,language,readme,title
0,Python,"English ‚àô Êó•Êú¨Ë™û ‚àô ÁÆÄ‰Ωì‰∏≠Êñá ‚àô ÁπÅÈ´î‰∏≠Êñá | ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé ‚àô ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‚àô Portugu√™s do Brasil ‚àô Deutsch ‚àô ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ ‚àô ◊¢◊ë◊®◊ô◊™ ‚àô Italiano ‚àô ÈüìÂúãË™û ‚àô ŸÅÿßÿ±ÿ≥€å ‚àô Polski ‚àô —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ ‚àô Espa√±ol ‚àô ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‚àô T√ºrk√ße ‚àô ti·∫øng Vi·ªát ‚àô Fran√ßais | Add Translation
The System Design Primer




Motivation

Learn how to design large-scale systems.
Prep for the system design interview.

Learn how to design large-scale systems
Learning how to design scalable systems will help you become a better engineer.
System design is a broad topic.  There is a vast amount of resources scattered throughout the web on system design principles.
This repo is an organized collection of resources to help you learn how to build systems at scale.
Learn from the open source community
This is a continually updated, open source project.
Contributions are welcome!
Prep for the system design interview
In addition to coding interviews, system design is a required component of the technical interview process at many tech companies.
Practice common system design interview questions and compare your results with sample solutions: discussions, code, and diagrams.
Additional topics for interview prep:

Study guide
How to approach a system design interview question
System design interview questions, with solutions
Object-oriented design interview questions, with solutions
Additional system design interview questions

Anki flashcards




The provided Anki flashcard decks use spaced repetition to help you retain key system design concepts.

System design deck
System design exercises deck
Object oriented design exercises deck

Great for use while on-the-go.
Coding Resource: Interactive Coding Challenges
Looking for resources to help you prep for the Coding Interview?




Check out the sister repo Interactive Coding Challenges, which contains an additional Anki deck:

Coding deck

Contributing

Learn from the community.

Feel free to submit pull requests to help:

Fix errors
Improve sections
Add new sections
Translate

Content that needs some polishing is placed under development.
Review the Contributing Guidelines.
Index of system design topics

Summaries of various system design topics, including pros and cons.  Everything is a trade-off.
Each section contains links to more in-depth resources.






System design topics: start here

Step 1: Review the scalability video lecture
Step 2: Review the scalability article
Next steps


Performance vs scalability
Latency vs throughput
Availability vs consistency

CAP theorem

CP - consistency and partition tolerance
AP - availability and partition tolerance




Consistency patterns

Weak consistency
Eventual consistency
Strong consistency


Availability patterns

Fail-over
Replication
Availability in numbers


Domain name system
Content delivery network

Push CDNs
Pull CDNs


Load balancer

Active-passive
Active-active
Layer 4 load balancing
Layer 7 load balancing
Horizontal scaling


Reverse proxy (web server)

Load balancer vs reverse proxy


Application layer

Microservices
Service discovery


Database

Relational database management system (RDBMS)

Master-slave replication
Master-master replication
Federation
Sharding
Denormalization
SQL tuning


NoSQL

Key-value store
Document store
Wide column store
Graph Database


SQL or NoSQL


Cache

Client caching
CDN caching
Web server caching
Database caching
Application caching
Caching at the database query level
Caching at the object level
When to update the cache

Cache-aside
Write-through
Write-behind (write-back)
Refresh-ahead




Asynchronism

Message queues
Task queues
Back pressure


Communication

Transmission control protocol (TCP)
User datagram protocol (UDP)
Remote procedure call (RPC)
Representational state transfer (REST)


Security
Appendix

Powers of two table
Latency numbers every programmer should know
Additional system design interview questions
Real world architectures
Company architectures
Company engineering blogs


Under development
Credits
Contact info
License

Study guide

Suggested topics to review based on your interview timeline (short, medium, long).


Q: For interviews, do I need to know everything here?
A: No, you don't need to know everything here to prepare for the interview.
What you are asked in an interview depends on variables such as:

How much experience you have
What your technical background is
What positions you are interviewing for
Which companies you are interviewing with
Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.
Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

Short timeline - Aim for breadth with system design topics.  Practice by solving some interview questions.
Medium timeline - Aim for breadth and some depth with system design topics.  Practice by solving many interview questions.
Long timeline - Aim for breadth and more depth with system design topics.  Practice by solving most interview questions.





Short
Medium
Long




Read through the System design topics to get a broad understanding of how systems work
üëç
üëç
üëç


Read through a few articles in the Company engineering blogs for the companies you are interviewing with
üëç
üëç
üëç


Read through a few Real world architectures
üëç
üëç
üëç


Review How to approach a system design interview question
üëç
üëç
üëç


Work through System design interview questions with solutions
Some
Many
Most


Work through Object-oriented design interview questions with solutions
Some
Many
Most


Review Additional system design interview questions
Some
Many
Most



How to approach a system design interview question

How to tackle a system design interview question.

The system design interview is an open-ended conversation.  You are expected to lead it.
You can use the following steps to guide the discussion.  To help solidify this process, work through the System design interview questions with solutions section using the following steps.
Step 1: Outline use cases, constraints, and assumptions
Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

Who is going to use it?
How are they going to use it?
How many users are there?
What does the system do?
What are the inputs and outputs of the system?
How much data do we expect to handle?
How many requests per second do we expect?
What is the expected read to write ratio?

Step 2: Create a high level design
Outline a high level design with all important components.

Sketch the main components and connections
Justify your ideas

Step 3: Design core components
Dive into details for each core component.  For example, if you were asked to design a url shortening service, discuss:

Generating and storing a hash of the full url

MD5 and Base62
Hash collisions
SQL or NoSQL
Database schema


Translating a hashed url to the full url

Database lookup


API and object-oriented design

Step 4: Scale the design
Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

Load balancer
Horizontal scaling
Caching
Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using principles of scalable system design.
Back-of-the-envelope calculations
You might be asked to do some estimates by hand.  Refer to the Appendix for the following resources:

Use back of the envelope calculations
Powers of two table
Latency numbers every programmer should know

Source(s) and further reading
Check out the following links to get a better idea of what to expect:

How to ace a systems design interview
The system design interview
Intro to Architecture and Systems Design Interviews

System design interview questions with solutions

Common system design interview questions with sample discussions, code, and diagrams.
Solutions linked to content in the solutions/ folder.




Question





Design Pastebin.com (or Bit.ly)
Solution


Design the Twitter timeline and search (or Facebook feed and search)
Solution


Design a web crawler
Solution


Design Mint.com
Solution


Design the data structures for a social network
Solution


Design a key-value store for a search engine
Solution


Design Amazon's sales ranking by category feature
Solution


Design a system that scales to millions of users on AWS
Solution


Add a system design question
Contribute



Design Pastebin.com (or Bit.ly)
View exercise and solution

Design the Twitter timeline and search (or Facebook feed and search)
View exercise and solution

Design a web crawler
View exercise and solution

Design Mint.com
View exercise and solution

Design the data structures for a social network
View exercise and solution

Design a key-value store for a search engine
View exercise and solution

Design Amazon's sales ranking by category feature
View exercise and solution

Design a system that scales to millions of users on AWS
View exercise and solution

Object-oriented design interview questions with solutions

Common object-oriented design interview questions with sample discussions, code, and diagrams.
Solutions linked to content in the solutions/ folder.


Note: This section is under development




Question





Design a hash map
Solution


Design a least recently used cache
Solution


Design a call center
Solution


Design a deck of cards
Solution


Design a parking lot
Solution


Design a chat server
Solution


Design a circular array
Contribute


Add an object-oriented design question
Contribute



System design topics: start here
New to system design?
First, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.
Step 1: Review the scalability video lecture
Scalability Lecture at Harvard

Topics covered:

Vertical scaling
Horizontal scaling
Caching
Load balancing
Database replication
Database partitioning



Step 2: Review the scalability article
Scalability

Topics covered:

Clones
Databases
Caches
Asynchronism



Next steps
Next, we'll look at high-level trade-offs:

Performance vs scalability
Latency vs throughput
Availability vs consistency

Keep in mind that everything is a trade-off.
Then we'll dive into more specific topics such as DNS, CDNs, and load balancers.
Performance vs scalability
A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.1
Another way to look at performance vs scalability:

If you have a performance problem, your system is slow for a single user.
If you have a scalability problem, your system is fast for a single user but slow under heavy load.

Source(s) and further reading

A word on scalability
Scalability, availability, stability, patterns

Latency vs throughput
Latency is the time to perform some action or to produce some result.
Throughput is the number of such actions or results per unit of time.
Generally, you should aim for maximal throughput with acceptable latency.
Source(s) and further reading

Understanding latency vs throughput

Availability vs consistency
CAP theorem



Source: CAP theorem revisited

In a distributed computer system, you can only support two of the following guarantees:

Consistency - Every read receives the most recent write or an error
Availability - Every request receives a response, without guarantee that it contains the most recent version of the information
Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures

Networks aren't reliable, so you'll need to support partition tolerance.  You'll need to make a software tradeoff between consistency and availability.
CP - consistency and partition tolerance
Waiting for a response from the partitioned node might result in a timeout error.  CP is a good choice if your business needs require atomic reads and writes.
AP - availability and partition tolerance
Responses return the most recent version of the data available on a node, which might not be the latest.  Writes might take some time to propagate when the partition is resolved.
AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors.
Source(s) and further reading

CAP theorem revisited
A plain english introduction to CAP theorem
CAP FAQ

Consistency patterns
With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data.  Recall the definition of consistency from the CAP theorem - Every read receives the most recent write or an error.
Weak consistency
After a write, reads may or may not see it.  A best effort approach is taken.
This approach is seen in systems such as memcached.  Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games.  For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.
Eventual consistency
After a write, reads will eventually see it (typically within milliseconds).  Data is replicated asynchronously.
This approach is seen in systems such as DNS and email.  Eventual consistency works well in highly available systems.
Strong consistency
After a write, reads will see it.  Data is replicated synchronously.
This approach is seen in file systems and RDBMSes.  Strong consistency works well in systems that need transactions.
Source(s) and further reading

Transactions across data centers

Availability patterns
There are two main patterns to support high availability: fail-over and replication.
Fail-over
Active-passive
With active-passive fail-over, heartbeats are sent between the active and the passive server on standby.  If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.
The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby.  Only the active server handles traffic.
Active-passive failover can also be referred to as master-slave failover.
Active-active
In active-active, both servers are managing traffic, spreading the load between them.
If the servers are public-facing, the DNS would need to know about the public IPs of both servers.  If the servers are internal-facing, application logic would need to know about both servers.
Active-active failover can also be referred to as master-master failover.
Disadvantage(s): failover

Fail-over adds more hardware and additional complexity.
There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.

Replication
Master-slave and master-master
This topic is further discussed in the Database section:

Master-slave replication
Master-master replication

Availability in numbers
Availability is often quantified by uptime (or downtime) as a percentage of time the service is available.  Availability is generally measured in number of 9s--a service with 99.99% availability is described as having four 9s.
99.9% availability - three 9s



Duration
Acceptable downtime




Downtime per year
8h 45min 57s


Downtime per month
43m 49.7s


Downtime per week
10m 4.8s


Downtime per day
1m 26.4s



99.99% availability - four 9s



Duration
Acceptable downtime




Downtime per year
52min 35.7s


Downtime per month
4m 23s


Downtime per week
1m 5s


Downtime per day
8.6s



Availability in parallel vs in sequence
If a service consists of multiple components prone to failure, the service's overall availability depends on whether the components are in sequence or in parallel.
In sequence
Overall availability decreases when two components with availability < 100% are in sequence:
Availability (Total) = Availability (Foo) * Availability (Bar)

If both Foo and Bar each had 99.9% availability, their total availability in sequence would be 99.8%.
In parallel
Overall availability increases when two components with availability < 100% are in parallel:
Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))

If both Foo and Bar each had 99.9% availability, their total availability in parallel would be 99.9999%.
Domain name system



Source: DNS security presentation

A Domain Name System (DNS) translates a domain name such as www.example.com to an IP address.
DNS is hierarchical, with a few authoritative servers at the top level.  Your router or ISP provides information about which DNS server(s) to contact when doing a lookup.  Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays.  DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL).

NS record (name server) - Specifies the DNS servers for your domain/subdomain.
MX record (mail exchange) - Specifies the mail servers for accepting messages.
A record (address) - Points a name to an IP address.
CNAME (canonical) - Points a name to another name or CNAME (example.com to www.example.com) or to an A record.

Services such as CloudFlare and Route 53 provide managed DNS services.  Some DNS services can route traffic through various methods:

Weighted round robin

Prevent traffic from going to servers under maintenance
Balance between varying cluster sizes
A/B testing


Latency-based
Geolocation-based

Disadvantage(s): DNS

Accessing a DNS server introduces a slight delay, although mitigated by caching described above.
DNS server management could be complex and is generally managed by governments, ISPs, and large companies.
DNS services have recently come under DDoS attack, preventing users from accessing websites such as Twitter without knowing Twitter's IP address(es).

Source(s) and further reading

DNS architecture
Wikipedia
DNS articles

Content delivery network



Source: Why use a CDN

A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user.  Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content.  The site's DNS resolution will tell clients which server to contact.
Serving content from CDNs can significantly improve performance in two ways:

Users receive content at data centers close to them
Your servers do not have to serve requests that the CDN fulfills

Push CDNs
Push CDNs receive new content whenever changes occur on your server.  You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN.  You can configure when content expires and when it is updated.  Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.
Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs.  Content is placed on the CDNs once, instead of being re-pulled at regular intervals.
Pull CDNs
Pull CDNs grab new content from your server when the first user requests the content.  You leave the content on your server and rewrite URLs to point to the CDN.  This results in a slower request until the content is cached on the CDN.
A time-to-live (TTL) determines how long content is cached.  Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.
Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.
Disadvantage(s): CDN

CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.
Content might be stale if it is updated before the TTL expires it.
CDNs require changing URLs for static content to point to the CDN.

Source(s) and further reading

Globally distributed content delivery
The differences between push and pull CDNs
Wikipedia

Load balancer



Source: Scalable system design patterns

Load balancers distribute incoming client requests to computing resources such as application servers and databases.  In each case, the load balancer returns the response from the computing resource to the appropriate client.  Load balancers are effective at:

Preventing requests from going to unhealthy servers
Preventing overloading resources
Helping eliminate single points of failure

Load balancers can be implemented with hardware (expensive) or with software such as HAProxy.
Additional benefits include:

SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations

Removes the need to install X.509 certificates on each server


Session persistence - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions

To protect against failures, it's common to set up multiple load balancers, either in active-passive or active-active mode.
Load balancers can route traffic based on various metrics, including:

Random
Least loaded
Session/cookies
Round robin or weighted round robin
Layer 4
Layer 7

Layer 4 load balancing
Layer 4 load balancers look at info at the transport layer to decide how to distribute requests.  Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet.  Layer 4 load balancers forward network packets to and from the upstream server, performing Network Address Translation (NAT).
Layer 7 load balancing
Layer 7 load balancers look at the application layer to decide how to distribute requests.  This can involve contents of the header, message, and cookies.  Layer 7 load balancers terminates network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server.  For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.
At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.
Horizontal scaling
Load balancers can also help with horizontal scaling, improving performance and availability.  Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called Vertical Scaling.  It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.
Disadvantage(s): horizontal scaling

Scaling horizontally introduces complexity and involves cloning servers

Servers should be stateless: they should not contain any user-related data like sessions or profile pictures
Sessions can be stored in a centralized data store such as a database (SQL, NoSQL) or a persistent cache (Redis, Memcached)


Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out

Disadvantage(s): load balancer

The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.
Introducing a load balancer to help eliminate single points of failure results in increased complexity.
A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.

Source(s) and further reading

NGINX architecture
HAProxy architecture guide
Scalability
Wikipedia
Layer 4 load balancing
Layer 7 load balancing
ELB listener config

Reverse proxy (web server)



Source: Wikipedia


A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public.  Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.
Additional benefits include:

Increased security - Hide information about backend servers, blacklist IPs, limit number of connections per client
Increased scalability and flexibility - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration
SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations

Removes the need to install X.509 certificates on each server


Compression - Compress server responses
Caching - Return the response for cached requests
Static content - Serve static content directly

HTML/CSS/JS
Photos
Videos
Etc



Load balancer vs reverse proxy

Deploying a load balancer is useful when you have multiple servers.  Often, load balancers  route traffic to a set of servers serving the same function.
Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.
Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.

Disadvantage(s): reverse proxy

Introducing a reverse proxy results in increased complexity.
A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a failover) further increases complexity.

Source(s) and further reading

Reverse proxy vs load balancer
NGINX architecture
HAProxy architecture guide
Wikipedia

Application layer



Source: Intro to architecting systems for scale

Separating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently.  Adding a new API results in adding application servers without necessarily adding additional web servers.  The single responsibility principle advocates for small and autonomous services that work together.  Small teams with small services can plan more aggressively for rapid growth.
Workers in the application layer also help enable asynchronism.
Microservices
Related to this discussion are microservices, which can be described as a suite of independently deployable, small, modular services.  Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. 1
Pinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.
Service Discovery
Systems such as Consul, Etcd, and Zookeeper can help services find each other by keeping track of registered names, addresses, and ports.  Health checks help verify service integrity and are often done using an HTTP endpoint.  Both Consul and Etcd have a built in key-value store that can be useful for storing config values and other shared data.
Disadvantage(s): application layer

Adding an application layer with loosely coupled services requires a different approach from an architectural, operations, and process viewpoint (vs a monolithic system).
Microservices can add complexity in terms of deployments and operations.

Source(s) and further reading

Intro to architecting systems for scale
Crack the system design interview
Service oriented architecture
Introduction to Zookeeper
Here's what you need to know about building microservices

Database



Source: Scaling up to your first 10 million users

Relational database management system (RDBMS)
A relational database like SQL is a collection of data items organized in tables.
ACID is a set of properties of relational database transactions.

Atomicity - Each transaction is all or nothing
Consistency - Any transaction will bring the database from one valid state to another
Isolation - Executing transactions concurrently has the same results as if the transactions were executed serially
Durability - Once a transaction has been committed, it will remain so

There are many techniques to scale a relational database: master-slave replication, master-master replication, federation, sharding, denormalization, and SQL tuning.
Master-slave replication
The master serves reads and writes, replicating writes to one or more slaves, which serve only reads.  Slaves can also replicate to additional slaves in a tree-like fashion.  If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.



Source: Scalability, availability, stability, patterns

Disadvantage(s): master-slave replication

Additional logic is needed to promote a slave to a master.
See Disadvantage(s): replication for points related to both master-slave and master-master.

Master-master replication
Both masters serve reads and writes and coordinate with each other on writes.  If either master goes down, the system can continue to operate with both reads and writes.



Source: Scalability, availability, stability, patterns

Disadvantage(s): master-master replication

You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.
Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.
Conflict resolution comes more into play as more write nodes are added and as latency increases.
See Disadvantage(s): replication for points related to both master-slave and master-master.

Disadvantage(s): replication

There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.
Writes are replayed to the read replicas.  If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.
The more read slaves, the more you have to replicate, which leads to greater replication lag.
On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.
Replication adds more hardware and additional complexity.

Source(s) and further reading: replication

Scalability, availability, stability, patterns
Multi-master replication

Federation



Source: Scaling up to your first 10 million users

Federation (or functional partitioning) splits up databases by function.  For example, instead of a single, monolithic database, you could have three databases: forums, users, and products, resulting in less read and write traffic to each database and therefore less replication lag.  Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality.  With no single central master serializing writes you can write in parallel, increasing throughput.
Disadvantage(s): federation

Federation is not effective if your schema requires huge functions or tables.
You'll need to update your application logic to determine which database to read and write.
Joining data from two databases is more complex with a server link.
Federation adds more hardware and additional complexity.

Source(s) and further reading: federation

Scaling up to your first 10 million users

Sharding



Source: Scalability, availability, stability, patterns

Sharding distributes data across different databases such that each database can only manage a subset of the data.  Taking a users database as an example, as the number of users increases, more shards are added to the cluster.
Similar to the advantages of federation, sharding results in less read and write traffic, less replication, and more cache hits.  Index size is also reduced, which generally improves performance with faster queries.  If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss.  Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.
Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.
Disadvantage(s): sharding

You'll need to update your application logic to work with shards, which could result in complex SQL queries.
Data distribution can become lopsided in a shard.  For example, a set of power users on a shard could result in increased load to that shard compared to others.

Rebalancing adds additional complexity.  A sharding function based on consistent hashing can reduce the amount of transferred data.


Joining data from multiple shards is more complex.
Sharding adds more hardware and additional complexity.

Source(s) and further reading: sharding

The coming of the shard
Shard database architecture
Consistent hashing

Denormalization
Denormalization attempts to improve read performance at the expense of some write performance.  Redundant copies of the data are written in multiple tables to avoid expensive joins.  Some RDBMS such as PostgreSQL and Oracle support materialized views which handle the work of storing redundant information and keeping redundant copies consistent.
Once data becomes distributed with techniques such as federation and sharding, managing joins across data centers further increases complexity.  Denormalization might circumvent the need for such complex joins.
In most systems, reads can heavily outnumber writes 100:1 or even 1000:1.  A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.
Disadvantage(s): denormalization

Data is duplicated.
Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.
A denormalized database under heavy write load might perform worse than its normalized counterpart.

Source(s) and further reading: denormalization

Denormalization

SQL tuning
SQL tuning is a broad topic and many books have been written as reference.
It's important to benchmark and profile to simulate and uncover bottlenecks.

Benchmark - Simulate high-load situations with tools such as ab.
Profile - Enable tools such as the slow query log to help track performance issues.

Benchmarking and profiling might point you to the following optimizations.
Tighten up the schema

MySQL dumps to disk in contiguous blocks for fast access.
Use CHAR instead of VARCHAR for fixed-length fields.

CHAR effectively allows for fast, random access, whereas with VARCHAR, you must find the end of a string before moving onto the next one.


Use TEXT for large blocks of text such as blog posts.  TEXT also allows for boolean searches.  Using a TEXT field results in storing a pointer on disk that is used to locate the text block.
Use INT for larger numbers up to 2^32 or 4 billion.
Use DECIMAL for currency to avoid floating point representation errors.
Avoid storing large BLOBS, store the location of where to get the object instead.
VARCHAR(255) is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS.
Set the NOT NULL constraint where applicable to improve search performance.

Use good indices

Columns that you are querying (SELECT, GROUP BY, ORDER BY, JOIN) could be faster with indices.
Indices are usually represented as self-balancing B-tree that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time.
Placing an index can keep the data in memory, requiring more space.
Writes could also be slower since the index also needs to be updated.
When loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices.

Avoid expensive joins

Denormalize where performance demands it.

Partition tables

Break up a table by putting hot spots in a separate table to help keep it in memory.

Tune the query cache

In some cases, the query cache could lead to performance issues.

Source(s) and further reading: SQL tuning

Tips for optimizing MySQL queries
Is there a good reason i see VARCHAR(255) used so often?
How do null values affect performance?
Slow query log

NoSQL
NoSQL is a collection of data items represented in a key-value store, document store, wide column store, or a graph database.  Data is denormalized, and joins are generally done in the application code.  Most NoSQL stores lack true ACID transactions and favor eventual consistency.
BASE is often used to describe the properties of NoSQL databases.  In comparison with the CAP Theorem, BASE chooses availability over consistency.

Basically available - the system guarantees availability.
Soft state - the state of the system may change over time, even without input.
Eventual consistency - the system will become consistent over a period of time, given that the system doesn't receive input during that period.

In addition to choosing between SQL or NoSQL, it is helpful to understand which type of NoSQL database best fits your use case(s).  We'll review key-value stores, document stores, wide column stores, and graph databases in the next section.
Key-value store

Abstraction: hash table

A key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD.  Data stores can maintain keys in lexicographic order, allowing efficient retrieval of key ranges.  Key-value stores can allow for storing of metadata with a value.
Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer.  Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.
A key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.
Source(s) and further reading: key-value store

Key-value database
Disadvantages of key-value stores
Redis architecture
Memcached architecture

Document store

Abstraction: key-value store with documents stored as values

A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object.  Document stores provide APIs or a query language to query based on the internal structure of the document itself.  Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.
Based on the underlying implementation, documents are organized by collections, tags, metadata, or directories.  Although documents can be organized or grouped together, documents may have fields that are completely different from each other.
Some document stores like MongoDB and CouchDB also provide a SQL-like language to perform complex queries.  DynamoDB supports both key-values and documents.
Document stores provide high flexibility and are often used for working with occasionally changing data.
Source(s) and further reading: document store

Document-oriented database
MongoDB architecture
CouchDB architecture
Elasticsearch architecture

Wide column store



Source: SQL & NoSQL, a brief history


Abstraction: nested map ColumnFamily<RowKey, Columns<ColKey, Value, Timestamp>>

A wide column store's basic unit of data is a column (name/value pair).  A column can be grouped in column families (analogous to a SQL table).  Super column families further group column families.  You can access each column independently with a row key, and columns with the same row key form a row.  Each value contains a timestamp for versioning and for conflict resolution.
Google introduced Bigtable as the first wide column store, which influenced the open-source HBase often-used in the Hadoop ecosystem, and Cassandra from Facebook.  Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.
Wide column stores offer high availability and high scalability.  They are often used for very large data sets.
Source(s) and further reading: wide column store

SQL & NoSQL, a brief history
Bigtable architecture
HBase architecture
Cassandra architecture

Graph database



Source: Graph database


Abstraction: graph

In a graph database, each node is a record and each arc is a relationship between two nodes.  Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.
Graphs databases offer high performance for data models with complex relationships, such as a social network.  They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources.  Many graphs can only be accessed with REST APIs.
Source(s) and further reading: graph

Graph database
Neo4j
FlockDB

Source(s) and further reading: NoSQL

Explanation of base terminology
NoSQL databases a survey and decision guidance
Scalability
Introduction to NoSQL
NoSQL patterns

SQL or NoSQL



Source: Transitioning from RDBMS to NoSQL

Reasons for SQL:

Structured data
Strict schema
Relational data
Need for complex joins
Transactions
Clear patterns for scaling
More established: developers, community, code, tools, etc
Lookups by index are very fast

Reasons for NoSQL:

Semi-structured data
Dynamic or flexible schema
Non-relational data
No need for complex joins
Store many TB (or PB) of data
Very data intensive workload
Very high throughput for IOPS

Sample data well-suited for NoSQL:

Rapid ingest of clickstream and log data
Leaderboard or scoring data
Temporary data, such as a shopping cart
Frequently accessed ('hot') tables
Metadata/lookup tables

Source(s) and further reading: SQL or NoSQL

Scaling up to your first 10 million users
SQL vs NoSQL differences

Cache



Source: Scalable system design patterns

Caching improves page load times and can reduce the load on your servers and databases.  In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.
Databases often benefit from a uniform distribution of reads and writes across its partitions.  Popular items can skew the distribution, causing bottlenecks.  Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.
Client caching
Caches can be located on the client side (OS or browser), server side, or in a distinct cache layer.
CDN caching
CDNs are considered a type of cache.
Web server caching
Reverse proxies and caches such as Varnish can serve static and dynamic content directly.  Web servers can also cache requests, returning responses without having to contact application servers.
Database caching
Your database usually includes some level of caching in a default configuration, optimized for a generic use case.  Tweaking these settings for specific usage patterns can further boost performance.
Application caching
In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage.  Since the data is held in RAM, it is much faster than typical databases where data is stored on disk.  RAM is more limited than disk, so cache invalidation algorithms such as least recently used (LRU) can help invalidate 'cold' entries and keep 'hot' data in RAM.
Redis has the following additional features:

Persistence option
Built-in data structures such as sorted sets and lists

There are multiple levels you can cache that fall into two general categories: database queries and objects:

Row level
Query-level
Fully-formed serializable objects
Fully-rendered HTML

Generally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.
Caching at the database query level
Whenever you query the database, hash the query as a key and store the result to the cache.  This approach suffers from expiration issues:

Hard to delete a cached result with complex queries
If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell

Caching at the object level
See your data as an object, similar to what you do with your application code.  Have your application assemble the dataset from the database into a class instance or a data structure(s):

Remove the object from cache if its underlying data has changed
Allows for asynchronous processing: workers assemble objects by consuming the latest cached object

Suggestions of what to cache:

User sessions
Fully rendered web pages
Activity streams
User graph data

When to update the cache
Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.
Cache-aside



Source: From cache to in-memory data grid

The application is responsible for reading and writing from storage.  The cache does not interact with storage directly.  The application does the following:

Look for entry in cache, resulting in a cache miss
Load entry from the database
Add entry to cache
Return entry

def get_user(self, user_id):
    user = cache.get(""user.{0}"", user_id)
    if user is None:
        user = db.query(""SELECT * FROM users WHERE user_id = {0}"", user_id)
        if user is not None:
            key = ""user.{0}"".format(user_id)
            cache.set(key, json.dumps(user))
    return user
Memcached is generally used in this manner.
Subsequent reads of data added to cache are fast.  Cache-aside is also referred to as lazy loading.  Only requested data is cached, which avoids filling up the cache with data that isn't requested.
Disadvantage(s): cache-aside

Each cache miss results in three trips, which can cause a noticeable delay.
Data can become stale if it is updated in the database.  This issue is mitigated by setting a time-to-live (TTL) which forces an update of the cache entry, or by using write-through.
When a node fails, it is replaced by a new, empty node, increasing latency.

Write-through



Source: Scalability, availability, stability, patterns

The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:

Application adds/updates entry in cache
Cache synchronously writes entry to data store
Return

Application code:
set_user(12345, {""foo"":""bar""})
Cache code:
def set_user(user_id, values):
    user = db.query(""UPDATE Users WHERE id = {0}"", user_id, values)
    cache.set(user_id, user)
Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast.  Users are generally more tolerant of latency when updating data than reading data.  Data in the cache is not stale.
Disadvantage(s): write through

When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database.  Cache-aside in conjunction with write through can mitigate this issue.
Most data written might never be read, which can be minimized with a TTL.

Write-behind (write-back)



Source: Scalability, availability, stability, patterns

In write-behind, the application does the following:

Add/update entry in cache
Asynchronously write entry to the data store, improving write performance

Disadvantage(s): write-behind

There could be data loss if the cache goes down prior to its contents hitting the data store.
It is more complex to implement write-behind than it is to implement cache-aside or write-through.

Refresh-ahead



Source: From cache to in-memory data grid

You can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.
Refresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.
Disadvantage(s): refresh-ahead

Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead.

Disadvantage(s): cache

Need to maintain consistency between caches and the source of truth such as the database through cache invalidation.
Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.
Need to make application changes such as adding Redis or memcached.

Source(s) and further reading

From cache to in-memory data grid
Scalable system design patterns
Introduction to architecting systems for scale
Scalability, availability, stability, patterns
Scalability
AWS ElastiCache strategies
Wikipedia

Asynchronism



Source: Intro to architecting systems for scale

Asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line.  They can also help by doing time-consuming work in advance, such as periodic aggregation of data.
Message queues
Message queues receive, hold, and deliver messages.  If an operation is too slow to perform inline, you can use a message queue with the following workflow:

An application publishes a job to the queue, then notifies the user of job status
A worker picks up the job from the queue, processes it, then signals the job is complete

The user is not blocked and the job is processed in the background.  During this time, the client might optionally do a small amount of processing to make it seem like the task has completed.  For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.
Redis is useful as a simple message broker but messages can be lost.
RabbitMQ is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.
Amazon SQS is hosted but can have high latency and has the possibility of messages being delivered twice.
Task queues
Tasks queues receive tasks and their related data, runs them, then delivers their results.  They can support scheduling and can be used to run computationally-intensive jobs in the background.
Celery has support for scheduling and primarily has python support.
Back pressure
If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance.  Back pressure can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue.  Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later.  Clients can retry the request at a later time, perhaps with exponential backoff.
Disadvantage(s): asynchronism

Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.

Source(s) and further reading

It's all a numbers game
Applying back pressure when overloaded
Little's law
What is the difference between a message queue and a task queue?

Communication



Source: OSI 7 layer model

Hypertext transfer protocol (HTTP)
HTTP is a method for encoding and transporting data between a client and a server.  It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.  HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.
A basic HTTP request consists of a verb (method) and a resource (endpoint).  Below are common HTTP verbs:



Verb
Description
Idempotent*
Safe
Cacheable




GET
Reads a resource
Yes
Yes
Yes


POST
Creates a resource or trigger a process that handles data
No
No
Yes if response contains freshness info


PUT
Creates or replace a resource
Yes
No
No


PATCH
Partially updates a resource
No
No
Yes if response contains freshness info


DELETE
Deletes a resource
Yes
No
No



*Can be called many times without different outcomes.
HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP.
Source(s) and further reading: HTTP

What is HTTP?
Difference between HTTP and TCP
Difference between PUT and PATCH

Transmission control protocol (TCP)



Source: How to make a multiplayer game

TCP is a connection-oriented protocol over an IP network.  Connection is established and terminated using a handshake.  All packets sent are guaranteed to reach the destination in the original order and without corruption through:

Sequence numbers and checksum fields for each packet
Acknowledgement packets and automatic retransmission

If the sender does not receive a correct response, it will resend the packets.  If there are multiple timeouts, the connection is dropped.  TCP also implements flow control and congestion control.  These guarantees cause delays and generally result in less efficient transmission than UDP.
To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage.  It can be expensive to have a large number of open connections between web server threads and say, a memcached server.  Connection pooling can help in addition to switching to UDP where applicable.
TCP is useful for applications that require high reliability but are less time critical.  Some examples include web servers, database info, SMTP, FTP, and SSH.
Use TCP over UDP when:

You need all of the data to arrive intact
You want to automatically make a best estimate use of the network throughput

User datagram protocol (UDP)



Source: How to make a multiplayer game

UDP is connectionless.  Datagrams (analogous to packets) are guaranteed only at the datagram level.  Datagrams might reach their destination out of order or not at all.  UDP does not support congestion control.  Without the guarantees that TCP support, UDP is generally more efficient.
UDP can broadcast, sending datagrams to all devices on the subnet.  This is useful with DHCP because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.
UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.
Use UDP over TCP when:

You need the lowest latency
Late data is worse than loss of data
You want to implement your own error correction

Source(s) and further reading: TCP and UDP

Networking for game programming
Key differences between TCP and UDP protocols
Difference between TCP and UDP
Transmission control protocol
User datagram protocol
Scaling memcache at Facebook

Remote procedure call (RPC)



Source: Crack the system design interview

In an RPC, a client causes a procedure to execute on a different address space, usually a remote server.  The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program.  Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls.  Popular RPC frameworks include Protobuf, Thrift, and Avro.
RPC is a request-response protocol:

Client program - Calls the client stub procedure.  The parameters are pushed onto the stack like a local procedure call.
Client stub procedure - Marshals (packs) procedure id and arguments into a request message.
Client communication module - OS sends the message from the client to the server.
Server communication module - OS passes the incoming packets to the server stub procedure.
Server stub procedure -  Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments.
The server response repeats the steps above in reverse order.

Sample RPC calls:
GET /someoperation?data=anId

POST /anotheroperation
{
  ""data"":""anId"";
  ""anotherdata"": ""another value""
}

RPC is focused on exposing behaviors.  RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.
Choose a native library (aka SDK) when:

You know your target platform.
You want to control how your ""logic"" is accessed.
You want to control how error control happens off your library.
Performance and end user experience is your primary concern.

HTTP APIs following REST tend to be used more often for public APIs.
Disadvantage(s): RPC

RPC clients become tightly coupled to the service implementation.
A new API must be defined for every new operation or use case.
It can be difficult to debug RPC.
You might not be able to leverage existing technologies out of the box.  For example, it might require additional effort to ensure RPC calls are properly cached on caching servers such as Squid.

Representational state transfer (REST)
REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server.  The server provides a representation of resources and actions that can either manipulate or get a new representation of resources.  All communication must be stateless and cacheable.
There are four qualities of a RESTful interface:

Identify resources (URI in HTTP) - use the same URI regardless of any operation.
Change with representations (Verbs in HTTP) - use verbs, headers, and body.
Self-descriptive error message (status response in HTTP) - Use status codes, don't reinvent the wheel.
HATEOAS (HTML interface for HTTP) - your web service should be fully accessible in a browser.

Sample REST calls:
GET /someresources/anId

PUT /someresources/anId
{""anotherdata"": ""another value""}

REST is focused on exposing data.  It minimizes the coupling between client/server and is often used for public HTTP APIs.  REST uses a more generic and uniform method of exposing resources through URIs, representation through headers, and actions through verbs such as GET, POST, PUT, DELETE, and PATCH.  Being stateless, REST is great for horizontal scaling and partitioning.
Disadvantage(s): REST

With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy.  For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path.  With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.
REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case.  For example, moving expired documents to the archive folder might not cleanly fit within these verbs.
Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.
Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.

RPC and REST calls comparison



Operation
RPC
REST




Signup
POST /signup
POST /persons


Resign
POST /resign{""personid"": ""1234""}
DELETE /persons/1234


Read a person
GET /readPerson?personid=1234
GET /persons/1234


Read a person‚Äôs items list
GET /readUsersItemsList?personid=1234
GET /persons/1234/items


Add an item to a person‚Äôs items
POST /addItemToUsersItemsList{""personid"": ""1234"";""itemid"": ""456""}
POST /persons/1234/items{""itemid"": ""456""}


Update an item
POST /modifyItem{""itemid"": ""456"";""key"": ""value""}
PUT /items/456{""key"": ""value""}


Delete an item
POST /removeItem{""itemid"": ""456""}
DELETE /items/456




Source: Do you really know why you prefer REST over RPC

Source(s) and further reading: REST and RPC

Do you really know why you prefer REST over RPC
When are RPC-ish approaches more appropriate than REST?
REST vs JSON-RPC
Debunking the myths of RPC and REST
What are the drawbacks of using REST
Crack the system design interview
Thrift
Why REST for internal use and not RPC

Security
This section could use some updates.  Consider contributing!
Security is a broad topic.  Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:

Encrypt in transit and at rest.
Sanitize all user inputs or any input parameters exposed to user to prevent XSS and SQL injection.
Use parameterized queries to prevent SQL injection.
Use the principle of least privilege.

Source(s) and further reading

API security checklist
Security guide for developers
OWASP top ten

Appendix
You'll sometimes be asked to do 'back-of-the-envelope' estimates.  For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take.  The Powers of two table and Latency numbers every programmer should know are handy references.
Powers of two table
Power           Exact Value         Approx Value        Bytes
---------------------------------------------------------------
7                             128
8                             256
10                           1024   1 thousand           1 KB
16                         65,536                       64 KB
20                      1,048,576   1 million            1 MB
30                  1,073,741,824   1 billion            1 GB
32                  4,294,967,296                        4 GB
40              1,099,511,627,776   1 trillion           1 TB

Source(s) and further reading

Powers of two

Latency numbers every programmer should know
Latency Comparison Numbers
--------------------------
L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                           25   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy            10,000   ns       10 us
Send 1 KB bytes over 1 Gbps network     10,000   ns       10 us
Read 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD
Read 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD
Send packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms

Notes
-----
1 ns = 10^-9 seconds
1 us = 10^-6 seconds = 1,000 ns
1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns

Handy metrics based on numbers above:

Read sequentially from disk at 30 MB/s
Read sequentially from 1 Gbps Ethernet at 100 MB/s
Read sequentially from SSD at 1 GB/s
Read sequentially from main memory at 4 GB/s
6-7 world-wide round trips per second
2,000 round trips per second within a data center

Latency numbers visualized

Source(s) and further reading

Latency numbers every programmer should know - 1
Latency numbers every programmer should know - 2
Designs, lessons, and advice from building large distributed systems
Software Engineering Advice from Building Large-Scale Distributed Systems

Additional system design interview questions

Common system design interview questions, with links to resources on how to solve each.




Question
Reference(s)




Design a file sync service like Dropbox
youtube.com


Design a search engine like Google
queue.acm.orgstackexchange.comardendertat.comstanford.edu


Design a scalable web crawler like Google
quora.com


Design Google docs
code.google.comneil.fraser.name


Design a key-value store like Redis
slideshare.net


Design a cache system like Memcached
slideshare.net


Design a recommendation system like Amazon's
hulu.comijcai13.org


Design a tinyurl system like Bitly
n00tc0d3r.blogspot.com


Design a chat app like WhatsApp
highscalability.com


Design a picture sharing system like Instagram
highscalability.comhighscalability.com


Design the Facebook news feed function
quora.comquora.comslideshare.net


Design the Facebook timeline function
facebook.comhighscalability.com


Design the Facebook chat function
erlang-factory.comfacebook.com


Design a graph search function like Facebook's
facebook.comfacebook.comfacebook.com


Design a content delivery network like CloudFlare
figshare.com


Design a trending topic system like Twitter's
michael-noll.comsnikolov .wordpress.com


Design a random ID generation system
blog.twitter.comgithub.com


Return the top k requests during a time interval
cs.ucsb.eduwpi.edu


Design a system that serves data from multiple data centers
highscalability.com


Design an online multiplayer card game
indieflashblog.combuildnewgames.com


Design a garbage collection system
stuffwithstuff.comwashington.edu


Design an API rate limiter
https://stripe.com/blog/


Add a system design question
Contribute



Real world architectures

Articles on how real world systems are designed.




Source: Twitter timelines at scale

Don't focus on nitty gritty details for the following articles, instead:

Identify shared principles, common technologies, and patterns within these articles
Study what problems are solved by each component, where it works, where it doesn't
Review the lessons learned




Type
System
Reference(s)




Data processing
MapReduce - Distributed data processing from Google
research.google.com


Data processing
Spark - Distributed data processing from Databricks
slideshare.net


Data processing
Storm - Distributed data processing from Twitter
slideshare.net







Data store
Bigtable - Distributed column-oriented database from Google
harvard.edu


Data store
HBase - Open source implementation of Bigtable
slideshare.net


Data store
Cassandra - Distributed column-oriented database from Facebook
slideshare.net


Data store
DynamoDB - Document-oriented database from Amazon
harvard.edu


Data store
MongoDB - Document-oriented database
slideshare.net


Data store
Spanner - Globally-distributed database from Google
research.google.com


Data store
Memcached - Distributed memory caching system
slideshare.net


Data store
Redis - Distributed memory caching system with persistence and value types
slideshare.net







File system
Google File System (GFS) - Distributed file system
research.google.com


File system
Hadoop File System (HDFS) - Open source implementation of GFS
apache.org







Misc
Chubby - Lock service for loosely-coupled distributed systems from Google
research.google.com


Misc
Dapper - Distributed systems tracing infrastructure
research.google.com


Misc
Kafka - Pub/sub message queue from LinkedIn
slideshare.net


Misc
Zookeeper - Centralized infrastructure and services enabling synchronization
slideshare.net



Add an architecture
Contribute



Company architectures



Company
Reference(s)




Amazon
Amazon architecture


Cinchcast
Producing 1,500 hours of audio every day


DataSift
Realtime datamining At 120,000 tweets per second


DropBox
How we've scaled Dropbox


ESPN
Operating At 100,000 duh nuh nuhs per second


Google
Google architecture


Instagram
14 million users, terabytes of photosWhat powers Instagram


Justin.tv
Justin.Tv's live video broadcasting architecture


Facebook
Scaling memcached at FacebookTAO: Facebook‚Äôs distributed data store for the social graphFacebook‚Äôs photo storageHow Facebook Live Streams To 800,000 Simultaneous Viewers


Flickr
Flickr architecture


Mailbox
From 0 to one million users in 6 weeks


Netflix
A 360 Degree View Of The Entire Netflix StackNetflix: What Happens When You Press Play?


Pinterest
From 0 To 10s of billions of page views a month18 million visitors, 10x growth, 12 employees


Playfish
50 million monthly users and growing


PlentyOfFish
PlentyOfFish architecture


Salesforce
How they handle 1.3 billion transactions a day


Stack Overflow
Stack Overflow architecture


TripAdvisor
40M visitors, 200M dynamic page views, 30TB data


Tumblr
15 billion page views a month


Twitter
Making Twitter 10000 percent fasterStoring 250 million tweets a day using MySQL150M active users, 300K QPS, a 22 MB/S firehoseTimelines at scaleBig and small data at TwitterOperations at Twitter: scaling beyond 100 million usersHow Twitter Handles 3,000 Images Per Second


Uber
How Uber scales their real-time market platformLessons Learned From Scaling Uber To 2000 Engineers, 1000 Services, And 8000 Git Repositories


WhatsApp
The WhatsApp architecture Facebook bought for $19 billion


YouTube
YouTube scalabilityYouTube architecture



Company engineering blogs

Architectures for companies you are interviewing with.
Questions you encounter might be from the same domain.


Airbnb Engineering
Atlassian Developers
AWS Blog
Bitly Engineering Blog
Box Blogs
Cloudera Developer Blog
Dropbox Tech Blog
Engineering at Quora
Ebay Tech Blog
Evernote Tech Blog
Etsy Code as Craft
Facebook Engineering
Flickr Code
Foursquare Engineering Blog
GitHub Engineering Blog
Google Research Blog
Groupon Engineering Blog
Heroku Engineering Blog
Hubspot Engineering Blog
High Scalability
Instagram Engineering
Intel Software Blog
Jane Street Tech Blog
LinkedIn Engineering
Microsoft Engineering
Microsoft Python Engineering
Netflix Tech Blog
Paypal Developer Blog
Pinterest Engineering Blog
Quora Engineering
Reddit Blog
Salesforce Engineering Blog
Slack Engineering Blog
Spotify Labs
Twilio Engineering Blog
Twitter Engineering
Uber Engineering Blog
Yahoo Engineering Blog
Yelp Engineering Blog
Zynga Engineering Blog

Source(s) and further reading
Looking to add a blog?  To avoid duplicating work, consider adding your company blog to the following repo:

kilimchoi/engineering-blogs

Under development
Interested in adding a section or helping complete one in-progress?  Contribute!

Distributed computing with MapReduce
Consistent hashing
Scatter gather
Contribute

Credits
Credits and sources are provided throughout this repo.
Special thanks to:

Hired in tech
Cracking the coding interview
High scalability
checkcheckzz/system-design-interview
shashank88/system_design
mmcgrana/services-engineering
System design cheat sheet
A distributed systems reading list
Cracking the system design interview

Contact info
Feel free to contact me to discuss any issues, questions, or comments.
My contact info can be found on my GitHub page.
License
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2017 Donne Martin

Creative Commons Attribution 4.0 International License (CC BY 4.0)

http://creativecommons.org/licenses/by/4.0/

",GitHub - donnemartin/system-design-primer: Learn how to design large-scale systems. Prep for the system design interview.  Includes Anki flashcards.
1,Python,"Awesome Python 
A curated list of awesome Python frameworks, libraries, software and resources.
Inspired by awesome-php.

Awesome Python

Admin Panels
Algorithms and Design Patterns
Asynchronous Programming
Audio
Authentication
Build Tools
Built-in Classes Enhancement
Caching
ChatOps Tools
CMS
Code Analysis
Command-line Interface Development
Command-line Tools
Compatibility
Computer Vision
Concurrency and Parallelism
Configuration
Cryptography
Data Analysis
Data Validation
Data Visualization
Database Drivers
Database
Date and Time
Debugging Tools
Deep Learning
DevOps Tools
Distributed Computing
Distribution
Documentation
Downloader
E-commerce
Editor Plugins and IDEs
Email
Environment Management
Files
Foreign Function Interface
Forms
Functional Programming
Game Development
Geolocation
GUI Development
Hardware
HTML Manipulation
HTTP Clients
Image Processing
Implementations
Interactive Interpreter
Internationalization
Job Scheduler
Logging
Machine Learning
Miscellaneous
Natural Language Processing
Network Virtualization
News Feed
ORM
Package Management
Package Repositories
Permissions
Processes
Recommender Systems
RESTful API
Robotics
RPC Servers
Science
Search
Serialization
Serverless Frameworks
Specific Formats Processing
Static Site Generator
Tagging
Task Queues
Template Engine
Testing
Text Processing
Third-party APIs
URL Manipulation
Video
Web Asset Management
Web Content Extracting
Web Crawling
Web Frameworks
WebSocket
WSGI Servers


Resources

Podcasts
Twitter
Websites
Weekly


Contributing


Admin Panels
Libraries for administrative interfaces.

ajenti - The admin panel your servers deserve.
django-grappelli - A jazzy skin for the Django Admin-Interface.
django-jet - Modern responsive template for the Django admin interface with improved functionality.
django-suit - Alternative Django Admin-Interface (free only for Non-commercial use).
django-xadmin - Drop-in replacement of Django admin comes with lots of goodies.
jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django)
flask-admin - Simple and extensible administrative interface framework for Flask.
flower - Real-time monitor and web admin for Celery.
wooey - A Django app which creates automatic web UIs for Python scripts.

Algorithms and Design Patterns
Python implementation of algorithms and design patterns.

algorithms - Minimal examples of data structures and algorithms in Python.
PyPattyrn - A simple yet effective library for implementing common design patterns.
python-patterns - A collection of design patterns in Python.
sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.

Asynchronous Programming

asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.

awesome-asyncio


uvloop - Ultra fast asyncio event loop.
Twisted - An event-driven networking engine.

Audio
Libraries for manipulating audio and its metadata.

Audio

audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.
dejavu - Audio fingerprinting and recognition.
mingus - An advanced music theory and notation package with MIDI file and playback support.
pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications.
pydub - Manipulate audio with a simple and easy high level interface.
TimeSide - Open web audio processing framework.


Metadata

beets - A music library manager and MusicBrainz tagger.
eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata.
mutagen - A Python module to handle audio metadata.
tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files.



Authentication
Libraries for implementing authentications schemes.

OAuth

authlib - JavaScript Object Signing and Encryption draft implementation.
django-allauth - Authentication app for Django that ""just works.""
django-oauth-toolkit - OAuth 2 goodies for Django.
oauthlib - A generic and thorough implementation of the OAuth request-signing logic.
python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers.
python-social-auth - An easy-to-setup social authentication mechanism.


JWT

pyjwt - JSON Web Token implementation in Python.
python-jose - A JOSE implementation in Python.
python-jwt - A module for generating and verifying JSON Web Tokens.



Build Tools
Compile software from source code.

BitBake - A make-like build tool for embedded Linux.
buildout - A build system for creating, assembling and deploying applications from multiple parts.
PlatformIO - A console tool to build code with different development platforms.
pybuilder - A continuous build tool written in pure Python.
SCons - A software construction tool.

Built-in Classes Enhancement
Libraries for enhancing Python built-in classes.

dataclasses - (Python standard library) Data classes.
attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions.
bidict - Efficient, Pythonic bidirectional map data structures and related functionality..
Box - Python dictionaries with advanced dot notation access.
DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation.

CMS
Content Management Systems.

wagtail - A Django content management system.
django-cms - An Open source enterprise CMS based on the Django.
feincms - One of the most advanced Content Management Systems built on Django.
Kotti - A high-level, Pythonic web application framework built on Pyramid.
mezzanine - A powerful, consistent, and flexible content management platform.
plone - A CMS built on top of the open source application server Zope.
quokka - Flexible, extensible, small CMS powered by Flask and MongoDB.

Caching
Libraries for caching data.

beaker - A WSGI middleware for sessions and caching.
django-cache-machine - Automatic caching and invalidation for Django models.
django-cacheops - A slick ORM cache with automatic granular event-driven invalidation.
dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors.
HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention.
pylibmc - A Python wrapper around the libmemcached interface.
python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis.

ChatOps Tools
Libraries for chatbot development.

errbot - The easiest and most popular chatbot to implement ChatOps.

Code Analysis
Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis.

Code Analysis

coala - Language independent and easily extendable code analysis application.
code2flow - Turn your Python and JavaScript code into DOT flowcharts.
prospector - A tool to analyse Python code.
pycallgraph - A library that visualises the flow (call graph) of your Python application.


Code Linters

flake8 - A wrapper around pycodestyle, pyflakes and McCabe.

awesome-flake8-extensions


pylint - A fully customizable source code analyzer.
pylama - A code audit tool for Python and JavaScript.
wemake-python-styleguide - The strictest and most opinionated python linter ever.


Code Formatters

black - The uncompromising Python code formatter.
yapf - Yet another Python code formatter from Google.


Static Type Checkers, also see awesome-python-typing

mypy - Check variable types during compile time.
pyre-check - Performant type checking.


Static Type Annotations Generators

MonkeyType - A system for Python that generates static type annotations by collecting runtime types



Command-line Interface Development
Libraries for building command-line applications.

Command-line Application Development

cement - CLI Application Framework for Python.
click - A package for creating beautiful command line interfaces in a composable way.
cliff - A framework for creating command-line programs with multi-level commands.
clint - Python Command-line Application Tools.
docopt - Pythonic command line arguments parser.
python-fire - A library for creating command line interfaces from absolutely any Python object.
python-prompt-toolkit - A library for building powerful interactive command lines.


Terminal Rendering

asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations).
bashplotlib - Making basic plots in the terminal.
colorama - Cross-platform colored terminal text.
tqdm - Fast, extensible progress bar for loops and CLI.



Command-line Tools
Useful CLI-based tools for productivity.

Productivity Tools

cookiecutter - A command-line utility that creates projects from cookiecutters (project templates).
doitlive - A tool for live presentations in the terminal.
howdoi - Instant coding answers via the command line.
PathPicker - Select files out of bash output.
percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX.
thefuck - Correcting your previous console command.
tmuxp - A tmux session manager.
try - A dead simple CLI to try out python packages - it's never been easier.


CLI Enhancements

httpie - A command line HTTP client, a user-friendly cURL replacement.
kube-shell - An integrated shell for working with the Kubernetes CLI.
mycli - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.
pgcli - Postgres CLI with autocompletion and syntax highlighting.
saws - A Supercharged aws-cli.



Compatibility
Libraries for migrating from Python 2 to 3.

python-future - The missing compatibility layer between Python 2 and Python 3.
python-modernize - Modernizes Python code for eventual Python 3 migration.
six - Python 2 and 3 compatibility utilities.

Computer Vision
Libraries for computer vision.

OpenCV - Open Source Computer Vision Library.
pytesseract - Another wrapper for Google Tesseract OCR.
SimpleCV - An open source framework for building computer vision applications.

Concurrency and Parallelism
Libraries for concurrent and parallel execution. Also see awesome-asyncio.

concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables.
multiprocessing - (Python standard library) Process-based parallelism.
eventlet - Asynchronous framework with WSGI support.
gevent - A coroutine-based Python networking library that uses greenlet.
uvloop - Ultra fast implementation of asyncio event loop on top of libuv.
scoop - Scalable Concurrent Operations in Python.

Configuration
Libraries for storing and parsing configuration options.

configobj - INI file parser with validation.
configparser - (Python standard library) INI file parser.
profig - Config from multiple formats with value conversion.
python-decouple - Strict separation of settings from code.

Cryptography

cryptography - A package designed to expose cryptographic primitives and recipes to Python developers.
paramiko - The leading native Python SSHv2 protocol library.
passlib - Secure password storage/hashing library, very high level.
pynacl - Python binding to the Networking and Cryptography (NaCl) library.

Data Analysis
Libraries for data analyzing.

Blaze - NumPy and Pandas interface to Big Data.
Open Mining - Business Intelligence (BI) in Pandas interface.
Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Optimus - Agile Data Science Workflows made easy with PySpark.

Data Validation
Libraries for validating data. Used for forms in many cases.

Cerberus - A lightweight and extensible data validation library.
colander - Validating and deserializing data obtained via XML, JSON, an HTML form post.
jsonschema - An implementation of JSON Schema for Python.
schema - A library for validating Python data structures.
Schematics - Data Structure Validation.
valideer - Lightweight extensible data validation and adaptation library.
voluptuous - A Python data validation library.

Data Visualization
Libraries for visualizing data. Also see awesome-javascript.

Altair - Declarative statistical visualization library for Python.
Bokeh - Interactive Web Plotting for Python.
bqplot - Interactive Plotting Library for the Jupyter Notebook
Dash - Built on top of Flask, React and Plotly aimed at analytical web applications.

awesome-dash


plotnine - A grammar of graphics for Python based on ggplot2.
Matplotlib - A Python 2D plotting library.
Pygal - A Python SVG Charts Creator.
PyGraphviz - Python interface to Graphviz.
PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.
Seaborn - Statistical data visualization using Matplotlib.
VisPy - High-performance scientific visualization based on OpenGL.

Database
Databases implemented in Python.

pickleDB - A simple and lightweight key-value store for Python.
tinydb - A tiny, document-oriented database.
ZODB - A native object database for Python. A key-value and object graph database.

Database Drivers
Libraries for connecting and operating databases.

MySQL - awesome-mysql

mysqlclient - MySQL connector with Python 3 support (mysql-python fork).
PyMySQL - A pure Python MySQL driver compatible to mysql-python.


PostgreSQL - awesome-postgres

psycopg2 - The most popular PostgreSQL adapter for Python.
queries - A wrapper of the psycopg2 library for interacting with PostgreSQL.


Other Relational Databases

pymssql - A simple database interface to Microsoft SQL Server.
SuperSQLite - A supercharged SQLite library built on top of apsw.


NoSQL Databases

cassandra-driver - The Python Driver for Apache Cassandra.
happybase - A developer-friendly library for Apache HBase.
kafka-python - The Python client for Apache Kafka.
py2neo - A client library and toolkit for working with Neo4j.
pymongo - The official Python client for MongoDB.
redis-py - The Python client for Redis.


Asynchronous Clients

motor - The async Python driver for MongoDB.



Date and Time
Libraries for working with dates and times.

Chronyk - A Python 3 library for parsing human-written times and dates.
dateutil - Extensions to the standard Python datetime module.
delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes.
moment - A Python library for dealing with dates/times. Inspired by Moment.js.
Pendulum - Python datetimes made easy.
PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string.
pytz - World timezone definitions, modern and historical. Brings the tz database into Python.
when.py - Providing user-friendly functions to help perform common date and time actions.
maya - Datetimes for Humans.

Debugging Tools
Libraries for debugging code.

pdb-like Debugger

ipdb - IPython-enabled pdb.
pdb++ - Another drop-in replacement for pdb.
pudb - A full-screen, console-based Python debugger.
wdb - An improbable web debugger through WebSockets.


Tracing

lptrace - strace for Python programs.
manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt.
pyringe - Debugger capable of attaching to and injecting code into Python processes.
python-hunter - A flexible code tracing toolkit.


Profiler

line_profiler - Line-by-line profiling.
memory_profiler - Monitor Memory usage of Python code.
profiling - An interactive Python profiler.
py-spy - A sampling profiler for Python programs. Written in Rust.
pyflame - A ptracing profiler For Python.
vprof - Visual Python profiler.


Others

icecream - Inspect variables, expressions, and program execution with a single, simple function call.
django-debug-toolbar - Display various debug information for Django.
django-devserver - A drop-in replacement for Django's runserver.
flask-debugtoolbar - A port of the django-debug-toolbar to flask.
pyelftools - Parsing and analyzing ELF files and DWARF debugging information.



Deep Learning
Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning.

caffe - A fast open framework for deep learning..
keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.
mxnet - A deep learning framework designed for both efficiency and flexibility.
pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration.
SerpentAI - Game agent framework. Use any video game as a deep learning sandbox.
tensorflow - The most popular Deep Learning framework created by Google.
Theano - A library for fast numerical computation.

DevOps Tools
Software and libraries for DevOps.

ansible - A radically simple IT automation platform.
cloudinit - A multi-distribution package that handles early initialization of a cloud instance.
cuisine - Chef-like functionality for Fabric.
docker-compose - Fast, isolated development environments using Docker.
fabric - A simple, Pythonic tool for remote execution and deployment.
fabtools - Tools for writing awesome Fabric files.
honcho - A Python clone of Foreman, for managing Procfile-based applications.
OpenStack - Open source software for building private and public clouds.
pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect.
psutil - A cross-platform process and system utilities module.
saltstack - Infrastructure automation and management system.
supervisor - Supervisor process control system for UNIX.

Distributed Computing
Frameworks and libraries for Distributed Computing.

Batch Processing

PySpark - Apache Spark Python API.
dask - A flexible parallel computing library for analytic computing.
luigi - A module that helps you build complex pipelines of batch jobs.
mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services.
Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem.


Stream Processing

faust - A stream processing library, porting the ideas from Kafka Streams to Python.
streamparse - Run Python code against real-time streams of data via Apache Storm.



Distribution
Libraries to create packaged executables for release distribution.

dh-virtualenv - Build and distribute a virtualenv as a Debian package.
Nuitka - Compile scripts, modules, packages to an executable or extension module.
py2app - Freezes Python scripts (Mac OS X).
py2exe - Freezes Python scripts (Windows).
PyInstaller - Converts Python programs into stand-alone executables (cross-platform).
pynsist - A tool to build Windows installers, installers bundle Python itself.

Documentation
Libraries for generating project documentation.

sphinx - Python Documentation generator.

awesome-sphinxdoc


pdoc - Epydoc replacement to auto generate API documentation for Python libraries.
pycco - The literate-programming-style documentation generator.

Downloader
Libraries for downloading.

s3cmd - A command line tool for managing Amazon S3 and CloudFront.
s4cmd - Super S3 command line tool, good for higher performance.
you-get - A YouTube/Youku/Niconico video downloader written in Python 3.
youtube-dl - A small command-line program to download videos from YouTube.

E-commerce
Frameworks and libraries for e-commerce and payments.

alipay - Unofficial Alipay API for Python.
Cartridge - A shopping cart app built using the Mezzanine.
django-oscar - An open-source e-commerce framework for Django.
django-shop - A Django based shop system.
merchant - A Django app to accept payments from various payment processors.
money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange.
python-currencies - Display money format and its filthy currencies.
forex-python - Foreign exchange rates, Bitcoin price index and currency conversion.
saleor - An e-commerce storefront for Django.
shoop - An open source E-Commerce platform based on Django.

Editor Plugins and IDEs

Emacs

elpy - Emacs Python Development Environment.


Sublime Text

anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.
SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi.


Vim

jedi-vim - Vim bindings for the Jedi auto-completion library for Python.
python-mode - An all in one plugin for turning Vim into a Python IDE.
YouCompleteMe - Includes Jedi-based completion engine for Python.


Visual Studio

PTVS - Python Tools for Visual Studio.


Visual Studio Code

Python - The official VSCode extension with rich support for Python.


IDE

PyCharm - Commercial Python IDE by JetBrains. Has free community edition available.
spyder - Open Source Python IDE.



Email
Libraries for sending and parsing email.

envelopes - Mailing for human beings.
flanker - An email address and Mime parsing library.
imbox - Python IMAP for Humans.
inbox.py - Python SMTP Server for Humans.
lamson - Pythonic SMTP Application Server.
Marrow Mailer - High-performance extensible mail delivery framework.
modoboa - A mail hosting and management platform including a modern and simplified Web UI.
Nylas Sync Engine - Providing a RESTful API on top of a powerful email sync platform.
yagmail - Yet another Gmail/SMTP client.

Environment Management
Libraries for Python version and virtual environment management.

pyenv - Simple Python version management.
pipenv - Python Development Workflow for Humans.
poetry - Python dependency management and packaging made easy.
virtualenv - A tool to create isolated Python environments.

Files
Libraries for file manipulation and MIME type detection.

mimetypes - (Python standard library) Map filenames to MIME types.
path.py - A module wrapper for os.path.
pathlib - (Python standard library) An cross-platform, object-oriented path library.
PyFilesystem2 - Python's filesystem abstraction layer.
python-magic - A Python interface to the libmagic file type identification library.
Unipath - An object-oriented approach to file/directory operations.
watchdog - API and shell utilities to monitor file system events.

Foreign Function Interface
Libraries for providing foreign function interface.

cffi - Foreign Function Interface for Python calling C code.
ctypes - (Python standard library) Foreign Function Interface for Python calling C code.
PyCUDA - A Python wrapper for Nvidia's CUDA API.
SWIG - Simplified Wrapper and Interface Generator.

Forms
Libraries for working with forms.

Deform - Python HTML form generation library influenced by the formish form generation library.
django-bootstrap3 - Bootstrap 3 integration with Django.
django-bootstrap4 - Bootstrap 4 integration with Django.
django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way.
django-remote-forms - A platform independent Django form serializer.
WTForms - A flexible forms validation and rendering library.

Functional Programming
Functional Programming with Python.

Coconut - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.
CyToolz - Cython implementation of Toolz: High performance functional utilities.
fn.py - Functional programming in Python: implementation of missing features to enjoy FP.
funcy - A fancy and practical functional tools.
Toolz - A collection of functional utilities for iterators, functions, and dictionaries.

GUI Development
Libraries for working with graphical user interface applications.

curses - Built-in wrapper for ncurses used to create terminal GUI applications.
Eel - A library for making simple Electron-like offline HTML/JS GUI apps.
enaml - Creating beautiful user-interfaces with Declarative Syntax like QML.
Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.
Gooey - Turn command line programs into a full GUI application with one line.
kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.
pyglet - A cross-platform windowing and multimedia library for Python.
PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).
PyQt - Python bindings for the Qt cross-platform application and UI framework.
PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi.
pywebview - A lightweight cross-platform native wrapper around a webview component.
Tkinter - Tkinter is Python's de-facto standard GUI package.
Toga - A Python native, OS native GUI toolkit.
urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.
wxPython - A blending of the wxWidgets C++ class library with the Python.

Game Development
Awesome game development libraries.

Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications.
Harfang3D - Python framework for 3D, VR and game development.
Panda3D - 3D game engine developed by Disney.
Pygame - Pygame is a set of Python modules designed for writing games.
PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.
PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs.
PySDL2 - A ctypes based wrapper for the SDL2 library.
RenPy - A Visual Novel engine.

Geolocation
Libraries for geocoding addresses and working with latitudes and longitudes.

django-countries - A Django app that provides a country field for models and forms.
GeoDjango - A world-class geographic web framework.
GeoIP - Python API for MaxMind GeoIP Legacy Database.
geojson - Python bindings and utilities for GeoJSON.
geopy - Python Geocoding Toolbox.
pygeoip - Pure Python GeoIP API.

HTML Manipulation
Libraries for working with HTML and XML.

BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.
bleach - A whitelist-based HTML sanitization and text linkification library.
cssutils - A CSS library for Python.
html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments.
lxml - A very fast, easy-to-use and versatile library for handling HTML and XML.
MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python.
pyquery - A jQuery-like library for parsing HTML.
untangle - Converts XML documents to Python objects for easy access.
WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF.
xmldataset - Simple XML Parsing.
xmltodict - Working with XML feel like you are working with JSON.

HTTP Clients
Libraries for working with HTTP.

grequests - requests + gevent for asynchronous HTTP requests.
httplib2 - Comprehensive HTTP client library.
requests - HTTP Requests for Humans.
treq - Python requests like API built on top of Twisted's HTTP client.
urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.

Hardware
Libraries for programming with hardware.

ino - Command line toolkit for working with Arduino.
keyboard - Hook and simulate global keyboard events on Windows and Linux.
mouse - Hook and simulate global mouse events on Windows and Linux.
Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.
PyUserInput - A module for cross-platform control of the mouse and keyboard.
scapy - A brilliant packet manipulation library.
wifi - A Python library and command line tool for working with WiFi on Linux.

Image Processing
Libraries for manipulating images.

hmap - Image histogram remapping.
imgSeek - A project for searching a collection of images using visual similarity.
nude.py - Nudity detection.
pagan - Retro identicon (Avatar) generation based on input string and hash.
pillow - Pillow is the friendly PIL fork.
pyBarcode - Create barcodes in Python without needing PIL.
pygram - Instagram-like image filters.
python-qrcode - A pure Python QR Code generator.
Quads - Computer art based on quadtrees.
scikit-image - A Python library for (scientific) image processing.
thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.
wand - Python bindings for MagickWand, C API for ImageMagick.

Implementations
Implementations of Python.

CPython - Default, most widely used implementation of the Python programming language written in C.
Cython - Optimizing Static Compiler for Python.
CLPython - Implementation of the Python programming language written in Common Lisp.
Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).
IronPython - Implementation of the Python programming language written in C#.
Jython - Implementation of Python programming language written in Java for the JVM.
MicroPython - A lean and efficient Python programming language implementation.
Numba - Python JIT compiler to LLVM aimed at scientific Python.
PeachPy - x86-64 assembler embedded in Python.
Pyjion - A JIT for Python based upon CoreCLR.
PyPy - A very fast and compliant implementation of the Python language.
Pyston - A Python implementation using JIT techniques.
Stackless Python - An enhanced version of the Python programming language.

Interactive Interpreter
Interactive Python interpreters (REPL).

bpython - A fancy interface to the Python interpreter.
Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively.

awesome-jupyter


ptpython - Advanced Python REPL built on top of the python-prompt-toolkit.

Internationalization
Libraries for working with i18n.

Babel - An internationalization library for Python.
PyICU - A wrapper of International Components for Unicode C++ library (ICU).

Job Scheduler
Libraries for scheduling jobs.

APScheduler - A light but powerful in-process task scheduler that lets you schedule functions.
django-schedule - A calendaring app for Django.
doit - A task runner and build tool.
gunnery - Multipurpose task execution tool for distributed systems with web-based interface.
Joblib - A set of tools to provide lightweight pipelining in Python.
Plan - Writing crontab file in Python like a charm.
schedule - Python job scheduling for humans.
Spiff - A powerful workflow engine implemented in pure Python.
TaskFlow - A Python library that helps to make task execution easy, consistent and reliable.
Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows.

Logging
Libraries for generating and working with logs.

Eliot - Logging for complex & distributed systems.
logbook - Logging replacement for Python.
logging - (Python standard library) Logging facility for Python.
raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.

Machine Learning
Libraries for Machine Learning. Also see awesome-machine-learning.

H2O - Open Source Fast Scalable Machine Learning Platform.
Metrics - Machine learning evaluation metrics.
NuPIC - Numenta Platform for Intelligent Computing.
scikit-learn - The most popular Python library for Machine Learning.
Spark ML - Apache Spark's scalable Machine Learning library.
vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit.
xgboost - A scalable, portable, and distributed gradient boosting library.

Microsoft Windows
Python programming on Microsoft Windows.

Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.
pythonlibs - Unofficial Windows binaries for Python extension packages.
PythonNet - Python Integration with the .NET Common Language Runtime (CLR).
PyWin32 - Python Extensions for Windows.
WinPython - Portable development environment for Windows 7/8.

Miscellaneous
Useful libraries or tools that don't fit in the categories above.

blinker - A fast Python in-process signal/event dispatching system.
boltons - A set of pure-Python utilities.
itsdangerous - Various helpers to pass trusted data to untrusted environments.
pluginbase - A simple but flexible plugin system for Python.
tryton - A general purpose business framework.

Natural Language Processing
Libraries for working with human languages.

General

gensim - Topic Modeling for Humans.
langid.py - Stand-alone language identification system.
nltk - A leading platform for building Python programs to work with human language data.
pattern - A web mining module for the Python.
polyglot - Natural language pipeline supporting hundreds of languages.
pytext - A natural language modeling framework based on PyTorch.
PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research.
spacy - A library for industrial-strength natural language processing in Python and Cython.
stanfordnlp - The Stanford NLP Group's official Python library, supporting 50+ languages.


Chinese

jieba - The most popular Chinese text segmentation library.
pkuseg-python - A toolkit for Chinese word segmentation in various domains.
snownlp - A library for processing Chinese text.
funNLP - A collection of tools and datasets for Chinese NLP.



Network Virtualization
Tools and libraries for Virtual Networking and SDN (Software Defined Networking).

mininet - A popular network emulator and API written in Python.
napalm - Cross-vendor API to manipulate network devices.
pox - A Python-based SDN control applications, such as OpenFlow SDN controllers.

News Feed
Libraries for building user's activities.

django-activity-stream - Generating generic activity streams from the actions on your site.
Stream Framework - Building news feed and notification systems using Cassandra and Redis.

ORM
Libraries that implement Object-Relational Mapping or data mapping techniques.

Relational Databases

Django Models - The Django ORM.
SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper.

awesome-sqlalchemy


dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.
orator -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.
orm - An async ORM.
peewee - A small, expressive ORM.
pony - ORM that provides a generator-oriented interface to SQL.
pydal - A pure Python Database Abstraction Layer.


NoSQL Databases

hot-redis - Rich Python data types for Redis.
mongoengine - A Python Object-Document-Mapper for working with MongoDB.
PynamoDB - A Pythonic interface for Amazon DynamoDB.
redisco - A Python Library for Simple Models and Containers Persisted in Redis.



Package Management
Libraries for package and dependency management.

pip - The Python package and dependency manager.

PyPI
pip-tools - A set of tools to keep your pinned Python dependencies fresh.


conda - Cross-platform, Python-agnostic binary package manager.

Package Repositories
Local PyPI repository server and proxies.

warehouse - Next generation Python Package Repository (PyPI).
bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA).
devpi - PyPI server and packaging/testing/release tool.
localshop - Local PyPI server (custom packages and auto-mirroring of pypi).

Permissions
Libraries that allow or deny users access to data or functionality.

django-guardian - Implementation of per object permissions for Django 1.2+
django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database.

Processes
Libraries for starting and communicating with OS processes.

delegator.py - Subprocesses for Humans 2.0.
sarge - Yet another wrapper for subprocess.
sh - A full-fledged subprocess replacement for Python.

Recommender Systems
Libraries for building recommender systems.

annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage.
fastFM - A library for Factorization Machines.
implicit - A fast Python implementation of collaborative filtering for implicit datasets.
libffm - A library for Field-aware Factorization Machine (FFM).
lightfm - A Python implementation of a number of popular recommendation algorithms.
spotlight - Deep recommender models using PyTorch.
Surprise - A scikit for building and analyzing recommender systems.
tensorrec - A Recommendation Engine Framework in TensorFlow.

RESTful API
Libraries for building RESTful APIs.

Django

django-rest-framework - A powerful and flexible toolkit to build web APIs.
django-tastypie - Creating delicious APIs for Django apps.


Flask

eve - REST API framework powered by Flask, MongoDB and good intentions.
flask-api - Browsable Web APIs for Flask.
flask-restful - Quickly building REST APIs for Flask.


Pyramid

cornice - A RESTful framework for Pyramid.


Framework agnostic

apistar - A smart Web API framework, designed for Python 3.
falcon - A high-performance framework for building cloud APIs and web app backends.
fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.
hug - A Python 3 framework for cleanly exposing APIs.
sandman2 - Automated REST APIs for existing database-driven systems.
sanic - A Python 3.6+ web server and web framework that's written to go fast.
vibora - Fast, efficient and asynchronous Web framework inspired by Flask.



Robotics
Libraries for robotics.

PythonRobotics - This is a compilation of various robotics algorithms with visualizations.
rospy - This is a library for ROS (Robot Operating System).

RPC Servers
RPC-compatible servers.

zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack.

Science
Libraries for scientific computing. Also see Python-for-Scientists

astropy - A community Python library for Astronomy.
bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis.
bccb - Collection of useful code related to biological analysis.
Biopython - Biopython is a set of freely available tools for biological computation.
cclib - A library for parsing and interpreting the results of computational chemistry packages.
Colour - Implementing a comprehensive number of colour theory transformations and algorithms.
NetworkX - A high-productivity software for complex networks.
NIPY - A collection of neuroimaging toolkits.
NumPy - A fundamental package for scientific computing with Python.
Open Babel - A chemical toolbox designed to speak the many languages of chemical data.
ObsPy - A Python toolbox for seismology.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.
PyMC - Markov Chain Monte Carlo sampling toolkit.
QuTiP - Quantum Toolbox in Python.
RDKit - Cheminformatics and Machine Learning Software.
SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
statsmodels - Statistical modeling and econometrics in Python.
SymPy - A Python library for symbolic mathematics.
Zipline - A Pythonic algorithmic trading library.
SimPy -  A process-based discrete-event simulation framework.

Search
Libraries and software for indexing and performing search queries on data.

elasticsearch-py - The official low-level Python client for Elasticsearch.
elasticsearch-dsl-py - The official high-level Python client for Elasticsearch.
django-haystack - Modular search for Django.
pysolr - A lightweight Python wrapper for Apache Solr.
whoosh - A fast, pure Python search engine library.

Serialization
Libraries for serializing complex data types

marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes.
pysimdjson - A Python bindings for simdjson.
python-rapidjson - A Python wrapper around RapidJSON.
ultrajson - A fast JSON decoder and encoder written in C with Python bindings.

Serverless Frameworks
Frameworks for developing serverless Python code.

python-lambda - A toolkit for developing and deploying Python code in AWS Lambda.
Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway.

Specific Formats Processing
Libraries for parsing and manipulating specific text formats.

General

tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML.


Office

openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.
pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.
python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files.
python-pptx - Python library for creating and updating PowerPoint (.pptx) files.
unoconv - Convert between any document format supported by LibreOffice/OpenOffice.
XlsxWriter - A Python module for creating Excel .xlsx files.
xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.
xlwt / xlrd - Writing and reading data and formatting information from Excel files.


PDF

PDFMiner - A tool for extracting information from PDF documents.
PyPDF2 - A library capable of splitting, merging and transforming PDF pages.
ReportLab - Allowing Rapid creation of rich PDF documents.


Markdown

Mistune - Fastest and full featured pure Python parsers of Markdown.
Python-Markdown - A Python implementation of John Gruber‚Äôs Markdown.


YAML

PyYAML - YAML implementations for Python.


CSV

csvkit - Utilities for converting to and working with CSV.


Archive

unp - A command line tool that can unpack archives easily.



Static Site Generator
Static site generator is a software that takes some text + templates as input and produces HTML files on the output.

mkdocs - Markdown friendly documentation generator.
pelican - Static site generator that supports Markdown and reST syntax.
lektor - An easy to use static CMS and blog engine.
nikola - A static website and blog generator.

Tagging
Libraries for tagging items.

django-taggit - Simple tagging for Django.

Task Queues
Libraries for working with task queues.

celery - An asynchronous task queue/job queue based on distributed message passing.
huey - Little multi-threaded task queue.
mrq - A distributed worker task queue in Python using Redis & gevent.
rq - Simple job queues for Python.

Template Engine
Libraries and tools for templating and lexing.

Jinja2 - A modern and designer friendly templating language.
Genshi - Python templating toolkit for generation of web-aware output.
Mako - Hyperfast and lightweight templating for the Python platform.

Testing
Libraries for testing codebases and generating test data.

Testing Frameworks

pytest - A mature full-featured Python testing tool.
hypothesis - Hypothesis is an advanced Quickcheck style property based testing library.
nose2 - The successor to nose, based on `unittest2.
Robot Framework - A generic test automation framework.
unittest - (Python standard library) Unit testing framework.


Test Runners

green - A clean, colorful test runner.
mamba - The definitive testing tool for Python. Born under the banner of BDD.
tox - Auto builds and tests distributions in multiple Python versions


GUI / Web Testing

locust - Scalable user load testing tool written in Python.
PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings.
Selenium - Python bindings for Selenium WebDriver.
sixpack - A language-agnostic A/B Testing framework.
splinter - Open source tool for testing web applications.


Mock

mock - (Python standard library) A mocking and patching library.
doublex - Powerful test doubles framework for Python.
freezegun - Travel through time by mocking the datetime module.
httmock - A mocking library for requests for Python 2.6+ and 3.2+.
httpretty - HTTP request mock tool for Python.
mocket - A socket mock framework with gevent/asyncio/SSL support.
responses - A utility library for mocking out the requests Python library.
VCR.py - Record and replay HTTP interactions on your tests.


Object Factories

factory_boy - A test fixtures replacement for Python.
mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.
model_mommy - Creating random fixtures for testing in Django.


Code Coverage

coverage - Code coverage measurement.


Fake Data

mimesis - is a Python library that help you generate fake data.
fake2db - Fake database generator.
faker - A Python package that generates fake data.
radar - Generate random datetime / time.



Text Processing
Libraries for parsing and manipulating plain texts.

General

chardet - Python 2/3 compatible character encoding detector.
difflib - (Python standard library) Helpers for computing deltas.
ftfy - Makes Unicode text less broken and more consistent automagically.
fuzzywuzzy - Fuzzy String Matching.
Levenshtein - Fast computation of Levenshtein distance and string similarity.
pangu.py - Paranoid text spacing.
pyfiglet - An implementation of figlet written in Python.
pypinyin - Convert Chinese hanzi (Êº¢Â≠ó) to pinyin (ÊãºÈü≥).
textdistance - Compute distance between sequences with 30+ algorithms.
unidecode - ASCII transliterations of Unicode text.


Slugify

awesome-slugify - A Python slugify library that can preserve unicode.
python-slugify - A Python slugify library that translates unicode to ASCII.
unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency.


Unique identifiers

hashids - Implementation of hashids in Python.
shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs.


Parser

ply - Implementation of lex and yacc parsing tools for Python.
pygments - A generic syntax highlighter.
pyparsing - A general purpose framework for generating parsers.
python-nameparser - Parsing human names into their individual components.
python-phonenumbers - Parsing, formatting, storing and validating international phone numbers.
python-user-agents - Browser user agent parser.
sqlparse - A non-validating SQL parser.



Third-party APIs
Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries.

apache-libcloud - One Python library for all clouds.
boto3 - Python interface to Amazon Web Services.
django-wordpress - WordPress models and views for Django.
facebook-sdk - Facebook Platform Python SDK.
google-api-python-client - Google APIs Client Library for Python.
gspread - Google Spreadsheets Python API.
twython - A Python wrapper for the Twitter API.

URL Manipulation
Libraries for parsing URLs.

furl - A small Python library that makes parsing and manipulating URLs easy.
purl - A simple, immutable URL class with a clean API for interrogation and manipulation.
pyshorteners - A pure Python URL shortening lib.
webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.

Video
Libraries for manipulating video and GIFs.

moviepy - A module for script-based movie editing with many formats, including animated GIFs.
scikit-video - Video processing routines for SciPy.

Web Asset Management
Tools for managing, compressing and minifying website assets.

django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file.
django-pipeline - An asset packaging library for Django.
django-storages - A collection of custom storage back ends for Django.
fanstatic - Packages, optimizes, and serves static file dependencies as Python packages.
fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP.
flask-assets - Helps you integrate webassets into your Flask app.
webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources.

Web Content Extracting
Libraries for extracting web contents.

html2text - Convert HTML to Markdown-formatted text.
lassie - Web Content Retrieval for Humans.
micawber - A small library for extracting rich content from URLs.
newspaper - News extraction, article extraction and content curation in Python.
python-readability - Fast Python port of arc90's readability tool.
requests-html - Pythonic HTML Parsing for Humans.
sumy - A module for automatic summarization of text documents and HTML pages.
textract - Extract text from any document, Word, PowerPoint, PDFs, etc.
toapi - Every web site provides APIs.

Web Crawling
Libraries to automate web scraping.

cola - A distributed crawling framework.
feedparser - Universal feed parser.
grab - Site scraping framework.
MechanicalSoup - A Python library for automating interaction with websites.
pyspider - A powerful spider system.
robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser.
scrapy - A fast high-level screen scraping and web crawling framework.
portia - Visual scraping for Scrapy.

Web Frameworks
Traditional full stack web frameworks. Also see RESTful API

Synchronous

Django - The most popular web framework in Python.

awesome-django


Flask - A microframework for Python.

awesome-flask


Pyramid - A small, fast, down-to-earth, open source Python web framework.

awesome-pyramid


Masonite - The modern and developer centric Python web framework.


Asynchronous

Tornado - A web framework and asynchronous networking library.



WebSocket
Libraries for working with WebSocket.

autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio.
channels - Developer-friendly asynchrony for Django.
websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity.

WSGI Servers
WSGI-compatible web servers.

bjoern - Asynchronous, very fast and written in C.
gunicorn - Pre-forked, partly written in C.
uWSGI - A project aims at developing a full stack for building hosting services, written in C.
waitress - Multi-threaded, powers Pyramid.
werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.

Resources
Where to discover new Python libraries.
Podcasts

From Python Import Podcast
Podcast.init
Python Bytes
Python Testing
Radio Free Python
Talk Python To Me
Test and Code

Twitter

@codetengu
@getpy
@importpython
@planetpython
@pycoders
@pypi
@pythontrending
@PythonWeekly
@TalkPython
@realpython

Websites

/r/CoolGithubProjects
/r/Python
Awesome Python @LibHunt
Django Packages
Full Stack Python
Python Cheatsheet
Python ZEEF
Python ÂºÄÂèëÁ§æÂå∫
Real Python
Trending Python repositories on GitHub today
–°–æ–æ–±—â–µ—Å—Ç–≤–æ Python –ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤

Weekly

CodeTengu Weekly Á¢ºÂ§©ÁãóÈÄ±Âàä
Import Python Newsletter
Pycoder's Weekly
Python Weekly
Python Tricks

Contributing
Your contributions are always welcome! Please take a look at the contribution guidelines first.
I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding üëç to them. Pull requests will be merged when their votes reach 20.

If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.
","GitHub - vinta/awesome-python: A curated list of awesome Python frameworks, libraries, software and resources"
2,Python,"Public APIs 
A collective list of free APIs for use in software and web development.
Sponsor:

A public API for this project can be found here - thanks to DigitalOcean for helping us provide this service!
For information on contributing to this project, please see the contributing guide.
Please note a passing build status indicates all listed APIs are available since the last update. A failing build status indicates that 1 or more services may be unavailable at the moment.
Index

Animals
Anime
Anti-Malware
Art & Design
Books
Business
Calendar
Cloud Storage & File Sharing
Continuous Integration
Cryptocurrency
Currency Exchange
Data Validation
Development
Dictionaries
Documents & Productivity
Environment
Events
Finance
Food & Drink
Fraud Prevention
Games & Comics
Geocoding
Government
Health
Jobs
Machine Learning
Music
News
Open Data
Open Source Projects
Patent
Personality
Photography
Science & Math
Security
Shopping
Social
Sports & Fitness
Test Data
Text Analysis
Tracking
Transportation
URL Shorteners
Vehicle
Video
Weather

Animals



API
Description
Auth
HTTPS
CORS




Cat Facts
Daily cat facts
No
Yes
No


Cats
Pictures of cats from Tumblr
apiKey
Yes
Unknown


Dogs
Based on the Stanford Dogs Dataset
No
Yes
Yes


HTTPCat
Cat for every HTTP Status
No
Yes
Unknown


IUCN
IUCN Red List of Threatened Species
apiKey
No
Unknown


Movebank
Movement and Migration data of animals
No
Yes
Unknown


Petfinder
Adoption
OAuth
Yes
Yes


PlaceGOAT
Placeholder goat images
No
Yes
Unknown


RandomCat
Random pictures of cats
No
Yes
Yes


RandomDog
Random pictures of dogs
No
Yes
Yes


RandomFox
Random pictures of foxes
No
Yes
No


RescueGroups
Adoption
No
Yes
Unknown


Shibe.Online
Random pictures of Shibu Inu, cats or birds
No
No
No



‚¨Ü Back to Index
Anime



API
Description
Auth
HTTPS
CORS




AniList
Anime discovery & tracking
OAuth
Yes
Unknown


AnimeNewsNetwork
Anime industry news
No
Yes
Yes


Jikan
Unofficial MyAnimeList API
No
Yes
Yes


Kitsu
Anime discovery platform
OAuth
Yes
Unknown


Studio Ghibli
Resources from Studio Ghibli films
No
Yes
Unknown



‚¨Ü Back to Index
Anti-Malware



API
Description
Auth
HTTPS
CORS




AbuseIPDB
IP/domain/URL reputation
apiKey
Yes
Unknown


AlienVault Open Threat Exchange (OTX)
IP/domain/URL reputation
apiKey
Yes
Unknown


Google Safe Browsing
Google Link/Domain Flagging
apiKey
Yes
Unknown


Metacert
Metacert Link Flagging
apiKey
Yes
Unknown


VirusTotal
VirusTotal File/URL Analysis
apiKey
Yes
Unknown


Web Of Trust (WOT)
Website reputation
apiKey
Yes
Unknown



‚¨Ü Back to Index
Art & Design



API
Description
Auth
HTTPS
CORS




Behance
Design
apiKey
Yes
Unknown


Cooper Hewitt
Smithsonian Design Museum
apiKey
Yes
Unknown


Dribbble
Design
OAuth
No
Unknown


Harvard Art Museums
Art
apiKey
No
Unknown


Iconfinder
Icons
apiKey
Yes
Unknown


Icons8
Icons
OAuth
Yes
Unknown


Noun Project
Icons
OAuth
No
Unknown


Rijksmuseum
Art
apiKey
Yes
Unknown



‚¨Ü Back to Index
Books



API
Description
Auth
HTTPS
CORS




Bhagavad Gita
Bhagavad Gita text
OAuth
Yes
Yes


BookNomads
Books published in the Netherlands and Flanders (about 2.5 million), book covers and related data
No
Yes
Unknown


British National Bibliography
Books
No
No
Unknown


Goodreads
Books
apiKey
Yes
Unknown


Google Books
Books
OAuth
Yes
Unknown


LibGen
Library Genesis search engine
No
No
Unknown


Open Library
Books, book covers and related data
No
Yes
Unknown


Penguin Publishing
Books, book covers and related data
No
Yes
Unknown



‚¨Ü Back to Index
Business



API
Description
Auth
HTTPS
CORS




Charity Search
Non-profit charity data
apiKey
No
Unknown


Clearbit Logo
Search for company logos and embed them in your projects
apiKey
Yes
Unknown


Domainsdb.info
Registered Domain Names Search
No
Yes
Unknown


Freelancer
Hire freelancers to get work done
OAuth
Yes
Unknown


Gmail
Flexible, RESTful access to the user's inbox
OAuth
Yes
Unknown


Google Analytics
Collect, configure and analyze your data to reach the right audience
OAuth
Yes
Unknown


MailboxValidator
Validate email address to improve deliverability
apiKey
Yes
Unknown


mailgun
Email Service
apiKey
Yes
Unknown


markerapi
Trademark Search
No
No
Unknown


Ticksel
Friendly website analytics made for humans
No
Yes
Unknown


Trello
Boards, lists and cards to help you organize and prioritize your projects
OAuth
Yes
Unknown



‚¨Ü Back to Index
Calendar



API
Description
Auth
HTTPS
CORS




Calendar Index
Worldwide Holidays and Working Days
apiKey
Yes
Yes


Church Calendar
Catholic liturgical calendar
No
No
Unknown


Czech Namedays Calendar
Lookup for a name and returns nameday date
No
No
Unknown


Google Calendar
Display, create and modify Google calendar events
OAuth
Yes
Unknown


Hebrew Calendar
Convert between Gregorian and Hebrew, fetch Shabbat and Holiday times, etc
No
No
Unknown


Holidays
Historical data regarding holidays
apiKey
Yes
Unknown


LectServe
Protestant liturgical calendar
No
No
Unknown


Nager.Date
Public holidays for more than 90 countries
No
Yes
No


Namedays Calendar
Provides namedays for multiple countries
No
Yes
Yes


Non-Working Days
Database of ICS files for non working days
No
Yes
Unknown


Russian Calendar
Check if a date is a Russian holiday or not
No
Yes
No



‚¨Ü Back to Index
Cloud Storage & File Sharing



API
Description
Auth
HTTPS
CORS




Box
File Sharing and Storage
OAuth
Yes
Unknown


Dropbox
File Sharing and Storage
OAuth
Yes
Unknown


Google Drive
File Sharing and Storage
OAuth
Yes
Unknown


OneDrive
File Sharing and Storage
OAuth
Yes
Unknown


Pastebin
Plain Text Storage
apiKey
Yes
Unknown


Temporal
IPFS based file storage and sharing with optional IPNS naming
apiKey
Yes
No


WeTransfer
File Sharing
apiKey
Yes
Yes



‚¨Ü Back to Index
Continuous Integration



API
Description
Auth
HTTPS
CORS




CircleCI
Automate the software development process using continuous integration and continuous delivery
apiKey
Yes
Unknown


Codeship
Codeship is a Continuous Integration Platform in the cloud
apiKey
Yes
Unknown


Travis CI
Sync your GitHub projects with Travis CI to test your code in minutes
apiKey
Yes
Unknown



‚¨Ü Back to Index
Cryptocurrency



API
Description
Auth
HTTPS
CORS




Binance
Exchange for Trading Cryptocurrencies based in China
apiKey
Yes
Unknown


BitcoinAverage
Digital Asset Price Data for the blockchain industry
apiKey
Yes
Unknown


BitcoinCharts
Financial and Technical Data related to the Bitcoin Network
No
Yes
Unknown


Bitfinex
Cryptocurrency Trading Platform
apiKey
Yes
Unknown


Bitmex
Real-Time Cryptocurrency derivatives trading platform based in Hong Kong
apiKey
Yes
Unknown


Bittrex
Next Generation Crypto Trading Platform
apiKey
Yes
Unknown


Block
Bitcoin Payment, Wallet & Transaction Data
apiKey
Yes
Unknown


Blockchain
Bitcoin Payment, Wallet & Transaction Data
No
Yes
Unknown


CoinAPI
All Currency Exchanges integrate under a single api
apiKey
Yes
No


Coinbase
Bitcoin, Bitcoin Cash, Litecoin and Ethereum Prices
apiKey
Yes
Unknown


Coinbase Pro
Cryptocurrency Trading Platform
apiKey
Yes
Unknown


CoinDesk
Bitcoin Price Index
No
No
Unknown


CoinGecko
Cryptocurrency Price, Market, and Developer/Social Data
No
Yes
Yes


Coinigy
Interacting with Coinigy Accounts and Exchange Directly
apiKey
Yes
Unknown


CoinLayer
Real-time Crypto Currency Exchange Rates
apiKey
Yes
Unknown


Coinlib
Crypto Currency Prices
apiKey
Yes
Unknown


Coinlore
Cryptocurrencies prices, volume and more
No
Yes
Unknown


CoinMarketCap
Cryptocurrencies Prices
apiKey
Yes
Unknown


Coinpaprika
Cryptocurrencies prices, volume and more
No
Yes
Yes


CoinRanking
Live Cryptocurrency data
No
Yes
Unknown


CryptoCompare
Cryptocurrencies Comparison
No
Yes
Unknown


Cryptonator
Cryptocurrencies Exchange Rates
No
Yes
Unknown


Gemini
Cryptocurrencies Exchange
No
Yes
Unknown


ICObench
Various information on listing, ratings, stats, and more
apiKey
Yes
Unknown


Livecoin
Cryptocurrency Exchange
No
Yes
Unknown


MercadoBitcoin
Brazilian Cryptocurrency Information
No
Yes
Unknown


Nexchange
Automated cryptocurrency exchange service
No
No
Yes


NiceHash
Largest Crypto Mining Marketplace
apiKey
Yes
Unknown


Poloniex
US based digital asset exchange
apiKey
Yes
Unknown


WorldCoinIndex
Cryptocurrencies Prices
apiKey
Yes
Unknown



‚¨Ü Back to Index
Currency Exchange



API
Description
Auth
HTTPS
CORS




1Forge
Forex currency market data
apiKey
Yes
Unknown


Currencylayer
Exchange rates and currency conversion
apiKey
Yes
Unknown


Czech National Bank
A collection of exchange rates
No
Yes
Unknown


ExchangeRate-API
Free currency conversion
No
Yes
Yes


Exchangeratesapi.io
Exchange rates with currency conversion
No
Yes
Yes


Fixer.io
Exchange rates and currency conversion
apiKey
Yes
Unknown


Frankfurter
Exchange rates, currency conversion and time series
No
Yes
Yes


ratesapi
Free exchange rates and historical rates
No
Yes
Unknown



‚¨Ü Back to Index
Data Validation



API
Description
Auth
HTTPS
CORS




Cloudmersive Validate
Validate email addresses, phone numbers, VAT numbers and domain names
apiKey
Yes
Yes


languagelayer
Language detection
No
Yes
Unknown


Lob.com
US Address Verification
apiKey
Yes
Unknown


mailboxlayer
Email address validation
No
Yes
Unknown


NumValidate
Open Source phone number validation
No
Yes
Unknown


numverify
Phone number validation
No
Yes
Unknown


PurgoMalum
Content validator against profanity & obscenity
No
No
Unknown


US Autocomplete
Enter address data quickly with real-time address suggestions
apiKey
Yes
Yes


US Extract
Extract postal addresses from any text including emails
apiKey
Yes
Yes


US Street Address
Validate and append data for any US postal address
apiKey
Yes
Yes


vatlayer
VAT number validation
No
Yes
Unknown



‚¨Ü Back to Index
Development



API
Description
Auth
HTTPS
CORS




24 Pull Requests
Project to promote open source collaboration during December
No
Yes
Yes


Agify.io
Estimates the age from a first name
No
Yes
Yes


ApiFlash
Chrome based screenshot API for developers
apiKey
Yes
Unknown


Apility.io
IP, Domains and Emails anti-abuse API blocklist
No
Yes
Yes


APIs.guru
Wikipedia for Web APIs, OpenAPI/Swagger specs for public APIs
No
Yes
Unknown


BetterMeta
Return a site's meta tags in JSON format
X-Mashape-Key
Yes
Unknown


Bitbucket
Pull public information for a Bitbucket account
No
Yes
Unknown


Bored
Find random activities to fight boredom
No
Yes
Unknown


Browshot
Easily make screenshots of web pages in any screen size, as any device
apiKey
Yes
Unknown


CDNJS
Library info on CDNJS
No
Yes
Unknown


Changelogs.md
Structured changelog metadata from open source projects
No
Yes
Unknown


CountAPI
Free and simple counting service. You can use it to track page hits and specific events
No
Yes
Yes


DigitalOcean Status
Status of all DigitalOcean services
No
Yes
Unknown


DomainDb Info
Domain name search to find all domains containing particular words/phrases/etc
No
Yes
Unknown


Faceplusplus
A tool to detect face
OAuth
Yes
Unknown


Genderize.io
Estimates a gender from a first name
No
Yes
Yes


GitHub
Make use of GitHub repositories, code and user info programmatically
OAuth
Yes
Yes


Gitlab
Automate GitLab interaction programmatically
OAuth
Yes
Unknown


Gitter
Chat for GitHub
OAuth
Yes
Unknown


HTTP2.Pro
Test endpoints for client and server HTTP/2 protocol support
No
Yes
Unknown


IBM Text to Speech
Convert text to speech
apiKey
Yes
Yes


import.io
Retrieve structured data from a website or RSS feed
apiKey
Yes
Unknown


IPify
A simple IP Address API
No
Yes
Unknown


IPinfo
Another simple IP Address API
No
Yes
Unknown


JSON 2 JSONP
Convert JSON to JSONP (on-the-fly) for easy cross-domain data requests using client-side JavaScript
No
Yes
Unknown


JSONbin.io
Free JSON storage service. Ideal for small scale Web apps, Websites and Mobile apps
apiKey
Yes
Yes


Judge0
Compile and run source code
No
Yes
Unknown


Let's Validate
Uncovers the technologies used on websites and URL to thumbnail
No
Yes
Unknown


License-API
Unofficial REST API for choosealicense.com
No
Yes
No


LiveEdu
Live Coding Streaming
OAuth
Yes
Unknown


MAC address vendor lookup
Retrieve vendor details and other information regarding a given MAC address or an OUI
apiKey
Yes
Yes


Myjson
A simple JSON store for your web or mobile app
No
No
Unknown


Nationalize.io
Estimate the nationality of a first name
No
Yes
Yes


OOPSpam
Multiple spam filtering service
No
Yes
Yes


Plino
Spam filtering system
No
Yes
Unknown


Postman
Tool for testing APIs
apiKey
Yes
Unknown


ProxyCrawl
Scraping and crawling anticaptcha service
apiKey
Yes
Unknown


Public APIs
A collective list of free JSON APIs for use in web development
No
Yes
Unknown


Pusher Beams
Push notifications for Android & iOS
apiKey
Yes
Unknown


QR code
Create an easy to read QR code and URL shortener
No
Yes
Yes


QR code
Generate and decode / read QR code graphics
No
Yes
Unknown


QuickChart
Generate chart and graph images
No
Yes
Yes


ReqRes
A hosted REST-API ready to respond to your AJAX requests
No
Yes
Unknown


Scrape Website Email
Grabs email addresses from a URL
X-Mashape-Key
Yes
Unknown


ScraperApi
Easily build scalable web scrapers
apiKey
Yes
Unknown


ScreenshotAPI.net
Create pixel-perfect website screenshots
apiKey
Yes
Yes


SHOUTCLOUD
ALL-CAPS AS A SERVICE
No
No
Unknown


StackExchange
Q&A forum for developers
OAuth
Yes
Unknown


Verse
Check what's the latest version of your favorite open-source project
No
Yes
Unknown


XML to JSON
Integration developer utility APIs
No
Yes
Unknown



‚¨Ü Back to Index
Dictionaries



API
Description
Auth
HTTPS
CORS




Lingua Robot
Word definitions, pronunciations, synonyms, antonyms and others
apiKey
Yes
Yes


Merriam-Webster
Dictionary and Thesaurus Data
apiKey
Yes
Unknown


OwlBot
Definitions with example sentence and photo if available
apiKey
Yes
Yes


Oxford
Dictionary Data
apiKey
Yes
No


Wordnik
Dictionary Data
apiKey
No
Unknown


Words
Definitions and synonyms for more than 150,000 words
apiKey
Yes
Unknown



‚¨Ü Back to Index
Documents & Productivity



API
Description
Auth
HTTPS
CORS




Cloudmersive Document and Data Conversion
HTML/URL to PDF/PNG, Office documents to PDF, image conversion
apiKey
Yes
Yes


File.io
File Sharing
No
Yes
Unknown


Mercury
Web parser
apiKey
Yes
Unknown


pdflayer
HTML/URL to PDF
apiKey
Yes
Unknown


Pocket
Bookmarking service
OAuth
Yes
Unknown


PrexView
Data from XML or JSON to PDF, HTML or Image
apiKey
Yes
Unknown


Restpack
Provides screenshot, HTML to PDF and content extraction APIs
apiKey
Yes
Unknown


Todoist
Todo Lists
OAuth
Yes
Unknown


Vector Express
Free vector file converting API
No
No
Yes


WakaTime
Automated time tracking leaderboards for programmers
No
Yes
Unknown


Wunderlist
Todo Lists
OAuth
Yes
Unknown



‚¨Ü Back to Index
Environment



API
Description
Auth
HTTPS
CORS




AirVisual
Air quality and weather data
apiKey
Yes
Unknown


Gr√ºnstromIndex
Green Power Index for Germany (Gr√ºnstromindex/GSI)
No
No
Yes


OpenAQ
Open air quality data
apiKey
Yes
Unknown


PM25.in
Air quality of China
apiKey
No
Unknown


PVWatts
Energy production photovoltaic (PV) energy systems
apiKey
Yes
Unknown


UK Carbon Intensity
The Official Carbon Intensity API for Great Britain developed by National Grid
No
Yes
Unknown



‚¨Ü Back to Index
Events



API
Description
Auth
HTTPS
CORS




Eventbrite
Find events
OAuth
Yes
Unknown


Picatic
Sell tickets anywhere
apiKey
Yes
Unknown


Ticketmaster
Search events, attractions, or venues
apiKey
Yes
Unknown



‚¨Ü Back to Index
Finance



API
Description
Auth
HTTPS
CORS




Alpha Vantage
Realtime and historical stock data
apiKey
Yes
Unknown


Barchart OnDemand
Stock, Futures and Forex Market Data
apiKey
Yes
Unknown


Consumer Financial Protection Bureau
Financial services consumer complaint data
apiKey
Yes
Unknown


Financial Modeling Prep
Stock information and data
No
Yes
Unknown


IEX
Realtime stock data
No
Yes
Yes


IEX Cloud
Realtime & Historical Stock and Market Data
apiKey
Yes
Yes


IG
Spreadbetting and CFD Market Data
apiKey
Yes
Unknown


Plaid
Connect with users‚Äô bank accounts and access transaction data
apiKey
Yes
Unknown


Razorpay IFSC
Indian Financial Systems Code (Bank Branch Codes)
No
Yes
Unknown


RoutingNumbers.info
ACH/NACHA Bank Routing Numbers
No
Yes
Unknown


Tradier
US equity/option market data (delayed, intraday, historical)
OAuth
Yes
Yes


VAT Rates
A collection of all VAT rates for EU countries
No
Yes
Unknown


YNAB
Budgeting & Planning
OAuth
Yes
Yes



‚¨Ü Back to Index
Food & Drink



API
Description
Auth
HTTPS
CORS




Edamam
Recipe Search
apiKey
Yes
Unknown


LCBO
Alcohol
apiKey
Yes
Unknown


Open Brewery DB
Breweries, Cideries and Craft Beer Bottle Shops
No
Yes
Yes


Open Food Facts
Food Products Database
No
Yes
Unknown


PunkAPI
Brewdog Beer Recipes
No
Yes
Unknown


Recipe Puppy
Food
No
No
Unknown


TacoFancy
Community-driven taco database
No
No
Unknown


The Report of the Week
Food & Drink Reviews
No
Yes
Unknown


TheCocktailDB
Cocktail Recipes
apiKey
Yes
Yes


TheMealDB
Meal Recipes
apiKey
Yes
Yes


What's on the menu?
NYPL human-transcribed historical menu collection
apiKey
No
Unknown


Zomato
Discover restaurants
apiKey
Yes
Unknown



‚¨Ü Back to Index
Fraud Prevention



API
Description
Auth
HTTPS
CORS




FraudLabs Pro
Screen order information using AI to detect frauds
apiKey
Yes
Unknown


Whitepages Pro
Global identity verification with phone, address, email and IP
apiKey
Yes
Unknown


Whitepages Pro
Phone reputation to detect spammy phones
apiKey
Yes
Unknown


Whitepages Pro
Get an owner‚Äôs name, address, demographics based on the phone number
apiKey
Yes
Unknown


Whitepages Pro
Phone number validation, line_type, carrier append
apiKey
Yes
Unknown


Whitepages Pro
Get normalized physical address, residents, address type and validity
apiKey
Yes
Unknown



‚¨Ü Back to Index
Games & Comics



API
Description
Auth
HTTPS
CORS




Age of Empires II
Get information about Age of Empires II resources
No
Yes
Unknown


AmiiboAPI
Amiibo Information
No
No
Yes


Battle.net
Blizzard Entertainment
apiKey
Yes
Unknown


Chuck Norris Database
Jokes
No
No
Unknown


Clash of Clans
Clash of Clans Game Information
apiKey
Yes
Unknown


Clash Royale
Clash Royale Game Information
apiKey
Yes
Unknown


Comic Vine
Comics
No
Yes
Unknown


Deck of Cards
Deck of Cards
No
No
Unknown


Destiny The Game
Bungie Platform API
apiKey
Yes
Unknown


Dota 2
Provides information about Player stats , Match stats, Rankings for Dota 2
No
Yes
Unknown


Dungeons and Dragons
Reference for 5th edition spells, classes, monsters, and more
No
No
No


Eve Online
Third-Party Developer Documentation
OAuth
Yes
Unknown


Final Fantasy XIV
Final Fantasy XIV Game data API
No
Yes
Yes


Fortnite
Fortnite Stats & Cosmetics
apiKey
Yes
Yes


Fortnite
Fortnite Stats
apiKey
Yes
Unknown


Giant Bomb
Video Games
No
Yes
Unknown


Guild Wars 2
Guild Wars 2 Game Information
apiKey
Yes
Unknown


Halo
Halo 5 and Halo Wars 2 Information
apiKey
Yes
Unknown


Hearthstone
Hearthstone Cards Information
X-Mashape-Key
Yes
Unknown


Hypixel
Hypixel player stats
apiKey
Yes
Unknown


IGDB.com
Video Game Database
apiKey
Yes
Unknown


JokeAPI
Programming, Miscellaneous and Dark Jokes
No
Yes
Yes


Jokes
Programming and general jokes
No
Yes
Unknown


Jservice
Jeopardy Question Database
No
No
Unknown


Magic The Gathering
Magic The Gathering Game Information
No
No
Unknown


Marvel
Marvel Comics
apiKey
No
Unknown


mod.io
Cross Platform Mod API
apiKey
Yes
Unknown


Open Trivia
Trivia Questions
No
Yes
Unknown


PandaScore
E-sports games and results
apiKey
Yes
Unknown


PlayerUnknown's Battlegrounds
PUBG Stats
apiKey
Yes
Unknown


Pok√©api
Pok√©mon Information
No
Yes
Unknown


Pok√©mon TCG
Pok√©mon TCG Information
No
Yes
Unknown


Rick and Morty
All the Rick and Morty information, including images
No
Yes
Yes


Riot Games
League of Legends Game Information
apiKey
Yes
Unknown


Scryfall
Magic: The Gathering database
No
Yes
Yes


Steam
Steam Client Interaction
OAuth
Yes
Unknown


SuperHeroes
All SuperHeroes and Villains data from all universes under a single API
apiKey
Yes
Unknown


Tronald Dump
The dumbest things Donald Trump has ever said
No
Yes
Unknown


Vainglory
Vainglory Players, Matches and Telemetry
apiKey
Yes
Yes


Wargaming.net
Wargaming.net info and stats
apiKey
Yes
No


xkcd
Retrieve xkcd comics as JSON
No
Yes
No



‚¨Ü Back to Index
Geocoding



API
Description
Auth
HTTPS
CORS




adresse.data.gouv.fr
Address database of France, geocoding and reverse
No
Yes
Unknown


Battuta
A (country/region/city) in-cascade location API
apiKey
No
Unknown


Bing Maps
Create/customize digital maps based on Bing Maps data
apiKey
Yes
Unknown


bng2latlong
Convert British OSGB36 easting and northing (British National Grid) to WGS84 latitude and longitude
No
Yes
Yes


CitySDK
Open APIs for select European cities
No
Yes
Unknown


Daum Maps
Daum Maps provide multiple APIs for Korean maps
apiKey
No
Unknown


FreeGeoIP
Free geo ip information, no registration required. 15k/hour rate limit
No
Yes
Yes


GeoApi
French geographical data
No
Yes
Unknown


Geocod.io
Address geocoding / reverse geocoding in bulk
apiKey
Yes
Unknown


Geocode.xyz
Provides worldwide forward/reverse geocoding, batch geocoding and geoparsing
No
Yes
Unknown


GeoDataSource
Geocoding of city name by using latitude and longitude coordinates
apiKey
Yes
Unknown


GeoJS
IP geolocation with ChatOps integration
No
Yes
Yes


GeoNames
Place names and other geographical data
No
No
Unknown


geoPlugin
IP geolocation and currency conversion
No
Yes
Yes


Google Earth Engine
A cloud-based platform for planetary-scale environmental data analysis
apiKey
Yes
Unknown


Google Maps
Create/customize digital maps based on Google Maps data
apiKey
Yes
Unknown


HelloSalut
Get hello translation following user language
No
Yes
Unknown


HERE Maps
Create/customize digital maps based on HERE Maps data
apiKey
Yes
Unknown


Indian Cities
Get all Indian cities in a clean JSON Format
No
Yes
Yes


IP 2 Country
Map an IP to a country
No
Yes
Unknown


IP Address Details
Find geolocation with ip address
No
Yes
Unknown


IP Location
Find location with ip address
No
No
Unknown


IP Location
Find IP address location information
No
Yes
Unknown


IP Sidekick
Geolocation API that returns extra information about an IP address
apiKey
Yes
Unknown


IP Vigilante
Free IP Geolocation API
No
Yes
Unknown


IP2Location
IP geolocation web service to get more than 55 parameters
apiKey
Yes
Unknown


IP2Proxy
Detect proxy and VPN using IP address
apiKey
Yes
Unknown


IPGeolocationAPI.com
Locate your visitors by IP with country details
No
Yes
Yes


IPInfoDB
Free Geolocation tools and APIs for country, region, city and time zone lookup by IP address
apiKey
Yes
Unknown


ipstack
Locate and identify website visitors by IP address
apiKey
Yes
Unknown


Kwelo Network
Locate and get detailed information on IP address
No
Yes
Yes


LocationIQ
Provides forward/reverse geocoding and batch geocoding
apiKey
Yes
Yes


Mapbox
Create/customize beautiful digital maps
apiKey
Yes
Unknown


Mexico
Mexico RESTful zip codes API
No
Yes
Unknown


One Map, Singapore
Singapore Land Authority REST API services for Singapore addresses
apiKey
Yes
Unknown


OnWater
Determine if a lat/lon is on water or land
No
Yes
Unknown


OpenCage
Forward and reverse geocoding using open data
apiKey
Yes
Yes


OpenStreetMap
Navigation, geolocation and geographical data
OAuth
No
Unknown


PostcodeData.nl
Provide geolocation data based on postcode for Dutch addresses
No
No
Unknown


Postcodes.io
Postcode lookup & Geolocation for the UK
No
Yes
Yes


REST Countries
Get information about countries via a RESTful API
No
Yes
Unknown


SmartIP.io
IP Geolocation and Threat Intelligence API
apiKey
Yes
Yes


Uebermaps
Discover and share maps with friends
apiKey
Yes
Unknown


US ZipCode
Validate and append data for any US ZipCode
apiKey
Yes
Yes


Utah AGRC
Utah Web API for geocoding Utah addresses
apiKey
Yes
Unknown


ViaCep
Brazil RESTful zip codes API
No
Yes
Unknown


ZipCodeAPI
US zip code distance, radius and location API
apiKey
Yes
Unknown


Zippopotam
Get information about place such as country, city, state, etc
No
No
Unknown



‚¨Ü Back to Index
Government



API
Description
Auth
HTTPS
CORS




BCLaws
Access to the laws of British Columbia
No
No
Unknown


BusinessUSA
Authoritative information on U.S. programs, events, services and more
apiKey
Yes
Unknown


Census.gov
The US Census Bureau provides various APIs and data sets on demographics and businesses
No
Yes
Unknown


City, Lyon Opendata
Lyon(FR) City Open Data
apiKey
Yes
Unknown


City, Nantes Opendata
Nantes(FR) City Open Data
apiKey
Yes
Unknown


City, Prague Opendata
Prague(CZ) City Open Data
No
No
Unknown


Code.gov
The primary platform for Open Source and code sharing for the U.S. Federal Government
apiKey
Yes
Unknown


Colorado Data Engine
Formatted and geolocated Colorado public data
No
Yes
Unknown


Colorado Information Marketplace
Colorado State Government Open Data
No
Yes
Unknown


Data USA
US Public Data
No
Yes
Unknown


Data.gov
US Government Data
apiKey
Yes
Unknown


Data.parliament.uk
Contains live datasets including information about petitions, bills, MP votes, attendance and more
No
No
Unknown


District of Columbia Open Data
Contains D.C. government public datasets, including crime, GIS, financial data, and so on
No
Yes
Unknown


EPA
Web services and data sets from the US Environmental Protection Agency
No
Yes
Unknown


FEC
Information on campaign donations in federal elections
apiKey
Yes
Unknown


Federal Register
The Daily Journal of the United States Government
No
Yes
Unknown


Food Standards Agency
UK food hygiene rating data API
No
No
Unknown


Open Government, Australia
Australian Government Open Data
No
Yes
Unknown


Open Government, Belgium
Belgium Government Open Data
No
Yes
Unknown


Open Government, Canada
Canadian Government Open Data
No
No
Unknown


Open Government, France
French Government Open Data
apiKey
Yes
Unknown


Open Government, India
Indian Government Open Data
apiKey
Yes
Unknown


Open Government, Italy
Italy Government Open Data
No
Yes
Unknown


Open Government, New Zealand
New Zealand Government Open Data
No
Yes
Unknown


Open Government, Romania
Romania Government Open Data
No
No
Unknown


Open Government, Taiwan
Taiwan Government Open Data
No
Yes
Unknown


Open Government, USA
United States Government Open Data
No
Yes
Unknown


Regulations.gov
Federal regulatory materials to increase understanding of the Federal rule making process
apiKey
Yes
Unknown


Represent by Open North
Find Canadian Government Representatives
No
Yes
Unknown


USAspending.gov
US federal spending data
No
Yes
Unknown



‚¨Ü Back to Index
Health



API
Description
Auth
HTTPS
CORS




BetterDoctor
Detailed information about doctors in your area
apiKey
Yes
Unknown


Diabetes
Logging and retrieving diabetes information
No
No
Unknown


Flutrack
Influenza-like symptoms with geotracking
No
No
Unknown


Healthcare.gov
Educational content about the US Health Insurance Marketplace
No
Yes
Unknown


Lexigram
NLP that extracts mentions of clinical concepts from text, gives access to clinical ontology
apiKey
Yes
Unknown


Makeup
Makeup Information
No
No
Unknown


Medicare
Access to the data from the CMS - medicare.gov
No
Yes
Unknown


NPPES
National Plan & Provider Enumeration System, info on healthcare providers registered in US
No
Yes
Unknown


Nutritionix
Worlds largest verified nutrition database
apiKey
Yes
Unknown


openFDA
Public FDA data about drugs, devices and foods
No
Yes
Unknown


USDA Nutrients
National Nutrient Database for Standard Reference
No
Yes
Unknown



‚¨Ü Back to Index
Jobs



API
Description
Auth
HTTPS
CORS




Adzuna
Job board aggregator
apiKey
Yes
Unknown


Authentic Jobs
Job board for designers, hackers and creative pros
apiKey
Yes
Unknown


Careerjet
Job search engine
apiKey
No
Unknown


Github Jobs
Jobs for software developers
No
Yes
Yes


GraphQL Jobs
Jobs with GraphQL
No
Yes
Yes


Indeed
Job board aggregator
apiKey
Yes
Unknown


Jobs2Careers
Job aggregator
apiKey
Yes
Unknown


Jooble
Job search engine
apiKey
Yes
Unknown


Juju
Job search engine
apiKey
No
Unknown


Open Skills
Job titles, skills and related jobs data
No
No
Unknown


Reed
Job board aggregator
apiKey
Yes
Unknown


Search.gov Jobs
Tap into a list of current jobs openings with the United States government
No
Yes
Unknown


The Muse
Job board and company profiles
apiKey
Yes
Unknown


Upwork
Freelance job board and management system
OAuth
Yes
Unknown


USAJOBS
US government job board
apiKey
Yes
Unknown


ZipRecruiter
Job search app and website
apiKey
Yes
Unknown



‚¨Ü Back to Index
Machine Learning



API
Description
Auth
HTTPS
CORS




Clarifai
Computer Vision
OAuth
Yes
Unknown


Cloudmersive
Image captioning, face recognition, NSFW classification
apiKey
Yes
Yes


Deepcode
AI for code review
No
Yes
Unknown


Dialogflow
Natural Language Processing
apiKey
Yes
Unknown


Keen IO
Data Analytics
apiKey
Yes
Unknown


Time Door
A time series analysis API
apiKey
Yes
Yes


Unplugg
Forecasting API for timeseries data
apiKey
Yes
Unknown


Wit.ai
Natural Language Processing
OAuth
Yes
Unknown



‚¨Ü Back to Index
Music



API
Description
Auth
HTTPS
CORS




AI Mastering
Automated Music Mastering
apiKey
Yes
Yes


Bandsintown
Music Events
No
Yes
Unknown


Deezer
Music
OAuth
Yes
Unknown


Discogs
Music
OAuth
Yes
Unknown


Genius
Crowdsourced lyrics and music knowledge
OAuth
Yes
Unknown


Genrenator
Music genre generator
No
Yes
Unknown


iTunes Search
Software products
No
Yes
Unknown


Jamendo
Music
OAuth
Yes
Unknown


KKBOX
Get music libraries, playlists, charts, and perform out of KKBOX's platform
OAuth
Yes
Unknown


LastFm
Music
apiKey
Yes
Unknown


Lyrics.ovh
Simple API to retrieve the lyrics of a song
No
Yes
Unknown


Mixcloud
Music
OAuth
Yes
Yes


MusicBrainz
Music
No
Yes
Unknown


Musikki
Music
apiKey
Yes
Unknown


Musixmatch
Music
apiKey
Yes
Unknown


Openwhyd
Download curated playlists of streaming tracks (YouTube, SoundCloud, etc...)
No
Yes
No


Songkick
Music Events
OAuth
Yes
Unknown


Songsterr
Provides guitar, bass and drums tabs and chords
No
Yes
Unknown


SoundCloud
Allow users to upload and share sounds
OAuth
Yes
Unknown


Spotify
View Spotify music catalog, manage users' libraries, get recommendations and more
OAuth
Yes
Unknown


TasteDive
Similar artist API (also works for movies and TV shows)
apiKey
Yes
Unknown


TheAudioDB
Music
apiKey
Yes
Unknown


Vagalume
Crowdsourced lyrics and music knowledge
apiKey
Yes
Unknown



‚¨Ü Back to Index
News



API
Description
Auth
HTTPS
CORS




Associated Press
Search for news and metadata from Associated Press
apiKey
Yes
Unknown


Chronicling America
Provides access to millions of pages of historic US newspapers from the Library of Congress
No
No
Unknown


Currents
Latest news published in various news sources, blogs and forums
apiKey
Yes
Yes


Feedbin
RSS reader
OAuth
Yes
Unknown


Feedster
Searchable and categorized collections of RSS feeds
apiKey
Yes
Unknown


New York Times
Provides news
apiKey
Yes
Unknown


News
Headlines currently published on a range of news sources and blogs
apiKey
Yes
Unknown


NPR One
Personalized news listening experience from NPR
OAuth
Yes
Unknown


The Guardian
Access all the content the Guardian creates, categorised by tags and section
apiKey
Yes
Unknown


The Old Reader
RSS reader
apiKey
Yes
Unknown



‚¨Ü Back to Index
Open Data



API
Description
Auth
HTTPS
CORS




18F
Unofficial US Federal Government API Development
No
No
Unknown


Abbreviation
Get abbreviations and meanings
X-Mashape-Key
Yes
Unknown


Archive.org
The Internet Archive
No
Yes
Unknown


ARSAT
ARSAT public data
apiKey
Yes
Unknown


Callook.info
United States ham radio callsigns
No
Yes
Unknown


CARTO
Location Information Prediction
apiKey
Yes
Unknown


Celebinfo
Celebrity information
X-Mashape-Key
Yes
Unknown


CivicFeed
News articles and public datasets
apiKey
Yes
Unknown


Datakick
The open product database
apiKey
Yes
Unknown


Enigma Public
Broadest collection of public data
apiKey
Yes
Yes


fonoApi
Mobile Device Description
No
Yes
Unknown


French Address Search
Address search via the French Government
No
Yes
Unknown


LinkPreview
Get JSON formatted summary with title, description and preview image for any requested URL
apiKey
Yes
Yes


Marijuana Strains
Marijuana strains, races, flavors and effects
apiKey
No
Unknown


Microlink.io
Extract structured data from any website
No
Yes
Yes


OpenCorporates
Data on corporate entities and directors in many countries
apiKey
Yes
Unknown


Qmeta
Global Search Engine
apiKey
Yes
Unknown


Quandl
Stock Market Data
No
Yes
Unknown


Recreation Information Database
Recreational areas, federal lands, historic sites, museums, and other attractions/resources(US)
apiKey
Yes
Unknown


Scoop.it
Content Curation Service
apiKey
No
Unknown


Teleport
Quality of Life Data
No
Yes
Unknown


Universities List
University names, countries and domains
No
Yes
Unknown


University of Oslo
Courses, lecture videos, detailed information for courses etc. for the University of Oslo (Norway)
No
Yes
Unknown


UPC database
More than 1.5 million barcode numbers from all around the world
apiKey
Yes
Unknown


Wikidata
Collaboratively edited knowledge base operated by the Wikimedia Foundation
OAuth
Yes
Unknown


Wikipedia
Mediawiki Encyclopedia
No
Yes
Unknown


Yelp
Find Local Business
OAuth
Yes
Unknown



‚¨Ü Back to Index
Open Source Projects



API
Description
Auth
HTTPS
CORS




Countly
Countly web analytics
No
No
Unknown


Drupal.org
Drupal.org
No
Yes
Unknown


Evil Insult Generator
Evil Insults
No
Yes
Yes


Libraries.io
Open source software libraries
apiKey
Yes
Unknown



‚¨Ü Back to Index
Patent



API
Description
Auth
HTTPS
CORS




EPO
European patent search system api
OAuth
Yes
Unknown


TIPO
Taiwan patent search system api
apiKey
Yes
Unknown


USPTO
USA patent api services
No
Yes
Unknown



‚¨Ü Back to Index
Personality



API
Description
Auth
HTTPS
CORS




Advice Slip
Generate random advice slips
No
Yes
Unknown


chucknorris.io
JSON API for hand curated Chuck Norris jokes
No
Yes
Unknown


FavQs.com
FavQs allows you to collect, discover and share your favorite quotes
apiKey
Yes
Unknown


FOAAS
Fuck Off As A Service
No
No
Unknown


Forismatic
Inspirational Quotes
No
No
Unknown


icanhazdadjoke
The largest selection of dad jokes on the internet
No
Yes
Unknown


kanye.rest
REST API for random Kanye West quotes
No
Yes
Yes


Medium
Community of readers and writers offering unique perspectives on ideas
OAuth
Yes
Unknown


NaMoMemes
Memes on Narendra Modi
No
Yes
Unknown


Programming Quotes
Programming Quotes API for open source projects
No
Yes
Unknown


Quote Garden
REST API for more than 5000 famous quotes
No
Yes
Unknown


Quotes on Design
Inspirational Quotes
No
Yes
Unknown


Traitify
Assess, collect and analyze Personality
No
Yes
Unknown


tronalddump.io
Api & web archive for the things Donald Trump has said
No
Yes
Unknown



‚¨Ü Back to Index
Photography



API
Description
Auth
HTTPS
CORS




Flickr
Flickr Services
OAuth
Yes
Unknown


Getty Images
Build applications using the world's most powerful imagery
OAuth
Yes
Unknown


Gfycat
Jiffier GIFs
OAuth
Yes
Unknown


Giphy
Get all your gifs
apiKey
Yes
Unknown


Gyazo
Upload images
apiKey
Yes
Unknown


Imgur
Images
OAuth
Yes
Unknown


Lorem Picsum
Images from Unsplash
No
Yes
Unknown


Pexels
Free Stock Photos and Videos
apiKey
Yes
Yes


Pixabay
Photography
apiKey
Yes
Unknown


Pixhost
Upload images, photos, galleries
No
Yes
Unknown


PlaceKitten
Resizable kitten placeholder images
No
Yes
Unknown


ScreenShotLayer
URL 2 Image
No
Yes
Unknown


Unsplash
Photography
OAuth
Yes
Unknown


Wallhaven
Wallpapers
apiKey
Yes
Unknown



‚¨Ü Back to Index
Science & Math



API
Description
Auth
HTTPS
CORS




arcsecond.io
Multiple astronomy data sources
No
Yes
Unknown


CORE
Access the world's Open Access research papers
apiKey
Yes
Unknown


GBIF
Global Biodiversity Information Facility
No
Yes
Yes


iDigBio
Access millions of museum specimens from organizations around the world
No
Yes
Unknown


inspirehep.net
High Energy Physics info. system
No
Yes
Unknown


ITIS
Integrated Taxonomic Information System
No
Yes
Unknown


Launch Library
Upcoming Space Launches
No
Yes
Unknown


Minor Planet Center
Asterank.com Information
No
No
Unknown


NASA
NASA data, including imagery
No
Yes
Unknown


NASA APOD (unofficial API)
API for getting APOD (Astronomy Image of the Day) images along with metadata
No
Yes
Yes


Newton
Symbolic and Arithmetic Math Calculator
No
Yes
Unknown


Numbers
Facts about numbers
No
No
Unknown


Open Notify
ISS astronauts, current location, etc
No
No
Unknown


Open Science Framework
Repository and archive for study designs, research materials, data, manuscripts, etc
No
Yes
Unknown


SHARE
A free, open, dataset about research and scholarly activities
No
Yes
Unknown


SpaceX
Company, vehicle, launchpad and launch data
No
Yes
Unknown


Sunrise and Sunset
Sunset and sunrise times for a given latitude and longitude
No
Yes
Unknown


Trefle
Botanical data for plant species
apiKey
Yes
Unknown


USGS Earthquake Hazards Program
Earthquakes data real-time
No
Yes
Unknown


USGS Water Services
Water quality and level info for rivers and lakes
No
Yes
Unknown


World Bank
World Data
No
No
Unknown



‚¨Ü Back to Index
Security



API
Description
Auth
HTTPS
CORS




Censys.io
Search engine for Internet connected host and devices
apiKey
Yes
No


CRXcavator
Chrome extension risk scoring
apiKey
Yes
Unknown


FilterLists
Lists of filters for adblockers and firewalls
No
Yes
Unknown


HaveIBeenPwned
Passwords which have previously been exposed in data breaches
apiKey
Yes
Unknown


National Vulnerability Database
U.S. National Vulnerability Database
No
Yes
Unknown


SecurityTrails
Domain and IP related information such as current and historical WHOIS and DNS records
apiKey
Yes
Unknown


Shodan
Search engine for Internet connected devices
apiKey
Yes
Unknown


UK Police
UK Police data
No
Yes
Unknown



‚¨Ü Back to Index
Shopping



API
Description
Auth
HTTPS
CORS




Best Buy
Products, Buying Options, Categories, Recommendations, Stores and Commerce
apiKey
Yes
Unknown


Bratabase
Database of different types of Bra Sizes
OAuth
Yes
Unknown


eBay
Sell and Buy on eBay
OAuth
Yes
Unknown


Wal-Mart
Item price and availability
apiKey
Yes
Unknown


Wegmans
Wegmans Food Markets
apiKey
Yes
Unknown



‚¨Ü Back to Index
Social



API
Description
Auth
HTTPS
CORS




Buffer
Access to pending and sent updates in Buffer
OAuth
Yes
Unknown


Cisco Spark
Team Collaboration Software
OAuth
Yes
Unknown


Discord
Make bots for Discord, integrate Discord onto an external platform
OAuth
Yes
Unknown


Disqus
Communicate with Disqus data
OAuth
Yes
Unknown


Facebook
Facebook Login, Share on FB, Social Plugins, Analytics and more
OAuth
Yes
Unknown


Foursquare
Interact with Foursquare users and places (geolocation-based checkins, photos, tips, events, etc)
OAuth
Yes
Unknown


Fuck Off as a Service
Asks someone to fuck off
No
Yes
Unknown


Full Contact
Get Social Media profiles and contact Information
OAuth
Yes
Unknown


HackerNews
Social news for CS and entrepreneurship
No
Yes
Unknown


Instagram
Instagram Login, Share on Instagram, Social Plugins and more
OAuth
Yes
Unknown


LinkedIn
The foundation of all digital integrations with LinkedIn
OAuth
Yes
Unknown


Meetup.com
Data about Meetups from Meetup.com
apiKey
Yes
Unknown


Mixer
Game Streaming API
OAuth
Yes
Unknown


MySocialApp
Seamless Social Networking features, API, SDK to any app
apiKey
Yes
Unknown


Open Collective
Get Open Collective data
No
Yes
Unknown


Pinterest
The world's catalog of ideas
OAuth
Yes
Unknown


PWRTelegram bot
Boosted version of the Telegram bot API
OAuth
Yes
Unknown


Reddit
Homepage of the internet
OAuth
Yes
Unknown


SharedCount
Social media like and share data for any URL
apiKey
Yes
Unknown


Slack
Team Instant Messaging
OAuth
Yes
Unknown


Telegram Bot
Simplified HTTP version of the MTProto API for bots
OAuth
Yes
Unknown


Telegram MTProto
Read and write Telegram data
OAuth
Yes
Unknown


Trash Nothing
A freecycling community with thousands of free items posted every day
OAuth
Yes
Yes


Tumblr
Read and write Tumblr Data
OAuth
Yes
Unknown


Twitch
Game Streaming API
OAuth
Yes
Unknown


Twitter
Read and write Twitter data
OAuth
Yes
No


vk
Read and write vk data
OAuth
Yes
Unknown



‚¨Ü Back to Index
Sports & Fitness



API
Description
Auth
HTTPS
CORS




balldontlie
Ballldontlie provides access to stats data from the NBA
No
Yes
Yes


BikeWise
Bikewise is a place to learn about and report bike crashes, hazards and thefts
No
Yes
Unknown


Canadian Football League (CFL)
Official JSON API providing real-time league, team and player statistics about the CFL
apiKey
Yes
No


Cartola FC
The Cartola FC API serves to check the partial points of your team
No
Yes
Unknown


City Bikes
City Bikes around the world
No
No
Unknown


Cricket Live Scores
Live cricket scores
X-Mashape-Key
Yes
Unknown


Ergast F1
F1 data from the beginning of the world championships in 1950
No
Yes
Unknown


Fitbit
Fitbit Information
OAuth
Yes
Unknown


Football (Soccer) Videos
Embed codes for goals and highlights from Premier League, Bundesliga, Serie A and many more
No
Yes
Yes


Football Prediction
Predictions for upcoming football matches, odds, results and stats
X-Mashape-Key
Yes
Unknown


Football-Data.org
Football Data
No
No
Unknown


JCDecaux Bike
JCDecaux's self-service bicycles
apiKey
Yes
Unknown


NBA Stats
Current and historical NBA Statistics
No
Yes
Unknown


NFL Arrests
NFL Arrest Data
No
No
Unknown


NHL Records and Stats
NHL historical data and statistics
No
Yes
Unknown


Pro Motocross
The RESTful AMA Pro Motocross lap times for every racer on the start gate
No
No
Unknown


Strava
Connect with athletes, activities and more
OAuth
Yes
Unknown


SuredBits
Query sports data, including teams, players, games, scores and statistics
No
No
No


TheSportsDB
Crowd-Sourced Sports Data and Artwork
apiKey
Yes
Yes


Wger
Workout manager data as exercises, muscles or equipment
apiKey
Yes
Unknown



‚¨Ü Back to Index
Test Data



API
Description
Auth
HTTPS
CORS




Adorable Avatars
Generate random cartoon avatars
No
Yes
Unknown


Bacon Ipsum
A Meatier Lorem Ipsum Generator
No
Yes
Unknown


Dicebear Avatars
Generate random pixel-art avatars
No
Yes
No


FakeJSON
Service to generate test and fake data
apiKey
Yes
Yes


FHIR
Fast Healthcare Interoperability Resources test data
No
Yes
Unknown


Hipster Ipsum
Generates Hipster Ipsum text
No
No
Unknown


Identicon
Generates abstract avatar image
No
Yes
Yes


JSONPlaceholder
Fake data for testing and prototyping
No
No
Unknown


Lorem Text
Generates Lorem Ipsum text
X-Mashape-Key
Yes
Unknown


LoremPicsum
Generate placeholder pictures
No
No
Unknown


Loripsum
The ""lorem ipsum"" generator that doesn't suck
No
No
Unknown


RandomUser
Generates random user data
No
Yes
Unknown


RoboHash
Generate random robot/alien avatars
No
Yes
Unknown


This Person Does not Exist
Generates real-life faces of people who do not exist
No
Yes
Unknown


UI Names
Generate random fake names
No
Yes
Unknown


Yes No
Generate yes or no randomly
No
Yes
Unknown



‚¨Ü Back to Index
Text Analysis



API
Description
Auth
HTTPS
CORS




Aylien Text Analysis
A collection of information retrieval and natural language APIs
apiKey
Yes
Unknown


Cloudmersive Natural Language Processing
Natural language processing and text analysis
apiKey
Yes
Yes


Detect Language
Detects text language
apiKey
Yes
Unknown


Google Cloud Natural
Natural language understanding technology, including sentiment, entity and syntax analysis
apiKey
Yes
Unknown


Language Identification
Automatic language detection for any texts, supports over 175 languages
X-Mashape-Key
Yes
Unknown


Semantira
Text Analytics with sentiment analysis, categorization & named entity extraction
OAuth
Yes
Unknown


Watson Natural Language Understanding
Natural language processing for advanced text analysis
OAuth
Yes
Unknown



‚¨Ü Back to Index
Tracking



API
Description
Auth
HTTPS
CORS




Postmon
An API to query Brazilian ZIP codes and orders easily, quickly and free
No
No
Unknown


Sweden
Provides information about parcels in transport
apiKey
No
Unknown


UPS
Shipment and Address information
apiKey
Yes
Unknown


WhatPulse
Small application that measures your keyboard/mouse usage
No
Yes
Unknown



‚¨Ü Back to Index
Transportation



API
Description
Auth
HTTPS
CORS




ADS-B Exchange
Access real-time and historical data of any and all airborne aircraft
No
Yes
Unknown


AIS Hub
Real-time data of any marine and inland vessel equipped with AIS tracking system
apiKey
No
Unknown


AIS Web
Aeronautical information in digital media produced by the Department of Airspace Control (DECEA)
apiKey
No
Unknown


Amadeus Travel Innovation Sandbox
Travel Search - Limited usage
apiKey
Yes
Unknown


Bay Area Rapid Transit
Stations and predicted arrivals for BART
apiKey
No
Unknown


BlaBlaCar
Search car sharing trips
apiKey
Yes
Unknown


Community Transit
Transitland API
No
Yes
Unknown


Goibibo
API for travel search
apiKey
Yes
Unknown


GraphHopper
A-to-B routing with turn-by-turn instructions
apiKey
Yes
Unknown


Icelandic APIs
Open APIs that deliver services in or regarding Iceland
No
Yes
Unknown


Indian Railways
Indian Railways Information
apiKey
No
Unknown


Izi
Audio guide for travellers
apiKey
Yes
Unknown


Metro Lisboa
Delays in subway lines
No
No
No


Navitia
The open API for building cool stuff with transport data
apiKey
Yes
Unknown


REFUGE Restrooms
Provides safe restroom access for transgender, intersex and gender nonconforming individuals
No
Yes
Unknown


Schiphol Airport
Schiphol
apiKey
Yes
Unknown


TransitLand
Transit Aggregation
No
Yes
Unknown


Transport for Atlanta, US
Marta
No
No
Unknown


Transport for Auckland, New Zealand
Auckland Transport
No
Yes
Unknown


Transport for Belgium
Belgian transport API
No
Yes
Unknown


Transport for Berlin, Germany
Third-party VBB API
No
Yes
Unknown


Transport for Bordeaux, France
Bordeaux M√©tropole public transport and more (France)
apiKey
Yes
Unknown


Transport for Boston, US
MBTA API
No
No
Unknown


Transport for Budapest, Hungary
Budapest public transport API
No
Yes
Unknown


Transport for Chicago, US
CTA
No
No
Unknown


Transport for Czech Republic
Czech transport API
No
Yes
Unknown


Transport for Denver, US
RTD
No
No
Unknown


Transport for Finland
Finnish transport API
No
Yes
Unknown


Transport for Germany
Deutsche Bahn (DB) API
apiKey
No
Unknown


Transport for Grenoble, France
Grenoble public transport
No
No
No


Transport for Honolulu, US
Honolulu Transportation Information
apiKey
No
Unknown


Transport for India
India Public Transport API
apiKey
Yes
Unknown


Transport for Lisbon, Portugal
Data about buses routes, parking and traffic
apiKey
Yes
Unknown


Transport for London, England
TfL API
No
Yes
Unknown


Transport for Madrid, Spain
Madrid BUS transport API
apiKey
No
Unknown


Transport for Manchester, England
TfGM transport network data
apiKey
Yes
No


Transport for Minneapolis, US
NexTrip API
OAuth
No
Unknown


Transport for New York City, US
MTA
apiKey
No
Unknown


Transport for Norway
Norwegian transport API
No
No
Unknown


Transport for Ottawa, Canada
OC Transpo next bus arrival API
No
No
Unknown


Transport for Paris, France
Live schedules made simple
No
No
Unknown


Transport for Paris, France
RATP Open Data API
No
No
Unknown


Transport for Philadelphia, US
SEPTA APIs
No
No
Unknown


Transport for Sao Paulo, Brazil
SPTrans
OAuth
No
Unknown


Transport for Sweden
Public Transport consumer
OAuth
Yes
Unknown


Transport for Switzerland
Official Swiss Public Transport Open Data
apiKey
Yes
Unknown


Transport for Switzerland
Swiss public transport API
No
Yes
Unknown


Transport for The Netherlands
NS, only trains
apiKey
No
Unknown


Transport for The Netherlands
OVAPI, country-wide public transport
No
Yes
Unknown


Transport for Toronto, Canada
TTC
No
Yes
Unknown


Transport for United States
NextBus API
No
No
Unknown


Transport for Vancouver, Canada
TransLink
OAuth
Yes
Unknown


Transport for Victoria, AU
PTV API
apiKey
Yes
Unknown


Transport for Washington, US
Washington Metro transport API
OAuth
Yes
Unknown


Uber
Uber ride requests and price estimation
OAuth
Yes
Yes


WhereIsMyTransport
Platform for public transport data in emerging cities
OAuth
Yes
Unknown



‚¨Ü Back to Index
URL Shorteners



API
Description
Auth
HTTPS
CORS




Bitly
URL shortener and link management
OAuth
Yes
Unknown


CleanURI
URL shortener service
No
Yes
Yes


ClickMeter
Monitor, compare and optimize your marketing links
apiKey
Yes
Unknown


Rebrandly
Custom URL shortener for sharing branded links
apiKey
Yes
Unknown


Relink
Free and secure URL shortener
No
Yes
Yes



‚¨Ü Back to Index
Vehicle



API
Description
Auth
HTTPS
CORS




Brazilian Vehicles and Prices
Vehicles information from Funda√ß√£o Instituto de Pesquisas Econ√¥micas - Fipe
No
Yes
Unknown


Kelley Blue Book
Vehicle info, pricing, configuration, plus much more
apiKey
Yes
No


Mercedes-Benz
Telematics data, remotely access vehicle functions, car configurator, locate service dealers
apiKey
Yes
No


NHTSA
NHTSA Product Information Catalog and Vehicle Listing
No
Yes
Unknown


Smartcar
Lock and unlock vehicles and get data like odometer reading and location. Works on most new cars
OAuth
Yes
Yes



‚¨Ü Back to Index
Video



API
Description
Auth
HTTPS
CORS




An API of Ice And Fire
Game Of Thrones API
No
Yes
Unknown


Breaking Bad
Breaking Bad API
No
Yes
Unknown


Breaking Bad Quotes
Some Breaking Bad quotes
No
Yes
Unknown


Czech Television
TV programme of Czech TV
No
No
Unknown


Dailymotion
Dailymotion Developer API
OAuth
Yes
Unknown


Harry Potter
Harry Potter API
apiKey
Yes
Yes


Open Movie Database
Movie information
apiKey
Yes
Unknown


Ron Swanson Quotes
Television
No
Yes
Unknown


STAPI
Information on all things Star Trek
No
No
No


SWAPI
Star Wars Information
No
Yes
Unknown


The Lord of the Rings
The Lord of the Rings API
apiKey
Yes
Unknown


TMDb
Community-based movie data
apiKey
Yes
Unknown


Trakt
Movie and TV Data
apiKey
Yes
Yes


TVDB
Television data
apiKey
Yes
Unknown


TVMaze
TV Show Data
No
No
Unknown


Utelly
Check where a tv show or movie is available
X-Mashape-Key
Yes
Unknown


Vimeo
Vimeo Developer API
OAuth
Yes
Unknown


YouTube
Add YouTube functionality to your sites and apps
OAuth
Yes
Unknown



‚¨Ü Back to Index
Weather



API
Description
Auth
HTTPS
CORS




7Timer!
Weather, especially for Astroweather
No
No
Unknown


APIXU
Weather
apiKey
Yes
Unknown


Dark Sky
Weather
apiKey
Yes
No


MetaWeather
Weather
No
Yes
No


Meteorologisk Institutt
Weather and climate data
No
Yes
Unknown


NOAA Climate Data
Weather and climate data
apiKey
Yes
Unknown


ODWeather
Weather and weather webcams
No
No
Unknown


OpenUV
Real-time UV Index Forecast
apiKey
Yes
Unknown


OpenWeatherMap
Weather
apiKey
No
Unknown


Storm Glass
Global marine weather from multiple sources
apiKey
Yes
Yes


Weatherbit
Weather
apiKey
Yes
Unknown


Yahoo! Weather
Weather
apiKey
Yes
Unknown



‚¨Ü Back to Index
",GitHub - public-apis/public-apis: A collective list of free APIs for use in software and web development.
3,Python,"The Algorithms - Python
¬†
¬†
¬†
¬†
¬†
¬†
All algorithms implemented in Python (for education)
These implementations are for learning purposes. They may be less efficient than the implementations in the Python standard library.
Contribution Guidelines
Read our Contribution Guidelines before you contribute.
Community Channel
We're on Gitter! Please join us.
List of Algorithms
See our directory.

",GitHub - TheAlgorithms/Python: All Algorithms implemented in Python
4,Python,"TensorFlow Models
This repository contains a number of different models implemented in TensorFlow:
The official models are a collection of example models that use TensorFlow's high-level APIs. They are intended to be well-maintained, tested, and kept up to date with the latest stable TensorFlow API. They should also be reasonably optimized for fast performance while still being easy to read. We especially recommend newer TensorFlow users to start here.
The research models are a large collection of models implemented in TensorFlow by researchers. They are not officially supported or available in release branches; it is up to the individual researchers to maintain the models and/or provide support on issues and pull requests.
The samples folder contains code snippets and smaller models that demonstrate features of TensorFlow, including code presented in various blog posts.
The tutorials folder is a collection of models described in the TensorFlow tutorials.
Contribution guidelines
If you want to contribute to models, be sure to review the contribution guidelines.
License
Apache License 2.0
",GitHub - tensorflow/models: Models and examples built with TensorFlow
5,Python,"
youtube-dl - download videos from youtube.com or other video platforms

INSTALLATION
DESCRIPTION
OPTIONS
CONFIGURATION
OUTPUT TEMPLATE
FORMAT SELECTION
VIDEO SELECTION
FAQ
DEVELOPER INSTRUCTIONS
EMBEDDING YOUTUBE-DL
BUGS
COPYRIGHT

INSTALLATION
To install it right away for all UNIX users (Linux, macOS, etc.), type:
sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl

If you do not have curl, you can alternatively use a recent wget:
sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl

Windows users can download an .exe file and place it in any location on their PATH except for %SYSTEMROOT%\System32 (e.g. do not put in C:\Windows\System32).
You can also use pip:
sudo -H pip install --upgrade youtube-dl

This command will update youtube-dl if you have already installed it. See the pypi page for more information.
macOS users can install youtube-dl with Homebrew:
brew install youtube-dl

Or with MacPorts:
sudo port install youtube-dl

Alternatively, refer to the developer instructions for how to check out and work with the git repository. For further options, including PGP signatures, see the youtube-dl Download Page.
DESCRIPTION
youtube-dl is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.
youtube-dl [OPTIONS] URL [URL...]

OPTIONS
-h, --help                       Print this help text and exit
--version                        Print program version and exit
-U, --update                     Update this program to latest version. Make
                                 sure that you have sufficient permissions
                                 (run with sudo if needed)
-i, --ignore-errors              Continue on download errors, for example to
                                 skip unavailable videos in a playlist
--abort-on-error                 Abort downloading of further videos (in the
                                 playlist or the command line) if an error
                                 occurs
--dump-user-agent                Display the current browser identification
--list-extractors                List all supported extractors
--extractor-descriptions         Output descriptions of all supported
                                 extractors
--force-generic-extractor        Force extraction to use the generic
                                 extractor
--default-search PREFIX          Use this prefix for unqualified URLs. For
                                 example ""gvsearch2:"" downloads two videos
                                 from google videos for youtube-dl ""large
                                 apple"". Use the value ""auto"" to let
                                 youtube-dl guess (""auto_warning"" to emit a
                                 warning when guessing). ""error"" just throws
                                 an error. The default value ""fixup_error""
                                 repairs broken URLs, but emits an error if
                                 this is not possible instead of searching.
--ignore-config                  Do not read configuration files. When given
                                 in the global configuration file
                                 /etc/youtube-dl.conf: Do not read the user
                                 configuration in ~/.config/youtube-
                                 dl/config (%APPDATA%/youtube-dl/config.txt
                                 on Windows)
--config-location PATH           Location of the configuration file; either
                                 the path to the config or its containing
                                 directory.
--flat-playlist                  Do not extract the videos of a playlist,
                                 only list them.
--mark-watched                   Mark videos watched (YouTube only)
--no-mark-watched                Do not mark videos watched (YouTube only)
--no-color                       Do not emit color codes in output

Network Options:
--proxy URL                      Use the specified HTTP/HTTPS/SOCKS proxy.
                                 To enable SOCKS proxy, specify a proper
                                 scheme. For example
                                 socks5://127.0.0.1:1080/. Pass in an empty
                                 string (--proxy """") for direct connection
--socket-timeout SECONDS         Time to wait before giving up, in seconds
--source-address IP              Client-side IP address to bind to
-4, --force-ipv4                 Make all connections via IPv4
-6, --force-ipv6                 Make all connections via IPv6

Geo Restriction:
--geo-verification-proxy URL     Use this proxy to verify the IP address for
                                 some geo-restricted sites. The default
                                 proxy specified by --proxy (or none, if the
                                 option is not present) is used for the
                                 actual downloading.
--geo-bypass                     Bypass geographic restriction via faking
                                 X-Forwarded-For HTTP header
--no-geo-bypass                  Do not bypass geographic restriction via
                                 faking X-Forwarded-For HTTP header
--geo-bypass-country CODE        Force bypass geographic restriction with
                                 explicitly provided two-letter ISO 3166-2
                                 country code
--geo-bypass-ip-block IP_BLOCK   Force bypass geographic restriction with
                                 explicitly provided IP block in CIDR
                                 notation

Video Selection:
--playlist-start NUMBER          Playlist video to start at (default is 1)
--playlist-end NUMBER            Playlist video to end at (default is last)
--playlist-items ITEM_SPEC       Playlist video items to download. Specify
                                 indices of the videos in the playlist
                                 separated by commas like: ""--playlist-items
                                 1,2,5,8"" if you want to download videos
                                 indexed 1, 2, 5, 8 in the playlist. You can
                                 specify range: ""--playlist-items
                                 1-3,7,10-13"", it will download the videos
                                 at index 1, 2, 3, 7, 10, 11, 12 and 13.
--match-title REGEX              Download only matching titles (regex or
                                 caseless sub-string)
--reject-title REGEX             Skip download for matching titles (regex or
                                 caseless sub-string)
--max-downloads NUMBER           Abort after downloading NUMBER files
--min-filesize SIZE              Do not download any videos smaller than
                                 SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE              Do not download any videos larger than SIZE
                                 (e.g. 50k or 44.6m)
--date DATE                      Download only videos uploaded in this date
--datebefore DATE                Download only videos uploaded on or before
                                 this date (i.e. inclusive)
--dateafter DATE                 Download only videos uploaded on or after
                                 this date (i.e. inclusive)
--min-views COUNT                Do not download any videos with less than
                                 COUNT views
--max-views COUNT                Do not download any videos with more than
                                 COUNT views
--match-filter FILTER            Generic video filter. Specify any key (see
                                 the ""OUTPUT TEMPLATE"" for a list of
                                 available keys) to match if the key is
                                 present, !key to check if the key is not
                                 present, key > NUMBER (like ""comment_count
                                 > 12"", also works with >=, <, <=, !=, =) to
                                 compare against a number, key = 'LITERAL'
                                 (like ""uploader = 'Mike Smith'"", also works
                                 with !=) to match against a string literal
                                 and & to require multiple matches. Values
                                 which are not known are excluded unless you
                                 put a question mark (?) after the operator.
                                 For example, to only match videos that have
                                 been liked more than 100 times and disliked
                                 less than 50 times (or the dislike
                                 functionality is not available at the given
                                 service), but who also have a description,
                                 use --match-filter ""like_count > 100 &
                                 dislike_count <? 50 & description"" .
--no-playlist                    Download only the video, if the URL refers
                                 to a video and a playlist.
--yes-playlist                   Download the playlist, if the URL refers to
                                 a video and a playlist.
--age-limit YEARS                Download only videos suitable for the given
                                 age
--download-archive FILE          Download only videos not listed in the
                                 archive file. Record the IDs of all
                                 downloaded videos in it.
--include-ads                    Download advertisements as well
                                 (experimental)

Download Options:
-r, --limit-rate RATE            Maximum download rate in bytes per second
                                 (e.g. 50K or 4.2M)
-R, --retries RETRIES            Number of retries (default is 10), or
                                 ""infinite"".
--fragment-retries RETRIES       Number of retries for a fragment (default
                                 is 10), or ""infinite"" (DASH, hlsnative and
                                 ISM)
--skip-unavailable-fragments     Skip unavailable fragments (DASH, hlsnative
                                 and ISM)
--abort-on-unavailable-fragment  Abort downloading when some fragment is not
                                 available
--keep-fragments                 Keep downloaded fragments on disk after
                                 downloading is finished; fragments are
                                 erased by default
--buffer-size SIZE               Size of download buffer (e.g. 1024 or 16K)
                                 (default is 1024)
--no-resize-buffer               Do not automatically adjust the buffer
                                 size. By default, the buffer size is
                                 automatically resized from an initial value
                                 of SIZE.
--http-chunk-size SIZE           Size of a chunk for chunk-based HTTP
                                 downloading (e.g. 10485760 or 10M) (default
                                 is disabled). May be useful for bypassing
                                 bandwidth throttling imposed by a webserver
                                 (experimental)
--playlist-reverse               Download playlist videos in reverse order
--playlist-random                Download playlist videos in random order
--xattr-set-filesize             Set file xattribute ytdl.filesize with
                                 expected file size
--hls-prefer-native              Use the native HLS downloader instead of
                                 ffmpeg
--hls-prefer-ffmpeg              Use ffmpeg instead of the native HLS
                                 downloader
--hls-use-mpegts                 Use the mpegts container for HLS videos,
                                 allowing to play the video while
                                 downloading (some players may not be able
                                 to play it)
--external-downloader COMMAND    Use the specified external downloader.
                                 Currently supports
                                 aria2c,avconv,axel,curl,ffmpeg,httpie,wget
--external-downloader-args ARGS  Give these arguments to the external
                                 downloader

Filesystem Options:
-a, --batch-file FILE            File containing URLs to download ('-' for
                                 stdin), one URL per line. Lines starting
                                 with '#', ';' or ']' are considered as
                                 comments and ignored.
--id                             Use only video ID in file name
-o, --output TEMPLATE            Output filename template, see the ""OUTPUT
                                 TEMPLATE"" for all the info
--autonumber-start NUMBER        Specify the start value for %(autonumber)s
                                 (default is 1)
--restrict-filenames             Restrict filenames to only ASCII
                                 characters, and avoid ""&"" and spaces in
                                 filenames
-w, --no-overwrites              Do not overwrite files
-c, --continue                   Force resume of partially downloaded files.
                                 By default, youtube-dl will resume
                                 downloads if possible.
--no-continue                    Do not resume partially downloaded files
                                 (restart from beginning)
--no-part                        Do not use .part files - write directly
                                 into output file
--no-mtime                       Do not use the Last-modified header to set
                                 the file modification time
--write-description              Write video description to a .description
                                 file
--write-info-json                Write video metadata to a .info.json file
--write-annotations              Write video annotations to a
                                 .annotations.xml file
--load-info-json FILE            JSON file containing the video information
                                 (created with the ""--write-info-json""
                                 option)
--cookies FILE                   File to read cookies from and dump cookie
                                 jar in
--cache-dir DIR                  Location in the filesystem where youtube-dl
                                 can store some downloaded information
                                 permanently. By default
                                 $XDG_CACHE_HOME/youtube-dl or
                                 ~/.cache/youtube-dl . At the moment, only
                                 YouTube player files (for videos with
                                 obfuscated signatures) are cached, but that
                                 may change.
--no-cache-dir                   Disable filesystem caching
--rm-cache-dir                   Delete all filesystem cache files

Thumbnail images:
--write-thumbnail                Write thumbnail image to disk
--write-all-thumbnails           Write all thumbnail image formats to disk
--list-thumbnails                Simulate and list all available thumbnail
                                 formats

Verbosity / Simulation Options:
-q, --quiet                      Activate quiet mode
--no-warnings                    Ignore warnings
-s, --simulate                   Do not download the video and do not write
                                 anything to disk
--skip-download                  Do not download the video
-g, --get-url                    Simulate, quiet but print URL
-e, --get-title                  Simulate, quiet but print title
--get-id                         Simulate, quiet but print id
--get-thumbnail                  Simulate, quiet but print thumbnail URL
--get-description                Simulate, quiet but print video description
--get-duration                   Simulate, quiet but print video length
--get-filename                   Simulate, quiet but print output filename
--get-format                     Simulate, quiet but print output format
-j, --dump-json                  Simulate, quiet but print JSON information.
                                 See the ""OUTPUT TEMPLATE"" for a description
                                 of available keys.
-J, --dump-single-json           Simulate, quiet but print JSON information
                                 for each command-line argument. If the URL
                                 refers to a playlist, dump the whole
                                 playlist information in a single line.
--print-json                     Be quiet and print the video information as
                                 JSON (video is still being downloaded).
--newline                        Output progress bar as new lines
--no-progress                    Do not print progress bar
--console-title                  Display progress in console titlebar
-v, --verbose                    Print various debugging information
--dump-pages                     Print downloaded pages encoded using base64
                                 to debug problems (very verbose)
--write-pages                    Write downloaded intermediary pages to
                                 files in the current directory to debug
                                 problems
--print-traffic                  Display sent and read HTTP traffic
-C, --call-home                  Contact the youtube-dl server for debugging
--no-call-home                   Do NOT contact the youtube-dl server for
                                 debugging

Workarounds:
--encoding ENCODING              Force the specified encoding (experimental)
--no-check-certificate           Suppress HTTPS certificate validation
--prefer-insecure                Use an unencrypted connection to retrieve
                                 information about the video. (Currently
                                 supported only for YouTube)
--user-agent UA                  Specify a custom user agent
--referer URL                    Specify a custom referer, use if the video
                                 access is restricted to one domain
--add-header FIELD:VALUE         Specify a custom HTTP header and its value,
                                 separated by a colon ':'. You can use this
                                 option multiple times
--bidi-workaround                Work around terminals that lack
                                 bidirectional text support. Requires bidiv
                                 or fribidi executable in PATH
--sleep-interval SECONDS         Number of seconds to sleep before each
                                 download when used alone or a lower bound
                                 of a range for randomized sleep before each
                                 download (minimum possible number of
                                 seconds to sleep) when used along with
                                 --max-sleep-interval.
--max-sleep-interval SECONDS     Upper bound of a range for randomized sleep
                                 before each download (maximum possible
                                 number of seconds to sleep). Must only be
                                 used along with --min-sleep-interval.

Video Format Options:
-f, --format FORMAT              Video format code, see the ""FORMAT
                                 SELECTION"" for all the info
--all-formats                    Download all available video formats
--prefer-free-formats            Prefer free video formats unless a specific
                                 one is requested
-F, --list-formats               List all available formats of requested
                                 videos
--youtube-skip-dash-manifest     Do not download the DASH manifests and
                                 related data on YouTube videos
--merge-output-format FORMAT     If a merge is required (e.g.
                                 bestvideo+bestaudio), output to given
                                 container format. One of mkv, mp4, ogg,
                                 webm, flv. Ignored if no merge is required

Subtitle Options:
--write-sub                      Write subtitle file
--write-auto-sub                 Write automatically generated subtitle file
                                 (YouTube only)
--all-subs                       Download all the available subtitles of the
                                 video
--list-subs                      List all available subtitles for the video
--sub-format FORMAT              Subtitle format, accepts formats
                                 preference, for example: ""srt"" or
                                 ""ass/srt/best""
--sub-lang LANGS                 Languages of the subtitles to download
                                 (optional) separated by commas, use --list-
                                 subs for available language tags

Authentication Options:
-u, --username USERNAME          Login with this account ID
-p, --password PASSWORD          Account password. If this option is left
                                 out, youtube-dl will ask interactively.
-2, --twofactor TWOFACTOR        Two-factor authentication code
-n, --netrc                      Use .netrc authentication data
--video-password PASSWORD        Video password (vimeo, smotri, youku)

Adobe Pass Options:
--ap-mso MSO                     Adobe Pass multiple-system operator (TV
                                 provider) identifier, use --ap-list-mso for
                                 a list of available MSOs
--ap-username USERNAME           Multiple-system operator account login
--ap-password PASSWORD           Multiple-system operator account password.
                                 If this option is left out, youtube-dl will
                                 ask interactively.
--ap-list-mso                    List all supported multiple-system
                                 operators

Post-processing Options:
-x, --extract-audio              Convert video files to audio-only files
                                 (requires ffmpeg or avconv and ffprobe or
                                 avprobe)
--audio-format FORMAT            Specify audio format: ""best"", ""aac"",
                                 ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"", or
                                 ""wav""; ""best"" by default; No effect without
                                 -x
--audio-quality QUALITY          Specify ffmpeg/avconv audio quality, insert
                                 a value between 0 (better) and 9 (worse)
                                 for VBR or a specific bitrate like 128K
                                 (default 5)
--recode-video FORMAT            Encode the video to another format if
                                 necessary (currently supported:
                                 mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS        Give these arguments to the postprocessor
-k, --keep-video                 Keep the video file on disk after the post-
                                 processing; the video is erased by default
--no-post-overwrites             Do not overwrite post-processed files; the
                                 post-processed files are overwritten by
                                 default
--embed-subs                     Embed subtitles in the video (only for mp4,
                                 webm and mkv videos)
--embed-thumbnail                Embed thumbnail in the audio as cover art
--add-metadata                   Write metadata to the video file
--metadata-from-title FORMAT     Parse additional metadata like song title /
                                 artist from the video title. The format
                                 syntax is the same as --output. Regular
                                 expression with named capture groups may
                                 also be used. The parsed parameters replace
                                 existing values. Example: --metadata-from-
                                 title ""%(artist)s - %(title)s"" matches a
                                 title like ""Coldplay - Paradise"". Example
                                 (regex): --metadata-from-title
                                 ""(?P<artist>.+?) - (?P<title>.+)""
--xattrs                         Write metadata to the video file's xattrs
                                 (using dublin core and xdg standards)
--fixup POLICY                   Automatically correct known faults of the
                                 file. One of never (do nothing), warn (only
                                 emit a warning), detect_or_warn (the
                                 default; fix file if we can, warn
                                 otherwise)
--prefer-avconv                  Prefer avconv over ffmpeg for running the
                                 postprocessors
--prefer-ffmpeg                  Prefer ffmpeg over avconv for running the
                                 postprocessors (default)
--ffmpeg-location PATH           Location of the ffmpeg/avconv binary;
                                 either the path to the binary or its
                                 containing directory.
--exec CMD                       Execute a command on the file after
                                 downloading, similar to find's -exec
                                 syntax. Example: --exec 'adb push {}
                                 /sdcard/Music/ && rm {}'
--convert-subs FORMAT            Convert the subtitles to other format
                                 (currently supported: srt|ass|vtt|lrc)

CONFIGURATION
You can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at /etc/youtube-dl.conf and the user wide configuration file at ~/.config/youtube-dl/config. On Windows, the user wide configuration file locations are %APPDATA%\youtube-dl\config.txt or C:\Users\<user name>\youtube-dl.conf. Note that by default configuration file may not exist so you may need to create it yourself.
For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under Movies directory in your home directory:
# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s

Note that options in configuration file are just the same options aka switches used in regular command line calls thus there must be no whitespace after - or --, e.g. -o or --proxy but not - o or -- proxy.
You can use --ignore-config if you want to disable the configuration file for a particular youtube-dl run.
You can also use --config-location if you want to use custom configuration file for a particular youtube-dl run.
Authentication with .netrc file
You may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with --username and --password) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a .netrc file on a per extractor basis. For that you will need to create a .netrc file in your $HOME and restrict permissions to read/write by only you:
touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc

After that you can add credentials for an extractor in the following format, where extractor is the name of the extractor in lowercase:
machine <extractor> login <login> password <password>

For example:
machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password

To activate authentication with the .netrc file you should pass --netrc to youtube-dl or place it in the configuration file.
On Windows you may also need to setup the %HOME% environment variable manually. For example:
set HOME=%USERPROFILE%

OUTPUT TEMPLATE
The -o option allows users to indicate a template for the output file names.
tl;dr: navigate me to examples.
The basic usage is not to set any template arguments when downloading a single file, like in youtube-dl -o funny_video.flv ""https://some/video"". However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to python string formatting operations. For example, %(NAME)s or %(NAME)05d. To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:

id (string): Video identifier
title (string): Video title
url (string): Video URL
ext (string): Video filename extension
alt_title (string): A secondary title of the video
display_id (string): An alternative identifier for the video
uploader (string): Full name of the video uploader
license (string): License name the video is licensed under
creator (string): The creator of the video
release_date (string): The date (YYYYMMDD) when the video was released
timestamp (numeric): UNIX timestamp of the moment the video became available
upload_date (string): Video upload date (YYYYMMDD)
uploader_id (string): Nickname or id of the video uploader
channel (string): Full name of the channel the video is uploaded on
channel_id (string): Id of the channel
location (string): Physical location where the video was filmed
duration (numeric): Length of the video in seconds
view_count (numeric): How many users have watched the video on the platform
like_count (numeric): Number of positive ratings of the video
dislike_count (numeric): Number of negative ratings of the video
repost_count (numeric): Number of reposts of the video
average_rating (numeric): Average rating give by users, the scale used depends on the webpage
comment_count (numeric): Number of comments on the video
age_limit (numeric): Age restriction for the video (years)
is_live (boolean): Whether this video is a live stream or a fixed-length video
start_time (numeric): Time in seconds where the reproduction should start, as specified in the URL
end_time (numeric): Time in seconds where the reproduction should end, as specified in the URL
format (string): A human-readable description of the format
format_id (string): Format code specified by --format
format_note (string): Additional info about the format
width (numeric): Width of the video
height (numeric): Height of the video
resolution (string): Textual description of width and height
tbr (numeric): Average bitrate of audio and video in KBit/s
abr (numeric): Average audio bitrate in KBit/s
acodec (string): Name of the audio codec in use
asr (numeric): Audio sampling rate in Hertz
vbr (numeric): Average video bitrate in KBit/s
fps (numeric): Frame rate
vcodec (string): Name of the video codec in use
container (string): Name of the container format
filesize (numeric): The number of bytes, if known in advance
filesize_approx (numeric): An estimate for the number of bytes
protocol (string): The protocol that will be used for the actual download
extractor (string): Name of the extractor
extractor_key (string): Key name of the extractor
epoch (numeric): Unix epoch when creating the file
autonumber (numeric): Five-digit number that will be increased with each download, starting at zero
playlist (string): Name or id of the playlist that contains the video
playlist_index (numeric): Index of the video in the playlist padded with leading zeros according to the total length of the playlist
playlist_id (string): Playlist identifier
playlist_title (string): Playlist title
playlist_uploader (string): Full name of the playlist uploader
playlist_uploader_id (string): Nickname or id of the playlist uploader

Available for the video that belongs to some logical chapter or section:

chapter (string): Name or title of the chapter the video belongs to
chapter_number (numeric): Number of the chapter the video belongs to
chapter_id (string): Id of the chapter the video belongs to

Available for the video that is an episode of some series or programme:

series (string): Title of the series or programme the video episode belongs to
season (string): Title of the season the video episode belongs to
season_number (numeric): Number of the season the video episode belongs to
season_id (string): Id of the season the video episode belongs to
episode (string): Title of the video episode
episode_number (numeric): Number of the video episode within a season
episode_id (string): Id of the video episode

Available for the media that is a track or a part of a music album:

track (string): Title of the track
track_number (numeric): Number of the track within an album or a disc
track_id (string): Id of the track
artist (string): Artist(s) of the track
genre (string): Genre(s) of the track
album (string): Title of the album the track belongs to
album_type (string): Type of the album
album_artist (string): List of all artists appeared on the album
disc_number (numeric): Number of the disc or other physical medium the track belongs to
release_year (numeric): Year (YYYY) when the album was released

Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with NA.
For example for -o %(title)s-%(id)s.%(ext)s and an mp4 video with title youtube-dl test video and id BaW_jenozKcj, this will result in a youtube-dl test video-BaW_jenozKcj.mp4 file created in the current directory.
For numeric sequences you can use numeric related formatting, for example, %(view_count)05d will result in a string with view count padded with zeros up to 5 characters, like in 00042.
Output templates can also contain arbitrary hierarchical path, e.g. -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.
To use percent literals in an output template use %%. To output to stdout use -o -.
The current default template is %(title)s-%(id)s.%(ext)s.
In some cases, you don't want special characters such as ‰∏≠, spaces, or &, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the --restrict-filenames flag to get a shorter title:
Output template and Windows batch files
If you are using an output template inside a Windows batch file then you must escape plain percent characters (%) by doubling, so that -o ""%(title)s-%(id)s.%(ext)s"" should become -o ""%%(title)s-%%(id)s.%%(ext)s"". However you should not touch %'s that are not plain characters, e.g. environment variables for expansion should stay intact: -o ""C:\%HOMEPATH%\Desktop\%%(title)s.%%(ext)s"".
Output template examples
Note that on Windows you may need to use double quotes instead of single.
$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc
youtube-dl test video ''_√§‚Ü≠ùïê.mp4    # All kinds of weird characters

$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          # A simple file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ youtube-dl -o ""C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s"" https://videomore.ru/kino_v_detalayah/5_sezon/367617

# Stream the video being downloaded to stdout
$ youtube-dl -o - BaW_jenozKc
FORMAT SELECTION
By default youtube-dl tries to download the best available quality, i.e. if you want the best quality you don't need to pass any special options, youtube-dl will guess it for you by default.
But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called format selection based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.
The general syntax for format selection is --format FORMAT or shorter -f FORMAT where FORMAT is a selector expression, i.e. an expression that describes format or formats you would like to download.
tl;dr: navigate me to examples.
The simplest case is requesting a specific format, for example with -f 22 you can download the format with format code equal to 22. You can get the list of available format codes for particular video using --list-formats or -F. Note that these format codes are extractor specific.
You can also use a file extension (currently 3gp, aac, flv, m4a, mp3, mp4, ogg, wav, webm are supported) to download the best quality format of a particular file extension served as a single file, e.g. -f webm will download the best quality format with the webm extension served as a single file.
You can also use special names to select particular edge case formats:

best: Select the best quality format represented by a single file with video and audio.
worst: Select the worst quality format represented by a single file with video and audio.
bestvideo: Select the best quality video-only format (e.g. DASH video). May not be available.
worstvideo: Select the worst quality video-only format. May not be available.
bestaudio: Select the best quality audio only-format. May not be available.
worstaudio: Select the worst quality audio only-format. May not be available.

For example, to download the worst quality video-only format you can use -f worstvideo.
If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example -f 22/17/18 will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.
If you want to download several formats of the same video use a comma as a separator, e.g. -f 22,17,18 will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: -f 136/137/mp4/bestvideo,140/m4a/bestaudio.
You can also filter the video formats by putting a condition in brackets, as in -f ""best[height=720]"" (or -f ""[filesize>10M]"").
The following numeric meta fields can be used with comparisons <, <=, >, >=, = (equals), != (not equals):

filesize: The number of bytes, if known in advance
width: Width of the video, if known
height: Height of the video, if known
tbr: Average bitrate of audio and video in KBit/s
abr: Average audio bitrate in KBit/s
vbr: Average video bitrate in KBit/s
asr: Audio sampling rate in Hertz
fps: Frame rate

Also filtering work for comparisons = (equals), ^= (starts with), $= (ends with), *= (contains) and following string meta fields:

ext: File extension
acodec: Name of the audio codec in use
vcodec: Name of the video codec in use
container: Name of the container format
protocol: The protocol that will be used for the actual download, lower-case (http, https, rtsp, rtmp, rtmpe, mms, f4m, ism, http_dash_segments, m3u8, or m3u8_native)
format_id: A short description of the format

Any string comparison may be prefixed with negation ! in order to produce an opposite comparison, e.g. !*= (does not contain).
Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.
Formats for which the value is not known are excluded unless you put a question mark (?) after the operator. You can combine format filters, so -f ""[height <=? 720][tbr>500]"" selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.
You can merge the video and audio of two formats into a single file using -f <video-format>+<audio-format> (requires ffmpeg or avconv installed), for example -f bestvideo+bestaudio will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.
Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use -f '(mp4,webm)[height<480]'.
Since the end of April 2015 and version 2015.04.26, youtube-dl uses -f bestvideo+bestaudio/best as the default format selection (see #5447, #5456). If ffmpeg or avconv are installed this results in downloading bestvideo and bestaudio separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to best and results in downloading the best available quality served as a single file. best is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add -f bestvideo[height<=?1080]+bestaudio/best to your configuration file. Note that if you use youtube-dl to stream to stdout (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as -o -, youtube-dl still uses -f best format selection in order to start content delivery immediately to your player and not to wait until bestvideo and bestaudio are downloaded and muxed.
If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with -f best. You may want to add it to the configuration file in order not to type it every time you run youtube-dl.
Format selection examples
Note that on Windows you may need to use double quotes instead of single.
# Download best mp4 format available or any other best if no mp4 available
$ youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'

# Download best format available but no better than 480p
$ youtube-dl -f 'bestvideo[height<=480]+bestaudio/best[height<=480]'

# Download best video only format but no bigger than 50 MB
$ youtube-dl -f 'best[filesize<50M]'

# Download best format available via direct link over HTTP/HTTPS protocol
$ youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'

# Download the best video format and the best audio format without merging them
$ youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'
Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.
VIDEO SELECTION
Videos can be filtered by their upload date using the options --date, --datebefore or --dateafter. They accept dates in two formats:

Absolute dates: Dates in the format YYYYMMDD.
Relative dates: Dates in the format (now|today)[+-][0-9](day|week|month|year)(s)?

Examples:
# Download only the videos uploaded in the last 6 months
$ youtube-dl --dateafter now-6months

# Download only the videos uploaded on January 1, 1970
$ youtube-dl --date 19700101

$ # Download only the videos uploaded in the 200x decade
$ youtube-dl --dateafter 20000101 --datebefore 20091231
FAQ
How do I update youtube-dl?
If you've followed our manual installation instructions, you can simply run youtube-dl -U (or, on Linux, sudo youtube-dl -U).
If you have used pip, a simple sudo pip install -U youtube-dl is sufficient to update.
If you have installed youtube-dl using a package manager like apt-get or yum, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to https://yt-dl.org to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.
As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line like
sudo apt-get remove -y youtube-dl

Afterwards, simply follow our manual installation instructions:
sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
hash -r

Again, from then on you'll be able to update with sudo youtube-dl -U.
youtube-dl is extremely slow to start on Windows
Add a file exclusion for youtube-dl.exe in Windows Defender settings.
I'm getting an error Unable to extract OpenGraph title on YouTube playlists
YouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.
If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to report bugs to the Ubuntu packaging people - all they have to do is update the package to a somewhat recent version. See above for a way to update.
I'm getting an error when trying to use output template: error: using output template conflicts with using title, video ID or auto number
Make sure you are not using -o with any of these options -t, --title, --id, -A or --auto-number set in command line or in a configuration file. Remove the latter if any.
Do I always have to pass -citw?
By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, please file an issue where you explain that). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of -citw that is regularly useful is -i.
Can you please put the -b option back?
Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the -b option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the -f option and youtube-dl will try to download it.
I get HTTP error 402 when trying to download a video. What's this?
Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're considering to provide a way to let you solve the CAPTCHA, but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.
Do I need any other programs?
youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need avconv or ffmpeg. On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.
Videos or video formats streamed via RTMP protocol can only be downloaded when rtmpdump is installed. Downloading MMS and RTSP videos requires either mplayer or mpv to be installed.
I have downloaded a video but how can I play it?
Once the video is fully downloaded, use any video player, such as mpv, vlc or mplayer.
I extracted a video URL with -g, but it does not play on another machine / in my web browser.
It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the --cookies option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use --dump-user-agent to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with --dump-json.
It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.
Please bear in mind that some URL protocols are not supported by browsers out of the box, including RTMP. If you are using -g, your own downloader must support these as well.
If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use -o - to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.
ERROR: no fmt_url_map or conn information found in video info
YouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See above for how to update youtube-dl.
ERROR: unable to download video
YouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See above for how to update youtube-dl.
Video URL contains an ampersand and I'm getting some strange output [1] 2839 or 'v' is not recognized as an internal or external command
That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).
For example if your URL is https://www.youtube.com/watch?t=4&v=BaW_jenozKc you should end up with following command:
youtube-dl 'https://www.youtube.com/watch?t=4&v=BaW_jenozKc'
or
youtube-dl https://www.youtube.com/watch?t=4\&v=BaW_jenozKc
For Windows you have to use the double quotes:
youtube-dl ""https://www.youtube.com/watch?t=4&v=BaW_jenozKc""
ExtractorError: Could not find JS function u'OF'
In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See above for how to update youtube-dl.
HTTP Error 429: Too Many Requests or 402: Payment Required
These two error codes indicate that the service is blocking your IP address because of overuse. Contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the --proxy or --source-address options to select another IP address.
SyntaxError: Non-ASCII character
The error
File ""youtube-dl"", line 2
SyntaxError: Non-ASCII character '\x93' ...

means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.
What is this binary file? Where has the code gone?
Since June 2012 (#342) youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to youtube-dl.zip first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the __main__.py file. To recompile the executable, run make youtube-dl.
The exe throws an error due to missing MSVCR100.dll
To run the exe you need to install first the Microsoft Visual C++ 2010 Redistributable Package (x86).
On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?
If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.
To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, C:\bin, or C:\Users\<User name>\bin), put all the executables directly in there, and then set your PATH environment variable to include that directory.
From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing youtube-dl or ffmpeg, no matter what directory you're in.
How do I put downloads into a specific folder?
Use the -o to specify an output template, for example -o ""/home/user/videos/%(title)s-%(id)s.%(ext)s"". If you want this for all of your downloads, put the option into your configuration file.
How do I download a video starting with a -?
Either prepend https://www.youtube.com/watch?v= or separate the ID from the options with --:
youtube-dl -- -wNyEUrxzFU
youtube-dl ""https://www.youtube.com/watch?v=-wNyEUrxzFU""

How do I pass cookies to youtube-dl?
Use the --cookies option, for example --cookies /path/to/cookies/file.txt.
In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example, cookies.txt (for Chrome) or cookies.txt (for Firefox).
Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either # HTTP Cookie File or # Netscape HTTP Cookie File. Make sure you have correct newline format in the cookies file and convert newlines if necessary to correspond with your OS, namely CRLF (\r\n) for Windows and LF (\n) for Unix and Unix-like systems (Linux, macOS, etc.). HTTP Error 400: Bad Request when using --cookies is a good sign of invalid newline format.
Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around CAPTCHA some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).
How do I stream directly to media player?
You will first need to tell youtube-dl to stream media to stdout with -o -, and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to vlc can be achieved with:
youtube-dl -o - ""https://www.youtube.com/watch?v=BaW_jenozKcj"" | vlc -

How do I download only new videos from a playlist?
Use download-archive feature. With this feature you should initially download the complete playlist with --download-archive /path/to/download/archive/file.txt that will record identifiers of all the videos in a special file. Each subsequent run with the same --download-archive will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.
For example, at first,
youtube-dl --download-archive archive.txt ""https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re""

will download the complete PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re playlist and create a file archive.txt. Each subsequent run will only download new videos if any:
youtube-dl --download-archive archive.txt ""https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re""

Should I add --hls-prefer-native into my config?
When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.
When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.
In particular, the generic extractor (used when your website is not in the list of supported sites by youtube-dl cannot mandate one specific downloader.
If you put either --hls-prefer-native or --hls-prefer-ffmpeg into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to file an issue or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.
Can you add support for this anime video site, or site which shows current movies for free?
As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.
A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should not be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A ""fair use"" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.
Support requests for services that do purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.
How can I speed up work on my issue?
(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:
First of all, please do report the issue at our issue tracker. That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.
Please read the bug reporting instructions below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.
If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).
Feel free to bump the issue from time to time by writing a small comment (""Issue is still present in youtube-dl version ...from France, but fixed from Belgium""), but please not more than once a month. Please do not declare your issue as important or urgent.
How can I detect whether a given URL is supported by youtube-dl?
For one, have a look at the list of supported sites. Note that it can sometimes happen that the site changes its URL scheme (say, from https://example.com/video/1234567 to https://example.com/v/1234567 ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.
It is not possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches all URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.
If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an UnsupportedError exception if you run it from a Python program.
Why do I need to go through that much red tape when filing bugs?
Before we had the issue template, despite our extensive bug reporting instructions, about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said ""please install ffmpeg"", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.
youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of youtube-dl -v YOUR_URL_HERE is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.
DEVELOPER INSTRUCTIONS
Most users do not need to build youtube-dl and can download the builds or get them from their distribution.
To run youtube-dl as a developer, you don't need to build anything either. Simply execute
python -m youtube_dl

To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:
python -m unittest discover
python test/test_download.py
nosetests

See item 6 of new extractor tutorial for how to run extractor specific test cases.
If you want to create a build of youtube-dl yourself, you'll need

python
make (only GNU make is supported)
pandoc
zip
nosetests

Adding support for a new site
If you want to add support for a new site, first of all make sure this site is not dedicated to copyright infringement. youtube-dl does not support such sites thus pull requests adding support for them will be rejected.
After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called yourextractor):


Fork this repository


Check out the source code with:
 git clone git@github.com:YOUR_GITHUB_USERNAME/youtube-dl.git



Start a new git branch with
 cd youtube-dl
 git checkout -b yourextractor



Start with this simple template and save it to youtube_dl/extractor/yourextractor.py:
# coding: utf-8
from __future__ import unicode_literals

from .common import InfoExtractor


class YourExtractorIE(InfoExtractor):
    _VALID_URL = r'https?://(?:www\.)?yourextractor\.com/watch/(?P<id>[0-9]+)'
    _TEST = {
        'url': 'https://yourextractor.com/watch/42',
        'md5': 'TODO: md5 sum of the first 10241 bytes of the video file (use --test)',
        'info_dict': {
            'id': '42',
            'ext': 'mp4',
            'title': 'Video title goes here',
            'thumbnail': r're:^https?://.*\.jpg$',
            # TODO more properties, either as:
            # * A value
            # * MD5 checksum; start the string with md5:
            # * A regular expression; start the string with re:
            # * Any Python type (for example int or float)
        }
    }

    def _real_extract(self, url):
        video_id = self._match_id(url)
        webpage = self._download_webpage(url, video_id)

        # TODO more code goes here, for example ...
        title = self._html_search_regex(r'<h1>(.+?)</h1>', webpage, 'title')

        return {
            'id': video_id,
            'title': title,
            'description': self._og_search_description(webpage),
            'uploader': self._search_regex(r'<div[^>]+id=""uploader""[^>]*>([^<]+)<', webpage, 'uploader', fatal=False),
            # TODO more properties (see youtube_dl/extractor/common.py)
        }


Add an import in youtube_dl/extractor/extractors.py.


Run python test/test_download.py TestDownload.test_YourExtractor. This should fail at first, but you can continually re-run it until you're done. If you decide to add more than one test, then rename _TEST to _TESTS and make it into a list of dictionaries. The tests will then be named TestDownload.test_YourExtractor, TestDownload.test_YourExtractor_1, TestDownload.test_YourExtractor_2, etc. Note that tests with only_matching key in test's dict are not counted in.


Have a look at youtube_dl/extractor/common.py for possible helper methods and a detailed description of what your extractor should and may return. Add tests and code for as many as you want.


Make sure your code follows youtube-dl coding conventions and check the code with flake8:
 $ flake8 youtube_dl/extractor/yourextractor.py



Make sure your code works under all Python versions claimed supported by youtube-dl, namely 2.6, 2.7, and 3.2+.


When the tests pass, add the new files and commit them and push the result, like this:
$ git add youtube_dl/extractor/extractors.py
$ git add youtube_dl/extractor/yourextractor.py
$ git commit -m '[yourextractor] Add new extractor'
$ git push origin yourextractor



Finally, create a pull request. We'll then review and merge it.


In any case, thank you very much for your contributions!
youtube-dl coding conventions
This section introduces a guide lines for writing idiomatic, robust and future-proof extractor code.
Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.
Mandatory and optional metafields
For extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an information dictionary or simply info dict. Only the following meta fields in the info dict are considered mandatory for a successful extraction process by youtube-dl:

id (media identifier)
title (media title)
url (media download URL) or formats

In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats id and title as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken.
Any field apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields.
Example
Say you have some source dictionary meta that you've fetched as JSON with HTTP request and it has a key summary:
meta = self._download_json(url, video_id)
Assume at this point meta's layout is:
{
    ...
    ""summary"": ""some fancy summary text"",
    ...
}
Assume you want to extract summary and put it into the resulting info dict as description. Since description is an optional meta field you should be ready that this key may be missing from the meta dict, so that you should extract it like:
description = meta.get('summary')  # correct
and not like:
description = meta['summary']  # incorrect
The latter will break extraction process with KeyError if summary disappears from meta at some later time but with the former approach extraction will just go ahead with description set to None which is perfectly fine (remember None is equivalent to the absence of data).
Similarly, you should pass fatal=False when extracting optional data from a webpage with _search_regex, _html_search_regex or similar methods, for instance:
description = self._search_regex(
    r'<span[^>]+id=""title""[^>]*>([^<]+)<',
    webpage, 'description', fatal=False)
With fatal set to False if _search_regex fails to extract description it will emit a warning and continue extraction.
You can also pass default=<some fallback value>, for example:
description = self._search_regex(
    r'<span[^>]+id=""title""[^>]*>([^<]+)<',
    webpage, 'description', default=None)
On failure this code will silently continue the extraction with description set to None. That is useful for metafields that may or may not be present.
Provide fallbacks
When extracting metadata try to do so from multiple sources. For example if title is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.
Example
Say meta from the previous example has a title and you are about to extract it. Since title is a mandatory meta field you should end up with something like:
title = meta['title']
If title disappears from meta in future due to some changes on the hoster's side the extraction would fail since title is mandatory. That's expected.
Assume that you have some another source you can extract title from, for example og:title HTML meta of a webpage. In this case you can provide a fallback scenario:
title = meta.get('title') or self._og_search_title(webpage)
This code will try to extract from meta first and if it fails it will try extracting og:title from a webpage.
Regular expressions
Don't capture groups you don't use
Capturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.
Example
Don't capture id attribute name here since you can't use it for anything anyway.
Correct:
r'(?:id|ID)=(?P<id>\d+)'
Incorrect:
r'(id|ID)=(?P<id>\d+)'
Make regular expressions relaxed and flexible
When using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.
Example
Say you need to extract title from the following HTML code:
<span style=""position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;"" class=""title"">some fancy title</span>
The code for that task should look similar to:
title = self._search_regex(
    r'<span[^>]+class=""title""[^>]*>([^<]+)', webpage, 'title')
Or even better:
title = self._search_regex(
    r'<span[^>]+class=([""\'])title\1[^>]*>(?P<title>[^<]+)',
    webpage, 'title', group='title')
Note how you tolerate potential changes in the style attribute's value or switch from using double quotes to single for class attribute:
The code definitely should not look like:
title = self._search_regex(
    r'<span style=""position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;"" class=""title"">(.*?)</span>',
    webpage, 'title', group='title')
Long lines policy
There is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.
For example, you should never split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:
Correct:
'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
Incorrect:
'https://www.youtube.com/watch?v=FqZTN594JQw&list='
'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
Inline values
Extracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.
Example
Correct:
title = self._html_search_regex(r'<title>([^<]+)</title>', webpage, 'title')
Incorrect:
TITLE_RE = r'<title>([^<]+)</title>'
# ...some lines of code...
title = self._html_search_regex(TITLE_RE, webpage, 'title')
Collapse fallbacks
Multiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.
Example
Good:
description = self._html_search_meta(
    ['og:description', 'description', 'twitter:description'],
    webpage, 'description', default=None)
Unwieldy:
description = (
    self._og_search_description(webpage, default=None)
    or self._html_search_meta('description', webpage, default=None)
    or self._html_search_meta('twitter:description', webpage, default=None))
Methods supporting list of patterns are: _search_regex, _html_search_regex, _og_search_property, _html_search_meta.
Trailing parentheses
Always move trailing parentheses after the last argument.
Example
Correct:
    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list)
Incorrect:
    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list,
)
Use convenience conversion and parsing functions
Wrap all extracted numeric data into safe functions from youtube_dl/utils.py: int_or_none, float_or_none. Use them for string to number conversions as well.
Use url_or_none for safe URL processing.
Use try_get for safe metadata extraction from parsed JSON.
Use unified_strdate for uniform upload_date or any YYYYMMDD meta field extraction, unified_timestamp for uniform timestamp extraction, parse_filesize for filesize extraction, parse_count for count meta fields extraction, parse_resolution, parse_duration for duration extraction, parse_age_limit for age_limit extraction.
Explore youtube_dl/utils.py for more useful convenience functions.
More examples
Safely extract optional description from parsed JSON
description = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)
Safely extract more optional metadata
video = try_get(response, lambda x: x['result']['video'][0], dict) or {}
description = video.get('summary')
duration = float_or_none(video.get('durationMs'), scale=1000)
view_count = int_or_none(video.get('views'))
EMBEDDING YOUTUBE-DL
youtube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to create a report.
From a Python program, you can embed youtube-dl in a more powerful fashion, like this:
from __future__ import unicode_literals
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
Most likely, you'll want to use various options. For a list of options available, have a look at youtube_dl/YoutubeDL.py. For a start, if you want to intercept youtube-dl's output, set a logger object.
Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:
from __future__ import unicode_literals
import youtube_dl


class MyLogger(object):
    def debug(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now converting ...')


ydl_opts = {
    'format': 'bestaudio/best',
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
BUGS
Bugs and suggestions should be reported at: https://github.com/ytdl-org/youtube-dl/issues. Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel #youtube-dl on freenode (webchat).
Please include the full output of youtube-dl when run with -v, i.e. add -v flag to your command line, copy the whole output and post it in the issue body wrapped in ``` for better formatting. It should look similar to this:
$ youtube-dl -v <your command line>
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...

Do not post screenshots of verbose logs; only plain text is acceptable.
The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.
Please re-read your issue once again to avoid a couple of common mistakes (you can and should use this as a checklist):
Is the description of the issue itself sufficient?
We often get issue reports that we cannot really decipher. While in most cases we eventually get the required information after asking back multiple times, this poses an unnecessary drain on our resources. Many contributors, including myself, are also not native speakers, so we may misread some parts.
So please elaborate on what feature you are requesting, or what bug you want to be fixed. Make sure that it's obvious

What the problem is
How it could be fixed
How your proposed solution would look like

If your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.
For bug reports, this means that your report should contain the complete output of youtube-dl when called with the -v flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.
If your server has multiple IPs or you suspect censorship, adding --call-home may be a good idea to get more diagnostics. If the error is ERROR: Unable to extract ... and you cannot reproduce it from multiple countries, add --dump-pages (warning: this will yield a rather large output, redirect it to the file log.txt by adding >log.txt 2>&1 to your command-line) or upload the .dump files you get when you add --write-pages somewhere.
Site support requests must contain an example URL. An example URL is a URL you might want to download, like https://www.youtube.com/watch?v=BaW_jenozKc. There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. https://www.youtube.com/) is not an example URL.
Are you using the latest version?
Before reporting any issue, type youtube-dl -U. This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.
Is the issue already documented?
Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the GitHub Issues of this repository. If there is an issue, feel free to write something along the lines of ""This affects me as well, with version 2015.01.01. Here is some more information on the issue: ..."". While some issues may be old, a new post into them often spurs rapid activity.
Why are existing options not enough?
Before requesting a new feature, please have a quick peek at the list of supported options. Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do not solve your problem.
Is there enough context in your bug report?
People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).
We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.
Does the issue involve one problem, and one problem only?
Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.
In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.
Is anyone going to need the feature?
Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.
Is your question about youtube-dl?
It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.
COPYRIGHT
youtube-dl is released into the public domain by the copyright holders.
This README file was originally written by Daniel Bolton and is likewise released into the public domain.
",GitHub - ytdl-org/youtube-dl: Command-line program to download videos from YouTube.com and other video sites
6,Python,"The Fuck     
The Fuck is a magnificent app, inspired by a @liamosaur
tweet,
that corrects errors in previous console commands.
Is The Fuck too slow? Try the experimental instant mode!

More examples:
‚ûú apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

‚ûú fuck
sudo apt-get install vim [enter/‚Üë/‚Üì/ctrl+c]
[sudo] password for nvbn:
Reading package lists... Done
...
‚ûú git push
fatal: The current branch master has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin master


‚ûú fuck
git push --set-upstream origin master [enter/‚Üë/‚Üì/ctrl+c]
Counting objects: 9, done.
...
‚ûú puthon
No command 'puthon' found, did you mean:
 Command 'python' from package 'python-minimal' (main)
 Command 'python' from package 'python3' (main)
zsh: command not found: puthon

‚ûú fuck
python [enter/‚Üë/‚Üì/ctrl+c]
Python 3.4.2 (default, Oct  8 2014, 13:08:17)
...
‚ûú git brnch
git: 'brnch' is not a git command. See 'git --help'.

Did you mean this?
    branch

‚ûú fuck
git branch [enter/‚Üë/‚Üì/ctrl+c]
* master
‚ûú lein rpl
'rpl' is not a task. See 'lein help'.

Did you mean this?
         repl

‚ûú fuck
lein repl [enter/‚Üë/‚Üì/ctrl+c]
nREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848
REPL-y 0.3.1
...
If you're not afraid of blindly running corrected commands, the
require_confirmation settings option can be disabled:
‚ûú apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

‚ûú fuck
sudo apt-get install vim
[sudo] password for nvbn:
Reading package lists... Done
...
Requirements

python (3.4+)
pip
python-dev

Installation
On OS X, you can install The Fuck via Homebrew (or via Linuxbrew on Linux):
brew install thefuck
On Ubuntu / Mint, install The Fuck with the following commands:
sudo apt update
sudo apt install python3-dev python3-pip python3-setuptools
sudo pip3 install thefuck
On FreeBSD, install The Fuck with the following commands:
pkg install thefuck
On ChromeOS, install The Fuck using chromebrew with the following command:
crew install thefuck
On other systems, install The Fuck  by using pip:
pip install thefuck
Alternatively, you may use an OS package manager (OS X, Ubuntu, Arch).
#
It is recommended that you place this command in your .bash_profile,
.bashrc, .zshrc or other startup script:
eval $(thefuck --alias)
# You can use whatever you want as an alias, like for Mondays:
eval $(thefuck --alias FUCK)
Or in your shell config (Bash, Zsh, Fish, Powershell, tcsh).
Changes are only available in a new shell session. To make changes immediately
available, run source ~/.bashrc (or your shell config file like .zshrc).
To run fixed commands without confirmation, use the --yeah option (or just -y for short, or --hard if you're especially frustrated):
fuck --yeah
To fix commands recursively until succeeding, use the -r option:
fuck -r
Updating
pip3 install thefuck --upgrade
Note: Alias functionality was changed in v1.34 of The Fuck
How it works
The Fuck attempts to match the previous command with a rule. If a match is
found, a new command is created using the matched rule and executed. The
following rules are enabled by default:

adb_unknown_command ‚Äì fixes misspelled commands like adb logcta;
ag_literal ‚Äì adds -Q to ag when suggested;
aws_cli ‚Äì fixes misspelled commands like aws dynamdb scan;
az_cli ‚Äì fixes misspelled commands like az providers;
cargo ‚Äì runs cargo build instead of cargo;
cargo_no_command ‚Äì fixes wrongs commands like cargo buid;
cat_dir ‚Äì replaces cat with ls when you try to cat a directory;
cd_correction ‚Äì spellchecks and correct failed cd commands;
cd_mkdir ‚Äì creates directories before cd'ing into them;
cd_parent ‚Äì changes cd.. to cd ..;
chmod_x ‚Äì add execution bit;
choco_install ‚Äì append common suffixes for chocolatey packages;
composer_not_command ‚Äì fixes composer command name;
cp_omitting_directory ‚Äì adds -a when you cp directory;
cpp11 ‚Äì adds missing -std=c++11 to g++ or clang++;
dirty_untar ‚Äì fixes tar x command that untarred in the current directory;
dirty_unzip ‚Äì fixes unzip command that unzipped in the current directory;
django_south_ghost ‚Äì adds --delete-ghost-migrations to failed because ghosts django south migration;
django_south_merge ‚Äì adds --merge to inconsistent django south migration;
docker_login ‚Äì executes a docker login and repeats the previous command;
docker_not_command ‚Äì fixes wrong docker commands like docker tags;
docker_image_being_used_by_container ‚Äê removes the container that is using the image before removing the image;
dry ‚Äì fixes repetitions like git git push;
fab_command_not_found ‚Äì fix misspelled fabric commands;
fix_alt_space ‚Äì replaces Alt+Space with Space character;
fix_file ‚Äì opens a file with an error in your $EDITOR;
gem_unknown_command ‚Äì fixes wrong gem commands;
git_add ‚Äì fixes ""pathspec 'foo' did not match any file(s) known to git."";
git_add_force ‚Äì adds --force to git add <pathspec>... when paths are .gitignore'd;
git_bisect_usage ‚Äì fixes git bisect strt, git bisect goood, git bisect rset, etc. when bisecting;
git_branch_delete ‚Äì changes git branch -d to git branch -D;
git_branch_delete_checked_out ‚Äì changes git branch -d to git checkout master && git branch -D when trying to delete a checked out branch;
git_branch_exists ‚Äì offers git branch -d foo, git branch -D foo or git checkout foo when creating a branch that already exists;
git_branch_list ‚Äì catches git branch list in place of git branch and removes created branch;
git_checkout ‚Äì fixes branch name or creates new branch;
git_commit_amend ‚Äì offers git commit --amend after previous commit;
git_commit_reset ‚Äì offers git reset HEAD~ after previous commit;
git_diff_no_index ‚Äì adds --no-index to previous git diff on untracked files;
git_diff_staged ‚Äì adds --staged to previous git diff with unexpected output;
git_fix_stash ‚Äì fixes git stash commands (misspelled subcommand and missing save);
git_flag_after_filename ‚Äì fixes fatal: bad flag '...' after filename
git_help_aliased ‚Äì fixes git help <alias> commands replacing  with the aliased command;
git_merge ‚Äì adds remote to branch names;
git_merge_unrelated ‚Äì adds --allow-unrelated-histories when required
git_not_command ‚Äì fixes wrong git commands like git brnch;
git_pull ‚Äì sets upstream before executing previous git pull;
git_pull_clone ‚Äì clones instead of pulling when the repo does not exist;
git_pull_uncommitted_changes ‚Äì stashes changes before pulling and pops them afterwards;
git_push ‚Äì adds --set-upstream origin $branch to previous failed git push;
git_push_different_branch_names ‚Äì fixes pushes when local brach name does not match remote branch name;
git_push_pull ‚Äì runs git pull when push was rejected;
git_push_without_commits ‚Äì Creates an initial commit if you forget and only git add ., when setting up a new project;
git_rebase_no_changes ‚Äì runs git rebase --skip instead of git rebase --continue when there are no changes;
git_remote_delete ‚Äì replaces git remote delete remote_name with git remote remove remote_name;
git_rm_local_modifications ‚Äì  adds -f or --cached when you try to rm a locally modified file;
git_rm_recursive ‚Äì adds -r when you try to rm a directory;
git_rm_staged ‚Äì  adds -f or --cached when you try to rm a file with staged changes
git_rebase_merge_dir ‚Äì offers git rebase (--continue | --abort | --skip) or removing the .git/rebase-merge dir when a rebase is in progress;
git_remote_seturl_add ‚Äì runs git remote add when git remote set_url on nonexistent remote;
git_stash ‚Äì stashes your local modifications before rebasing or switching branch;
git_stash_pop ‚Äì adds your local modifications before popping stash, then resets;
git_tag_force ‚Äì adds --force to git tag <tagname> when the tag already exists;
git_two_dashes ‚Äì adds a missing dash to commands like git commit -amend or git rebase -continue;
go_run ‚Äì appends .go extension when compiling/running Go programs;
go_unknown_command ‚Äì fixes wrong go commands, for example go bulid;
gradle_no_task ‚Äì fixes not found or ambiguous gradle task;
gradle_wrapper ‚Äì replaces gradle with ./gradlew;
grep_arguments_order ‚Äì fixes grep arguments order for situations like grep -lir . test;
grep_recursive ‚Äì adds -r when you try to grep directory;
grunt_task_not_found ‚Äì fixes misspelled grunt commands;
gulp_not_task ‚Äì fixes misspelled gulp tasks;
has_exists_script ‚Äì prepends ./ when script/binary exists;
heroku_multiple_apps ‚Äì add --app <app> to heroku commands like heroku pg;
heroku_not_command ‚Äì fixes wrong heroku commands like heroku log;
history ‚Äì tries to replace command with most similar command from history;
hostscli ‚Äì tries to fix hostscli usage;
ifconfig_device_not_found ‚Äì fixes wrong device names like wlan0 to wlp2s0;
java ‚Äì removes .java extension when running Java programs;
javac ‚Äì appends missing .java when compiling Java files;
lein_not_task ‚Äì fixes wrong lein tasks like lein rpl;
long_form_help ‚Äì changes -h to --help when the short form version is not supported
ln_no_hard_link ‚Äì catches hard link creation on directories, suggest symbolic link;
ln_s_order ‚Äì fixes ln -s arguments order;
ls_all ‚Äì adds -A to ls when output is empty;
ls_lah ‚Äì adds -lah to ls;
man ‚Äì changes manual section;
man_no_space ‚Äì fixes man commands without spaces, for example mandiff;
mercurial ‚Äì fixes wrong hg commands;
missing_space_before_subcommand ‚Äì fixes command with missing space like npminstall;
mkdir_p ‚Äì adds -p when you try to create a directory without parent;
mvn_no_command ‚Äì adds clean package to mvn;
mvn_unknown_lifecycle_phase ‚Äì fixes misspelled life cycle phases with mvn;
npm_missing_script ‚Äì fixes npm custom script name in npm run-script <script>;
npm_run_script ‚Äì adds missing run-script for custom npm scripts;
npm_wrong_command ‚Äì fixes wrong npm commands like npm urgrade;
no_command ‚Äì fixes wrong console commands, for example vom/vim;
no_such_file ‚Äì creates missing directories with mv and cp commands;
open ‚Äì either prepends http:// to address passed to open or create a new file or directory and passes it to open;
pip_install ‚Äì fixes permission issues with pip install commands by adding --user or prepending sudo if necessary;
pip_unknown_command ‚Äì fixes wrong pip commands, for example pip instatl/pip install;
php_s ‚Äì replaces -s by -S when trying to run a local php server;
port_already_in_use ‚Äì kills process that bound port;
prove_recursively ‚Äì adds -r when called with directory;
pyenv_no_such_command ‚Äì fixes wrong pyenv commands like pyenv isntall or pyenv list;
python_command ‚Äì prepends python when you try to run non-executable/without ./ python script;
python_execute ‚Äì appends missing .py when executing Python files;
quotation_marks ‚Äì fixes uneven usage of ' and "" when containing args';
path_from_history ‚Äì replaces not found path with similar absolute path from history;
react_native_command_unrecognized ‚Äì fixes unrecognized react-native commands;
remove_shell_prompt_literal ‚Äì remove leading shell prompt symbol $, common when copying commands from documentations;
remove_trailing_cedilla ‚Äì remove trailing cedillas √ß, a common typo for european keyboard layouts;
rm_dir ‚Äì adds -rf when you try to remove a directory;
scm_correction ‚Äì corrects wrong scm like hg log to git log;
sed_unterminated_s ‚Äì adds missing '/' to sed's s commands;
sl_ls ‚Äì changes sl to ls;
ssh_known_hosts ‚Äì removes host from known_hosts on warning;
sudo ‚Äì prepends sudo to previous command if it failed because of permissions;
sudo_command_from_user_path ‚Äì runs commands from users $PATH with sudo;
switch_lang ‚Äì switches command from your local layout to en;
systemctl ‚Äì correctly orders parameters of confusing systemctl;
terraform_init.py ‚Äì run terraform init before plan or apply;
test.py ‚Äì runs py.test instead of test.py;
touch ‚Äì creates missing directories before ""touching"";
tsuru_login ‚Äì runs tsuru login if not authenticated or session expired;
tsuru_not_command ‚Äì fixes wrong tsuru commands like tsuru shell;
tmux ‚Äì fixes tmux commands;
unknown_command ‚Äì fixes hadoop hdfs-style ""unknown command"", for example adds missing '-' to the command on hdfs dfs ls;
unsudo ‚Äì removes sudo from previous command if a process refuses to run on super user privilege.
vagrant_up ‚Äì starts up the vagrant instance;
whois ‚Äì fixes whois command;
workon_doesnt_exists ‚Äì fixes virtualenvwrapper env name os suggests to create new.
yarn_alias ‚Äì fixes aliased yarn commands like yarn ls;
yarn_command_not_found ‚Äì fixes misspelled yarn commands;
yarn_command_replaced ‚Äì fixes replaced yarn commands;
yarn_help ‚Äì makes it easier to open yarn documentation;

The following rules are enabled by default on specific platforms only:

apt_get ‚Äì installs app from apt if it not installed (requires python-commandnotfound / python3-commandnotfound);
apt_get_search ‚Äì changes trying to search using apt-get with searching using apt-cache;
apt_invalid_operation ‚Äì fixes invalid apt and apt-get calls, like apt-get isntall vim;
apt_list_upgradable ‚Äì helps you run apt list --upgradable after apt update;
apt_upgrade ‚Äì helps you run apt upgrade after apt list --upgradable;
brew_cask_dependency ‚Äì installs cask dependencies;
brew_install ‚Äì fixes formula name for brew install;
brew_reinstall ‚Äì turns brew install <formula> into brew reinstall <formula>;
brew_link ‚Äì adds --overwrite --dry-run if linking fails;
brew_uninstall ‚Äì adds --force to brew uninstall if multiple versions were installed;
brew_unknown_command ‚Äì fixes wrong brew commands, for example brew docto/brew doctor;
brew_update_formula ‚Äì turns brew update <formula> into brew upgrade <formula>;
dnf_no_such_command ‚Äì fixes mistyped DNF commands;
nixos_cmd_not_found ‚Äì installs apps on NixOS;
pacman ‚Äì installs app with pacman if it is not installed (uses yay or yaourt if available);
pacman_not_found ‚Äì fixes package name with pacman, yay or yaourt.
yum_invalid_operation ‚Äì fixes invalid yum calls, like yum isntall vim;

The following commands are bundled with The Fuck, but are not enabled by
default:

git_push_force ‚Äì adds --force-with-lease to a git push (may conflict with git_push_pull);
rm_root ‚Äì adds --no-preserve-root to rm -rf / command.

Creating your own rules
To add your own rule, create a file named your-rule-name.py
in ~/.config/thefuck/rules. The rule file must contain two functions:
match(command: Command) -> bool
get_new_command(command: Command) -> str | list[str]
Additionally, rules can contain optional functions:
side_effect(old_command: Command, fixed_command: str) -> None
Rules can also contain the optional variables enabled_by_default, requires_output and priority.
Command has three attributes: script, output and script_parts.
Your rule should not change Command.
Rules api changed in 3.0: To access a rule's settings, import it with
from thefuck.conf import settings
settings is a special object assembled from ~/.config/thefuck/settings.py,
and values from env (see more below).
A simple example rule for running a script with sudo:
def match(command):
    return ('permission denied' in command.output.lower()
            or 'EACCES' in command.output)


def get_new_command(command):
    return 'sudo {}'.format(command.script)

# Optional:
enabled_by_default = True

def side_effect(command, fixed_command):
    subprocess.call('chmod 777 .', shell=True)

priority = 1000  # Lower first, default is 1000

requires_output = True
More examples of rules,
utility functions for rules,
app/os-specific helpers.
Settings
Several The Fuck parameters can be changed in the file $XDG_CONFIG_HOME/thefuck/settings.py
($XDG_CONFIG_HOME defaults to ~/.config):

rules ‚Äì list of enabled rules, by default thefuck.const.DEFAULT_RULES;
exclude_rules ‚Äì list of disabled rules, by default [];
require_confirmation ‚Äì requires confirmation before running new command, by default True;
wait_command ‚Äì max amount of time in seconds for getting previous command output;
no_colors ‚Äì disable colored output;
priority ‚Äì dict with rules priorities, rule with lower priority will be matched first;
debug ‚Äì enables debug output, by default False;
history_limit ‚Äì numeric value of how many history commands will be scanned, like 2000;
alter_history ‚Äì push fixed command to history, by default True;
wait_slow_command ‚Äì max amount of time in seconds for getting previous command output if it in slow_commands list;
slow_commands ‚Äì list of slow commands;
num_close_matches ‚Äì maximum number of close matches to suggest, by default 3.

An example of settings.py:
rules = ['sudo', 'no_command']
exclude_rules = ['git_push']
require_confirmation = True
wait_command = 10
no_colors = False
priority = {'sudo': 100, 'no_command': 9999}
debug = False
history_limit = 9999
wait_slow_command = 20
slow_commands = ['react-native', 'gradle']
num_close_matches = 5
Or via environment variables:

THEFUCK_RULES ‚Äì list of enabled rules, like DEFAULT_RULES:rm_root or sudo:no_command;
THEFUCK_EXCLUDE_RULES ‚Äì list of disabled rules, like git_pull:git_push;
THEFUCK_REQUIRE_CONFIRMATION ‚Äì require confirmation before running new command, true/false;
THEFUCK_WAIT_COMMAND ‚Äì max amount of time in seconds for getting previous command output;
THEFUCK_NO_COLORS ‚Äì disable colored output, true/false;
THEFUCK_PRIORITY ‚Äì priority of the rules, like no_command=9999:apt_get=100,
rule with lower priority will be matched first;
THEFUCK_DEBUG ‚Äì enables debug output, true/false;
THEFUCK_HISTORY_LIMIT ‚Äì how many history commands will be scanned, like 2000;
THEFUCK_ALTER_HISTORY ‚Äì push fixed command to history true/false;
THEFUCK_WAIT_SLOW_COMMAND ‚Äì max amount of time in seconds for getting previous command output if it in slow_commands list;
THEFUCK_SLOW_COMMANDS ‚Äì list of slow commands, like lein:gradle;
THEFUCK_NUM_CLOSE_MATCHES ‚Äì maximum number of close matches to suggest, like 5.

For example:
export THEFUCK_RULES='sudo:no_command'
export THEFUCK_EXCLUDE_RULES='git_pull:git_push'
export THEFUCK_REQUIRE_CONFIRMATION='true'
export THEFUCK_WAIT_COMMAND=10
export THEFUCK_NO_COLORS='false'
export THEFUCK_PRIORITY='no_command=9999:apt_get=100'
export THEFUCK_HISTORY_LIMIT='2000'
export THEFUCK_NUM_CLOSE_MATCHES='5'
Third-party packages with rules
If you'd like to make a specific set of non-public rules, but would still like
to share them with others, create a package named thefuck_contrib_* with
the following structure:
thefuck_contrib_foo
  thefuck_contrib_foo
    rules
      __init__.py
      *third-party rules*
    __init__.py
    *third-party-utils*
  setup.py

The Fuck will find rules located in the rules module.
Experimental instant mode
The default behavior of The Fuck requires time to re-run previous commands.
When in instant mode, The Fuck saves time by logging output with script,
then reading the log.

Currently, instant mode only supports Python 3 with bash or zsh. zsh's autocorrect function also needs to be disabled in order for thefuck to work properly.
To enable instant mode, add --enable-experimental-instant-mode
to the alias initialization in .bashrc, .bash_profile or .zshrc.
For example:
eval $(thefuck --alias --enable-experimental-instant-mode)
Developing
See CONTRIBUTING.md
License MIT
Project License can be found here.
",GitHub - nvbn/thefuck: Magnificent app which corrects your previous console command.
7,Python,"Flask
Flask is a lightweight WSGI web application framework. It is designed
to make getting started quick and easy, with the ability to scale up to
complex applications. It began as a simple wrapper around Werkzeug
and Jinja and has become one of the most popular Python web
application frameworks.
Flask offers suggestions, but doesn't enforce any dependencies or
project layout. It is up to the developer to choose the tools and
libraries they want to use. There are many extensions provided by the
community that make adding new functionality easy.

Installing
Install and update using pip:
pip install -U Flask


A Simple Example
from flask import Flask

app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello, World!""
$ env FLASK_APP=hello.py flask run
 * Serving Flask app ""hello""
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)


Contributing
For guidance on setting up a development environment and how to make a
contribution to Flask, see the contributing guidelines.

Donate
The Pallets organization develops and supports Flask and the libraries
it uses. In order to grow the community of contributors and users, and
allow the maintainers to devote more time to the projects, please
donate today.

Links

Website: https://palletsprojects.com/p/flask/
Documentation: https://flask.palletsprojects.com/
Releases: https://pypi.org/project/Flask/
Code: https://github.com/pallets/flask
Issue tracker: https://github.com/pallets/flask/issues
Test status: https://dev.azure.com/pallets/flask/_build
Official chat: https://discord.gg/t6rrQZH

",GitHub - pallets/flask: The Python micro framework for building web applications.
8,Python,"Keras: Deep Learning for humans



You have just found Keras.
Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.
Use Keras if you need a deep learning library that:

Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).
Supports both convolutional networks and recurrent networks, as well as combinations of the two.
Runs seamlessly on CPU and GPU.

Read the documentation at Keras.io.
Keras is compatible with: Python 2.7-3.6.

Multi-backend Keras and tf.keras:
At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0. tf.keras is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).
Keras 2.2.5 was the last release of Keras implementing the 2.2.* API. It was the last release to only support TensorFlow 1 (as well as Theano and CNTK).
The current release is Keras 2.3.0, which makes significant API changes and add support for TensorFlow 2.0. The 2.3.0 release will be the last major release of multi-backend Keras. Multi-backend Keras is superseded by tf.keras.
Bugs present in multi-backend Keras will only be fixed until April 2020 (as part of minor releases).
For more information about the future of Keras, see the Keras meeting notes.

Guiding principles


User friendliness. Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.


Modularity. A model is understood as a sequence or a graph of standalone, fully configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions and regularization schemes are all standalone modules that you can combine to create new models.


Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.


Work with Python. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.



Getting started: 30 seconds to Keras
The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers.
Here is the Sequential model:
from keras.models import Sequential

model = Sequential()
Stacking layers is as easy as .add():
from keras.layers import Dense

model.add(Dense(units=64, activation='relu', input_dim=100))
model.add(Dense(units=10, activation='softmax'))
Once your model looks good, configure its learning process with .compile():
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))
You can now iterate on your training data in batches:
# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.
model.fit(x_train, y_train, epochs=5, batch_size=32)
Alternatively, you can feed batches to your model manually:
model.train_on_batch(x_batch, y_batch)
Evaluate your performance in one line:
loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
Or generate predictions on new data:
classes = model.predict(x_test, batch_size=128)
Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?
For a more in-depth tutorial about Keras, you can check out:

Getting started with the Sequential model
Getting started with the functional API

In the examples folder of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.

Installation
Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.

TensorFlow installation instructions.
Theano installation instructions.
CNTK installation instructions.

You may also consider installing the following optional dependencies:

cuDNN (recommended if you plan on running Keras on GPU).
HDF5 and h5py (required if you plan on saving Keras models to disk).
graphviz and pydot (used by visualization utilities to plot model graphs).

Then, you can install Keras itself. There are two ways to install Keras:

Install Keras from PyPI (recommended):

Note: These installation steps assume that you are on a Linux or Mac environment.
If you are on Windows, you will need to remove sudo to run the commands below.
sudo pip install keras
If you are using a virtualenv, you may want to avoid using sudo:
pip install keras

Alternatively: install Keras from the GitHub source:

First, clone Keras using git:
git clone https://github.com/keras-team/keras.git
Then, cd to the Keras folder and run the install command:
cd keras
sudo python setup.py install

Configuring your Keras backend
By default, Keras will use TensorFlow as its tensor manipulation library. Follow these instructions to configure the Keras backend.

Support
You can ask questions and join the development discussion:

On the Keras Google group.
On the Keras Slack channel. Use this link to request an invitation to the channel.

You can also post bug reports and feature requests (only) in GitHub issues. Make sure to read our guidelines first.

Why this name, Keras?
Keras (Œ∫Œ≠œÅŒ±œÇ) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words Œ∫Œ≠œÅŒ±œÇ (horn) / Œ∫œÅŒ±ŒØŒΩœâ (fulfill), and ·ºêŒªŒ≠œÜŒ±œÇ (ivory) / ·ºêŒªŒµœÜŒ±ŒØœÅŒøŒºŒ±Œπ (deceive).
Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).

""Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them."" Homer, Odyssey 19. 562 ff (Shewring translation).


",GitHub - keras-team/keras: Deep Learning for humans
9,Python,"Django
Django is a high-level Python Web framework that encourages rapid development
and clean, pragmatic design. Thanks for checking it out.
All documentation is in the ""docs"" directory and online at
https://docs.djangoproject.com/en/stable/. If you're just getting started,
here's how we recommend you read the docs:

First, read docs/intro/install.txt for instructions on installing Django.
Next, work through the tutorials in order (docs/intro/tutorial01.txt,
docs/intro/tutorial02.txt, etc.).
If you want to set up an actual deployment server, read
docs/howto/deployment/index.txt for instructions.
You'll probably want to read through the topical guides (in docs/topics)
next; from there you can jump to the HOWTOs (in docs/howto) for specific
problems, and check out the reference (docs/ref) for gory details.
See docs/README for instructions on building an HTML version of the docs.

Docs are updated rigorously. If you find any problems in the docs, or think
they should be clarified in any way, please take 30 seconds to fill out a
ticket here: https://code.djangoproject.com/newticket
To get more help:

Join the #django channel on irc.freenode.net. Lots of helpful people hang
out there. See https://en.wikipedia.org/wiki/Wikipedia:IRC/Tutorial if you're
new to IRC.
Join the django-users mailing list, or read the archives, at
https://groups.google.com/group/django-users.

To contribute to Django:

Check out https://docs.djangoproject.com/en/dev/internals/contributing/ for
information about getting involved.

To run Django's test suite:

Follow the instructions in the ""Unit tests"" section of
docs/internals/contributing/writing-code/unit-tests.txt, published online at
https://docs.djangoproject.com/en/dev/internals/contributing/writing-code/unit-tests/#running-the-unit-tests

",GitHub - django/django: The Web framework for perfectionists with deadlines.
10,Python,"HTTPie: a CLI, cURL-like tool for humans
HTTPie (pronounced aitch-tee-tee-pie) is a command line HTTP client.
Its goal is to make CLI interaction with web services as human-friendly
as possible. It provides a simple http command that allows for sending
arbitrary HTTP requests using a simple and natural syntax, and displays
colorized output. HTTPie can be used for testing, debugging, and
generally interacting with HTTP servers.
 
   


Contents

1¬†¬†¬†Main features
2¬†¬†¬†Installation
2.1¬†¬†¬†macOS
2.2¬†¬†¬†Linux
2.3¬†¬†¬†Windows, etc.
2.4¬†¬†¬†Python version
2.5¬†¬†¬†Unstable version


3¬†¬†¬†Usage
3.1¬†¬†¬†Examples


4¬†¬†¬†HTTP method
5¬†¬†¬†Request URL
5.1¬†¬†¬†Querystring parameters
5.2¬†¬†¬†URL shortcuts for localhost
5.3¬†¬†¬†Other default schemes


6¬†¬†¬†Request items
6.1¬†¬†¬†Escaping rules


7¬†¬†¬†JSON
7.1¬†¬†¬†Default behaviour
7.2¬†¬†¬†Explicit JSON
7.3¬†¬†¬†Non-string JSON fields


8¬†¬†¬†Forms
8.1¬†¬†¬†Regular forms
8.2¬†¬†¬†File upload forms


9¬†¬†¬†HTTP headers
9.1¬†¬†¬†Default request headers
9.2¬†¬†¬†Empty headers and header un-setting
9.3¬†¬†¬†Limiting response headers


10¬†¬†¬†Cookies
11¬†¬†¬†Authentication
11.1¬†¬†¬†Basic auth
11.2¬†¬†¬†Digest auth
11.3¬†¬†¬†Password prompt
11.4¬†¬†¬†.netrc
11.5¬†¬†¬†Auth plugins


12¬†¬†¬†HTTP redirects
12.1¬†¬†¬†Follow Location
12.2¬†¬†¬†Showing intermediary redirect responses
12.3¬†¬†¬†Limiting maximum redirects followed


13¬†¬†¬†Proxies
13.1¬†¬†¬†Environment variables
13.2¬†¬†¬†SOCKS


14¬†¬†¬†HTTPS
14.1¬†¬†¬†Server SSL certificate verification
14.2¬†¬†¬†Custom CA bundle
14.3¬†¬†¬†Client side SSL certificate
14.4¬†¬†¬†SSL version


15¬†¬†¬†Output options
15.1¬†¬†¬†What parts of the HTTP exchange should be printed
15.2¬†¬†¬†Viewing intermediary requests/responses
15.3¬†¬†¬†Conditional body download


16¬†¬†¬†Redirected Input
16.1¬†¬†¬†Request data from a filename


17¬†¬†¬†Terminal output
17.1¬†¬†¬†Colors and formatting
17.2¬†¬†¬†Binary data


18¬†¬†¬†Redirected output
19¬†¬†¬†Download mode
19.1¬†¬†¬†Downloaded filename
19.2¬†¬†¬†Piping while downloading
19.3¬†¬†¬†Resuming downloads
19.4¬†¬†¬†Other notes


20¬†¬†¬†Streamed responses
20.1¬†¬†¬†Disabling buffering
20.2¬†¬†¬†Examples use cases


21¬†¬†¬†Sessions
21.1¬†¬†¬†Named sessions
21.2¬†¬†¬†Anonymous sessions
21.3¬†¬†¬†Readonly session


22¬†¬†¬†Config
22.1¬†¬†¬†Config file directory
22.2¬†¬†¬†Configurable options
22.2.1¬†¬†¬†default_options


22.3¬†¬†¬†Un-setting previously specified options


23¬†¬†¬†Scripting
23.1¬†¬†¬†Best practices


24¬†¬†¬†Meta
24.1¬†¬†¬†Interface design
24.2¬†¬†¬†User support
24.3¬†¬†¬†Related projects
24.3.1¬†¬†¬†Dependencies
24.3.2¬†¬†¬†HTTPie friends
24.3.3¬†¬†¬†Alternatives


24.4¬†¬†¬†Contributing
24.5¬†¬†¬†Change log
24.6¬†¬†¬†Artwork
24.7¬†¬†¬†Licence
24.8¬†¬†¬†Authors





1¬†¬†¬†Main features

Expressive and intuitive syntax
Formatted and colorized terminal output
Built-in JSON support
Forms and file uploads
HTTPS, proxies, and authentication
Arbitrary request data
Custom headers
Persistent sessions
Wget-like downloads
Linux, macOS and Windows support
Plugins
Documentation
Test coverage



2¬†¬†¬†Installation

2.1¬†¬†¬†macOS
On macOS, HTTPie can be installed via Homebrew
(recommended):
$ brew install httpie
A MacPorts port is also available:
$ port install httpie

2.2¬†¬†¬†Linux
Most Linux distributions provide a package that can be installed using the
system package manager, for example:
# Debian, Ubuntu, etc.
$ apt-get install httpie
# Fedora
$ dnf install httpie
# CentOS, RHEL, ...
$ yum install httpie
# Arch Linux
$ pacman -S httpie

2.3¬†¬†¬†Windows, etc.
A universal installation method (that works on Windows, Mac OS X, Linux, ‚Ä¶,
and always provides the latest version) is to use pip:
# Make sure we have an up-to-date version of pip and setuptools:
$ pip install --upgrade pip setuptools

$ pip install --upgrade httpie
(If pip installation fails for some reason, you can try
easy_install httpie as a fallback.)

2.4¬†¬†¬†Python version
Starting with version 2.0.0 (currently under development) Python 3.6+ is required.

2.5¬†¬†¬†Unstable version
You can also install the latest unreleased development version directly from
the master branch on GitHub.  It is a work-in-progress of a future stable
release so the experience might be not as smooth.


On macOS you can install it with Homebrew:
$ brew install httpie --HEAD
Otherwise with pip:
$ pip install --upgrade https://github.com/jakubroztocil/httpie/archive/master.tar.gz
Verify that now we have the
current development version identifier
with the -dev suffix, for example:
$ http --version
1.0.0-dev

3¬†¬†¬†Usage
Hello World:
$ http httpie.org
Synopsis:
$ http [flags] [METHOD] URL [ITEM [ITEM]]
See also http --help.

3.1¬†¬†¬†Examples
Custom HTTP method, HTTP headers and JSON data:
$ http PUT example.org X-API-Token:123 name=John
Submitting forms:
$ http -f POST example.org hello=World
See the request that is being sent using one of the output options:
$ http -v example.org
Use Github API to post a comment on an
issue
with authentication:
$ http -a USERNAME POST https://api.github.com/repos/jakubroztocil/httpie/issues/83/comments body='HTTPie is awesome! :heart:'
Upload a file using redirected input:
$ http example.org < file.json
Download a file and save it via redirected output:
$ http example.org/file > file
Download a file wget style:
$ http --download example.org/file
Use named sessions to make certain aspects or the communication persistent
between requests to the same host:
$ http --session=logged-in -a username:password httpbin.org/get API-Key:123

$ http --session=logged-in httpbin.org/headers
Set a custom Host header to work around missing DNS records:
$ http localhost:8000 Host:example.com

4¬†¬†¬†HTTP method
The name of the HTTP method comes right before the URL argument:
$ http DELETE example.org/todos/7
Which looks similar to the actual Request-Line that is sent:
DELETE /todos/7 HTTP/1.1
When the METHOD argument is omitted from the command, HTTPie defaults to
either GET (with no request data) or POST (with request data).

5¬†¬†¬†Request URL
The only information HTTPie needs to perform a request is a URL.
The default scheme is, somewhat unsurprisingly, http://,
and can be omitted from the argument ‚Äì http example.org works just fine.

5.1¬†¬†¬†Querystring parameters
If you find yourself manually constructing URLs with querystring parameters
on the terminal, you may appreciate the param==value syntax for appending
URL parameters.
With that, you don't have to worry about escaping the &
separators for your shell. Additionally, any special characters in the
parameter name or value get automatically URL-escaped
(as opposed to parameters specified in the full URL, which HTTPie doesn‚Äôt
modify).
$ http https://api.github.com/search/repositories q==httpie per_page==1
GET /search/repositories?q=httpie&per_page=1 HTTP/1.1

5.2¬†¬†¬†URL shortcuts for localhost
Additionally, curl-like shorthand for localhost is supported.
This means that, for example :3000 would expand to http://localhost:3000
If the port is omitted, then port 80 is assumed.
$ http :/foo
GET /foo HTTP/1.1
Host: localhost
$ http :3000/bar
GET /bar HTTP/1.1
Host: localhost:3000
$ http :
GET / HTTP/1.1
Host: localhost

5.3¬†¬†¬†Other default schemes
When HTTPie is invoked as https then the default scheme is https://
($ https example.org will make a request to https://example.org).
You can also use the --default-scheme <URL_SCHEME> option to create
shortcuts for other protocols than HTTP (possibly supported via plugins).
Example for the httpie-unixsocket plugin:
# Before
$ http http+unix://%2Fvar%2Frun%2Fdocker.sock/info
# Create an alias
$ alias http-unix='http --default-scheme=""http+unix""'
# Now the scheme can be omitted
$ http-unix %2Fvar%2Frun%2Fdocker.sock/info

6¬†¬†¬†Request items
There are a few different request item types that provide a
convenient mechanism for specifying HTTP headers, simple JSON and
form data, files, and URL parameters.
They are key/value pairs specified after the URL. All have in
common that they become part of the actual request that is sent and that
their type is distinguished only by the separator used:
:, =, :=, ==, @, =@, and :=@. The ones with an
@ expect a file path as value.


Item Type
Description



HTTP Headers
Name:Value
Arbitrary HTTP header, e.g. X-API-Token:123.

URL parameters
name==value
Appends the given name/value pair as a query
string parameter to the URL.
The == separator is used.

Data Fields
field=value,
field=@file.txt
Request data fields to be serialized as a JSON
object (default), or to be form-encoded
(--form, -f).

Raw JSON fields
field:=json,
field:=@file.json
Useful when sending JSON and one or
more fields need to be a Boolean, Number,
nested Object, or an Array,  e.g.,
meals:='[""ham"",""spam""]' or pies:=[1,2,3]
(note the quotes).

Form File Fields
field@/dir/file
Only available with --form, -f.
For example screenshot@~/Pictures/img.png.
The presence of a file field results
in a multipart/form-data request.



Note that data fields aren't the only way to specify request data:
Redirected input is a mechanism for passing arbitrary request data.

6.1¬†¬†¬†Escaping rules
You can use \ to escape characters that shouldn't be used as separators
(or parts thereof). For instance, foo\==bar will become a data key/value
pair (foo= and bar) instead of a URL parameter.
Often it is necessary to quote the values, e.g. foo='bar baz'.
If any of the field names or headers starts with a minus
(e.g., -fieldname), you need to place all such items after the special
token -- to prevent confusion with --arguments:
$ http httpbin.org/post  --  -name-starting-with-dash=foo -Unusual-Header:bar
POST /post HTTP/1.1
-Unusual-Header: bar
Content-Type: application/json

{
    ""-name-starting-with-dash"": ""foo""
}

7¬†¬†¬†JSON
JSON is the lingua franca of modern web services and it is also the
implicit content type HTTPie uses by default.
Simple example:
$ http PUT example.org name=John email=john@example.org
PUT / HTTP/1.1
Accept: application/json, */*
Accept-Encoding: gzip, deflate
Content-Type: application/json
Host: example.org

{
    ""name"": ""John"",
    ""email"": ""john@example.org""
}

7.1¬†¬†¬†Default behaviour
If your command includes some data request items, they are serialized as a JSON
object by default. HTTPie also automatically sets the following headers,
both of which can be overwritten:


Content-Type
application/json

Accept
application/json, */*




7.2¬†¬†¬†Explicit JSON
You can use --json, -j to explicitly set Accept
to application/json regardless of whether you are sending data
(it's a shortcut for setting the header via the usual header notation:
http url Accept:'application/json, */*'). Additionally,
HTTPie will try to detect JSON responses even when the
Content-Type is incorrectly text/plain or unknown.

7.3¬†¬†¬†Non-string JSON fields
Non-string fields use the := separator, which allows you to embed raw JSON
into the resulting object. Text and raw JSON files can also be embedded into
fields using =@ and :=@:
$ http PUT api.example.com/person/1 \
    name=John \
    age:=29 married:=false hobbies:='[""http"", ""pies""]' \  # Raw JSON
    description=@about-john.txt \   # Embed text file
    bookmarks:=@bookmarks.json      # Embed JSON file
PUT /person/1 HTTP/1.1
Accept: application/json, */*
Content-Type: application/json
Host: api.example.com

{
    ""age"": 29,
    ""hobbies"": [
        ""http"",
        ""pies""
    ],
    ""description"": ""John is a nice guy who likes pies."",
    ""married"": false,
    ""name"": ""John"",
    ""bookmarks"": {
        ""HTTPie"": ""https://httpie.org"",
    }
}
Please note that with this syntax the command gets unwieldy when sending
complex data. In that case it's always better to use redirected input:
$ http POST api.example.com/person/1 < person.json

8¬†¬†¬†Forms
Submitting forms is very similar to sending JSON requests. Often the only
difference is in adding the --form, -f option, which ensures that
data fields are serialized as, and Content-Type is set to,
application/x-www-form-urlencoded; charset=utf-8. It is possible to make
form data the implicit content type instead of JSON
via the config file.

8.1¬†¬†¬†Regular forms
$ http --form POST api.example.org/person/1 name='John Smith'
POST /person/1 HTTP/1.1
Content-Type: application/x-www-form-urlencoded; charset=utf-8

name=John+Smith

8.2¬†¬†¬†File upload forms
If one or more file fields is present, the serialization and content type is
multipart/form-data:
$ http -f POST example.com/jobs name='John Smith' cv@~/Documents/cv.pdf
The request above is the same as if the following HTML form were
submitted:
<form enctype=""multipart/form-data"" method=""post"" action=""http://example.com/jobs"">
    <input type=""text"" name=""name"" />
    <input type=""file"" name=""cv"" />
</form>
Note that @ is used to simulate a file upload form field, whereas
=@ just embeds the file content as a regular text field value.

9¬†¬†¬†HTTP headers
To set custom headers you can use the Header:Value notation:
$ http example.org  User-Agent:Bacon/1.0  'Cookie:valued-visitor=yes;foo=bar'  \
    X-Foo:Bar  Referer:https://httpie.org/
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Cookie: valued-visitor=yes;foo=bar
Host: example.org
Referer: https://httpie.org/
User-Agent: Bacon/1.0
X-Foo: Bar

9.1¬†¬†¬†Default request headers
There are a couple of default headers that HTTPie sets:
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
User-Agent: HTTPie/<version>
Host: <taken-from-URL>
Any of these except Host can be overwritten and some of them unset.

9.2¬†¬†¬†Empty headers and header un-setting
To unset a previously specified header
(such a one of the default headers), use Header::
$ http httpbin.org/headers Accept: User-Agent:
To send a header with an empty value, use Header;:
$ http httpbin.org/headers 'Header;'

9.3¬†¬†¬†Limiting response headers
The --max-headers=n options allows you to control the number of headers
HTTPie reads before giving up (the default 0, i.e., there‚Äôs no limit).
$ http --max-headers=100 httpbin.org/get

10¬†¬†¬†Cookies
HTTP clients send cookies to the server as regular HTTP headers. That means,
HTTPie does not offer any special syntax for specifying cookies ‚Äî the usual
Header:Value notation is used:
Send a single cookie:
$ http example.org Cookie:sessionid=foo
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Connection: keep-alive
Cookie: sessionid=foo
Host: example.org
User-Agent: HTTPie/0.9.9
Send multiple cookies
(note the header is quoted to prevent the shell from interpreting the ;):
$ http example.org 'Cookie:sessionid=foo;another-cookie=bar'
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Connection: keep-alive
Cookie: sessionid=foo;another-cookie=bar
Host: example.org
User-Agent: HTTPie/0.9.9
If you often deal with cookies in your requests, then chances are you'd appreciate
the sessions feature.

11¬†¬†¬†Authentication
The currently supported authentication schemes are Basic and Digest
(see auth plugins for more). There are two flags that control authentication:


--auth, -a
Pass a username:password pair as
the argument. Or, if you only specify a username
(-a username), you'll be prompted for
the password before the request is sent.
To send an empty password, pass username:.
The username:password@hostname URL syntax is
supported as well (but credentials passed via -a
have higher priority).

--auth-type, -A
Specify the auth mechanism. Possible values are
basic and digest. The default value is
basic so it can often be omitted.




11.1¬†¬†¬†Basic auth
$ http -a username:password example.org

11.2¬†¬†¬†Digest auth
$ http -A digest -a username:password example.org

11.3¬†¬†¬†Password prompt
$ http -a username example.org

11.4¬†¬†¬†.netrc
Authentication information from your ~/.netrc
file is by default honored as well.
For example:
$ cat ~/.netrc
machine httpbin.org
login httpie
password test
$ http httpbin.org/basic-auth/httpie/test
HTTP/1.1 200 OK
[...]
This can be disabled with the --ignore-netrc option:
$ http --ignore-netrc httpbin.org/basic-auth/httpie/test
HTTP/1.1 401 UNAUTHORIZED
[...]

11.5¬†¬†¬†Auth plugins
Additional authentication mechanism can be installed as plugins.
They can be found on the Python Package Index.
Here's a few picks:

httpie-api-auth: ApiAuth
httpie-aws-auth: AWS / Amazon S3
httpie-edgegrid: EdgeGrid
httpie-hmac-auth: HMAC
httpie-jwt-auth: JWTAuth (JSON Web Tokens)
httpie-negotiate: SPNEGO (GSS Negotiate)
httpie-ntlm: NTLM (NT LAN Manager)
httpie-oauth: OAuth
requests-hawk: Hawk


12¬†¬†¬†HTTP redirects
By default, HTTP redirects are not followed and only the first
response is shown:
$ http httpbin.org/redirect/3

12.1¬†¬†¬†Follow Location
To instruct HTTPie to follow the Location header of 30x responses
and show the final response instead, use the --follow, -F option:
$ http --follow httpbin.org/redirect/3

12.2¬†¬†¬†Showing intermediary redirect responses
If you additionally wish to see the intermediary requests/responses,
then use the --all option as well:
$ http --follow --all httpbin.org/redirect/3

12.3¬†¬†¬†Limiting maximum redirects followed
To change the default limit of maximum 30 redirects, use the
--max-redirects=<limit> option:
$ http --follow --all --max-redirects=5 httpbin.org/redirect/3

13¬†¬†¬†Proxies
You can specify proxies to be used through the --proxy argument for each
protocol (which is included in the value in case of redirects across protocols):
$ http --proxy=http:http://10.10.1.10:3128 --proxy=https:https://10.10.1.10:1080 example.org
With Basic authentication:
$ http --proxy=http:http://user:pass@10.10.1.10:3128 example.org

13.1¬†¬†¬†Environment variables
You can also configure proxies by environment variables ALL_PROXY,
HTTP_PROXY and HTTPS_PROXY, and the underlying Requests library will
pick them up as well. If you want to disable proxies configured through
the environment variables for certain hosts, you can specify them in NO_PROXY.
In your ~/.bash_profile:
export HTTP_PROXY=http://10.10.1.10:3128
export HTTPS_PROXY=https://10.10.1.10:1080
export NO_PROXY=localhost,example.com

13.2¬†¬†¬†SOCKS
Homebrew-installed HTTPie comes with SOCKS proxy support out of the box.
To enable SOCKS proxy support for non-Homebrew  installations, you'll
might need to install requests[socks] manually using pip:
$ pip install -U requests[socks]
Usage is the same as for other types of proxies:
$ http --proxy=http:socks5://user:pass@host:port --proxy=https:socks5://user:pass@host:port example.org

14¬†¬†¬†HTTPS

14.1¬†¬†¬†Server SSL certificate verification
To skip the host's SSL certificate verification, you can pass --verify=no
(default is yes):
$ http --verify=no https://example.org

14.2¬†¬†¬†Custom CA bundle
You can also use --verify=<CA_BUNDLE_PATH> to set a custom CA bundle path:
$ http --verify=/ssl/custom_ca_bundle https://example.org

14.3¬†¬†¬†Client side SSL certificate
To use a client side certificate for the SSL communication, you can pass
the path of the cert file with --cert:
$ http --cert=client.pem https://example.org
If the private key is not contained in the cert file you may pass the
path of the key file with --cert-key:
$ http --cert=client.crt --cert-key=client.key https://example.org

14.4¬†¬†¬†SSL version
Use the --ssl=<PROTOCOL> to specify the desired protocol version to use.
This will default to SSL v2.3 which will negotiate the highest protocol that both
the server and your installation of OpenSSL support. The available protocols
are ssl2.3, ssl3, tls1, tls1.1, tls1.2, tls1.3. (The actually
available set of protocols may vary depending on your OpenSSL installation.)
# Specify the vulnerable SSL v3 protocol to talk to an outdated server:
$ http --ssl=ssl3 https://vulnerable.example.org

15¬†¬†¬†Output options
By default, HTTPie only outputs the final response and the whole response
message is printed (headers as well as the body). You can control what should
be printed via several options:


--headers, -h
Only the response headers are printed.

--body, -b
Only the response body is printed.

--verbose, -v
Print the whole HTTP exchange (request and response).
This option also enables --all (see below).

--print, -p
Selects parts of the HTTP exchange.



--verbose can often be useful for debugging the request and generating
documentation examples:
$ http --verbose PUT httpbin.org/put hello=world
PUT /put HTTP/1.1
Accept: application/json, */*
Accept-Encoding: gzip, deflate
Content-Type: application/json
Host: httpbin.org
User-Agent: HTTPie/0.2.7dev

{
    ""hello"": ""world""
}


HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 477
Content-Type: application/json
Date: Sun, 05 Aug 2012 00:25:23 GMT
Server: gunicorn/0.13.4

{
    [‚Ä¶]
}

15.1¬†¬†¬†What parts of the HTTP exchange should be printed
All the other output options are under the hood just shortcuts for
the more powerful --print, -p. It accepts a string of characters each
of which represents a specific part of the HTTP exchange:


Character
Stands for



H
request headers

B
request body

h
response headers

b
response body



Print request and response headers:
$ http --print=Hh PUT httpbin.org/put hello=world

15.2¬†¬†¬†Viewing intermediary requests/responses
To see all the HTTP communication, i.e. the final request/response as
well as any possible  intermediary requests/responses, use the --all
option. The intermediary HTTP communication include followed redirects
(with --follow), the first unauthorized request when HTTP digest
authentication is used (--auth=digest), etc.
# Include all responses that lead to the final one:
$ http --all --follow httpbin.org/redirect/3
The intermediary requests/response are by default formatted according to
--print, -p (and its shortcuts described above). If you'd like to change
that, use the --history-print, -P option. It takes the same
arguments as --print, -p but applies to the intermediary requests only.
# Print the intermediary requests/responses differently than the final one:
$ http -A digest -a foo:bar --all -p Hh -P H httpbin.org/digest-auth/auth/foo/bar

15.3¬†¬†¬†Conditional body download
As an optimization, the response body is downloaded from the server
only if it's part of the output. This is similar to performing a HEAD
request, except that it applies to any HTTP method you use.
Let's say that there is an API that returns the whole resource when it is
updated, but you are only interested in the response headers to see the
status code after an update:
$ http --headers PATCH example.org/Really-Huge-Resource name='New Name'
Since we are only printing the HTTP headers here, the connection to the server
is closed as soon as all the response headers have been received.
Therefore, bandwidth and time isn't wasted downloading the body
which you don't care about. The response headers are downloaded always,
even if they are not part of the output

16¬†¬†¬†Redirected Input
The universal method for passing request data is through redirected stdin
(standard input)‚Äîpiping. Such data is buffered and then with no further
processing used as the request body. There are multiple useful ways to use
piping:
Redirect from a file:
$ http PUT example.com/person/1 X-API-Token:123 < person.json
Or the output of another program:
$ grep '401 Unauthorized' /var/log/httpd/error_log | http POST example.org/intruders
You can use echo for simple data:
$ echo '{""name"": ""John""}' | http PATCH example.com/person/1 X-API-Token:123
You can also use a Bash here string:
$ http example.com/ <<<'{""name"": ""John""}'
You can even pipe web services together using HTTPie:
$ http GET https://api.github.com/repos/jakubroztocil/httpie | http POST httpbin.org/post
You can use cat to enter multiline data on the terminal:
$ cat | http POST example.com
<paste>
^D
$ cat | http POST example.com/todos Content-Type:text/plain
- buy milk
- call parents
^D
On OS X, you can send the contents of the clipboard with pbpaste:
$ pbpaste | http PUT example.com
Passing data through stdin cannot be combined with data fields specified
on the command line:
$ echo 'data' | http POST example.org more=data   # This is invalid
To prevent HTTPie from reading stdin data you can use the
--ignore-stdin option.

16.1¬†¬†¬†Request data from a filename
An alternative to redirected stdin is specifying a filename (as
@/path/to/file) whose content is used as if it came from stdin.
It has the advantage that the Content-Type
header is automatically set to the appropriate value based on the
filename extension. For example, the following request sends the
verbatim contents of that XML file with Content-Type: application/xml:
$ http PUT httpbin.org/put @/data/file.xml

17¬†¬†¬†Terminal output
HTTPie does several things by default in order to make its terminal output
easy to read.

17.1¬†¬†¬†Colors and formatting
Syntax highlighting is applied to HTTP headers and bodies (where it makes
sense). You can choose your preferred color scheme via the --style option
if you don't like the default one (see $ http --help for the possible
values).
Also, the following formatting is applied:

HTTP headers are sorted by name.
JSON data is indented, sorted by keys, and unicode escapes are converted
to the characters they represent.

One of these options can be used to control output processing:


--pretty=all
Apply both colors and formatting.
Default for terminal output.

--pretty=colors
Apply colors.

--pretty=format
Apply formatting.

--pretty=none
Disables output processing.
Default for redirected output.




17.2¬†¬†¬†Binary data
Binary data is suppressed for terminal output, which makes it safe to perform
requests to URLs that send back binary data. Binary data is suppressed also in
redirected, but prettified output. The connection is closed as soon as we know
that the response body is binary,
$ http example.org/Movie.mov
You will nearly instantly see something like this:
HTTP/1.1 200 OK
Accept-Ranges: bytes
Content-Encoding: gzip
Content-Type: video/quicktime
Transfer-Encoding: chunked

+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+

18¬†¬†¬†Redirected output
HTTPie uses a different set of defaults for redirected output than for
terminal output. The differences being:

Formatting and colors aren't applied (unless --pretty is specified).
Only the response body is printed (unless one of the output options is set).
Also, binary data isn't suppressed.

The reason is to make piping HTTPie's output to another programs and
downloading files work with no extra flags. Most of the time, only the raw
response body is of an interest when the output is redirected.
Download a file:
$ http example.org/Movie.mov > Movie.mov
Download an image of Octocat, resize it using ImageMagick, upload it elsewhere:
$ http octodex.github.com/images/original.jpg | convert - -resize 25% -  | http example.org/Octocats
Force colorizing and formatting, and show both the request and the response in
less pager:
$ http --pretty=all --verbose example.org | less -R
The -R flag tells less to interpret color escape sequences included
HTTPie`s output.
You can create a shortcut for invoking HTTPie with colorized and paged output
by adding the following to your ~/.bash_profile:
function httpless {
    # `httpless example.org'
    http --pretty=all --print=hb ""$@"" | less -R;
}

19¬†¬†¬†Download mode
HTTPie features a download mode in which it acts similarly to wget.
When enabled using the --download, -d flag, response headers are printed to
the terminal (stderr), and a progress bar is shown while the response body
is being saved to a file.
$ http --download https://github.com/jakubroztocil/httpie/archive/master.tar.gz
HTTP/1.1 200 OK
Content-Disposition: attachment; filename=httpie-master.tar.gz
Content-Length: 257336
Content-Type: application/x-gzip

Downloading 251.30 kB to ""httpie-master.tar.gz""
Done. 251.30 kB in 2.73862s (91.76 kB/s)

19.1¬†¬†¬†Downloaded filename
There are three mutually exclusive ways through which HTTPie determines
the output filename (with decreasing priority):

You can explicitly provide it via --output, -o.
The file gets overwritten if it already exists
(or appended to with --continue, -c).
The server may specify the filename in the optional Content-Disposition
response header. Any leading dots are stripped from a server-provided filename.
The last resort HTTPie uses is to generate the filename from a combination
of the request URL and the response Content-Type.
The initial URL is always used as the basis for
the generated filename ‚Äî even if there has been one or more redirects.

To prevent data loss by overwriting, HTTPie adds a unique numerical suffix to the
filename when necessary (unless specified with --output, -o).

19.2¬†¬†¬†Piping while downloading
You can also redirect the response body to another program while the response
headers and progress are still shown in the terminal:
$ http -d https://github.com/jakubroztocil/httpie/archive/master.tar.gz |  tar zxf -

19.3¬†¬†¬†Resuming downloads
If --output, -o is specified, you can resume a partial download using the
--continue, -c option. This only works with servers that support
Range requests and 206 Partial Content responses. If the server doesn't
support that, the whole file will simply be downloaded:
$ http -dco file.zip example.org/file

19.4¬†¬†¬†Other notes

The --download option only changes how the response body is treated.
You can still set custom headers, use sessions, --verbose, -v, etc.
--download always implies --follow (redirects are followed).
HTTPie exits with status code 1 (error) if the body hasn't been fully
downloaded.
Accept-Encoding cannot be set with --download.


20¬†¬†¬†Streamed responses
Responses are downloaded and printed in chunks which allows for streaming
and large file downloads without using too much memory. However, when
colors and formatting is applied, the whole response is buffered and only
then processed at once.

20.1¬†¬†¬†Disabling buffering
You can use the --stream, -S flag to make two things happen:

The output is flushed in much smaller chunks without any buffering,
which makes HTTPie behave kind of like tail -f for URLs.
Streaming becomes enabled even when the output is prettified: It will be
applied to each line of the response and flushed immediately. This makes
it possible to have a nice output for long-lived requests, such as one
to the Twitter streaming API.


20.2¬†¬†¬†Examples use cases
Prettified streamed response:
$ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track='Justin Bieber'
Streamed output by small chunks al√° tail -f:
# Send each new tweet (JSON object) mentioning ""Apple"" to another
# server as soon as it arrives from the Twitter streaming API:
$ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track=Apple \
| while read tweet; do echo ""$tweet"" | http POST example.org/tweets ; done

21¬†¬†¬†Sessions
By default, every request HTTPie makes is completely independent of any
previous ones to the same host.
However, HTTPie also supports persistent
sessions via the --session=SESSION_NAME_OR_PATH option. In a session,
custom HTTP headers (except for the ones starting with Content- or If-),
authentication, and cookies
(manually specified or sent by the server) persist between requests
to the same host.
# Create a new session
$ http --session=/tmp/session.json example.org API-Token:123

# Re-use an existing session ‚Äî API-Token will be set:
$ http --session=/tmp/session.json example.org
All session data, including credentials, cookie data,
and custom headers are stored in plain text.
That means session files can also be created and edited manually in a text
editor‚Äîthey are regular JSON. It also means that they can be read by anyone
who has access to the session file.

21.1¬†¬†¬†Named sessions
You can create one or more named session per host. For example, this is how
you can create a new session named user1 for example.org:
$ http --session=user1 -a user1:password example.org X-Foo:Bar
From now on, you can refer to the session by its name. When you choose to
use the session again, any previously specified authentication or HTTP headers
will automatically be set:
$ http --session=user1 example.org
To create or reuse a different session, simple specify a different name:
$ http --session=user2 -a user2:password example.org X-Bar:Foo
Named sessions‚Äôs data is stored in JSON files in the the sessions
subdirectory of the config directory:
~/.httpie/sessions/<host>/<name>.json
(%APPDATA%\httpie\sessions\<host>\<name>.json on Windows).

21.2¬†¬†¬†Anonymous sessions
Instead of a name, you can also directly specify a path to a session file. This
allows for sessions to be re-used across multiple hosts:
$ http --session=/tmp/session.json example.org
$ http --session=/tmp/session.json admin.example.org
$ http --session=~/.httpie/sessions/another.example.org/test.json example.org
$ http --session-read-only=/tmp/session.json example.org

21.3¬†¬†¬†Readonly session
To use an existing session file without updating it from the request/response
exchange once it is created, specify the session name via
--session-read-only=SESSION_NAME_OR_PATH instead.

22¬†¬†¬†Config
HTTPie uses a simple config.json file. The file doesn‚Äôt exist by default
but you can create it manually.

22.1¬†¬†¬†Config file directory
The default location of the configuration file is ~/.httpie/config.json
(or %APPDATA%\httpie\config.json on Windows).
The config directory can be changed by setting the $HTTPIE_CONFIG_DIR
environment variable:
$ export HTTPIE_CONFIG_DIR=/tmp/httpie
$ http example.org
To view the exact location run http --debug.

22.2¬†¬†¬†Configurable options
Currently HTTPie offers a single configurable option:

22.2.1¬†¬†¬†default_options
An Array (by default empty) of default options that should be applied to
every invocation of HTTPie.
For instance, you can use this config option to change your default color theme:
$ cat ~/.httpie/config.json
{
    ""default_options"": [
      ""--style=fruity""
    ]
}
Even though it is technically possible to include there any of HTTPie‚Äôs
options, it is not recommended to modify the default behaviour in a way
that would break your compatibility with the wider world as that can
generate a lot of confusion.

22.3¬†¬†¬†Un-setting previously specified options
Default options from the config file, or specified any other way,
can be unset for a particular invocation via --no-OPTION arguments passed
on the command line (e.g., --no-style or --no-session).

23¬†¬†¬†Scripting
When using HTTPie from shell scripts, it can be handy to set the
--check-status flag. It instructs HTTPie to exit with an error if the
HTTP status is one of 3xx, 4xx, or 5xx. The exit status will
be 3 (unless --follow is set), 4, or 5,
respectively.
#!/bin/bash

if http --check-status --ignore-stdin --timeout=2.5 HEAD example.org/health &> /dev/null; then
    echo 'OK!'
else
    case $? in
        2) echo 'Request timed out!' ;;
        3) echo 'Unexpected HTTP 3xx Redirection!' ;;
        4) echo 'HTTP 4xx Client Error!' ;;
        5) echo 'HTTP 5xx Server Error!' ;;
        6) echo 'Exceeded --max-redirects=<n> redirects!' ;;
        *) echo 'Other Error!' ;;
    esac
fi

23.1¬†¬†¬†Best practices
The default behaviour of automatically reading stdin is typically not
desirable during non-interactive invocations. You most likely want to
use the --ignore-stdin option to disable it.
It is a common gotcha that without this option HTTPie seemingly hangs.
What happens is that when HTTPie is invoked for example from a cron job,
stdin is not connected to a terminal.
Therefore, rules for redirected input apply, i.e., HTTPie starts to read it
expecting that the request body will be passed through.
And since there's no data nor EOF, it will be stuck. So unless you're
piping some data to HTTPie, this flag should be used in scripts.
Also, it might be good to set a connection --timeout limit to prevent
your program from hanging if the server never responds.

24¬†¬†¬†Meta

24.1¬†¬†¬†Interface design
The syntax of the command arguments closely corresponds to the actual HTTP
requests sent over the wire. It has the advantage  that it's easy to remember
and read. It is often possible to translate an HTTP request to an HTTPie
argument list just by inlining the request elements. For example, compare this
HTTP request:
POST /collection HTTP/1.1
X-API-Key: 123
User-Agent: Bacon/1.0
Content-Type: application/x-www-form-urlencoded

name=value&name2=value2
with the HTTPie command that sends it:
$ http -f POST example.org/collection \
  X-API-Key:123 \
  User-Agent:Bacon/1.0 \
  name=value \
  name2=value2
Notice that both the order of elements and the syntax is very similar,
and that only a small portion of the command is used to control HTTPie and
doesn't directly correspond to any part of the request (here it's only -f
asking HTTPie to send a form request).
The two modes, --pretty=all (default for terminal) and --pretty=none
(default for redirected output), allow for both user-friendly interactive use
and usage from scripts, where HTTPie serves as a generic HTTP client.
As HTTPie is still under heavy development, the existing command line
syntax and some of the --OPTIONS may change slightly before
HTTPie reaches its final version 1.0. All changes are recorded in the
change log.

24.2¬†¬†¬†User support
Please use the following support channels:

GitHub issues
for bug reports and feature requests.
Our Gitter chat room
to ask questions, discuss features, and for general discussion.
StackOverflow
to ask questions (please make sure to use the
httpie tag).
Tweet directly to @clihttp.
You can also tweet directly to @jakubroztocil.


24.3¬†¬†¬†Related projects

24.3.1¬†¬†¬†Dependencies
Under the hood, HTTPie uses these two amazing libraries:

Requests
‚Äî Python HTTP library for humans
Pygments
‚Äî Python syntax highlighter


24.3.2¬†¬†¬†HTTPie friends
HTTPie plays exceptionally well with the following tools:

jq
‚Äî CLI JSON processor that
works great in conjunction with HTTPie
http-prompt
‚Äî  interactive shell for HTTPie featuring autocomplete
and command syntax highlighting


24.3.3¬†¬†¬†Alternatives

httpcat ‚Äî a lower-level sister utility
of HTTPie for constructing raw HTTP requests on the command line.
curl ‚Äî a ""Swiss knife"" command line tool and
an exceptional library for transferring data with URLs.


24.4¬†¬†¬†Contributing
See CONTRIBUTING.rst.

24.5¬†¬†¬†Change log
See CHANGELOG.

24.6¬†¬†¬†Artwork

Logo by Cl√°udia Delgado.
Animation by Allen Smith of GitHub.


24.7¬†¬†¬†Licence
BSD-3-Clause: LICENSE.

24.8¬†¬†¬†Authors
Jakub Roztocil  (@jakubroztocil) created HTTPie and these fine people
have contributed.
","GitHub - jakubroztocil/httpie: As easy as HTTPie /aitch-tee-tee-pie/  ü•ß Modern command line HTTP client ‚Äì user-friendly curl alternative with intuitive UI, JSON support, syntax highlighting, wget-like downloads, extensions, etc.  https://twitter.com/clihttp"
11,Python,"Awesome Machine Learning 
A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by awesome-php.
If you want to contribute to this list (please do), send me a pull request or contact me @josephmisiti.
Also, a listed repository should be deprecated if:

Repository's owner explicitly say that ""this library is not maintained"".
Not committed for long time (2~3 years).

Further resources:


For a list of free machine learning books available for download, go here.


For a list of (mostly) free machine learning courses available online, go here.


For a list of blogs and newsletters on data science and machine learning, go here.


For a list of free-to-attend meetups and local events, go here.


Table of Contents
Frameworks and Libraries

Awesome Machine Learning 

Table of Contents

Frameworks and Libraries
Tools


APL

General-Purpose Machine Learning


C

General-Purpose Machine Learning
Computer Vision


C++

Computer Vision
General-Purpose Machine Learning
Natural Language Processing
Speech Recognition
Sequence Analysis
Gesture Detection


Common Lisp

General-Purpose Machine Learning


Clojure

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Crystal

General-Purpose Machine Learning


Elixir

General-Purpose Machine Learning
Natural Language Processing


Erlang

General-Purpose Machine Learning


Go

Natural Language Processing
General-Purpose Machine Learning
Spatial analysis and geometry
Data Analysis / Data Visualization
Computer vision


Haskell

General-Purpose Machine Learning


Java

Natural Language Processing
General-Purpose Machine Learning
Speech Recognition
Data Analysis / Data Visualization
Deep Learning


Javascript

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning
Misc
Demos and Scripts


Julia

General-Purpose Machine Learning
Natural Language Processing
Data Analysis / Data Visualization
Misc Stuff / Presentations


Lua

General-Purpose Machine Learning
Demos and Scripts


Matlab

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


.NET

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Objective C

General-Purpose Machine Learning


OCaml

General-Purpose Machine Learning


Perl

Data Analysis / Data Visualization
General-Purpose Machine Learning


Perl 6

Data Analysis / Data Visualization
General-Purpose Machine Learning


PHP

Natural Language Processing
General-Purpose Machine Learning


Python

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc Scripts / iPython Notebooks / Codebases
Neural Networks
Kaggle Competition Source Code
Reinforcement Learning


Ruby

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc


Rust

General-Purpose Machine Learning


R

General-Purpose Machine Learning
Data Analysis / Data Visualization


SAS

General-Purpose Machine Learning
Data Analysis / Data Visualization
Natural Language Processing
Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning


Tools

Neural Networks
Misc


Credits
ÂÜô‰∏™ËÑöÊú¨ÊääÂÆÉ‰ª¨Áà¨‰∏ãÊù• - Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning



Tools

Neural Networks
Misc

Credits

APL

General-Purpose Machine Learning

naive-apl - Naive Bayesian Classifier implementation in APL. [Deprecated]


C

General-Purpose Machine Learning

Darknet - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.
Recommender - A C library for product recommendations/suggestions using collaborative filtering (CF).
Hybrid Recommender System - A hybrid recommender system based upon scikit-learn algorithms. [Deprecated]
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.


Computer Vision

CCV - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.
VLFeat - VLFeat is an open and portable library of computer vision algorithms, which has Matlab toolbox.


C++

Computer Vision

DLib - DLib has C++ and Python interfaces for face detection and training general object detectors.
EBLearn - Eblearn is an object-oriented C++ library that implements various machine learning models [Deprecated]
OpenCV - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.
VIGRA - VIGRA is a generic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.


General-Purpose Machine Learning

BanditLib - A simple Multi-armed Bandit library. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.
CNTK - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.
CUDA - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]
DeepDetect - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.
Distributed Machine learning Tool Kit (DMTK) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.
DLib - A suite of ML tools designed to be easy to imbed in other applications.
DSSTNE - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.
DyNet - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.
Fido - A highly-modular C++ machine learning library for embedded electronics and robotics.
igraph - General purpose graph library.
Intel(R) DAAL - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.
LightGBM - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.
libfm - A generic approach that allows to mimic most factorization models by feature engineering.
MLDB - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.
mlpack - A scalable C++ machine learning library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
proNet-core - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.
PyCUDA - Python interface to CUDA
ROOT - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.
shark - A fast, modular, feature-rich open-source C++ machine learning library.
Shogun - The Shogun Machine Learning Toolbox.
sofia-ml - Suite of fast incremental algorithms.
Stan - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.
Timbl - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.
Vowpal Wabbit (VW) - A fast out-of-core learning system.
Warp-CTC - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.
XGBoost - A parallelized optimized general purpose gradient boosting library.
ThunderGBM - A fast library for GBDTs and Random Forests on GPUs.
ThunderSVM - A fast SVM library on GPUs and CPUs.
LKYDeepNN - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
Featuretools - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering ""primitives"".
skynet - A library for learning neural network, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.
Feast - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.
Polyaxon - A platform for reproducible and scalable machine learning and deep learning.


Natural Language Processing

BLLIP Parser - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).
colibri-core - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
CRF++ - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks. [Deprecated]
CRFsuite - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. [Deprecated]
frog - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.
libfolia - C++ library for the FoLiA format
MeTA - MeTA : ModErn Text Analysis is a C++ Data Sciences Toolkit that facilitates mining big text data.
MIT Information Extraction Toolkit - C, C++, and Python tools for named entity recognition and relation extraction
ucto - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.


Speech Recognition

Kaldi - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.


Sequence Analysis

ToPS - This is an objected-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. [Deprecated]


Gesture Detection

grt - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.


Common Lisp

General-Purpose Machine Learning

mgl - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.
mgl-gpr - Evolutionary algorithms. [Deprecated]
cl-libsvm - Wrapper for the libsvm support vector machine library. [Deprecated]
cl-online-learning - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).
cl-random-forest - Implementation of Random Forest in Common Lisp.


Clojure

Natural Language Processing

Clojure-openNLP - Natural Language Processing in Clojure (opennlp).
Infections-clj - Rails-like inflection library for Clojure and ClojureScript.


General-Purpose Machine Learning

Touchstone - Clojure A/B testing library. [Deprecated]
Clojush - The Push programming language and the PushGP genetic programming system implemented in Clojure.
Infer - Inference and machine learning in Clojure. [Deprecated]
Clj-ML - A machine learning library for Clojure built on top of Weka and friends. [Deprecated]
DL4CLJ - Clojure wrapper for Deeplearning4j.
Encog - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). [Deprecated]
Fungp - A genetic programming library for Clojure. [Deprecated]
Statistiker - Basic Machine Learning algorithms in Clojure. [Deprecated]
clortex - General Machine Learning library using Numenta‚Äôs Cortical Learning Algorithm. [Deprecated]
comportex - Functionally composable Machine Learning library using Numenta‚Äôs Cortical Learning Algorithm. [Deprecated]
cortex - Neural networks, regression and feature learning in Clojure.
lambda-ml - Simple, concise implementations of machine learning techniques and utilities in Clojure.


Data Analysis / Data Visualization

Incanter - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.
PigPen - Map-Reduce for Clojure.
Envision - Clojure Data Visualisation library, based on Statistiker and D3.


Crystal

General-Purpose Machine Learning

machine - Simple machine learning algorithm.
crystal-fann - FANN (Fast Artificial Neural Network) binding.


Elixir

General-Purpose Machine Learning

Simple Bayes - A Simple Bayes / Naive Bayes implementation in Elixir.
emel - A simple and functional machine learning library written in Elixir.
Tensorflex - Tensorflow bindings for the Elixir programming language.


Natural Language Processing

Stemmer - An English (Porter2) stemming implementation in Elixir.


Erlang

General-Purpose Machine Learning

Disco - Map Reduce in Erlang. [Deprecated]
Yanni - ANN neural networks using Erlangs leightweight processes.


Go

Natural Language Processing

snowball - Snowball Stemmer for Go.
word-embedding - Word Embeddings: the full implementation of word2vec, GloVe in Go.
sentences - Golang implementation of Punkt sentence tokenizer.
go-ngram - In-memory n-gram index with compression. [Deprecated]
paicehusk - Golang implementation of the Paice/Husk Stemming Algorithm. [Deprecated]
go-porterstemmer - A native Go clean room implementation of the Porter Stemming algorithm. [Deprecated]


General-Purpose Machine Learning

birdland - A recommendation library in Go.
eaopt - An evolutionary optimization library.
leaves - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.
gobrain - Neural Networks written in Go.
go-mxnet-predictor - Go binding for MXNet c_predict_api to do inference with pre-trained model.
go-ml-transpiler - An open source Go transpiler for machine learning models.
golearn - Machine learning for Go.
goml - Machine learning library written in pure Go.
gorgonia - Deep learning in Go.
gorse - An offline recommender system backend based on collaborative filtering written in Go.
therfoo - An embedded deep learning library for Go.
neat - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). [Deprecated]
go-pr - Pattern recognition package in Go lang. [Deprecated]
go-ml - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. [Deprecated]
GoNN - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. [Deprecated]
bayesian - Naive Bayesian Classification for Golang. [Deprecated]
go-galib - Genetic Algorithms library written in Go / Golang. [Deprecated]
Cloudforest - Ensembles of decision trees in Go/Golang. [Deprecated]
go-dnn - Deep Neural Networks for Golang (powered by MXNet)


Spatial analysis and geometry

go-geom - Go library to handle geometries.
gogeo - Spherical geometry in Go.


Data Analysis / Data Visualization

gota - Dataframes.
gonum/mat - A linear algebra package for Go.
gonum/optimize - Implementations of optimization algorithms.
gonum/plot - A plotting library.
gonum/stat - A statistics library.
SVGo - The Go Language library for SVG generation.
glot - Glot is a plotting library for Golang built on top of gnuplot.
globe - Globe wireframe visualization.
gonum/graph - General-purpose graph library.
go-graph - Graph library for Go/Golang language. [Deprecated]
RF - Random forests implementation in Go. [Deprecated]


Computer vision

GoCV - Package for computer vision using OpenCV 4 and beyond.


Haskell

General-Purpose Machine Learning

haskell-ml - Haskell implementations of various ML algorithms. [Deprecated]
HLearn - a suite of libraries for interpreting machine learning models according to their algebraic structure. [Deprecated]
hnn - Haskell Neural Network library.
hopfield-networks - Hopfield Networks for unsupervised learning in Haskell. [Deprecated]
DNNGraph - A DSL for deep neural networks. [Deprecated]
LambdaNet - Configurable Neural Networks in Haskell. [Deprecated]


Java

Natural Language Processing

Cortical.io - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.
IRIS - Cortical.io's FREE NLP, Retina API Analysis Tool (written in JavaFX!) - See the Tutorial Video.
CoreNLP - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.
Stanford Parser - A natural language parser is a program that works out the grammatical structure of sentences.
Stanford POS Tagger - A Part-Of-Speech Tagger (POS Tagger).
Stanford Name Entity Recognizer - Stanford NER is a Java implementation of a Named Entity Recognizer.
Stanford Word Segmenter - Tokenization of raw text is a standard pre-processing step for many NLP tasks.
Tregex, Tsurgeon and Semgrex - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for ""tree regular expressions"").
Stanford Phrasal: A Phrase-Based Translation System
Stanford English Tokenizer - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.
Stanford Tokens Regex - A tokenizer divides text into a sequence of tokens, which roughly correspond to ""words"".
Stanford Temporal Tagger - SUTime is a library for recognizing and normalizing time expressions.
Stanford SPIED - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.
Stanford Topic Modeling Toolbox - Topic modeling tools to social scientists and others who wish to perform analysis on datasets.
Twitter Text Java - A Java implementation of Twitter's text processing library.
MALLET - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.
OpenNLP - a machine learning based toolkit for the processing of natural language text.
LingPipe - A tool kit for processing text using computational linguistics.
ClearTK - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. [Deprecated]
Apache cTAKES - Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.
NLP4J - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. [Deprecated]
CogcompNLP - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example illinois-core-utilities which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, illinois-edison a library for feature extraction from illinois-core-utilities data structures and many other packages.


General-Purpose Machine Learning

aerosolve - A machine learning library by Airbnb designed from the ground up to be human friendly.
AMIDST Toolbox - A Java Toolbox for Scalable Probabilistic Machine Learning.
Datumbox - Machine Learning framework for rapid development of Machine Learning and Statistical applications.
ELKI - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
H2O - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.
htm.java - General Machine Learning library using Numenta‚Äôs Cortical Learning Algorithm.
liblinear-java - Java version of liblinear.
Mahout - Distributed machine learning.
Meka - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Neuroph - Neuroph is lightweight Java neural network framework
ORYX - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.
Samoa SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.
RankLib - RankLib is a library of learning to rank algorithms. [Deprecated]
rapaio - statistics, data mining and machine learning toolbox in Java.
RapidMiner - RapidMiner integration into Java code.
Stanford Classifier - A classifier is a machine learning tool that will take data items and place them into one of k classes.
SmileMiner - Statistical Machine Intelligence & Learning Engine.
SystemML - flexible, scalable machine learning (ML) language.
Weka - Weka is a collection of machine learning algorithms for data mining tasks.
LBJava - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.


Speech Recognition

CMU Sphinx - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.


Data Analysis / Data Visualization

Flink - Open source platform for distributed stream and batch data processing.
Hadoop - Hadoop/HDFS.
Onyx - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.
Spark - Spark is a fast and general engine for large-scale data processing.
Storm - Storm is a distributed realtime computation system.
Impala - Real-time Query for Hadoop.
DataMelt - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.
Dr. Michael Thomas Flanagan's Java Scientific Library [Deprecated]


Deep Learning

Deeplearning4j - Scalable deep learning for industry with parallel GPUs.
Keras Beginner Tutorial - Friendly guide on using Keras to implement a simple Neural Network in Python


Javascript

Natural Language Processing

Twitter-text - A JavaScript implementation of Twitter's text processing library.
natural - General natural language facilities for node.
Knwl.js - A Natural Language Processor in JS.
Retext - Extensible system for analyzing and manipulating natural language.
NLP Compromise - Natural Language processing in the browser.
nlp.js - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more


Data Analysis / Data Visualization

D3.js
High Charts
NVD3.js
dc.js
chartjs
dimple
amCharts
D3xter - Straight forward plotting built on D3. [Deprecated]
statkit - Statistics kit for JavaScript. [Deprecated]
datakit - A lightweight framework for data analysis in JavaScript
science.js - Scientific and statistical computing in JavaScript. [Deprecated]
Z3d - Easily make interactive 3d plots built on Three.js [Deprecated]
Sigma.js - JavaScript library dedicated to graph drawing.
C3.js - customizable library based on D3.js for easy chart drawing.
Datamaps - Customizable SVG map/geo visualizations using D3.js. [Deprecated]
ZingChart - library written on Vanilla JS for big data visualization.
cheminfo - Platform for data visualization and analysis, using the visualizer project.
Learn JS Data
AnyChart
FusionCharts
Nivo - built on top of the awesome d3 and Reactjs libraries


General-Purpose Machine Learning

Auto ML - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!
Convnet.js - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING] [Deprecated]
Clusterfck - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser. [Deprecated]
Clustering.js - Clustering algorithms implemented in Javascript for Node.js and the browser. [Deprecated]
Decision Trees - NodeJS Implementation of Decision Tree using ID3 Algorithm. [Deprecated]
DN2A - Digital Neural Networks Architecture. [Deprecated]
figue - K-means, fuzzy c-means and agglomerative clustering.
Gaussian Mixture Model - Unsupervised machine learning with multivariate Gaussian mixture model.
Node-fann - FANN (Fast Artificial Neural Network Library) bindings for Node.js [Deprecated]
Keras.js - Run Keras models in the browser, with GPU support provided by WebGL 2.
Kmeans.js - Simple Javascript implementation of the k-means algorithm, for node.js and the browser. [Deprecated]
LDA.js - LDA topic modeling for Node.js
Learning.js - Javascript implementation of logistic regression/c4.5 decision tree [Deprecated]
machinelearn.js - Machine Learning library for the web, Node.js and developers
mil-tokyo - List of several machine learning libraries.
Node-SVM - Support Vector Machine for Node.js
Brain - Neural networks in JavaScript [Deprecated]
Brain.js - Neural networks in JavaScript - continued community fork of Brain.
Bayesian-Bandit - Bayesian bandit implementation for Node and the browser. [Deprecated]
Synaptic - Architecture-free neural network library for Node.js and the browser.
kNear - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.
NeuralN - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. [Deprecated]
kalman - Kalman filter for Javascript. [Deprecated]
shaman - Node.js library with support for both simple and multiple linear regression. [Deprecated]
ml.js - Machine learning and numerical analysis tools for Node.js and the Browser!
ml5 - Friendly machine learning for the web!
Pavlov.js - Reinforcement learning using Markov Decision Processes.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TensorFlow.js - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.
JSMLT - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see visualml.io).
xgboost-node - Run XGBoost model and make predictions in Node.js.
Netron - Visualizer for machine learning models.
WebDNN - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.


Misc

stdlib - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.
sylvester - Vector and Matrix math for JavaScript. [Deprecated]
simple-statistics - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.
regression-js - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.
Lyric - Linear Regression library. [Deprecated]
GreatCircle - Library for calculating great circle distance.
MLPleaseHelp - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at https://jgreenemi.github.io/MLPleaseHelp/, provided via Github Pages.


Demos and Scripts

The Bot - Example of how the neural network learns to predict the angle between two points created with Synaptic.
Half Beer - Beer glass classifier created with Synaptic.
NSFWJS - Indecent content checker with TensorFlow.js
Rock Paper Scissors - Rock Paper Scissors trained in the browser with TensorFlow.js


Julia

General-Purpose Machine Learning

MachineLearning - Julia Machine Learning library. [Deprecated]
MLBase - A set of functions to support the development of machine learning algorithms.
PGM - A Julia framework for probabilistic graphical models.
DA - Julia package for Regularized Discriminant Analysis.
Regression - Algorithms for regression analysis (e.g. linear regression and logistic regression). [Deprecated]
Local Regression - Local regression, so smooooth!
Naive Bayes - Simple Naive Bayes implementation in Julia. [Deprecated]
Mixed Models - A Julia package for fitting (statistical) mixed-effects models.
Simple MCMC - basic mcmc sampler implemented in Julia. [Deprecated]
Distances - Julia module for Distance evaluation.
Decision Tree - Decision Tree Classifier and Regressor.
Neural - A neural network in Julia.
MCMC - MCMC tools for Julia. [Deprecated]
Mamba - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.
GLM - Generalized linear models in Julia.
Gaussian Processes - Julia package for Gaussian processes.
Online Learning [Deprecated]
GLMNet - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.
Clustering - Basic functions for clustering data: k-means, dp-means, etc.
SVM - SVM's for Julia. [Deprecated]
Kernel Density - Kernel density estimators for julia.
MultivariateStats - Methods for dimensionality reduction.
NMF - A Julia package for non-negative matrix factorization.
ANN - Julia artificial neural networks. [Deprecated]
Mocha - Deep Learning framework for Julia inspired by Caffe. [Deprecated]
XGBoost - eXtreme Gradient Boosting Package in Julia.
ManifoldLearning - A Julia package for manifold learning and nonlinear dimensionality reduction.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Merlin - Flexible Deep Learning Framework in Julia.
ROCAnalysis - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.
GaussianMixtures - Large scale Gaussian Mixture Models.
ScikitLearn - Julia implementation of the scikit-learn API.
Knet - Ko√ß University Deep Learning Framework.
Flux - Relax! Flux is the ML library that doesn't make you tensor


Natural Language Processing

Topic Models - TopicModels for Julia. [Deprecated]
Text Analysis - Julia package for text analysis.
Word Tokenizers - Tokenizers for Natural Language Processing in Julia
Corpus Loaders - A julia package providing a variety of loaders for various NLP corpora.
Embeddings - Functions and data dependencies for loading various word embeddings
Languages - Julia package for working with various human languages
WordNet - A Julia package for Princeton's WordNet


Data Analysis / Data Visualization

Graph Layout - Graph layout algorithms in pure Julia.
LightGraphs - Graph modeling and analysis.
Data Frames Meta - Metaprogramming tools for DataFrames.
Julia Data - library for working with tabular data in Julia. [Deprecated]
Data Read - Read files from Stata, SAS, and SPSS.
Hypothesis Tests - Hypothesis tests for Julia.
Gadfly - Crafty statistical graphics for Julia.
Stats - Statistical tests for Julia.
RDataSets - Julia package for loading many of the data sets available in R.
DataFrames - library for working with tabular data in Julia.
Distributions - A Julia package for probability distributions and associated functions.
Data Arrays - Data structures that allow missing values. [Deprecated]
Time Series - Time series toolkit for Julia.
Sampling - Basic sampling algorithms for Julia.


Misc Stuff / Presentations

DSP - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).
JuliaCon Presentations - Presentations for JuliaCon.
SignalProcessing - Signal Processing tools for Julia.
Images - An image library for Julia.
DataDeps - Reproducible data setup for reproducible science.


Lua

General-Purpose Machine Learning

Torch7

cephes - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. [Deprecated]
autograd - Autograd automatically differentiates native Torch code. Inspired by the original Python version.
graph - Graph package for Torch. [Deprecated]
randomkit - Numpy's randomkit, wrapped for Torch. [Deprecated]
signal - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.
nn - Neural Network package for Torch.
torchnet - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.
nngraph - This package provides graphical computation for nn library in Torch7.
nnx - A completely unstable and experimental package that extends Torch's builtin nn library.
rnn - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.
dpnn - Many useful features that aren't part of the main nn package.
dp - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. [Deprecated]
optim - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.
unsup - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). [Deprecated]
manifold - A package to manipulate manifolds.
svm - Torch-SVM library. [Deprecated]
lbfgs - FFI Wrapper for liblbfgs. [Deprecated]
vowpalwabbit - An old vowpalwabbit interface to torch. [Deprecated]
OpenGM - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. [Deprecated]
spaghetti - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu [Deprecated]
LuaSHKit - A lua wrapper around the Locality sensitive hashing library SHKit [Deprecated]
kernel smoothing - KNN, kernel-weighted average, local linear regression smoothers. [Deprecated]
cutorch - Torch CUDA Implementation.
cunn - Torch CUDA Neural Network Implementation.
imgraph - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. [Deprecated]
videograph - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. [Deprecated]
saliency - code and tools around integral images. A library for finding interest points based on fast integral histograms. [Deprecated]
stitch - allows us to use hugin to stitch images and apply same stitching to a video sequence. [Deprecated]
sfm - A bundle adjustment/structure from motion package. [Deprecated]
fex - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. [Deprecated]
OverFeat - A state-of-the-art generic dense feature extractor. [Deprecated]
wav2letter - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.


Numeric Lua
Lunatic Python
SciLua
Lua - Numerical Algorithms [Deprecated]
Lunum [Deprecated]


Demos and Scripts

Core torch7 demos repository.

linear-regression, logistic-regression
face detector (training and detection as separate demos)
mst-based-segmenter
train-a-digit-classifier
train-autoencoder
optical flow demo
train-on-housenumbers
train-on-cifar
tracking with deep nets
kinect demo
filter-bank visualization
saliency-networks


Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)
Music Tagging - Music Tagging scripts for torch7.
torch-datasets - Scripts to load several popular datasets including:

BSR 500
CIFAR-10
COIL
Street View House Numbers
MNIST
NORB


Atari2600 - Scripts to generate a dataset with static frames from the Arcade Learning Environment.


Matlab

Computer Vision

Contourlets - MATLAB source code that implements the contourlet transform and its utility functions.
Shearlets - MATLAB code for shearlet transform.
Curvelets - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.
Bandlets - MATLAB code for bandlet transform.
mexopencv - Collection and a development kit of MATLAB mex functions for OpenCV library.


Natural Language Processing

NLP - An NLP library for Matlab.


General-Purpose Machine Learning

Training a deep autoencoder or a classifier
on MNIST digits - Training a deep autoencoder or a classifier
on MNIST digits[DEEP LEARNING].
Convolutional-Recursive Deep Learning for 3D Object Classification - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].
Spider - The spider is intended to be a complete object orientated environment for machine learning in Matlab.
LibSVM - A Library for Support Vector Machines.
ThunderSVM - An Open-Source SVM Library on GPUs and CPUs
LibLinear - A Library for Large Linear Classification.
Machine Learning Module - Class on machine w/ PDF, lectures, code
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
Pattern Recognition Toolbox - A complete object-oriented environment for machine learning in Matlab.
Pattern Recognition and Machine Learning - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Machine Learning in MatLab/Octave - examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.


Data Analysis / Data Visualization

matlab_bgl - MatlabBGL is a Matlab package for working with graphs.
gaimc - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.


.NET

Computer Vision

OpenCVDotNet - A wrapper for the OpenCV project to be used with .NET applications.
Emgu CV - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.
AForge.NET - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.
Accord.NET - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.


Natural Language Processing

Stanford.NLP for .NET - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.


General-Purpose Machine Learning

Accord-Framework -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.
Accord.MachineLearning - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.
DiffSharp - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
GeneticSharp - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.
Infer.NET - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customised solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.
ML.NET - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.
Neural Network Designer - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feed back. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.
Synapses - Neural network library in F#.
Vulpes - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.


Data Analysis / Data Visualization

numl - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.
Math.NET Numerics - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and every day use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.
Sho - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.


Objective C

General-Purpose Machine Learning

YCML - A Machine Learning framework for Objective-C and Swift (OS X / iOS).
MLPNeuralNet - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural network. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. [Deprecated]
MAChineLearning - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.
BPN-NeuralNetwork - It implemented 3 layers neural network ( Input Layer, Hidden Layer and Output Layer ) and it named Back Propagation Neural Network (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. [Deprecated]
Multi-Perceptron-NeuralNetwork - it implemented multi-perceptrons neural network („Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ) based on Back Propagation Neural Network (BPN) and designed unlimited-hidden-layers.
KRHebbian-Algorithm - It is a non-supervisor and self-learning algorithm (adjust the weights) in neural network of Machine Learning. [Deprecated]
KRKmeans-Algorithm - It implemented K-Means the clustering and classification algorithm. It could be used in data mining and image compression. [Deprecated]
KRFuzzyCMeans-Algorithm - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression. [Deprecated]


OCaml

General-Purpose Machine Learning

Oml - A general statistics and machine learning library.
GPR - Efficient Gaussian Process Regression in OCaml.
Libra-Tk - Algorithms for learning and inference with discrete probabilistic models.
TensorFlow - OCaml bindings for TensorFlow.


Perl

Data Analysis / Data Visualization

Perl Data Language, a pluggable architecture for data and image processing, which can
be used for machine learning.


General-Purpose Machine Learning

MXnet for Deep Learning, in Perl,
also released in CPAN.
Perl Data Language,
using AWS machine learning platform from Perl.
Algorithm::SVMLight,
implementation of Support Vector Machines with SVMLight under it. [Deprecated]
Several machine learning and artificial intelligence models are
included in the AI
namespace. For instance, you can
find Na√Øve Bayes.


Perl 6

Support Vector Machines
Na√Øve Bayes

Data Analysis / Data Visualization

Perl Data Language,
a pluggable architecture for data and image processing, which can
be
used for machine learning.

General-Purpose Machine Learning

PHP

Natural Language Processing

jieba-php - Chinese Words Segmentation Utilities.


General-Purpose Machine Learning

PHP-ML - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Rubix ML - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.
19 Questions - A machine learning / bayesian inference assigning attributes to objects.


Python

Computer Vision

Scikit-Image - A collection of algorithms for image processing in Python.
SimpleCV - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.
Vigranumpy - Python bindings for the VIGRA C++ computer vision library.
OpenFace - Free and open source face recognition with deep neural networks.
PCV - Open source Python module for computer vision. [Deprecated]
face_recognition - Face recognition library that recognize and manipulate faces from Python or from the command line.
dockerface - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.
Detectron - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework.
albumentations - –ê fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.
pytessarct - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and ""read"" the text embedded in images.Python-tesseract is a wrapper for Google's Tesseract-OCR Engine>.
imutils - A library containg Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.
PyTorchCV - A PyTorch-Based Framework for Deep Learning in Computer Vision.
neural-style-pt - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).


Natural Language Processing

pkuseg-python - A better version of Jieba, developed by Peking University.
NLTK - A leading platform for building Python programs to work with human language data.
Pattern - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.
Quepy - A python framework to transform natural language questions to queries in a database query language.
TextBlob - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.
YAlign - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. [Deprecated]
jieba - Chinese Words Segmentation Utilities.
SnowNLP - A library for processing Chinese text.
spammy - A library for email Spam filtering built on top of nltk
loso - Another Chinese segmentation library. [Deprecated]
genius - A Chinese segment base on Conditional Random Field.
KoNLPy - A Python package for Korean natural language processing.
nut - Natural language Understanding Toolkit. [Deprecated]
Rosetta - Text processing tools and wrappers (e.g. Vowpal Wabbit)
BLLIP Parser - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). [Deprecated]
PyNLPl - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrasetables, GIZA++ alignments.
python-ucto - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).
python-frog - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)
python-zpar - Python bindings for ZPar, a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.
colibri-core - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
spaCy - Industrial strength NLP with Python and Cython.
PyStanfordDependencies - Python interface for converting Penn Treebank trees to Stanford Dependencies.
Distance - Levenshtein and Hamming distance computation. [Deprecated]
Fuzzy Wuzzy - Fuzzy String Matching in Python.
jellyfish - a python library for doing approximate and phonetic matching of strings.
editdistance - fast implementation of edit distance.
textacy - higher-level NLP built on Spacy.
stanford-corenlp-python - Python wrapper for Stanford CoreNLP [Deprecated]
CLTK - The Classical Language Toolkit.
rasa_nlu - turn natural language into structured data.
yase - Transcode sentence (or other sequence) to list of word vector .
Polyglot - Multilingual text (NLP) processing toolkit.
DrQA - Reading Wikipedia to answer open-domain questions.
Dedupe - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.
Snips NLU - Natural Language Understanding library for intent classification and entity extraction
NeuroNER - Named-entity recognition using neural networks providing state-of-the-art-results
DeepPavlov - conversational AI library with many pretrained Russian NLP models.
BigARTM - topic modelling platform.


General-Purpose Machine Learning

PyOD -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles.
steppy -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces very simple interface that enables clean machine learning pipeline design.
steppy-toolkit -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.
CNTK - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found here.
auto_ml - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.
machine learning - automated build consisting of a web-interface, and set of programmatic-interface API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.
XGBoost - Python bindings for eXtreme Gradient Boosting (Tree) Library.
Apache SINGA - An Apache Incubating project for developing an open source machine learning library.
Bayesian Methods for Hackers - Book/iPython notebooks on Probabilistic Programming in Python.
Featureforge A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
scikit-learn - A Python module for machine learning built on top of SciPy.
metric-learn - A Python module for metric learning.
SimpleAI Python implementation of many of the artificial intelligence algorithms described on the book ""Artificial Intelligence, a Modern Approach"". It focuses on providing an easy to use, well documented and tested library.
astroML - Machine Learning and Data Mining for Astronomy.
graphlab-create - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.
BigML - A library that contacts external servers.
pattern - Web mining module for Python.
NuPIC - Numenta Platform for Intelligent Computing.
Pylearn2 - A Machine Learning library based on Theano. [Deprecated]
keras - High-level neural networks frontend for TensorFlow, CNTK and Theano.
Lasagne - Lightweight library to build and train neural networks in Theano.
hebel - GPU-Accelerated Deep Learning Library in Python. [Deprecated]
Chainer - Flexible neural network framework.
prophet - Fast and automated time series forecasting framework by Facebook.
gensim - Topic Modelling for Humans.
topik - Topic modelling toolkit. [Deprecated]
PyBrain - Another Python Machine Learning Library.
Brainstorm - Fast, flexible and fun neural networks. This is the successor of PyBrain.
Surprise - A scikit for building and analyzing recommender systems.
Crab - A flexible, fast recommender engine. [Deprecated]
python-recsys - A Python library for implementing a Recommender System.
thinking bayes - Book on Bayesian Analysis.
Image-to-Image Translation with Conditional Adversarial Networks - Implementation of image to image (pix2pix) translation from the paper by isola et al.[DEEP LEARNING]
Restricted Boltzmann Machines -Restricted Boltzmann Machines in Python. [DEEP LEARNING]
Bolt - Bolt Online Learning Toolbox. [Deprecated]
CoverTree - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree [Deprecated]
nilearn - Machine learning for NeuroImaging in Python.
neuropredict - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing the much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.
imbalanced-learn - Python module to perform under sampling and over sampling with various techniques.
Shogun - The Shogun Machine Learning Toolbox.
Pyevolve - Genetic algorithm framework. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
breze - Theano based library for deep and recurrent neural networks.
Cortex - Open source platform for deploying machine learning models in production.
pyhsmm - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.
mrjob - A library to let Python program run on Hadoop.
SKLL - A wrapper around scikit-learn that makes it simpler to conduct experiments.
neurolab
Spearmint - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. [Deprecated]
Pebl - Python Environment for Bayesian Learning. [Deprecated]
Theano - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.
TensorFlow - Open source software library for numerical computation using data flow graphs.
pomegranate - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.
python-timbl - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.
deap - Evolutionary algorithm framework.
pydeep - Deep Learning In Python. [Deprecated]
mlxtend - A library consisting of useful tools for data science and machine learning tasks.
neon - Nervana's high-performance Python-based Deep Learning framework [DEEP LEARNING].
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.
Neural Networks and Deep Learning - Code samples for my book ""Neural Networks and Deep Learning"" [DEEP LEARNING].
Annoy - Approximate nearest neighbours implementation.
TPOT - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.
pgmpy A python library for working with Probabilistic Graphical Models.
DIGITS - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.
Orange - Open source data visualization and data analysis for novices and experts.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
milk - Machine learning toolkit focused on supervised classification. [Deprecated]
TFLearn - Deep learning library featuring a higher-level API for TensorFlow.
REP - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. [Deprecated]
rgf_python - Python bindings for Regularized Greedy Forest (Tree) Library.
skbayes - Python package for Bayesian Machine Learning with scikit-learn API.
fuku-ml - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.
Xcessiv - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.
PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration
ML-From-Scratch - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.
Edward - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.
xRBM - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.
stacked_generalization - Implementation of machine learning stacking technic as handy library in Python.
modAL - A modular active learning framework for Python, built on top of scikit-learn.
Cogitare: A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.
Parris - Parris, the automated infrastructure setup tool for machine learning algorithms.
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.
Turi Create - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
mlens - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.
Netron - Visualizer for machine learning models.
Thampi - Machine Learning Prediction System on AWS Lambda
MindsDB - Open Source framework to streamline use of neural networks.
Microsoft Recommenders: Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.
StellarGraph: Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.
BentoML: Toolkit for package and deploy machine learning models for serving in production
MiraiML: An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.
numpy-ML: Reference implementations of ML models written in numpy
creme: A framework for online machine learning.
Neuraxle: A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.
Cornac - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.
JAX - JAX is Autograd and XLA, brought together for high-performance machine learning research.
fast.ai - A library simplifies training fast and accurate neural nets using modern best practices and already supports  vision, text, tabular, and collab (collaborative filtering) models ""out of the box""
Catalyst - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.
Fastai - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.


Data Analysis / Data Visualization

SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
NumPy - A fundamental package for scientific computing with Python.
Numba - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.
Mars - A tensor-based framework for large-scale data computation which often regarded as a parallel and distributed version of NumPy.
NetworkX - A high-productivity software for complex networks.
igraph - binding to igraph library - General purpose graph library.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Open Mining - Business Intelligence (BI) in Python (Pandas web interface) [Deprecated]
PyMC - Markov Chain Monte Carlo sampling toolkit.
zipline - A Pythonic algorithmic trading library.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.
SymPy - A Python library for symbolic mathematics.
statsmodels - Statistical modeling and econometrics in Python.
astropy - A community Python library for Astronomy.
matplotlib - A Python 2D plotting library.
bokeh - Interactive Web Plotting for Python.
plotly - Collaborative web plotting for Python and matplotlib.
altair - A Python to Vega translator.
d3py - A plotting library for Python, based on D3.js.
PyDexter - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.
ggplot - Same API as ggplot2 for R. [Deprecated]
ggfortify - Unified interface to ggplot2 popular R packages.
Kartograph.py - Rendering beautiful SVG maps in Python.
pygal - A Python SVG Charts Creator.
PyQtGraph - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.
pycascading [Deprecated]
Petrel - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.
Blaze - NumPy and Pandas interface to Big Data.
emcee - The Python ensemble sampling toolkit for affine-invariant MCMC.
windML - A Python Framework for Wind Energy Analysis and Prediction.
vispy - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.
cerebro2 A web-based visualization and debugging platform for NuPIC. [Deprecated]
NuPIC Studio An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! [Deprecated]
SparklingPandas Pandas on PySpark (POPS).
Seaborn - A python visualization library based on matplotlib.
bqplot - An API for plotting in Jupyter (IPython).
pastalog - Simple, realtime visualization of neural network training performance.
Superset - A data exploration platform designed to be visual, intuitive, and interactive.
Dora - Tools for exploratory data analysis in Python.
Ruffus - Computation Pipeline library for python.
SOMPY - Self Organizing Map written in Python (Uses neural networks for data analysis).
somoclu Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.
HDBScan - implementation of the hdbscan algorithm in Python - used for clustering
visualize_ML - A python package for data exploration and data analysis. [Deprecated]
scikit-plot - A visualization library for quick and easy generation of common plots in data analysis and machine learning.
Bowtie - A dashboard library for interactive visualizations using flask socketio and react.
lime - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.
PyCM - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters
Dash - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask
Lambdo - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.
TensorWatch - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.
dowel - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to logger.log().


Misc Scripts / iPython Notebooks / Codebases

Map/Reduce implementations of common ML algorithms: Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark.
BioPy - Biologically-Inspired and Machine Learning Algorithms in Python. [Deprecated]
SVM Explorer - Interactive SVM Explorer, using Dash and scikit-learn
pattern_classification
thinking stats 2
hyperopt
numpic
2012-paper-diginorm
A gallery of interesting IPython notebooks
ipython-notebooks
data-science-ipython-notebooks - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.
decision-weights
Sarah Palin LDA - Topic Modeling the Sarah Palin emails.
Diffusion Segmentation - A collection of image segmentation algorithms based on diffusion methods.
Scipy Tutorials - SciPy tutorials. This is outdated, check out scipy-lecture-notes.
Crab - A recommendation engine library for Python.
BayesPy - Bayesian Inference Tools in Python.
scikit-learn tutorials - Series of notebooks for learning scikit-learn.
sentiment-analyzer - Tweets Sentiment Analyzer
sentiment_classifier - Sentiment classifier using word sense disambiguation.
group-lasso - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.
jProcessing - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.
mne-python-notebooks - IPython notebooks for EEG/MEG data processing using mne-python.
Neon Course - IPython notebooks for a complete course around understanding Nervana's Neon.
pandas cookbook - Recipes for using Python's pandas library.
climin - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.
Allen Downey‚Äôs Data Science Course - Code for Data Science at Olin College, Spring 2014.
Allen Downey‚Äôs Think Bayes Code - Code repository for Think Bayes.
Allen Downey‚Äôs Think Complexity Code - Code for Allen Downey's book Think Complexity.
Allen Downey‚Äôs Think OS Code - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.
Python Programming for the Humanities - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.
GreatCircle - Library for calculating great circle distance.
Optunity examples - Examples demonstrating how to use Optunity in synergy with machine learning libraries.
Dive into Machine Learning  with Python Jupyter notebook and scikit-learn - ""I learned Python by hacking first, and getting serious later. I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.""
TDB - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.
Suiron - Machine Learning for RC Cars.
Introduction to machine learning with scikit-learn - IPython notebooks from Data School's video tutorials on scikit-learn.
Practical XGBoost in Python - comprehensive online course about using XGBoost in Python.
Introduction to Machine Learning with Python - Notebooks and code for the book ""Introduction to Machine Learning with Python""
Pydata book - Materials and IPython notebooks for ""Python for Data Analysis"" by Wes McKinney, published by O'Reilly Media
Homemade Machine Learning - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained
Prodmodel - Build tool for data science pipelines.
the-elements-of-statistical-learning - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.


Neural Networks

nn_builder - nn_builder is a python package that lets you build neural networks in 1 line
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg‚ÄìMarquardt algorithm.
=======
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. [Deprecated]
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg‚ÄìMarquardt algorithm. [Deprecated]
Data Driven Code - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.
Machine Learning, Data Science and Deep Learning with Python - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.


Kaggle Competition Source Code

open-solution-home-credit -> source code and experiments results for Home Credit Default Risk.
open-solution-googleai-object-detection -> source code and experiments results for Google AI Open Images - Object Detection Track.
open-solution-salt-identification -> source code and experiments results for TGS Salt Identification Challenge.
open-solution-ship-detection -> source code and experiments results for Airbus Ship Detection Challenge.
open-solution-data-science-bowl-2018 -> source code and experiments results for 2018 Data Science Bowl.
open-solution-value-prediction -> source code and experiments results for Santander Value Prediction Challenge.
open-solution-toxic-comments -> source code for Toxic Comment Classification Challenge.
wiki challenge - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.
kaggle insults - Kaggle Submission for ""Detecting Insults in Social Commentary"".
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
kaggle-cifar - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.
kaggle-blackbox - Deep learning made easy.
kaggle-accelerometer - Code for Accelerometer Biometric Competition at Kaggle.
kaggle-advertised-salaries - Predicting job salaries from ads - a Kaggle competition.
kaggle amazon - Amazon access control challenge.
kaggle-bestbuy_big - Code for the Best Buy competition at Kaggle.
kaggle-bestbuy_small
Kaggle Dogs vs. Cats - Code for Kaggle Dogs vs. Cats competition.
Kaggle Galaxy Challenge - Winning solution for the Galaxy Challenge on Kaggle.
Kaggle Gender - A Kaggle competition: discriminate gender based on handwriting.
Kaggle Merck - Merck challenge at Kaggle.
Kaggle Stackoverflow - Predicting closed questions on Stack Overflow.
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
wine-quality - Predicting wine quality.


Reinforcement Learning

DeepMind Lab - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.
Gym - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
Serpent.AI - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.
ViZDoom - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.
Roboschool - Open-source software for robot simulation, integrated with OpenAI Gym.
Retro - Retro Games in Gym
SLM Lab - Modular Deep Reinforcement Learning framework in PyTorch.
Coach - Reinforcement Learning Coach by Intel¬Æ AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms
garage - A toolkit for reproducible reinforcement learning research
metaworld - An open source robotics benchmark for meta- and multi-task reinforcement learning


Ruby

Natural Language Processing

Awesome NLP with Ruby - Curated link list for practical natural language processing in Ruby.
Treat - Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I‚Äôve encountered so far for Ruby.
Stemmer - Expose libstemmer_c to Ruby. [Deprecated]
Raspel - raspell is an interface binding for ruby. [Deprecated]
UEA Stemmer - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.
Twitter-text-rb - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.


General-Purpose Machine Learning

Awesome Machine Learning with Ruby - Curated list of ML related resources for Ruby.
Ruby Machine Learning - Some Machine Learning algorithms, implemented in Ruby. [Deprecated]
Machine Learning Ruby [Deprecated]
jRuby Mahout - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. [Deprecated]
CardMagic-Classifier - A general classifier module to allow Bayesian and other types of classifications.
rb-libsvm - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.
Scoruby - Creates Random Forest classifiers from PMML files.


Data Analysis / Data Visualization

rsruby - Ruby - R bridge.
data-visualization-ruby - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. [Deprecated]
ruby-plot - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. [Deprecated]
plot-rb - A plotting library in Ruby built on top of Vega and D3. [Deprecated]
scruffy - A beautiful graphing toolkit for Ruby.
SciRuby
Glean - A data management tool for humans. [Deprecated]
Bioruby
Arel [Deprecated]


Misc

Big Data For Chimps
Listof - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. Demo/Search for a list


Rust

General-Purpose Machine Learning

deeplearn-rs - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.
rustlearn - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.
rusty-machine - a pure-rust machine learning library.
leaf - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [Deprecated]
RustNN - RustNN is a feedforward neural network library. [Deprecated]
RusticSOM - A Rust library for Self Organising Maps (SOM).


R

General-Purpose Machine Learning

ahaz - ahaz: Regularization for semiparametric additive hazards regression. [Deprecated]
arules - arules: Mining Association Rules and Frequent Itemsets
biglasso - biglasso: Extending Lasso Model Fitting to Big Data in R.
bmrm - bmrm: Bundle Methods for Regularized Risk Minimization Package.
Boruta - Boruta: A wrapper algorithm for all-relevant feature selection.
bst - bst: Gradient Boosting.
C50 - C50: C5.0 Decision Trees and Rule-Based Models.
caret - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.
caretEnsemble - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. [Deprecated]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.
Clever Algorithms For Machine Learning
CORElearn - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.
CoxBoost - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks [Deprecated]
Cubist - Cubist: Rule- and Instance-Based Regression Modeling.
e1071 - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien
earth - earth: Multivariate Adaptive Regression Spline Models
elasticnet - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.
ElemStatLearn - ElemStatLearn: Data sets, functions and examples from the book: ""The Elements of Statistical Learning, Data Mining, Inference, and Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.
evtree - evtree: Evolutionary Learning of Globally Optimal Trees.
forecast - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.
forecastHybrid - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the ""forecast"" package.
fpc - fpc: Flexible procedures for clustering.
frbs - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. [Deprecated]
GAMBoost - GAMBoost: Generalized linear and additive models by likelihood based boosting. [Deprecated]
gamboostLSS - gamboostLSS: Boosting Methods for GAMLSS.
gbm - gbm: Generalized Boosted Regression Models.
glmnet - glmnet: Lasso and elastic-net regularized generalized linear models.
glmpath - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.
GMMBoost - GMMBoost: Likelihood-based Boosting for Generalized mixed models. [Deprecated]
grplasso - grplasso: Fitting user specified models with Group Lasso penalty.
grpreg - grpreg: Regularization paths for regression models with grouped covariates.
h2o - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.
hda - hda: Heteroscedastic Discriminant Analysis. [Deprecated]
Introduction to Statistical Learning
ipred - ipred: Improved Predictors.
kernlab - kernlab: Kernel-based Machine Learning Lab.
klaR - klaR: Classification and visualization.
L0Learn - L0Learn: Fast algorithms for best subset selection.
lars - lars: Least Angle Regression, Lasso and Forward Stagewise. [Deprecated]
lasso2 - lasso2: L1 constrained estimation aka ‚Äòlasso‚Äô.
LiblineaR - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.
LogicReg - LogicReg: Logic Regression.
Machine Learning For Hackers
maptree - maptree: Mapping, pruning, and graphing tree models. [Deprecated]
mboost - mboost: Model-Based Boosting.
medley - medley: Blending regression models, using a greedy stepwise approach.
mlr - mlr: Machine Learning in R.
ncvreg - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.
nnet - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. [Deprecated]
pamr - pamr: Pam: prediction analysis for microarrays. [Deprecated]
party - party: A Laboratory for Recursive Partytioning.
partykit - partykit: A Toolkit for Recursive Partytioning.
penalized - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.
penalizedLDA - penalizedLDA: Penalized classification using Fisher's linear discriminant. [Deprecated]
penalizedSVM - penalizedSVM: Feature Selection SVM using penalty functions.
quantregForest - quantregForest: Quantile Regression Forests.
randomForest - randomForest: Breiman and Cutler's random forests for classification and regression.
randomForestSRC - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).
rattle - rattle: Graphical user interface for data mining in R.
rda - rda: Shrunken Centroids Regularized Discriminant Analysis.
rdetools - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. [Deprecated]
REEMtree - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. [Deprecated]
relaxo - relaxo: Relaxed Lasso. [Deprecated]
rgenoud - rgenoud: R version of GENetic Optimization Using Derivatives
Rmalschains - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.
rminer - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. [Deprecated]
ROCR - ROCR: Visualizing the performance of scoring classifiers. [Deprecated]
RoughSets - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. [Deprecated]
rpart - rpart: Recursive Partitioning and Regression Trees.
RPMM - RPMM: Recursively Partitioned Mixture Model.
RSNNS - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).
RWeka - RWeka: R/Weka interface.
RXshrink - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.
sda - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. [Deprecated]
spectralGraphTopology - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.
SuperLearner - Multi-algorithm ensemble learning packages.
svmpath - svmpath: svmpath: the SVM Path algorithm. [Deprecated]
tgp - tgp: Bayesian treed Gaussian process models. [Deprecated]
tree - tree: Classification and regression trees.
varSelRF - varSelRF: Variable selection using random forests.
XGBoost.R - R binding for eXtreme Gradient Boosting (Tree) Library.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.
igraph - binding to igraph library - General purpose graph library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TDSP-Utilities - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).


Data Analysis / Data Visualization

ggplot2 - A data visualization package based on the grammar of graphics.
tmap for visualizing geospatial data with static maps and leaflet for interactive maps
tm and quanteda are the main packages for managing,  analyzing, and visualizing textual data.
shiny is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with htmlwidgets bringing javascript libraries to R. These include, plotly, dygraphs, highcharter, and several others.


SAS

General-Purpose Machine Learning

Visual Data Mining and Machine Learning - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.
Enterprise Miner - Data mining and machine learning that creates deployable models using a GUI or code.
Factory Miner - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.


Data Analysis / Data Visualization

SAS/STAT - For conducting advanced statistical analysis.
University Edition - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.


Natural Language Processing

Contextual Analysis - Add structure to unstructured text using a GUI.
Sentiment Analysis - Extract sentiment from text using a GUI.
Text Miner - Text mining using a GUI or code.


Demos and Scripts

ML_Tables - Concise cheat sheets containing machine learning best practices.
enlighten-apply - Example code and materials that illustrate applications of SAS machine learning techniques.
enlighten-integration - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.
enlighten-deep - Example code and materials that illustrate using neural networks with several hidden layers in SAS.
dm-flow - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.


Scala

Natural Language Processing

ScalaNLP - ScalaNLP is a suite of machine learning and numerical computing libraries.
Breeze - Breeze is a numerical processing library for Scala.
Chalk - Chalk is a natural language processing library. [Deprecated]
FACTORIE - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.
Montague - Montague is a semantic parsing library for Scala with an easy-to-use DSL.
Spark NLP - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.


Data Analysis / Data Visualization

MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Scalding - A Scala API for Cascading.
Summing Bird - Streaming MapReduce with Scalding and Storm.
Algebird - Abstract Algebra for Scala.
xerial - Data management utilities for Scala. [Deprecated]
PredictionIO - PredictionIO, a machine learning server for software developers and data engineers.
BIDMat - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.
Flink - Open source platform for distributed stream and batch data processing.
Spark Notebook - Interactive and Reactive Data Science using Scala and Spark.


General-Purpose Machine Learning

DeepLearning.scala - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.
Conjecture - Scalable Machine Learning in Scalding.
brushfire - Distributed decision tree ensemble learning in Scala.
ganitha - Scalding powered machine learning. [Deprecated]
adam - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.
bioscala - Bioinformatics for the Scala programming language
BIDMach - CPU and GPU-accelerated Machine Learning Library.
Figaro - a Scala library for constructing probabilistic models.
H2O Sparkling Water - H2O and Spark interoperability.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
DynaML - Scala Library/REPL for Machine Learning Research.
Saul - Flexible Declarative Learning-Based Programming.
SwiftLearner - Simply written algorithms to help study ML or write your own implementations.
Smile - Statistical Machine Intelligence and Learning Engine.
doddle-model - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.
TensorFlow Scala -   Strongly-typed Scala API for TensorFlow.


Scheme

Neural Networks

layer - Neural network inference from the command line, implemented in CHICKEN Scheme.


Swift

General-Purpose Machine Learning

Bender - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.
Swift AI - Highly optimized artificial intelligence and machine learning library written in Swift.
Swift for Tensorflow - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.
BrainCore - The iOS and OS X neural network framework.
swix - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. [Deprecated]
AIToolbox - A toolbox framework of AI modules written in Swift: Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.
MLKit - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.
Swift Brain - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...
Perfect TensorFlow - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Awesome CoreML - A curated list of pretrained CoreML models.
Awesome Core ML Models - A curated list of machine learning models in CoreML format.


TensorFlow

General-Purpose Machine Learning

Awesome TensorFlow - A list of all things related to TensorFlow.
Golden TensorFlow - A page of content on TensorFlow, including academic papers and links to related topics.


Tools

Neural Networks

layer - Neural network inference from the command line


Misc

ML Workspace - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).
Notebooks - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.
DVC - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.
Kedro - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.
guild.ai - Tool to log, analyze, compare and ""optimize"" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.
Sacred - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemestry/Biology. The community has built multiple add-ons leveraging the proposed standard.
MLFlow - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework anf language agnostic, take a look at all the built-in integrations.
More tools to improve the ML lifecycle: Catalyst, PachydermIO. The following are Github-alike and targetting teams Weights & Biases, Neptune.Ml, Comet.ml, Valohai.ai.


Credits

Some of the python libraries were cut-and-pasted from vinta
References for Go were mostly cut-and-pasted from gopherdata

","GitHub - josephmisiti/awesome-machine-learning: A curated list of awesome Machine Learning frameworks, libraries and software."
12,Python,"
    
    
    
    
     Python 3.7.4 (default, Sep  7 2019, 18:27:02)
     >>> import requests
     >>> r = requests.get('https://api.github.com/repos/psf/requests')
     >>> r.json()[""description""]
     'A simple, yet elegant HTTP library. Handcrafted, with ‚ô•, for the Python community.'
    

    
This software has been designed for you, with much joy,
by Kenneth Reitz & is protected by The Python Software Foundation.
   


¬†¬†
Requests is an elegant and simple HTTP library for Python, built with ‚ô•.
¬†
>>> import requests
>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
u'{""type"":""User""...'
>>> r.json()
{u'disk_usage': 368627, u'private_gists': 484, ...}

¬†
Requests allows you to send HTTP/1.1 requests extremely easily. There‚Äôs no need to manually add query strings to your URLs, or to form-encode your PUT & POST data ‚Äî but nowadays, just use the json method!
Requests is the most downloaded Python package today, pulling in around 14M downloads / week‚Äî according to GitHub, Requests is currently depended upon by 367_296 repositories. You may certainly put your trust in this code.
¬†



¬†
Supported Features & Best‚ÄìPractices
Requests is ready for the demands of building robust and reliable HTTP‚Äìspeak applications, for the needs of today.
         + International Domains and URLs       + Keep-Alive & Connection Pooling
         + Sessions with Cookie Persistence     + Browser-style SSL Verification
         + Basic & Digest Authentication        + Familiar `dict`‚Äìlike Cookies
         + Automatic Decompression of Content   + Automatic Content Decoding
         + Automatic Connection Pooling         + Unicode Response Bodies*
         + Multi-part File Uploads              + SOCKS Proxy Support
         + Connection Timeouts                  + Streaming Downloads
         + Automatic honoring of `.netrc`       + Chunked HTTP Requests

                            &, of course, rock‚Äìsolid stability!


‚ú® üç∞ ‚ú®¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†

¬†
Requests Module Installation
The recommended way to intall the requests module is to simply use pipenv (or pip, of
course):
$ pipenv install requests
Adding requests to Pipfile's [packages]‚Ä¶
‚úî Installation Succeeded
‚Ä¶
Requests officially supports Python 2.7 & 3.4‚Äì3.8.

P.S. ‚Äî¬†Documentation is Available at //requests.readthedocs.io.




¬†



¬†



","GitHub - psf/requests: A simple, yet elegant HTTP library."
13,Python,"
 
 
  
 
 


Ansible
Ansible is a radically simple IT automation system. It handles
configuration management, application deployment, cloud provisioning,
ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex
changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible website.

Design Principles

Have a dead simple setup process and a minimal learning curve.
Manage machines very quickly and in parallel.
Avoid custom-agents and additional open ports, be agentless by
leveraging the existing SSH daemon.
Describe infrastructure in a language that is both machine and human
friendly.
Focus on security and easy auditability/review/rewriting of content.
Manage new remote machines instantly, without bootstrapping any
software.
Allow module development in any dynamic language, not just Python.
Be usable as non-root.
Be the easiest IT automation system to use, ever.


Use Ansible
You can install a released version of Ansible via pip, a package manager, or
our release repository. See our
installation guide for details on installing Ansible
on a variety of platforms.
Red Hat offers supported builds of Ansible Engine.
Power users and developers can run the devel branch, which has the latest
features and fixes, directly. Although it is reasonably stable, you are more likely to encounter
breaking changes when running the devel branch. We recommend getting involved
in the Ansible community if you want to run the devel branch.

Get Involved

Read Community
Information for all
kinds of ways to contribute to and interact with the project,
including mailing list information and how to submit bug reports and
code to Ansible.
Join a Working Group, an organized community devoted to a specific technology domain or platform.
Submit a proposed code update through a pull request to the devel branch.
Talk to us before making larger changes
to avoid duplicate efforts. This not only helps everyone
know what is going on, it also helps save time and effort if we decide
some changes are needed.
For a list of email lists, IRC channels and Working Groups, see the
Communication page


Branch Info

The devel branch corresponds to the release actively under development.
The stable-2.X branches correspond to stable releases.
Create a branch based on devel and set up a dev environment if you want to open a PR.
See the Ansible release and maintenance page for information about active branches.


Roadmap
Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).
The Ansible Roadmap page details what is planned and how to influence the roadmap.

Authors
Ansible was created by Michael DeHaan
and has contributions from over 4700 users (and growing). Thanks everyone!
Ansible is sponsored by Red Hat, Inc.

License
GNU General Public License v3.0 or later
See COPYING to see the full text.
","GitHub - ansible/ansible: Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy. Avoid writing scripts or custom code to deploy and update your applications ‚Äî automate in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com/ansible/"
14,Python,"    
 
 


scikit-learn
scikit-learn is a Python module for machine learning built on top of
SciPy and is distributed under the 3-Clause BSD license.
The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the About us page
for a list of core contributors.
It is currently maintained by a team of volunteers.
Website: http://scikit-learn.org

Installation

Dependencies
scikit-learn requires:

Python (>= 3.5)
NumPy (>= 1.11.0)
SciPy (>= 0.17.0)
joblib (>= 0.11)

Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.
scikit-learn 0.21 and later require Python 3.5 or newer.
Scikit-learn plotting capabilities (i.e., functions start with plot_
and classes end with ""Display"") require Matplotlib (>= 1.5.1). For running the
examples Matplotlib >= 1.5.1 is required. A few examples require
scikit-image >= 0.12.3, a few examples require pandas >= 0.18.0.

User installation
If you already have a working installation of numpy and scipy,
the easiest way to install scikit-learn is using pip
pip install -U scikit-learn

or conda:
conda install scikit-learn

The documentation includes more detailed installation instructions.

Changelog
See the changelog
for a history of notable changes to scikit-learn.

Development
We welcome new contributors of all experience levels. The scikit-learn
community goals are to be helpful, welcoming, and effective. The
Development Guide
has detailed information about contributing code, documentation, tests, and
more. We've included some basic information in this README.

Important links

Official source code repo: https://github.com/scikit-learn/scikit-learn
Download releases: https://pypi.org/project/scikit-learn/
Issue tracker: https://github.com/scikit-learn/scikit-learn/issues


Source code
You can check the latest sources with the command:
git clone https://github.com/scikit-learn/scikit-learn.git


Contributing
To learn more about making a contribution to scikit-learn, please see our
Contributing guide.

Testing
After installation, you can launch the test suite from outside the
source directory (you will need to have pytest >= 3.3.0 installed):
pytest sklearn

See the web page http://scikit-learn.org/dev/developers/advanced_installation.html#testing
for more information.

Random number generation can be controlled during testing by setting
the SKLEARN_SEED environment variable.

Submitting a Pull Request
Before opening a Pull Request, have a look at the
full Contributing page to make sure your code complies
with our guidelines: http://scikit-learn.org/stable/developers/index.html

Project History
The project was started in 2007 by David Cournapeau as a Google Summer
of Code project, and since then many volunteers have contributed. See
the About us page
for a list of core contributors.
The project is currently maintained by a team of volunteers.
Note: scikit-learn was previously referred to as scikits.learn.

Help and Support

Documentation

HTML documentation (stable release): http://scikit-learn.org
HTML documentation (development version): http://scikit-learn.org/dev/
FAQ: http://scikit-learn.org/stable/faq.html


Communication

Mailing list: https://mail.python.org/mailman/listinfo/scikit-learn
IRC channel: #scikit-learn at webchat.freenode.net
Stack Overflow: https://stackoverflow.com/questions/tagged/scikit-learn
Website: http://scikit-learn.org


Citation
If you use scikit-learn in a scientific publication, we would appreciate citations: http://scikit-learn.org/stable/about.html#citing-scikit-learn
",GitHub - scikit-learn/scikit-learn: scikit-learn: machine learning in Python
15,Python,"Scrapy













Overview
Scrapy is a fast high-level web crawling and web scraping framework, used to
crawl websites and extract structured data from their pages. It can be used for
a wide range of purposes, from data mining to monitoring and automated testing.
Check the Scrapy homepage at https://scrapy.org for more information,
including a list of features.

Requirements

Python 3.5+
Works on Linux, Windows, Mac OSX, BSD


Install
The quick way:
pip install scrapy

See the install section in the documentation at
https://docs.scrapy.org/en/latest/intro/install.html for more details.

Documentation
Documentation is available online at https://docs.scrapy.org/ and in the docs
directory.

Releases
You can check https://docs.scrapy.org/en/latest/news.html for the release notes.

Community (blog, twitter, mail list, IRC)
See https://scrapy.org/community/ for details.

Contributing
See https://docs.scrapy.org/en/master/contributing.html for details.

Code of Conduct
Please note that this project is released with a Contributor Code of Conduct
(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).
By participating in this project you agree to abide by its terms.
Please report unacceptable behavior to opensource@scrapinghub.com.

Companies using Scrapy
See https://scrapy.org/companies/ for a list.

Commercial Support
See https://scrapy.org/support/ for details.
","GitHub - scrapy/scrapy: Scrapy, a fast high-level web crawling & scraping framework for Python."
16,Python,"Big List of Naughty Strings
The Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer walks into a bar.
Why Test Naughty Strings?
Even multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a zero-width space (U+200B) on Twitter:

Although this is not a malicious error, and typical users aren't Tweeting weird unicode, an ""internal server error"" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.
Usage
blns.txt consists of newline-delimited strings and comments which are preceded with #. The comments divide the strings into sections for easy manual reading and copy/pasting into input forms. For those who want to access the strings programmatically, a blns.json file is provided containing an array with all the comments stripped out (the scripts folder contains a Python script used to generate the blns.json).
Contributions
Feel free to send a pull request to add more strings, or additional sections. However, please do not send pull requests with very-long strings (255+ characters), as that makes the list much more difficult to view.
Likewise, please do not send pull requests which compromise manual usability of the file. This includes the EICAR test string, which can cause the file to be flagged by antivirus scanners, and files which alter the encoding of blns.txt. Also, do not send a null character (U+0000) string, as it changes the file format on GitHub to binary and renders it unreadable in pull requests. Finally, when adding or removing a string please update all files when you perform a pull request.
Disclaimer
The Big List of Naughty Strings is intended to be used for software you own and manage. Some of the Naughty Strings can indicate security vulnerabilities, and as a result using such strings with third-party software may be a crime. The maintainer is not responsible for any negative actions that result from the use of the list.
Additionally, the Big List of Naughty Strings is not a fully-comprehensive substitute for formal security/penetration testing for your service.
Library / Packages
Various implementations of the Big List of Naughty Strings have made it to various package managers.  Those are maintained by outside parties, but can be found here:



Library
Link




Node
https://www.npmjs.com/package/blns


Node
https://www.npmjs.com/package/big-list-of-naughty-strings


.NET
https://github.com/SimonCropp/NaughtyStrings



Please open a PR to list others.
Maintainer/Creator
Max Woolf (@minimaxir)
Social Media Discussions

June 10, 2015 [Hacker News]: Show HN: Big List of Naughty Strings for testing user-input data
August 17, 2015 [Reddit]: Big list of naughty strings.
February 9, 2016 [Reddit]: Big List of Naughty Strings
January 15, 2017 [Hacker News]: Naughty Strings: A list of strings likely to cause issues as user-input data
January 16, 2017 [Reddit]: Naughty Strings: A list of strings likely to cause issues as user-input data
November 16, 2018 [Hacker News]: Big List of Naughty Strings
November 16, 2018 [Reddit]: Naughty Strings - A list of strings which have a high probability of causing issues when used as user-input data

License
MIT
",GitHub - minimaxir/big-list-of-naughty-strings: The Big List of Naughty Strings is a list of strings which have a high probability of causing issues when used as user-input data.
17,Python,"Removed according to regulations.
",GitHub - shadowsocks/shadowsocks
18,Python,"Face Recognition
You can also read a translated version of this file in Chinese ÁÆÄ‰Ωì‰∏≠ÊñáÁâà or in Korean ÌïúÍµ≠Ïñ¥.
Recognize and manipulate faces from Python or from the command line with
the world's simplest face recognition library.
Built using dlib's state-of-the-art face recognition
built with deep learning. The model has an accuracy of 99.38% on the
Labeled Faces in the Wild benchmark.
This also provides a simple face_recognition command line tool that lets
you do face recognition on a folder of images from the command line!



Features
Find faces in pictures
Find all the faces that appear in a picture:

import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_locations = face_recognition.face_locations(image)
Find and manipulate facial features in pictures
Get the locations and outlines of each person's eyes, nose, mouth and chin.

import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)
Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff
like applying digital make-up (think 'Meitu'):

Identify faces in pictures
Recognize who appears in each photo.

import face_recognition
known_image = face_recognition.load_image_file(""biden.jpg"")
unknown_image = face_recognition.load_image_file(""unknown.jpg"")

biden_encoding = face_recognition.face_encodings(known_image)[0]
unknown_encoding = face_recognition.face_encodings(unknown_image)[0]

results = face_recognition.compare_faces([biden_encoding], unknown_encoding)
You can even use this library with other Python libraries to do real-time face recognition:

See this example for the code.
Online Demos
User-contributed shared Jupyter notebook demo (not officially supported): 
Installation
Requirements

Python 3.3+ or Python 2.7
macOS or Linux (Windows not officially supported, but might work)

Installation Options:
Installing on Mac or Linux
First, make sure you have dlib already installed with Python bindings:

How to install dlib from source on macOS or Ubuntu

Then, install this module from pypi using pip3 (or pip2 for Python 2):
pip3 install face_recognition
Alternatively, you can try this library with Docker, see this section.
If you are having trouble with installation, you can also try out a
pre-configured VM.
Installing on an Nvidia Jetson Nano board

Jetson Nano installation instructions

Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.



Installing on Raspberry Pi 2+

Raspberry Pi 2+ installation instructions

Installing on Windows
While Windows isn't officially supported, helpful users have posted instructions on how to install this library:

@masoudr's Windows 10 installation guide (dlib + face_recognition)

Installing a pre-configured Virtual Machine image

Download the pre-configured VM image (for VMware Player or VirtualBox).

Usage
Command-Line Interface
When you install face_recognition, you get two simple command-line
programs:

face_recognition - Recognize faces in a photograph or folder full for
photographs.
face_detection - Find faces in a photograph or folder full for photographs.

face_recognition command line tool
The face_recognition command lets you recognize faces in a photograph or
folder full  for photographs.
First, you need to provide a folder with one picture of each person you
already know. There should be one image file for each person with the
files named according to who is in the picture:

Next, you need a second folder with the files you want to identify:

Then in you simply run the command face_recognition, passing in
the folder of known people and the folder (or single image) with unknown
people and it tells you who is in each image:
$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
There's one line in the output for each face. The data is comma-separated
with the filename and the name of the person found.
An unknown_person is a face in the image that didn't match anyone in
your folder of known people.
face_detection command line tool
The face_detection command lets you find the location (pixel coordinatates)
of any faces in an image.
Just run the command face_detection, passing in a folder of images
to check (or a single image):
$ face_detection  ./folder_with_pictures/

examples/image1.jpg,65,215,169,112
examples/image2.jpg,62,394,211,244
examples/image2.jpg,95,941,244,792
It prints one line for each face that was detected. The coordinates
reported are the top, right, bottom and left coordinates of the face (in pixels).
Adjusting Tolerance / Sensitivity
If you are getting multiple matches for the same person, it might be that
the people in your photos look very similar and a lower tolerance value
is needed to make face comparisons more strict.
You can do that with the --tolerance parameter. The default tolerance
value is 0.6 and lower numbers make face comparisons more strict:
$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
If you want to see the face distance calculated for each match in order
to adjust the tolerance setting, you can use --show-distance true:
$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None
More Examples
If you simply want to know the names of the people in each photograph but don't
care about file names, you could do this:
$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2

Barack Obama
unknown_person
Speeding up Face Recognition
Face recognition can be done in parallel if you have a computer with
multiple CPU cores. For example, if your system has 4 CPU cores, you can
process about 4 times as many images in the same amount of time by using
all your CPU cores in parallel.
If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:
$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/
You can also pass in --cpus -1 to use all CPU cores in your system.
Python Module
You can import the face_recognition module and then easily manipulate
faces with just a couple of lines of code. It's super easy!
API Docs: https://face-recognition.readthedocs.io.
Automatically find all the faces in an image
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image)

# face_locations is now an array listing the co-ordinates of each face!
See this example
to try it out.
You can also opt-in to a somewhat more accurate deep-learning-based face detection model.
Note: GPU acceleration (via NVidia's CUDA library) is required for good
performance with this model. You'll also want to enable CUDA support
when compliling dlib.
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image, model=""cnn"")

# face_locations is now an array listing the co-ordinates of each face!
See this example
to try it out.
If you have a lot of images and a GPU, you can also
find faces in batches.
Automatically locate the facial features of a person in an image
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)

# face_landmarks_list is now an array with the locations of each facial feature in each face.
# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.
See this example
to try it out.
Recognize faces in images and identify who they are
import face_recognition

picture_of_me = face_recognition.load_image_file(""me.jpg"")
my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]

# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!

unknown_picture = face_recognition.load_image_file(""unknown.jpg"")
unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]

# Now we can see the two face encodings are of the same person with `compare_faces`!

results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)

if results[0] == True:
    print(""It's a picture of me!"")
else:
    print(""It's not a picture of me!"")
See this example
to try it out.
Python Code Examples
All the examples are available here.
Face Detection

Find faces in a photograph
Find faces in a photograph (using deep learning)
Find faces in batches of images w/ GPU (using deep learning)
Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)

Facial Features

Identify specific facial features in a photograph
Apply (horribly ugly) digital make-up

Facial Recognition

Find and recognize unknown faces in a photograph based on photographs of known people
Identify and draw boxes around each person in a photo
Compare faces by numeric face distance instead of only True/False matches
Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)
Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)
Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)
Recognize faces on a Raspberry Pi w/ camera
Run a web service to recognize faces via HTTP (Requires Flask to be installed)
Recognize faces with a K-nearest neighbors classifier
Train multiple images per person then recognize faces using a SVM

Creating a Standalone Executable
If you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.
Articles and Guides that cover face_recognition

My article on how Face Recognition works: Modern Face Recognition with Deep Learning

Covers the algorithms and how they generally work


Face recognition with OpenCV, Python, and deep learning by Adrian Rosebrock

Covers how to use face recognition in practice


Raspberry Pi Face Recognition by Adrian Rosebrock

Covers how to use this on a Raspberry Pi


Face clustering with Python by Adrian Rosebrock

Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning



How Face Recognition Works
If you want to learn how face location and recognition work instead of
depending on a black box library, read my article.
Caveats

The face recognition model is trained on adults and does not work very well on children. It tends to mix
up children quite easy using the default comparison threshold of 0.6.
Accuracy may vary between ethnic groups. Please see this wiki page for more details.

Deployment to Cloud Hosts (Heroku, AWS, etc)
Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an app
using it to a cloud hosting provider like Heroku or AWS.
To make things easier, there's an example Dockerfile in this repo that shows how to run an app built with
face_recognition in a Docker container. With that, you should be able to deploy
to any service that supports Docker images.
You can try the Docker image locally by running: docker-compose up --build
Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.
Having problems?
If you run into problems, please read the Common Errors section of the wiki before filing a github issue.
Thanks

Many, many thanks to Davis King (@nulhom)
for creating dlib and for providing the trained facial feature detection and face encoding models
used in this library. For more information on the ResNet that powers the face encodings, check out
his blog post.
Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,
pillow, etc, etc that makes this kind of stuff so easy and fun in Python.
Thanks to Cookiecutter and the
audreyr/cookiecutter-pypackage project template
for making Python project packaging way more tolerable.

",GitHub - ageitgey/face_recognition: The world's simplest facial recognition api for Python and the command line
19,Python,"Home Assistant 

Open source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.
Check out home-assistant.io for a
demo, installation instructions,
tutorials and documentation.


Featured integrations

The system is built using a modular approach so support for other devices or actions can be implemented easily. See also the section on architecture and the section on creating your own
components.
If you run into issues while using Home Assistant or during development
of a component, check the Home Assistant help section of our website for further help and information.
",GitHub - home-assistant/home-assistant: Open source home automation that puts local control and privacy first
20,JavaScript,"




freeCodeCamp.org's open-source codebase and curriculum
freeCodeCamp.org is a friendly community where you can learn to code for free. It is run by a donor-supported 501(c)(3) nonprofit to help  millions of busy adults transition into tech. Our community has already helped more than 10,000 people get their first developer job.
Our full-stack web development curriculum is completely free and self-paced. We have thousands of interactive coding challenges to help you expand your skills.
Table of Contents

Certifications
The Learning Platform
Reporting Bugs and Issues
Reporting Security Issues
Contributing
Platform, Build and Deployment Status
License

Certifications
freeCodeCamp.org offers several free developer certifications. Each of these certifications involves building 5 required web app projects, along with hundreds of optional coding challenges to help you prepare for those projects. We estimate that each certification will take a beginning programmer around 300 hours to earn.
Each of these 30 projects in the freeCodeCamp.org curriculum has its own agile user stories and automated tests. These help you build up your project incrementally and ensure you've fulfilled all the user stories before you submit it.
You can pull in these test suites through freeCodeCamp's CDN. This means you can build these projects on websites like CodePen and Glitch - or even on your local computer's development environment.
Once you‚Äôve earned a certification, you will always have it. You will always be able to link to it from your LinkedIn or r√©sum√©. And when your prospective employers or freelance clients click that link, they‚Äôll see a verified certification specific to you.
The one exception to this is if we discover violations of our Academic Honesty Policy. When we catch people unambiguously plagiarizing (submitting other people's code or projects as their own without citation), we do what all rigorous institutions of learning should do - we revoke their certifications and ban those people.
Here are our six core certifications:
1. Responsive Web Design Certification

Basic HTML and HTML5
Basic CSS
Applied Visual Design
Applied Accessibility
Responsive Web Design Principles
CSS Flexbox
CSS Grid


Projects: Tribute Page, Survey Form, Product Landing Page, Technical Documentation Page, Personal Portfolio Webpage

2. JavaScript Algorithms and Data Structures Certification

Basic JavaScript
ES6
Regular Expressions
Debugging
Basic Data Structures
Algorithm Scripting
Object-Oriented Programming
Functional Programming
Intermediate Algorithm Scripting


Projects: Palindrome Checker, Roman Numeral Converter, Caesar's Cipher, Telephone Number Validator, Cash Register

3. Front End Libraries Certification

Bootstrap
jQuery
Sass
React
Redux
React and Redux


Projects: Random Quote Machine, Markdown Previewer, Drum Machine, JavaScript Calculator, Pomodoro Clock

4. Data Visualization Certification

Data Visualization with D3
JSON APIs and Ajax


Projects: Bar Chart, Scatterplot Graph, Heat Map, Choropleth Map, Treemap Diagram

5. APIs and Microservices Certification

Managing Packages with Npm
Basic Node and Express
MongoDB and Mongoose


Projects: Timestamp Microservice, Request Header Parser, URL Shortener, Exercise Tracker, File Metadata Microservice

6. Information Security and Quality Assurance Certification

Information Security with HelmetJS
Quality Assurance and Testing with Chai
Advanced Node and Express


Projects: Metric-Imperial Converter, Issue Tracker, Personal Library, Stock Price Checker, Anonymous Message Board

Full Stack Development Certification
Once you have earned all 6 of these certifications, you'll be able to claim your freeCodeCamp.org Full Stack Development Certification. This final distinction signifies that you‚Äôve completed around 1,800 hours of coding with a wide range of web development tools.
Legacy Certifications
We also have 3 legacy certifications from our 2015 curriculum, which are still available. All of the required projects for these legacy certifications will remain available on freeCodeCamp.org.

Legacy Front End Development Certification
Legacy Data Visualization Certification
Legacy Back End Development Certification

The Learning Platform
This code is running live at freeCodeCamp.org.
Our community also has:

A forum where you can usually get programming help or project feedback within hours.
A YouTube channel with free courses on Python, SQL, Android, and a wide variety of other technologies.
A podcast with technology insights and inspiring stories from developers.
A Developer News publication, a free, open source, no-ads place to cross-post your blog articles.


Join our community here.

Reporting Bugs and Issues
If you think you've found a bug, first read the how to report a bug article and follow its instructions.
If you're confident it's a new bug and have confirmed that someone else is facing the same issue, go ahead and create a new GitHub issue. Be sure to include as much information as possible so we can reproduce the bug.
Reporting Security Issues
If you think you have found a vulnerability, please report responsibly. Don't create GitHub issues for security issues. Instead, please send an email to security@freecodecamp.org and we'll look into it immediately.
Contributing

Please follow these steps to contribute.

Platform, Build and Deployment Status
The general platform status for all our applications is available at status.freecodecamp.org. The build and deployment status for the code is available in our DevOps Guide.
License
Copyright ¬© 2019 freeCodeCamp.org
The content of this repository is bound by the following licenses:

The computer software is licensed under the BSD-3-Clause license.
The learning resources in the /curriculum directory including their subdirectories thereon are licensed under the CC-BY-SA-4.0 license.

",GitHub - freeCodeCamp/freeCodeCamp: The https://www.freeCodeCamp.org open source codebase and curriculum. Learn to code for free together with millions of people.
21,JavaScript,"










Supporting Vue.js
Vue.js is an MIT-licensed open source project with its ongoing development made possible entirely by the support of these awesome backers. If you'd like to join them, please consider:

Become a backer or sponsor on Patreon.
Become a backer or sponsor on Open Collective.
One-time donation via PayPal or crypto-currencies.

What's the difference between Patreon and OpenCollective?
Funds donated via Patreon go directly to support Evan You's full-time work on Vue.js. Funds donated via OpenCollective are managed with transparent expenses and will be used for compensating work and expenses for core team members or sponsoring community events. Your name/logo will receive proper recognition and exposure by donating on either platform.
Special Sponsors





Platinum Sponsors


























Platinum Sponsors (China)











Gold Sponsors




































































































































































Sponsors via Open Collective
Platinum


Gold






Introduction
Vue (pronounced /vjuÀê/, like view) is a progressive framework for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.
Browser Compatibility
Vue.js supports all browsers that are ES5-compliant (IE8 and below are not supported).
Ecosystem



Project
Status
Description




vue-router

Single-page application routing


vuex

Large-scale state management


vue-cli

Project scaffolding


vue-loader

Single File Component (*.vue file) loader for webpack


vue-server-renderer

Server-side rendering support


vue-class-component

TypeScript decorator for a class-based API


vue-rx

RxJS integration


vue-devtools

Browser DevTools extension



Documentation
To check out live examples and docs, visit vuejs.org.
Questions
For questions and support please use the official forum or community chat. The issue list of this repo is exclusively for bug reports and feature requests.
Issues
Please make sure to read the Issue Reporting Checklist before opening an issue. Issues not conforming to the guidelines may be closed immediately.
Changelog
Detailed changes for each release are documented in the release notes.
Stay In Touch

Twitter
Blog
Job Board

Contribution
Please make sure to read the Contributing Guide before making a pull request. If you have a Vue-related project/component/tool, add it with a pull request to this curated list!
Thank you to all the people who already contributed to Vue!

License
MIT
Copyright (c) 2013-present, Yuxi (Evan) You
","GitHub - vuejs/vue: üññ Vue.js is a progressive, incrementally-adoptable JavaScript framework for building UI on the web."
22,JavaScript,"React ¬∑    
React is a JavaScript library for building user interfaces.

Declarative: React makes it painless to create interactive UIs. Design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes. Declarative views make your code more predictable, simpler to understand, and easier to debug.
Component-Based: Build encapsulated components that manage their own state, then compose them to make complex UIs. Since component logic is written in JavaScript instead of templates, you can easily pass rich data through your app and keep state out of the DOM.
Learn Once, Write Anywhere: We don't make assumptions about the rest of your technology stack, so you can develop new features in React without rewriting existing code. React can also render on the server using Node and power mobile apps using React Native.

Learn how to use React in your own project.
Installation
React has been designed for gradual adoption from the start, and you can use as little or as much React as you need:

Use Online Playgrounds to get a taste of React.
Add React to a Website as a <script> tag in one minute.
Create a New React App if you're looking for a powerful JavaScript toolchain.

You can use React as a <script> tag from a CDN, or as a react package on npm.
Documentation
You can find the React documentation on the website.
Check out the Getting Started page for a quick overview.
The documentation is divided into several sections:

Tutorial
Main Concepts
Advanced Guides
API Reference
Where to Get Support
Contributing Guide

You can improve it by sending pull requests to this repository.
Examples
We have several examples on the website. Here is the first one to get you started:
function HelloMessage({ name }) {
  return <div>Hello {name}</div>;
}

ReactDOM.render(
  <HelloMessage name=""Taylor"" />,
  document.getElementById('container')
);
This example will render ""Hello Taylor"" into a container on the page.
You'll notice that we used an HTML-like syntax; we call it JSX. JSX is not required to use React, but it makes code more readable, and writing it feels like writing HTML. If you're using React as a <script> tag, read this section on integrating JSX; otherwise, the recommended JavaScript toolchains handle it automatically.
Contributing
The main purpose of this repository is to continue to evolve React core, making it faster and easier to use. Development of React happens in the open on GitHub, and we are grateful to the community for contributing bugfixes and improvements. Read below to learn how you can take part in improving React.
Code of Conduct
Facebook has adopted a Code of Conduct that we expect project participants to adhere to. Please read the full text so that you can understand what actions will and will not be tolerated.
Contributing Guide
Read our contributing guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React.
Good First Issues
To help you get your feet wet and get you familiar with our contribution process, we have a list of good first issues that contain bugs which have a relatively limited scope. This is a great place to get started.
License
React is MIT licensed.
","GitHub - facebook/react: A declarative, efficient, and flexible JavaScript library for building user interfaces."
23,JavaScript,"




Bootstrap

  Sleek, intuitive, and powerful front-end framework for faster and easier web development.
  
Explore Bootstrap docs ¬ª


Report bug
  ¬∑
  Request feature
  ¬∑
  Themes
  ¬∑
  Blog

Table of contents

Quick start
Status
What's included
Bugs and feature requests
Documentation
Contributing
Community
Versioning
Creators
Thanks
Copyright and license

Quick start
Several quick start options are available:

Download the latest release.
Clone the repo: git clone https://github.com/twbs/bootstrap.git
Install with npm: npm install bootstrap
Install with yarn: yarn add bootstrap@4.3.1
Install with Composer: composer require twbs/bootstrap:4.3.1
Install with NuGet: CSS: Install-Package bootstrap Sass: Install-Package bootstrap.sass

Read the Getting started page for information on the framework contents, templates and examples, and more.
Status














What's included
Within the download you'll find the following directories and files, logically grouping common assets and providing both compiled and minified variations. You'll see something like this:
bootstrap/
‚îî‚îÄ‚îÄ dist/
    ‚îú‚îÄ‚îÄ css/
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.min.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-grid.min.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.min.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-reboot.min.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.min.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap-utilities.min.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.css
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.css.map
    ‚îÇ   ‚îú‚îÄ‚îÄ bootstrap.min.css
    ‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.min.css.map
    ‚îî‚îÄ‚îÄ js/
        ‚îú‚îÄ‚îÄ bootstrap.bundle.js
        ‚îú‚îÄ‚îÄ bootstrap.bundle.js.map
        ‚îú‚îÄ‚îÄ bootstrap.bundle.min.js
        ‚îú‚îÄ‚îÄ bootstrap.bundle.min.js.map
        ‚îú‚îÄ‚îÄ bootstrap.esm.js
        ‚îú‚îÄ‚îÄ bootstrap.esm.js.map
        ‚îú‚îÄ‚îÄ bootstrap.esm.min.js
        ‚îú‚îÄ‚îÄ bootstrap.esm.min.js.map
        ‚îú‚îÄ‚îÄ bootstrap.js
        ‚îú‚îÄ‚îÄ bootstrap.js.map
        ‚îú‚îÄ‚îÄ bootstrap.min.js
        ‚îî‚îÄ‚îÄ bootstrap.min.js.map

We provide compiled CSS and JS (bootstrap.*), as well as compiled and minified CSS and JS (bootstrap.min.*). source maps (bootstrap.*.map) are available for use with certain browsers' developer tools. Bundled JS files (bootstrap.bundle.js and minified bootstrap.bundle.min.js) include Popper.
Bugs and feature requests
Have a bug or a feature request? Please first read the issue guidelines and search for existing and closed issues. If your problem or idea is not addressed yet, please open a new issue.
Documentation
Bootstrap's documentation, included in this repo in the root directory, is built with Hugo and publicly hosted on GitHub Pages at https://getbootstrap.com/. The docs may also be run locally.
Documentation search is powered by Algolia's DocSearch. Working on our search? Be sure to set debug: true in site/assets/js/src/search.js file.
Running documentation locally

Run npm install to install the Node.js dependencies, including Hugo (the site builder).
Run npm run test (or a specific npm script) to rebuild distributed CSS and JavaScript files, as well as our docs assets.
From the root /bootstrap directory, run npm run docs-serve in the command line.
Open http://localhost:9001/ in your browser, and voil√†.

Learn more about using Hugo by reading its documentation.
Documentation for previous releases
You can find all our previous releases docs on https://getbootstrap.com/docs/versions/.
Previous releases and their documentation are also available for download.
Contributing
Please read through our contributing guidelines. Included are directions for opening issues, coding standards, and notes on development.
Moreover, if your pull request contains JavaScript patches or features, you must include relevant unit tests. All HTML and CSS should conform to the Code Guide, maintained by Mark Otto.
Editor preferences are available in the editor config for easy use in common text editors. Read more and download plugins at https://editorconfig.org/.
Community
Get updates on Bootstrap's development and chat with the project maintainers and community members.

Follow @getbootstrap on Twitter.
Read and subscribe to The Official Bootstrap Blog.
Join the official Slack room.
Chat with fellow Bootstrappers in IRC. On the irc.freenode.net server, in the ##bootstrap channel.
Implementation help may be found at Stack Overflow (tagged bootstrap-4).
Developers should use the keyword bootstrap on packages which modify or add to the functionality of Bootstrap when distributing through npm or similar delivery mechanisms for maximum discoverability.

Versioning
For transparency into our release cycle and in striving to maintain backward compatibility, Bootstrap is maintained under the Semantic Versioning guidelines. Sometimes we screw up, but we adhere to those rules whenever possible.
See the Releases section of our GitHub project for changelogs for each release version of Bootstrap. Release announcement posts on the official Bootstrap blog contain summaries of the most noteworthy changes made in each release.
Creators
Mark Otto

https://twitter.com/mdo
https://github.com/mdo

Jacob Thornton

https://twitter.com/fat
https://github.com/fat

Thanks



Thanks to BrowserStack for providing the infrastructure that allows us to test in real browsers!
Backers
Thank you to all our backers! üôè [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]










Copyright and license
Code and documentation copyright 2011-2019 the Bootstrap Authors and Twitter, Inc. Code released under the MIT License. Docs released under Creative Commons.
","GitHub - twbs/bootstrap: The most popular HTML, CSS, and JavaScript framework for developing responsive, mobile first projects on the web."
24,JavaScript,"Airbnb JavaScript Style Guide() {
A mostly reasonable approach to JavaScript

Note: this guide assumes you are using Babel, and requires that you use babel-preset-airbnb or the equivalent. It also assumes you are installing shims/polyfills in your app, with airbnb-browser-shims or the equivalent.




This guide is available in other languages too. See Translation
Other Style Guides

ES5 (Deprecated)
React
CSS-in-JavaScript
CSS & Sass
Ruby

Table of Contents

Types
References
Objects
Arrays
Destructuring
Strings
Functions
Arrow Functions
Classes & Constructors
Modules
Iterators and Generators
Properties
Variables
Hoisting
Comparison Operators & Equality
Blocks
Control Statements
Comments
Whitespace
Commas
Semicolons
Type Casting & Coercion
Naming Conventions
Accessors
Events
jQuery
ECMAScript 5 Compatibility
ECMAScript 6+ (ES 2015+) Styles
Standard Library
Testing
Performance
Resources
In the Wild
Translation
The JavaScript Style Guide Guide
Chat With Us About JavaScript
Contributors
License
Amendments

Types



1.1 Primitives: When you access a primitive type you work directly on its value.

string
number
boolean
null
undefined
symbol

const foo = 1;
let bar = foo;

bar = 9;

console.log(foo, bar); // => 1, 9

Symbols cannot be faithfully polyfilled, so they should not be used when targeting browsers/environments that don‚Äôt support them natively.






1.2 Complex: When you access a complex type you work on a reference to its value.

object
array
function

const foo = [1, 2];
const bar = foo;

bar[0] = 9;

console.log(foo[0], bar[0]); // => 9, 9


‚¨Ü back to top
References



2.1 Use const for all of your references; avoid using var. eslint: prefer-const, no-const-assign

Why? This ensures that you can‚Äôt reassign your references, which can lead to bugs and difficult to comprehend code.

// bad
var a = 1;
var b = 2;

// good
const a = 1;
const b = 2;





2.2 If you must reassign references, use let instead of var. eslint: no-var

Why? let is block-scoped rather than function-scoped like var.

// bad
var count = 1;
if (true) {
  count += 1;
}

// good, use the let.
let count = 1;
if (true) {
  count += 1;
}





2.3 Note that both let and const are block-scoped.
// const and let only exist in the blocks they are defined in.
{
  let a = 1;
  const b = 1;
}
console.log(a); // ReferenceError
console.log(b); // ReferenceError


‚¨Ü back to top
Objects



3.1 Use the literal syntax for object creation. eslint: no-new-object
// bad
const item = new Object();

// good
const item = {};





3.2 Use computed property names when creating objects with dynamic property names.

Why? They allow you to define all the properties of an object in one place.

function getKey(k) {
  return `a key named ${k}`;
}

// bad
const obj = {
  id: 5,
  name: 'San Francisco',
};
obj[getKey('enabled')] = true;

// good
const obj = {
  id: 5,
  name: 'San Francisco',
  [getKey('enabled')]: true,
};





3.3 Use object method shorthand. eslint: object-shorthand
// bad
const atom = {
  value: 1,

  addValue: function (value) {
    return atom.value + value;
  },
};

// good
const atom = {
  value: 1,

  addValue(value) {
    return atom.value + value;
  },
};





3.4 Use property value shorthand. eslint: object-shorthand

Why? It is shorter and descriptive.

const lukeSkywalker = 'Luke Skywalker';

// bad
const obj = {
  lukeSkywalker: lukeSkywalker,
};

// good
const obj = {
  lukeSkywalker,
};





3.5 Group your shorthand properties at the beginning of your object declaration.

Why? It‚Äôs easier to tell which properties are using the shorthand.

const anakinSkywalker = 'Anakin Skywalker';
const lukeSkywalker = 'Luke Skywalker';

// bad
const obj = {
  episodeOne: 1,
  twoJediWalkIntoACantina: 2,
  lukeSkywalker,
  episodeThree: 3,
  mayTheFourth: 4,
  anakinSkywalker,
};

// good
const obj = {
  lukeSkywalker,
  anakinSkywalker,
  episodeOne: 1,
  twoJediWalkIntoACantina: 2,
  episodeThree: 3,
  mayTheFourth: 4,
};





3.6 Only quote properties that are invalid identifiers. eslint: quote-props

Why? In general we consider it subjectively easier to read. It improves syntax highlighting, and is also more easily optimized by many JS engines.

// bad
const bad = {
  'foo': 3,
  'bar': 4,
  'data-blah': 5,
};

// good
const good = {
  foo: 3,
  bar: 4,
  'data-blah': 5,
};





3.7 Do not call Object.prototype methods directly, such as hasOwnProperty, propertyIsEnumerable, and isPrototypeOf. eslint: no-prototype-builtins

Why? These methods may be shadowed by properties on the object in question - consider { hasOwnProperty: false } - or, the object may be a null object (Object.create(null)).

// bad
console.log(object.hasOwnProperty(key));

// good
console.log(Object.prototype.hasOwnProperty.call(object, key));

// best
const has = Object.prototype.hasOwnProperty; // cache the lookup once, in module scope.
console.log(has.call(object, key));
/* or */
import has from 'has'; // https://www.npmjs.com/package/has
console.log(has(object, key));





3.8 Prefer the object spread operator over Object.assign to shallow-copy objects. Use the object rest operator to get a new object with certain properties omitted.
// very bad
const original = { a: 1, b: 2 };
const copy = Object.assign(original, { c: 3 }); // this mutates `original` ‡≤†_‡≤†
delete copy.a; // so does this

// bad
const original = { a: 1, b: 2 };
const copy = Object.assign({}, original, { c: 3 }); // copy => { a: 1, b: 2, c: 3 }

// good
const original = { a: 1, b: 2 };
const copy = { ...original, c: 3 }; // copy => { a: 1, b: 2, c: 3 }

const { a, ...noA } = copy; // noA => { b: 2, c: 3 }


‚¨Ü back to top
Arrays



4.1 Use the literal syntax for array creation. eslint: no-array-constructor
// bad
const items = new Array();

// good
const items = [];





4.2 Use Array#push instead of direct assignment to add items to an array.
const someStack = [];

// bad
someStack[someStack.length] = 'abracadabra';

// good
someStack.push('abracadabra');





4.3 Use array spreads ... to copy arrays.
// bad
const len = items.length;
const itemsCopy = [];
let i;

for (i = 0; i < len; i += 1) {
  itemsCopy[i] = items[i];
}

// good
const itemsCopy = [...items];






4.4 To convert an iterable object to an array, use spreads ... instead of Array.from.
const foo = document.querySelectorAll('.foo');

// good
const nodes = Array.from(foo);

// best
const nodes = [...foo];





4.5 Use Array.from for converting an array-like object to an array.
const arrLike = { 0: 'foo', 1: 'bar', 2: 'baz', length: 3 };

// bad
const arr = Array.prototype.slice.call(arrLike);

// good
const arr = Array.from(arrLike);





4.6 Use Array.from instead of spread ... for mapping over iterables, because it avoids creating an intermediate array.
// bad
const baz = [...foo].map(bar);

// good
const baz = Array.from(foo, bar);





4.7 Use return statements in array method callbacks. It‚Äôs ok to omit the return if the function body consists of a single statement returning an expression without side effects, following 8.2. eslint: array-callback-return
// good
[1, 2, 3].map((x) => {
  const y = x + 1;
  return x * y;
});

// good
[1, 2, 3].map((x) => x + 1);

// bad - no returned value means `acc` becomes undefined after the first iteration
[[0, 1], [2, 3], [4, 5]].reduce((acc, item, index) => {
  const flatten = acc.concat(item);
});

// good
[[0, 1], [2, 3], [4, 5]].reduce((acc, item, index) => {
  const flatten = acc.concat(item);
  return flatten;
});

// bad
inbox.filter((msg) => {
  const { subject, author } = msg;
  if (subject === 'Mockingbird') {
    return author === 'Harper Lee';
  } else {
    return false;
  }
});

// good
inbox.filter((msg) => {
  const { subject, author } = msg;
  if (subject === 'Mockingbird') {
    return author === 'Harper Lee';
  }

  return false;
});





4.8 Use line breaks after open and before close array brackets if an array has multiple lines
// bad
const arr = [
  [0, 1], [2, 3], [4, 5],
];

const objectInArray = [{
  id: 1,
}, {
  id: 2,
}];

const numberInArray = [
  1, 2,
];

// good
const arr = [[0, 1], [2, 3], [4, 5]];

const objectInArray = [
  {
    id: 1,
  },
  {
    id: 2,
  },
];

const numberInArray = [
  1,
  2,
];


‚¨Ü back to top
Destructuring



5.1 Use object destructuring when accessing and using multiple properties of an object. eslint: prefer-destructuring

Why? Destructuring saves you from creating temporary references for those properties.

// bad
function getFullName(user) {
  const firstName = user.firstName;
  const lastName = user.lastName;

  return `${firstName} ${lastName}`;
}

// good
function getFullName(user) {
  const { firstName, lastName } = user;
  return `${firstName} ${lastName}`;
}

// best
function getFullName({ firstName, lastName }) {
  return `${firstName} ${lastName}`;
}





5.2 Use array destructuring. eslint: prefer-destructuring
const arr = [1, 2, 3, 4];

// bad
const first = arr[0];
const second = arr[1];

// good
const [first, second] = arr;





5.3 Use object destructuring for multiple return values, not array destructuring.

Why? You can add new properties over time or change the order of things without breaking call sites.

// bad
function processInput(input) {
  // then a miracle occurs
  return [left, right, top, bottom];
}

// the caller needs to think about the order of return data
const [left, __, top] = processInput(input);

// good
function processInput(input) {
  // then a miracle occurs
  return { left, right, top, bottom };
}

// the caller selects only the data they need
const { left, top } = processInput(input);


‚¨Ü back to top
Strings



6.1 Use single quotes '' for strings. eslint: quotes
// bad
const name = ""Capt. Janeway"";

// bad - template literals should contain interpolation or newlines
const name = `Capt. Janeway`;

// good
const name = 'Capt. Janeway';





6.2 Strings that cause the line to go over 100 characters should not be written across multiple lines using string concatenation.

Why? Broken strings are painful to work with and make code less searchable.

// bad
const errorMessage = 'This is a super long error that was thrown because \
of Batman. When you stop to think about how Batman had anything to do \
with this, you would get nowhere \
fast.';

// bad
const errorMessage = 'This is a super long error that was thrown because ' +
  'of Batman. When you stop to think about how Batman had anything to do ' +
  'with this, you would get nowhere fast.';

// good
const errorMessage = 'This is a super long error that was thrown because of Batman. When you stop to think about how Batman had anything to do with this, you would get nowhere fast.';





6.3 When programmatically building up strings, use template strings instead of concatenation. eslint: prefer-template template-curly-spacing

Why? Template strings give you a readable, concise syntax with proper newlines and string interpolation features.

// bad
function sayHi(name) {
  return 'How are you, ' + name + '?';
}

// bad
function sayHi(name) {
  return ['How are you, ', name, '?'].join();
}

// bad
function sayHi(name) {
  return `How are you, ${ name }?`;
}

// good
function sayHi(name) {
  return `How are you, ${name}?`;
}




6.4 Never use eval() on a string, it opens too many vulnerabilities. eslint: no-eval




6.5 Do not unnecessarily escape characters in strings. eslint: no-useless-escape

Why? Backslashes harm readability, thus they should only be present when necessary.

// bad
const foo = '\'this\' \i\s \""quoted\""';

// good
const foo = '\'this\' is ""quoted""';
const foo = `my name is '${name}'`;


‚¨Ü back to top
Functions



7.1 Use named function expressions instead of function declarations. eslint: func-style

Why? Function declarations are hoisted, which means that it‚Äôs easy - too easy - to reference the function before it is defined in the file. This harms readability and maintainability. If you find that a function‚Äôs definition is large or complex enough that it is interfering with understanding the rest of the file, then perhaps it‚Äôs time to extract it to its own module! Don‚Äôt forget to explicitly name the expression, regardless of whether or not the name is inferred from the containing variable (which is often the case in modern browsers or when using compilers such as Babel). This eliminates any assumptions made about the Error‚Äôs call stack. (Discussion)

// bad
function foo() {
  // ...
}

// bad
const foo = function () {
  // ...
};

// good
// lexical name distinguished from the variable-referenced invocation(s)
const short = function longUniqueMoreDescriptiveLexicalFoo() {
  // ...
};





7.2 Wrap immediately invoked function expressions in parentheses. eslint: wrap-iife

Why? An immediately invoked function expression is a single unit - wrapping both it, and its invocation parens, in parens, cleanly expresses this. Note that in a world with modules everywhere, you almost never need an IIFE.

// immediately-invoked function expression (IIFE)
(function () {
  console.log('Welcome to the Internet. Please follow me.');
}());




7.3 Never declare a function in a non-function block (if, while, etc). Assign the function to a variable instead. Browsers will allow you to do it, but they all interpret it differently, which is bad news bears. eslint: no-loop-func




7.4 Note: ECMA-262 defines a block as a list of statements. A function declaration is not a statement.
// bad
if (currentUser) {
  function test() {
    console.log('Nope.');
  }
}

// good
let test;
if (currentUser) {
  test = () => {
    console.log('Yup.');
  };
}





7.5 Never name a parameter arguments. This will take precedence over the arguments object that is given to every function scope.
// bad
function foo(name, options, arguments) {
  // ...
}

// good
function foo(name, options, args) {
  // ...
}





7.6 Never use arguments, opt to use rest syntax ... instead. eslint: prefer-rest-params

Why? ... is explicit about which arguments you want pulled. Plus, rest arguments are a real Array, and not merely Array-like like arguments.

// bad
function concatenateAll() {
  const args = Array.prototype.slice.call(arguments);
  return args.join('');
}

// good
function concatenateAll(...args) {
  return args.join('');
}





7.7 Use default parameter syntax rather than mutating function arguments.
// really bad
function handleThings(opts) {
  // No! We shouldn‚Äôt mutate function arguments.
  // Double bad: if opts is falsy it'll be set to an object which may
  // be what you want but it can introduce subtle bugs.
  opts = opts || {};
  // ...
}

// still bad
function handleThings(opts) {
  if (opts === void 0) {
    opts = {};
  }
  // ...
}

// good
function handleThings(opts = {}) {
  // ...
}





7.8 Avoid side effects with default parameters.

Why? They are confusing to reason about.

var b = 1;
// bad
function count(a = b++) {
  console.log(a);
}
count();  // 1
count();  // 2
count(3); // 3
count();  // 3





7.9 Always put default parameters last.
// bad
function handleThings(opts = {}, name) {
  // ...
}

// good
function handleThings(name, opts = {}) {
  // ...
}





7.10 Never use the Function constructor to create a new function. eslint: no-new-func

Why? Creating a function in this way evaluates a string similarly to eval(), which opens vulnerabilities.

// bad
var add = new Function('a', 'b', 'return a + b');

// still bad
var subtract = Function('a', 'b', 'return a - b');





7.11 Spacing in a function signature. eslint: space-before-function-paren space-before-blocks

Why? Consistency is good, and you shouldn‚Äôt have to add or remove a space when adding or removing a name.

// bad
const f = function(){};
const g = function (){};
const h = function() {};

// good
const x = function () {};
const y = function a() {};





7.12 Never mutate parameters. eslint: no-param-reassign

Why? Manipulating objects passed in as parameters can cause unwanted variable side effects in the original caller.

// bad
function f1(obj) {
  obj.key = 1;
}

// good
function f2(obj) {
  const key = Object.prototype.hasOwnProperty.call(obj, 'key') ? obj.key : 1;
}





7.13 Never reassign parameters. eslint: no-param-reassign

Why? Reassigning parameters can lead to unexpected behavior, especially when accessing the arguments object. It can also cause optimization issues, especially in V8.

// bad
function f1(a) {
  a = 1;
  // ...
}

function f2(a) {
  if (!a) { a = 1; }
  // ...
}

// good
function f3(a) {
  const b = a || 1;
  // ...
}

function f4(a = 1) {
  // ...
}





7.14 Prefer the use of the spread operator ... to call variadic functions. eslint: prefer-spread

Why? It‚Äôs cleaner, you don‚Äôt need to supply a context, and you can not easily compose new with apply.

// bad
const x = [1, 2, 3, 4, 5];
console.log.apply(console, x);

// good
const x = [1, 2, 3, 4, 5];
console.log(...x);

// bad
new (Function.prototype.bind.apply(Date, [null, 2016, 8, 5]));

// good
new Date(...[2016, 8, 5]);





7.15 Functions with multiline signatures, or invocations, should be indented just like every other multiline list in this guide: with each item on a line by itself, with a trailing comma on the last item. eslint: function-paren-newline
// bad
function foo(bar,
             baz,
             quux) {
  // ...
}

// good
function foo(
  bar,
  baz,
  quux,
) {
  // ...
}

// bad
console.log(foo,
  bar,
  baz);

// good
console.log(
  foo,
  bar,
  baz,
);


‚¨Ü back to top
Arrow Functions



8.1 When you must use an anonymous function (as when passing an inline callback), use arrow function notation. eslint: prefer-arrow-callback, arrow-spacing

Why? It creates a version of the function that executes in the context of this, which is usually what you want, and is a more concise syntax.


Why not? If you have a fairly complicated function, you might move that logic out into its own named function expression.

// bad
[1, 2, 3].map(function (x) {
  const y = x + 1;
  return x * y;
});

// good
[1, 2, 3].map((x) => {
  const y = x + 1;
  return x * y;
});





8.2 If the function body consists of a single statement returning an expression without side effects, omit the braces and use the implicit return. Otherwise, keep the braces and use a return statement. eslint: arrow-parens, arrow-body-style

Why? Syntactic sugar. It reads well when multiple functions are chained together.

// bad
[1, 2, 3].map((number) => {
  const nextNumber = number + 1;
  `A string containing the ${nextNumber}.`;
});

// good
[1, 2, 3].map((number) => `A string containing the ${number + 1}.`);

// good
[1, 2, 3].map((number) => {
  const nextNumber = number + 1;
  return `A string containing the ${nextNumber}.`;
});

// good
[1, 2, 3].map((number, index) => ({
  [index]: number,
}));

// No implicit return with side effects
function foo(callback) {
  const val = callback();
  if (val === true) {
    // Do something if callback returns true
  }
}

let bool = false;

// bad
foo(() => bool = true);

// good
foo(() => {
  bool = true;
});





8.3 In case the expression spans over multiple lines, wrap it in parentheses for better readability.

Why? It shows clearly where the function starts and ends.

// bad
['get', 'post', 'put'].map((httpMethod) => Object.prototype.hasOwnProperty.call(
    httpMagicObjectWithAVeryLongName,
    httpMethod,
  )
);

// good
['get', 'post', 'put'].map((httpMethod) => (
  Object.prototype.hasOwnProperty.call(
    httpMagicObjectWithAVeryLongName,
    httpMethod,
  )
));





8.4 Always include parentheses around arguments for clarity and consistency. eslint: arrow-parens

Why? Minimizes diff churn when adding or removing arguments.

// bad
[1, 2, 3].map(x => x * x);

// good
[1, 2, 3].map((x) => x * x);

// bad
[1, 2, 3].map(number => (
  `A long string with the ${number}. It‚Äôs so long that we don‚Äôt want it to take up space on the .map line!`
));

// good
[1, 2, 3].map((number) => (
  `A long string with the ${number}. It‚Äôs so long that we don‚Äôt want it to take up space on the .map line!`
));

// bad
[1, 2, 3].map(x => {
  const y = x + 1;
  return x * y;
});

// good
[1, 2, 3].map((x) => {
  const y = x + 1;
  return x * y;
});





8.5 Avoid confusing arrow function syntax (=>) with comparison operators (<=, >=). eslint: no-confusing-arrow
// bad
const itemHeight = (item) => item.height <= 256 ? item.largeSize : item.smallSize;

// bad
const itemHeight = (item) => item.height >= 256 ? item.largeSize : item.smallSize;

// good
const itemHeight = (item) => (item.height <= 256 ? item.largeSize : item.smallSize);

// good
const itemHeight = (item) => {
  const { height, largeSize, smallSize } = item;
  return height <= 256 ? largeSize : smallSize;
};





8.6 Enforce the location of arrow function bodies with implicit returns. eslint: implicit-arrow-linebreak
// bad
(foo) =>
  bar;

(foo) =>
  (bar);

// good
(foo) => bar;
(foo) => (bar);
(foo) => (
   bar
)


‚¨Ü back to top
Classes & Constructors



9.1 Always use class. Avoid manipulating prototype directly.

Why? class syntax is more concise and easier to reason about.

// bad
function Queue(contents = []) {
  this.queue = [...contents];
}
Queue.prototype.pop = function () {
  const value = this.queue[0];
  this.queue.splice(0, 1);
  return value;
};

// good
class Queue {
  constructor(contents = []) {
    this.queue = [...contents];
  }
  pop() {
    const value = this.queue[0];
    this.queue.splice(0, 1);
    return value;
  }
}





9.2 Use extends for inheritance.

Why? It is a built-in way to inherit prototype functionality without breaking instanceof.

// bad
const inherits = require('inherits');
function PeekableQueue(contents) {
  Queue.apply(this, contents);
}
inherits(PeekableQueue, Queue);
PeekableQueue.prototype.peek = function () {
  return this.queue[0];
};

// good
class PeekableQueue extends Queue {
  peek() {
    return this.queue[0];
  }
}





9.3 Methods can return this to help with method chaining.
// bad
Jedi.prototype.jump = function () {
  this.jumping = true;
  return true;
};

Jedi.prototype.setHeight = function (height) {
  this.height = height;
};

const luke = new Jedi();
luke.jump(); // => true
luke.setHeight(20); // => undefined

// good
class Jedi {
  jump() {
    this.jumping = true;
    return this;
  }

  setHeight(height) {
    this.height = height;
    return this;
  }
}

const luke = new Jedi();

luke.jump()
  .setHeight(20);





9.4 It‚Äôs okay to write a custom toString() method, just make sure it works successfully and causes no side effects.
class Jedi {
  constructor(options = {}) {
    this.name = options.name || 'no name';
  }

  getName() {
    return this.name;
  }

  toString() {
    return `Jedi - ${this.getName()}`;
  }
}





9.5 Classes have a default constructor if one is not specified. An empty constructor function or one that just delegates to a parent class is unnecessary. eslint: no-useless-constructor
// bad
class Jedi {
  constructor() {}

  getName() {
    return this.name;
  }
}

// bad
class Rey extends Jedi {
  constructor(...args) {
    super(...args);
  }
}

// good
class Rey extends Jedi {
  constructor(...args) {
    super(...args);
    this.name = 'Rey';
  }
}





9.6 Avoid duplicate class members. eslint: no-dupe-class-members

Why? Duplicate class member declarations will silently prefer the last one - having duplicates is almost certainly a bug.

// bad
class Foo {
  bar() { return 1; }
  bar() { return 2; }
}

// good
class Foo {
  bar() { return 1; }
}

// good
class Foo {
  bar() { return 2; }
}





9.7 Class methods should use this or be made into a static method unless an external library or framework requires to use specific non-static methods. Being an instance method should indicate that it behaves differently based on properties of the receiver. eslint: class-methods-use-this
// bad
class Foo {
  bar() {
    console.log('bar');
  }
}

// good - this is used
class Foo {
  bar() {
    console.log(this.bar);
  }
}

// good - constructor is exempt
class Foo {
  constructor() {
    // ...
  }
}

// good - static methods aren't expected to use this
class Foo {
  static bar() {
    console.log('bar');
  }
}


‚¨Ü back to top
Modules



10.1 Always use modules (import/export) over a non-standard module system. You can always transpile to your preferred module system.

Why? Modules are the future, let‚Äôs start using the future now.

// bad
const AirbnbStyleGuide = require('./AirbnbStyleGuide');
module.exports = AirbnbStyleGuide.es6;

// ok
import AirbnbStyleGuide from './AirbnbStyleGuide';
export default AirbnbStyleGuide.es6;

// best
import { es6 } from './AirbnbStyleGuide';
export default es6;





10.2 Do not use wildcard imports.

Why? This makes sure you have a single default export.

// bad
import * as AirbnbStyleGuide from './AirbnbStyleGuide';

// good
import AirbnbStyleGuide from './AirbnbStyleGuide';





10.3 And do not export directly from an import.

Why? Although the one-liner is concise, having one clear way to import and one clear way to export makes things consistent.

// bad
// filename es6.js
export { es6 as default } from './AirbnbStyleGuide';

// good
// filename es6.js
import { es6 } from './AirbnbStyleGuide';
export default es6;





10.4 Only import from a path in one place.
eslint: no-duplicate-imports

Why? Having multiple lines that import from the same path can make code harder to maintain.

// bad
import foo from 'foo';
// ‚Ä¶ some other imports ‚Ä¶ //
import { named1, named2 } from 'foo';

// good
import foo, { named1, named2 } from 'foo';

// good
import foo, {
  named1,
  named2,
} from 'foo';





10.5 Do not export mutable bindings.
eslint: import/no-mutable-exports

Why? Mutation should be avoided in general, but in particular when exporting mutable bindings. While this technique may be needed for some special cases, in general, only constant references should be exported.

// bad
let foo = 3;
export { foo };

// good
const foo = 3;
export { foo };





10.6 In modules with a single export, prefer default export over named export.
eslint: import/prefer-default-export

Why? To encourage more files that only ever export one thing, which is better for readability and maintainability.

// bad
export function foo() {}

// good
export default function foo() {}





10.7 Put all imports above non-import statements.
eslint: import/first

Why? Since imports are hoisted, keeping them all at the top prevents surprising behavior.

// bad
import foo from 'foo';
foo.init();

import bar from 'bar';

// good
import foo from 'foo';
import bar from 'bar';

foo.init();





10.8 Multiline imports should be indented just like multiline array and object literals.

Why? The curly braces follow the same indentation rules as every other curly brace block in the style guide, as do the trailing commas.

// bad
import {longNameA, longNameB, longNameC, longNameD, longNameE} from 'path';

// good
import {
  longNameA,
  longNameB,
  longNameC,
  longNameD,
  longNameE,
} from 'path';





10.9 Disallow Webpack loader syntax in module import statements.
eslint: import/no-webpack-loader-syntax

Why? Since using Webpack syntax in the imports couples the code to a module bundler. Prefer using the loader syntax in webpack.config.js.

// bad
import fooSass from 'css!sass!foo.scss';
import barCss from 'style!css!bar.css';

// good
import fooSass from 'foo.scss';
import barCss from 'bar.css';


‚¨Ü back to top
Iterators and Generators



11.1 Don‚Äôt use iterators. Prefer JavaScript‚Äôs higher-order functions instead of loops like for-in or for-of. eslint: no-iterator no-restricted-syntax

Why? This enforces our immutable rule. Dealing with pure functions that return values is easier to reason about than side effects.


Use map() / every() / filter() / find() / findIndex() / reduce() / some() / ... to iterate over arrays, and Object.keys() / Object.values() / Object.entries() to produce arrays so you can iterate over objects.

const numbers = [1, 2, 3, 4, 5];

// bad
let sum = 0;
for (let num of numbers) {
  sum += num;
}
sum === 15;

// good
let sum = 0;
numbers.forEach((num) => {
  sum += num;
});
sum === 15;

// best (use the functional force)
const sum = numbers.reduce((total, num) => total + num, 0);
sum === 15;

// bad
const increasedByOne = [];
for (let i = 0; i < numbers.length; i++) {
  increasedByOne.push(numbers[i] + 1);
}

// good
const increasedByOne = [];
numbers.forEach((num) => {
  increasedByOne.push(num + 1);
});

// best (keeping it functional)
const increasedByOne = numbers.map((num) => num + 1);





11.2 Don‚Äôt use generators for now.

Why? They don‚Äôt transpile well to ES5.






11.3 If you must use generators, or if you disregard our advice, make sure their function signature is spaced properly. eslint: generator-star-spacing

Why? function and * are part of the same conceptual keyword - * is not a modifier for function, function* is a unique construct, different from function.

// bad
function * foo() {
  // ...
}

// bad
const bar = function * () {
  // ...
};

// bad
const baz = function *() {
  // ...
};

// bad
const quux = function*() {
  // ...
};

// bad
function*foo() {
  // ...
}

// bad
function *foo() {
  // ...
}

// very bad
function
*
foo() {
  // ...
}

// very bad
const wat = function
*
() {
  // ...
};

// good
function* foo() {
  // ...
}

// good
const foo = function* () {
  // ...
};


‚¨Ü back to top
Properties



12.1 Use dot notation when accessing properties. eslint: dot-notation
const luke = {
  jedi: true,
  age: 28,
};

// bad
const isJedi = luke['jedi'];

// good
const isJedi = luke.jedi;





12.2 Use bracket notation [] when accessing properties with a variable.
const luke = {
  jedi: true,
  age: 28,
};

function getProp(prop) {
  return luke[prop];
}

const isJedi = getProp('jedi');





12.3 Use exponentiation operator ** when calculating exponentiations. eslint: no-restricted-properties.
// bad
const binary = Math.pow(2, 10);

// good
const binary = 2 ** 10;


‚¨Ü back to top
Variables



13.1 Always use const or let to declare variables. Not doing so will result in global variables. We want to avoid polluting the global namespace. Captain Planet warned us of that. eslint: no-undef prefer-const
// bad
superPower = new SuperPower();

// good
const superPower = new SuperPower();





13.2 Use one const or let declaration per variable or assignment. eslint: one-var

Why? It‚Äôs easier to add new variable declarations this way, and you never have to worry about swapping out a ; for a , or introducing punctuation-only diffs. You can also step through each declaration with the debugger, instead of jumping through all of them at once.

// bad
const items = getItems(),
    goSportsTeam = true,
    dragonball = 'z';

// bad
// (compare to above, and try to spot the mistake)
const items = getItems(),
    goSportsTeam = true;
    dragonball = 'z';

// good
const items = getItems();
const goSportsTeam = true;
const dragonball = 'z';





13.3 Group all your consts and then group all your lets.

Why? This is helpful when later on you might need to assign a variable depending on one of the previous assigned variables.

// bad
let i, len, dragonball,
    items = getItems(),
    goSportsTeam = true;

// bad
let i;
const items = getItems();
let dragonball;
const goSportsTeam = true;
let len;

// good
const goSportsTeam = true;
const items = getItems();
let dragonball;
let i;
let length;





13.4 Assign variables where you need them, but place them in a reasonable place.

Why? let and const are block scoped and not function scoped.

// bad - unnecessary function call
function checkName(hasName) {
  const name = getName();

  if (hasName === 'test') {
    return false;
  }

  if (name === 'test') {
    this.setName('');
    return false;
  }

  return name;
}

// good
function checkName(hasName) {
  if (hasName === 'test') {
    return false;
  }

  const name = getName();

  if (name === 'test') {
    this.setName('');
    return false;
  }

  return name;
}





13.5 Don‚Äôt chain variable assignments. eslint: no-multi-assign

Why? Chaining variable assignments creates implicit global variables.

// bad
(function example() {
  // JavaScript interprets this as
  // let a = ( b = ( c = 1 ) );
  // The let keyword only applies to variable a; variables b and c become
  // global variables.
  let a = b = c = 1;
}());

console.log(a); // throws ReferenceError
console.log(b); // 1
console.log(c); // 1

// good
(function example() {
  let a = 1;
  let b = a;
  let c = a;
}());

console.log(a); // throws ReferenceError
console.log(b); // throws ReferenceError
console.log(c); // throws ReferenceError

// the same applies for `const`





13.6 Avoid using unary increments and decrements (++, --). eslint no-plusplus

Why? Per the eslint documentation, unary increment and decrement statements are subject to automatic semicolon insertion and can cause silent errors with incrementing or decrementing values within an application. It is also more expressive to mutate your values with statements like num += 1 instead of num++ or num ++. Disallowing unary increment and decrement statements also prevents you from pre-incrementing/pre-decrementing values unintentionally which can also cause unexpected behavior in your programs.

// bad

const array = [1, 2, 3];
let num = 1;
num++;
--num;

let sum = 0;
let truthyCount = 0;
for (let i = 0; i < array.length; i++) {
  let value = array[i];
  sum += value;
  if (value) {
    truthyCount++;
  }
}

// good

const array = [1, 2, 3];
let num = 1;
num += 1;
num -= 1;

const sum = array.reduce((a, b) => a + b, 0);
const truthyCount = array.filter(Boolean).length;





13.7 Avoid linebreaks before or after = in an assignment. If your assignment violates max-len, surround the value in parens. eslint operator-linebreak.

Why? Linebreaks surrounding = can obfuscate the value of an assignment.

// bad
const foo =
  superLongLongLongLongLongLongLongLongFunctionName();

// bad
const foo
  = 'superLongLongLongLongLongLongLongLongString';

// good
const foo = (
  superLongLongLongLongLongLongLongLongFunctionName()
);

// good
const foo = 'superLongLongLongLongLongLongLongLongString';





13.8 Disallow unused variables. eslint: no-unused-vars

Why? Variables that are declared and not used anywhere in the code are most likely an error due to incomplete refactoring. Such variables take up space in the code and can lead to confusion by readers.

// bad

var some_unused_var = 42;

// Write-only variables are not considered as used.
var y = 10;
y = 5;

// A read for a modification of itself is not considered as used.
var z = 0;
z = z + 1;

// Unused function arguments.
function getX(x, y) {
    return x;
}

// good

function getXPlusY(x, y) {
  return x + y;
}

var x = 1;
var y = a + 2;

alert(getXPlusY(x, y));

// 'type' is ignored even if unused because it has a rest property sibling.
// This is a form of extracting an object that omits the specified keys.
var { type, ...coords } = data;
// 'coords' is now the 'data' object without its 'type' property.


‚¨Ü back to top
Hoisting



14.1 var declarations get hoisted to the top of their closest enclosing function scope, their assignment does not. const and let declarations are blessed with a new concept called Temporal Dead Zones (TDZ). It‚Äôs important to know why typeof is no longer safe.
// we know this wouldn‚Äôt work (assuming there
// is no notDefined global variable)
function example() {
  console.log(notDefined); // => throws a ReferenceError
}

// creating a variable declaration after you
// reference the variable will work due to
// variable hoisting. Note: the assignment
// value of `true` is not hoisted.
function example() {
  console.log(declaredButNotAssigned); // => undefined
  var declaredButNotAssigned = true;
}

// the interpreter is hoisting the variable
// declaration to the top of the scope,
// which means our example could be rewritten as:
function example() {
  let declaredButNotAssigned;
  console.log(declaredButNotAssigned); // => undefined
  declaredButNotAssigned = true;
}

// using const and let
function example() {
  console.log(declaredButNotAssigned); // => throws a ReferenceError
  console.log(typeof declaredButNotAssigned); // => throws a ReferenceError
  const declaredButNotAssigned = true;
}





14.2 Anonymous function expressions hoist their variable name, but not the function assignment.
function example() {
  console.log(anonymous); // => undefined

  anonymous(); // => TypeError anonymous is not a function

  var anonymous = function () {
    console.log('anonymous function expression');
  };
}





14.3 Named function expressions hoist the variable name, not the function name or the function body.
function example() {
  console.log(named); // => undefined

  named(); // => TypeError named is not a function

  superPower(); // => ReferenceError superPower is not defined

  var named = function superPower() {
    console.log('Flying');
  };
}

// the same is true when the function name
// is the same as the variable name.
function example() {
  console.log(named); // => undefined

  named(); // => TypeError named is not a function

  var named = function named() {
    console.log('named');
  };
}





14.4 Function declarations hoist their name and the function body.
function example() {
  superPower(); // => Flying

  function superPower() {
    console.log('Flying');
  }
}


For more information refer to JavaScript Scoping & Hoisting by Ben Cherry.


‚¨Ü back to top
Comparison Operators & Equality


15.1 Use === and !== over == and !=. eslint: eqeqeq




15.2 Conditional statements such as the if statement evaluate their expression using coercion with the ToBoolean abstract method and always follow these simple rules:

Objects evaluate to true
Undefined evaluates to false
Null evaluates to false
Booleans evaluate to the value of the boolean
Numbers evaluate to false if +0, -0, or NaN, otherwise true
Strings evaluate to false if an empty string '', otherwise true

if ([0] && []) {
  // true
  // an array (even an empty one) is an object, objects will evaluate to true
}





15.3 Use shortcuts for booleans, but explicit comparisons for strings and numbers.
// bad
if (isValid === true) {
  // ...
}

// good
if (isValid) {
  // ...
}

// bad
if (name) {
  // ...
}

// good
if (name !== '') {
  // ...
}

// bad
if (collection.length) {
  // ...
}

// good
if (collection.length > 0) {
  // ...
}




15.4 For more information see Truth Equality and JavaScript by Angus Croll.




15.5 Use braces to create blocks in case and default clauses that contain lexical declarations (e.g. let, const, function, and class). eslint: no-case-declarations

Why? Lexical declarations are visible in the entire switch block but only get initialized when assigned, which only happens when its case is reached. This causes problems when multiple case clauses attempt to define the same thing.

// bad
switch (foo) {
  case 1:
    let x = 1;
    break;
  case 2:
    const y = 2;
    break;
  case 3:
    function f() {
      // ...
    }
    break;
  default:
    class C {}
}

// good
switch (foo) {
  case 1: {
    let x = 1;
    break;
  }
  case 2: {
    const y = 2;
    break;
  }
  case 3: {
    function f() {
      // ...
    }
    break;
  }
  case 4:
    bar();
    break;
  default: {
    class C {}
  }
}





15.6 Ternaries should not be nested and generally be single line expressions. eslint: no-nested-ternary
// bad
const foo = maybe1 > maybe2
  ? ""bar""
  : value1 > value2 ? ""baz"" : null;

// split into 2 separated ternary expressions
const maybeNull = value1 > value2 ? 'baz' : null;

// better
const foo = maybe1 > maybe2
  ? 'bar'
  : maybeNull;

// best
const foo = maybe1 > maybe2 ? 'bar' : maybeNull;





15.7 Avoid unneeded ternary statements. eslint: no-unneeded-ternary
// bad
const foo = a ? a : b;
const bar = c ? true : false;
const baz = c ? false : true;

// good
const foo = a || b;
const bar = !!c;
const baz = !c;





15.8 When mixing operators, enclose them in parentheses. The only exception is the standard arithmetic operators: +, -, and ** since their precedence is broadly understood. We recommend enclosing / and * in parentheses because their precedence can be ambiguous when they are mixed.
eslint: no-mixed-operators

Why? This improves readability and clarifies the developer‚Äôs intention.

// bad
const foo = a && b < 0 || c > 0 || d + 1 === 0;

// bad
const bar = a ** b - 5 % d;

// bad
// one may be confused into thinking (a || b) && c
if (a || b && c) {
  return d;
}

// bad
const bar = a + b / c * d;

// good
const foo = (a && b < 0) || c > 0 || (d + 1 === 0);

// good
const bar = a ** b - (5 % d);

// good
if (a || (b && c)) {
  return d;
}

// good
const bar = a + (b / c) * d;


‚¨Ü back to top
Blocks



16.1 Use braces with all multi-line blocks. eslint: nonblock-statement-body-position
// bad
if (test)
  return false;

// good
if (test) return false;

// good
if (test) {
  return false;
}

// bad
function foo() { return false; }

// good
function bar() {
  return false;
}





16.2 If you‚Äôre using multi-line blocks with if and else, put else on the same line as your if block‚Äôs closing brace. eslint: brace-style
// bad
if (test) {
  thing1();
  thing2();
}
else {
  thing3();
}

// good
if (test) {
  thing1();
  thing2();
} else {
  thing3();
}





16.3 If an if block always executes a return statement, the subsequent else block is unnecessary. A return in an else if block following an if block that contains a return can be separated into multiple if blocks. eslint: no-else-return
// bad
function foo() {
  if (x) {
    return x;
  } else {
    return y;
  }
}

// bad
function cats() {
  if (x) {
    return x;
  } else if (y) {
    return y;
  }
}

// bad
function dogs() {
  if (x) {
    return x;
  } else {
    if (y) {
      return y;
    }
  }
}

// good
function foo() {
  if (x) {
    return x;
  }

  return y;
}

// good
function cats() {
  if (x) {
    return x;
  }

  if (y) {
    return y;
  }
}

// good
function dogs(x) {
  if (x) {
    if (z) {
      return y;
    }
  } else {
    return z;
  }
}


‚¨Ü back to top
Control Statements



17.1 In case your control statement (if, while etc.) gets too long or exceeds the maximum line length, each (grouped) condition could be put into a new line. The logical operator should begin the line.

Why? Requiring operators at the beginning of the line keeps the operators aligned and follows a pattern similar to method chaining. This also improves readability by making it easier to visually follow complex logic.

// bad
if ((foo === 123 || bar === 'abc') && doesItLookGoodWhenItBecomesThatLong() && isThisReallyHappening()) {
  thing1();
}

// bad
if (foo === 123 &&
  bar === 'abc') {
  thing1();
}

// bad
if (foo === 123
  && bar === 'abc') {
  thing1();
}

// bad
if (
  foo === 123 &&
  bar === 'abc'
) {
  thing1();
}

// good
if (
  foo === 123
  && bar === 'abc'
) {
  thing1();
}

// good
if (
  (foo === 123 || bar === 'abc')
  && doesItLookGoodWhenItBecomesThatLong()
  && isThisReallyHappening()
) {
  thing1();
}

// good
if (foo === 123 && bar === 'abc') {
  thing1();
}





17.2 Don't use selection operators in place of control statements.
// bad
!isRunning && startRunning();

// good
if (!isRunning) {
  startRunning();
}


‚¨Ü back to top
Comments



18.1 Use /** ... */ for multi-line comments.
// bad
// make() returns a new element
// based on the passed in tag name
//
// @param {String} tag
// @return {Element} element
function make(tag) {

  // ...

  return element;
}

// good
/**
 * make() returns a new element
 * based on the passed-in tag name
 */
function make(tag) {

  // ...

  return element;
}





18.2 Use // for single line comments. Place single line comments on a newline above the subject of the comment. Put an empty line before the comment unless it‚Äôs on the first line of a block.
// bad
const active = true;  // is current tab

// good
// is current tab
const active = true;

// bad
function getType() {
  console.log('fetching type...');
  // set the default type to 'no type'
  const type = this.type || 'no type';

  return type;
}

// good
function getType() {
  console.log('fetching type...');

  // set the default type to 'no type'
  const type = this.type || 'no type';

  return type;
}

// also good
function getType() {
  // set the default type to 'no type'
  const type = this.type || 'no type';

  return type;
}





18.3 Start all comments with a space to make it easier to read. eslint: spaced-comment
// bad
//is current tab
const active = true;

// good
// is current tab
const active = true;

// bad
/**
 *make() returns a new element
 *based on the passed-in tag name
 */
function make(tag) {

  // ...

  return element;
}

// good
/**
 * make() returns a new element
 * based on the passed-in tag name
 */
function make(tag) {

  // ...

  return element;
}




18.4 Prefixing your comments with FIXME or TODO helps other developers quickly understand if you‚Äôre pointing out a problem that needs to be revisited, or if you‚Äôre suggesting a solution to the problem that needs to be implemented. These are different than regular comments because they are actionable. The actions are FIXME: -- need to figure this out or TODO: -- need to implement.




18.5 Use // FIXME: to annotate problems.
class Calculator extends Abacus {
  constructor() {
    super();

    // FIXME: shouldn‚Äôt use a global here
    total = 0;
  }
}





18.6 Use // TODO: to annotate solutions to problems.
class Calculator extends Abacus {
  constructor() {
    super();

    // TODO: total should be configurable by an options param
    this.total = 0;
  }
}


‚¨Ü back to top
Whitespace



19.1 Use soft tabs (space character) set to 2 spaces. eslint: indent
// bad
function foo() {
‚àô‚àô‚àô‚àôlet name;
}

// bad
function bar() {
‚àôlet name;
}

// good
function baz() {
‚àô‚àôlet name;
}





19.2 Place 1 space before the leading brace. eslint: space-before-blocks
// bad
function test(){
  console.log('test');
}

// good
function test() {
  console.log('test');
}

// bad
dog.set('attr',{
  age: '1 year',
  breed: 'Bernese Mountain Dog',
});

// good
dog.set('attr', {
  age: '1 year',
  breed: 'Bernese Mountain Dog',
});





19.3 Place 1 space before the opening parenthesis in control statements (if, while etc.). Place no space between the argument list and the function name in function calls and declarations. eslint: keyword-spacing
// bad
if(isJedi) {
  fight ();
}

// good
if (isJedi) {
  fight();
}

// bad
function fight () {
  console.log ('Swooosh!');
}

// good
function fight() {
  console.log('Swooosh!');
}





19.4 Set off operators with spaces. eslint: space-infix-ops
// bad
const x=y+5;

// good
const x = y + 5;





19.5 End files with a single newline character. eslint: eol-last
// bad
import { es6 } from './AirbnbStyleGuide';
  // ...
export default es6;
// bad
import { es6 } from './AirbnbStyleGuide';
  // ...
export default es6;‚Üµ
‚Üµ
// good
import { es6 } from './AirbnbStyleGuide';
  // ...
export default es6;‚Üµ





19.6 Use indentation when making long method chains (more than 2 method chains). Use a leading dot, which
emphasizes that the line is a method call, not a new statement. eslint: newline-per-chained-call no-whitespace-before-property
// bad
$('#items').find('.selected').highlight().end().find('.open').updateCount();

// bad
$('#items').
  find('.selected').
    highlight().
    end().
  find('.open').
    updateCount();

// good
$('#items')
  .find('.selected')
    .highlight()
    .end()
  .find('.open')
    .updateCount();

// bad
const leds = stage.selectAll('.led').data(data).enter().append('svg:svg').classed('led', true)
    .attr('width', (radius + margin) * 2).append('svg:g')
    .attr('transform', `translate(${radius + margin},${radius + margin})`)
    .call(tron.led);

// good
const leds = stage.selectAll('.led')
    .data(data)
  .enter().append('svg:svg')
    .classed('led', true)
    .attr('width', (radius + margin) * 2)
  .append('svg:g')
    .attr('transform', `translate(${radius + margin},${radius + margin})`)
    .call(tron.led);

// good
const leds = stage.selectAll('.led').data(data);





19.7 Leave a blank line after blocks and before the next statement.
// bad
if (foo) {
  return bar;
}
return baz;

// good
if (foo) {
  return bar;
}

return baz;

// bad
const obj = {
  foo() {
  },
  bar() {
  },
};
return obj;

// good
const obj = {
  foo() {
  },

  bar() {
  },
};

return obj;

// bad
const arr = [
  function foo() {
  },
  function bar() {
  },
];
return arr;

// good
const arr = [
  function foo() {
  },

  function bar() {
  },
];

return arr;





19.8 Do not pad your blocks with blank lines. eslint: padded-blocks
// bad
function bar() {

  console.log(foo);

}

// bad
if (baz) {

  console.log(qux);
} else {
  console.log(foo);

}

// bad
class Foo {

  constructor(bar) {
    this.bar = bar;
  }
}

// good
function bar() {
  console.log(foo);
}

// good
if (baz) {
  console.log(qux);
} else {
  console.log(foo);
}





19.9 Do not use multiple blank lines to pad your code. eslint: no-multiple-empty-lines
// bad
class Person {
  constructor(fullName, email, birthday) {
    this.fullName = fullName;


    this.email = email;


    this.setAge(birthday);
  }


  setAge(birthday) {
    const today = new Date();


    const age = this.getAge(today, birthday);


    this.age = age;
  }


  getAge(today, birthday) {
    // ..
  }
}

// good
class Person {
  constructor(fullName, email, birthday) {
    this.fullName = fullName;
    this.email = email;
    this.setAge(birthday);
  }

  setAge(birthday) {
    const today = new Date();
    const age = getAge(today, birthday);
    this.age = age;
  }

  getAge(today, birthday) {
    // ..
  }
}





19.10 Do not add spaces inside parentheses. eslint: space-in-parens
// bad
function bar( foo ) {
  return foo;
}

// good
function bar(foo) {
  return foo;
}

// bad
if ( foo ) {
  console.log(foo);
}

// good
if (foo) {
  console.log(foo);
}





19.11 Do not add spaces inside brackets. eslint: array-bracket-spacing
// bad
const foo = [ 1, 2, 3 ];
console.log(foo[ 0 ]);

// good
const foo = [1, 2, 3];
console.log(foo[0]);





19.12 Add spaces inside curly braces. eslint: object-curly-spacing
// bad
const foo = {clark: 'kent'};

// good
const foo = { clark: 'kent' };





19.13 Avoid having lines of code that are longer than 100 characters (including whitespace). Note: per above, long strings are exempt from this rule, and should not be broken up. eslint: max-len

Why? This ensures readability and maintainability.

// bad
const foo = jsonData && jsonData.foo && jsonData.foo.bar && jsonData.foo.bar.baz && jsonData.foo.bar.baz.quux && jsonData.foo.bar.baz.quux.xyzzy;

// bad
$.ajax({ method: 'POST', url: 'https://airbnb.com/', data: { name: 'John' } }).done(() => console.log('Congratulations!')).fail(() => console.log('You have failed this city.'));

// good
const foo = jsonData
  && jsonData.foo
  && jsonData.foo.bar
  && jsonData.foo.bar.baz
  && jsonData.foo.bar.baz.quux
  && jsonData.foo.bar.baz.quux.xyzzy;

// good
$.ajax({
  method: 'POST',
  url: 'https://airbnb.com/',
  data: { name: 'John' },
})
  .done(() => console.log('Congratulations!'))
  .fail(() => console.log('You have failed this city.'));





19.14 Require consistent spacing inside an open block token and the next token on the same line. This rule also enforces consistent spacing inside a close block token and previous token on the same line. eslint: block-spacing
// bad
function foo() {return true;}
if (foo) { bar = 0;}

// good
function foo() { return true; }
if (foo) { bar = 0; }





19.15 Avoid spaces before commas and require a space after commas. eslint: comma-spacing
// bad
var foo = 1,bar = 2;
var arr = [1 , 2];

// good
var foo = 1, bar = 2;
var arr = [1, 2];





19.16 Enforce spacing inside of computed property brackets. eslint: computed-property-spacing
// bad
obj[foo ]
obj[ 'foo']
var x = {[ b ]: a}
obj[foo[ bar ]]

// good
obj[foo]
obj['foo']
var x = { [b]: a }
obj[foo[bar]]





19.17 Avoid spaces between functions and their invocations. eslint: func-call-spacing
// bad
func ();

func
();

// good
func();





19.18 Enforce spacing between keys and values in object literal properties. eslint: key-spacing
// bad
var obj = { ""foo"" : 42 };
var obj2 = { ""foo"":42 };

// good
var obj = { ""foo"": 42 };




19.19 Avoid trailing spaces at the end of lines. eslint: no-trailing-spaces




19.20 Avoid multiple empty lines, only allow one newline at the end of files, and avoid a newline at the beginning of files. eslint: no-multiple-empty-lines
// bad - multiple empty lines
var x = 1;


var y = 2;

// bad - 2+ newlines at end of file
var x = 1;
var y = 2;


// bad - 1+ newline(s) at beginning of file

var x = 1;
var y = 2;

// good
var x = 1;
var y = 2;



‚¨Ü back to top
Commas



20.1 Leading commas: Nope. eslint: comma-style
// bad
const story = [
    once
  , upon
  , aTime
];

// good
const story = [
  once,
  upon,
  aTime,
];

// bad
const hero = {
    firstName: 'Ada'
  , lastName: 'Lovelace'
  , birthYear: 1815
  , superPower: 'computers'
};

// good
const hero = {
  firstName: 'Ada',
  lastName: 'Lovelace',
  birthYear: 1815,
  superPower: 'computers',
};





20.2 Additional trailing comma: Yup. eslint: comma-dangle

Why? This leads to cleaner git diffs. Also, transpilers like Babel will remove the additional trailing comma in the transpiled code which means you don‚Äôt have to worry about the trailing comma problem in legacy browsers.

// bad - git diff without trailing comma
const hero = {
     firstName: 'Florence',
-    lastName: 'Nightingale'
+    lastName: 'Nightingale',
+    inventorOf: ['coxcomb chart', 'modern nursing']
};

// good - git diff with trailing comma
const hero = {
     firstName: 'Florence',
     lastName: 'Nightingale',
+    inventorOf: ['coxcomb chart', 'modern nursing'],
};
// bad
const hero = {
  firstName: 'Dana',
  lastName: 'Scully'
};

const heroes = [
  'Batman',
  'Superman'
];

// good
const hero = {
  firstName: 'Dana',
  lastName: 'Scully',
};

const heroes = [
  'Batman',
  'Superman',
];

// bad
function createHero(
  firstName,
  lastName,
  inventorOf
) {
  // does nothing
}

// good
function createHero(
  firstName,
  lastName,
  inventorOf,
) {
  // does nothing
}

// good (note that a comma must not appear after a ""rest"" element)
function createHero(
  firstName,
  lastName,
  inventorOf,
  ...heroArgs
) {
  // does nothing
}

// bad
createHero(
  firstName,
  lastName,
  inventorOf
);

// good
createHero(
  firstName,
  lastName,
  inventorOf,
);

// good (note that a comma must not appear after a ""rest"" element)
createHero(
  firstName,
  lastName,
  inventorOf,
  ...heroArgs
);


‚¨Ü back to top
Semicolons



21.1 Yup. eslint: semi

Why? When JavaScript encounters a line break without a semicolon, it uses a set of rules called Automatic Semicolon Insertion to determine whether or not it should regard that line break as the end of a statement, and (as the name implies) place a semicolon into your code before the line break if it thinks so. ASI contains a few eccentric behaviors, though, and your code will break if JavaScript misinterprets your line break. These rules will become more complicated as new features become a part of JavaScript. Explicitly terminating your statements and configuring your linter to catch missing semicolons will help prevent you from encountering issues.

// bad - raises exception
const luke = {}
const leia = {}
[luke, leia].forEach((jedi) => jedi.father = 'vader')

// bad - raises exception
const reaction = ""No! That‚Äôs impossible!""
(async function meanwhileOnTheFalcon() {
  // handle `leia`, `lando`, `chewie`, `r2`, `c3p0`
  // ...
}())

// bad - returns `undefined` instead of the value on the next line - always happens when `return` is on a line by itself because of ASI!
function foo() {
  return
    'search your feelings, you know it to be foo'
}

// good
const luke = {};
const leia = {};
[luke, leia].forEach((jedi) => {
  jedi.father = 'vader';
});

// good
const reaction = ""No! That‚Äôs impossible!"";
(async function meanwhileOnTheFalcon() {
  // handle `leia`, `lando`, `chewie`, `r2`, `c3p0`
  // ...
}());

// good
function foo() {
  return 'search your feelings, you know it to be foo';
}
Read more.


‚¨Ü back to top
Type Casting & Coercion


22.1 Perform type coercion at the beginning of the statement.




22.2 Strings: eslint: no-new-wrappers
// => this.reviewScore = 9;

// bad
const totalScore = new String(this.reviewScore); // typeof totalScore is ""object"" not ""string""

// bad
const totalScore = this.reviewScore + ''; // invokes this.reviewScore.valueOf()

// bad
const totalScore = this.reviewScore.toString(); // isn‚Äôt guaranteed to return a string

// good
const totalScore = String(this.reviewScore);





22.3 Numbers: Use Number for type casting and parseInt always with a radix for parsing strings. eslint: radix no-new-wrappers
const inputValue = '4';

// bad
const val = new Number(inputValue);

// bad
const val = +inputValue;

// bad
const val = inputValue >> 0;

// bad
const val = parseInt(inputValue);

// good
const val = Number(inputValue);

// good
const val = parseInt(inputValue, 10);





22.4 If for whatever reason you are doing something wild and parseInt is your bottleneck and need to use Bitshift for performance reasons, leave a comment explaining why and what you‚Äôre doing.
// good
/**
 * parseInt was the reason my code was slow.
 * Bitshifting the String to coerce it to a
 * Number made it a lot faster.
 */
const val = inputValue >> 0;





22.5 Note: Be careful when using bitshift operations. Numbers are represented as 64-bit values, but bitshift operations always return a 32-bit integer (source). Bitshift can lead to unexpected behavior for integer values larger than 32 bits. Discussion. Largest signed 32-bit Int is 2,147,483,647:
2147483647 >> 0; // => 2147483647
2147483648 >> 0; // => -2147483648
2147483649 >> 0; // => -2147483647





22.6 Booleans: eslint: no-new-wrappers
const age = 0;

// bad
const hasAge = new Boolean(age);

// good
const hasAge = Boolean(age);

// best
const hasAge = !!age;


‚¨Ü back to top
Naming Conventions



23.1 Avoid single letter names. Be descriptive with your naming. eslint: id-length
// bad
function q() {
  // ...
}

// good
function query() {
  // ...
}





23.2 Use camelCase when naming objects, functions, and instances. eslint: camelcase
// bad
const OBJEcttsssss = {};
const this_is_my_object = {};
function c() {}

// good
const thisIsMyObject = {};
function thisIsMyFunction() {}





23.3 Use PascalCase only when naming constructors or classes. eslint: new-cap
// bad
function user(options) {
  this.name = options.name;
}

const bad = new user({
  name: 'nope',
});

// good
class User {
  constructor(options) {
    this.name = options.name;
  }
}

const good = new User({
  name: 'yup',
});





23.4 Do not use trailing or leading underscores. eslint: no-underscore-dangle

Why? JavaScript does not have the concept of privacy in terms of properties or methods. Although a leading underscore is a common convention to mean ‚Äúprivate‚Äù, in fact, these properties are fully public, and as such, are part of your public API contract. This convention might lead developers to wrongly think that a change won‚Äôt count as breaking, or that tests aren‚Äôt needed. tl;dr: if you want something to be ‚Äúprivate‚Äù, it must not be observably present.

// bad
this.__firstName__ = 'Panda';
this.firstName_ = 'Panda';
this._firstName = 'Panda';

// good
this.firstName = 'Panda';

// good, in environments where WeakMaps are available
// see https://kangax.github.io/compat-table/es6/#test-WeakMap
const firstNames = new WeakMap();
firstNames.set(this, 'Panda');





23.5 Don‚Äôt save references to this. Use arrow functions or Function#bind.
// bad
function foo() {
  const self = this;
  return function () {
    console.log(self);
  };
}

// bad
function foo() {
  const that = this;
  return function () {
    console.log(that);
  };
}

// good
function foo() {
  return () => {
    console.log(this);
  };
}





23.6 A base filename should exactly match the name of its default export.
// file 1 contents
class CheckBox {
  // ...
}
export default CheckBox;

// file 2 contents
export default function fortyTwo() { return 42; }

// file 3 contents
export default function insideDirectory() {}

// in some other file
// bad
import CheckBox from './checkBox'; // PascalCase import/export, camelCase filename
import FortyTwo from './FortyTwo'; // PascalCase import/filename, camelCase export
import InsideDirectory from './InsideDirectory'; // PascalCase import/filename, camelCase export

// bad
import CheckBox from './check_box'; // PascalCase import/export, snake_case filename
import forty_two from './forty_two'; // snake_case import/filename, camelCase export
import inside_directory from './inside_directory'; // snake_case import, camelCase export
import index from './inside_directory/index'; // requiring the index file explicitly
import insideDirectory from './insideDirectory/index'; // requiring the index file explicitly

// good
import CheckBox from './CheckBox'; // PascalCase export/import/filename
import fortyTwo from './fortyTwo'; // camelCase export/import/filename
import insideDirectory from './insideDirectory'; // camelCase export/import/directory name/implicit ""index""
// ^ supports both insideDirectory.js and insideDirectory/index.js





23.7 Use camelCase when you export-default a function. Your filename should be identical to your function‚Äôs name.
function makeStyleGuide() {
  // ...
}

export default makeStyleGuide;





23.8 Use PascalCase when you export a constructor / class / singleton / function library / bare object.
const AirbnbStyleGuide = {
  es6: {
  },
};

export default AirbnbStyleGuide;





23.9 Acronyms and initialisms should always be all uppercased, or all lowercased.

Why? Names are for readability, not to appease a computer algorithm.

// bad
import SmsContainer from './containers/SmsContainer';

// bad
const HttpRequests = [
  // ...
];

// good
import SMSContainer from './containers/SMSContainer';

// good
const HTTPRequests = [
  // ...
];

// also good
const httpRequests = [
  // ...
];

// best
import TextMessageContainer from './containers/TextMessageContainer';

// best
const requests = [
  // ...
];





23.10 You may optionally uppercase a constant only if it (1) is exported, (2) is a const (it can not be reassigned), and (3) the programmer can trust it (and its nested properties) to never change.

Why? This is an additional tool to assist in situations where the programmer would be unsure if a variable might ever change. UPPERCASE_VARIABLES are letting the programmer know that they can trust the variable (and its properties) not to change.


What about all const variables? - This is unnecessary, so uppercasing should not be used for constants within a file. It should be used for exported constants however.
What about exported objects? - Uppercase at the top level of export (e.g. EXPORTED_OBJECT.key) and maintain that all nested properties do not change.

// bad
const PRIVATE_VARIABLE = 'should not be unnecessarily uppercased within a file';

// bad
export const THING_TO_BE_CHANGED = 'should obviously not be uppercased';

// bad
export let REASSIGNABLE_VARIABLE = 'do not use let with uppercase variables';

// ---

// allowed but does not supply semantic value
export const apiKey = 'SOMEKEY';

// better in most cases
export const API_KEY = 'SOMEKEY';

// ---

// bad - unnecessarily uppercases key while adding no semantic value
export const MAPPING = {
  KEY: 'value'
};

// good
export const MAPPING = {
  key: 'value'
};


‚¨Ü back to top
Accessors


24.1 Accessor functions for properties are not required.




24.2 Do not use JavaScript getters/setters as they cause unexpected side effects and are harder to test, maintain, and reason about. Instead, if you do make accessor functions, use getVal() and setVal('hello').
// bad
class Dragon {
  get age() {
    // ...
  }

  set age(value) {
    // ...
  }
}

// good
class Dragon {
  getAge() {
    // ...
  }

  setAge(value) {
    // ...
  }
}





24.3 If the property/method is a boolean, use isVal() or hasVal().
// bad
if (!dragon.age()) {
  return false;
}

// good
if (!dragon.hasAge()) {
  return false;
}





24.4 It‚Äôs okay to create get() and set() functions, but be consistent.
class Jedi {
  constructor(options = {}) {
    const lightsaber = options.lightsaber || 'blue';
    this.set('lightsaber', lightsaber);
  }

  set(key, val) {
    this[key] = val;
  }

  get(key) {
    return this[key];
  }
}


‚¨Ü back to top
Events



25.1 When attaching data payloads to events (whether DOM events or something more proprietary like Backbone events), pass an object literal (also known as a ""hash"") instead of a raw value. This allows a subsequent contributor to add more data to the event payload without finding and updating every handler for the event. For example, instead of:
// bad
$(this).trigger('listingUpdated', listing.id);

// ...

$(this).on('listingUpdated', (e, listingID) => {
  // do something with listingID
});
prefer:
// good
$(this).trigger('listingUpdated', { listingID: listing.id });

// ...

$(this).on('listingUpdated', (e, data) => {
  // do something with data.listingID
});


‚¨Ü back to top
jQuery



26.1 Prefix jQuery object variables with a $.
// bad
const sidebar = $('.sidebar');

// good
const $sidebar = $('.sidebar');

// good
const $sidebarBtn = $('.sidebar-btn');





26.2 Cache jQuery lookups.
// bad
function setSidebar() {
  $('.sidebar').hide();

  // ...

  $('.sidebar').css({
    'background-color': 'pink',
  });
}

// good
function setSidebar() {
  const $sidebar = $('.sidebar');
  $sidebar.hide();

  // ...

  $sidebar.css({
    'background-color': 'pink',
  });
}




26.3 For DOM queries use Cascading $('.sidebar ul') or parent > child $('.sidebar > ul'). jsPerf




26.4 Use find with scoped jQuery object queries.
// bad
$('ul', '.sidebar').hide();

// bad
$('.sidebar').find('ul').hide();

// good
$('.sidebar ul').hide();

// good
$('.sidebar > ul').hide();

// good
$sidebar.find('ul').hide();


‚¨Ü back to top
ECMAScript 5 Compatibility


27.1 Refer to Kangax‚Äôs ES5 compatibility table.

‚¨Ü back to top

ECMAScript 6+ (ES 2015+) Styles


28.1 This is a collection of links to the various ES6+ features.


Arrow Functions
Classes
Object Shorthand
Object Concise
Object Computed Properties
Template Strings
Destructuring
Default Parameters
Rest
Array Spreads
Let and Const
Exponentiation Operator
Iterators and Generators
Modules




28.2 Do not use TC39 proposals that have not reached stage 3.

Why? They are not finalized, and they are subject to change or to be withdrawn entirely. We want to use JavaScript, and proposals are not JavaScript yet.



‚¨Ü back to top
Standard Library
The Standard Library
contains utilities that are functionally broken but remain for legacy reasons.



29.1 Use Number.isNaN instead of global isNaN.
eslint: no-restricted-globals

Why? The global isNaN coerces non-numbers to numbers, returning true for anything that coerces to NaN.
If this behavior is desired, make it explicit.

// bad
isNaN('1.2'); // false
isNaN('1.2.3'); // true

// good
Number.isNaN('1.2.3'); // false
Number.isNaN(Number('1.2.3')); // true





29.2 Use Number.isFinite instead of global isFinite.
eslint: no-restricted-globals

Why? The global isFinite coerces non-numbers to numbers, returning true for anything that coerces to a finite number.
If this behavior is desired, make it explicit.

// bad
isFinite('2e3'); // true

// good
Number.isFinite('2e3'); // false
Number.isFinite(parseInt('2e3', 10)); // true


‚¨Ü back to top
Testing



30.1 Yup.
function foo() {
  return true;
}




30.2 No, but seriously:

Whichever testing framework you use, you should be writing tests!
Strive to write many small pure functions, and minimize where mutations occur.
Be cautious about stubs and mocks - they can make your tests more brittle.
We primarily use mocha and jest at Airbnb. tape is also used occasionally for small, separate modules.
100% test coverage is a good goal to strive for, even if it‚Äôs not always practical to reach it.
Whenever you fix a bug, write a regression test. A bug fixed without a regression test is almost certainly going to break again in the future.



‚¨Ü back to top
Performance

On Layout & Web Performance
String vs Array Concat
Try/Catch Cost In a Loop
Bang Function
jQuery Find vs Context, Selector
innerHTML vs textContent for script text
Long String Concatenation
Are JavaScript functions like map(), reduce(), and filter() optimized for traversing arrays?
Loading...

‚¨Ü back to top
Resources
Learning ES6+

Latest ECMA spec
ExploringJS
ES6 Compatibility Table
Comprehensive Overview of ES6 Features

Read This

Standard ECMA-262

Tools

Code Style Linters

ESlint - Airbnb Style .eslintrc
JSHint - Airbnb Style .jshintrc


Neutrino Preset - @neutrinojs/airbnb

Other Style Guides

Google JavaScript Style Guide
Google JavaScript Style Guide (Old)
jQuery Core Style Guidelines
Principles of Writing Consistent, Idiomatic JavaScript
StandardJS

Other Styles

Naming this in nested functions - Christian Johansen
Conditional Callbacks - Ross Allen
Popular JavaScript Coding Conventions on GitHub - JeongHoon Byun
Multiple var statements in JavaScript, not superfluous - Ben Alman

Further Reading

Understanding JavaScript Closures - Angus Croll
Basic JavaScript for the impatient programmer - Dr. Axel Rauschmayer
You Might Not Need jQuery - Zack Bloom & Adam Schwartz
ES6 Features - Luke Hoban
Frontend Guidelines - Benjamin De Cock

Books

JavaScript: The Good Parts - Douglas Crockford
JavaScript Patterns - Stoyan Stefanov
Pro JavaScript Design Patterns - Ross Harmes and Dustin Diaz
High Performance Web Sites: Essential Knowledge for Front-End Engineers - Steve Souders
Maintainable JavaScript - Nicholas C. Zakas
JavaScript Web Applications - Alex MacCaw
Pro JavaScript Techniques - John Resig
Smashing Node.js: JavaScript Everywhere - Guillermo Rauch
Secrets of the JavaScript Ninja - John Resig and Bear Bibeault
Human JavaScript - Henrik Joreteg
Superhero.js - Kim Joar Bekkelund, Mads Mob√¶k, & Olav Bjorkoy
JSBooks - Julien Bouquillon
Third Party JavaScript - Ben Vinegar and Anton Kovalyov
Effective JavaScript: 68 Specific Ways to Harness the Power of JavaScript - David Herman
Eloquent JavaScript - Marijn Haverbeke
You Don‚Äôt Know JS: ES6 & Beyond - Kyle Simpson

Blogs

JavaScript Weekly
JavaScript, JavaScript...
Bocoup Weblog
Adequately Good
NCZOnline
Perfection Kills
Ben Alman
Dmitry Baranovskiy
nettuts

Podcasts

JavaScript Air
JavaScript Jabber

‚¨Ü back to top
In the Wild
This is a list of organizations that are using this style guide. Send us a pull request and we'll add you to the list.

123erfasst: 123erfasst/javascript
3blades: 3Blades
4Catalyzer: 4Catalyzer/javascript
Aan Zee: AanZee/javascript
Adult Swim: adult-swim/javascript
Airbnb: airbnb/javascript
AltSchool: AltSchool/javascript
Apartmint: apartmint/javascript
Ascribe: ascribe/javascript
Avalara: avalara/javascript
Avant: avantcredit/javascript
Axept: axept/javascript
BashPros: BashPros/javascript
Billabong: billabong/javascript
Bisk: bisk
Bonhomme: bonhommeparis/javascript
Brainshark: brainshark/javascript
CaseNine: CaseNine/javascript
Cerner: Cerner
Chartboost: ChartBoost/javascript-style-guide
Coeur d'Alene Tribe: www.cdatribe-nsn.gov
ComparaOnline: comparaonline/javascript
Compass Learning: compasslearning/javascript-style-guide
DailyMotion: dailymotion/javascript
DoSomething: DoSomething/eslint-config
Digitpaint digitpaint/javascript
Drupal: www.drupal.org
Ecosia: ecosia/javascript
Evernote: evernote/javascript-style-guide
Evolution Gaming: evolution-gaming/javascript
EvozonJs: evozonjs/javascript
ExactTarget: ExactTarget/javascript
Expensify Expensify/Style-Guide
Flexberry: Flexberry/javascript-style-guide
Gawker Media: gawkermedia
General Electric: GeneralElectric/javascript
Generation Tux: GenerationTux/javascript
GoodData: gooddata/gdc-js-style
GreenChef: greenchef/javascript
Grooveshark: grooveshark/javascript
Grupo-Abraxas: Grupo-Abraxas/javascript
Happeo: happeo/javascript
Honey: honeyscience/javascript
How About We: howaboutwe/javascript
Huballin: huballin
HubSpot: HubSpot/javascript
Hyper: hyperoslo/javascript-playbook
InterCity Group: intercitygroup/javascript-style-guide
Jam3: Jam3/Javascript-Code-Conventions
JeopardyBot: kesne/jeopardy-bot
JSSolutions: JSSolutions/javascript
Kaplan Komputing: kaplankomputing/javascript
KickorStick: kickorstick
Kinetica Solutions: kinetica/javascript
LEINWAND: LEINWAND/javascript
Lonely Planet: lonelyplanet/javascript
M2GEN: M2GEN/javascript
Mighty Spring: mightyspring/javascript
MinnPost: MinnPost/javascript
MitocGroup: MitocGroup/javascript
ModCloth: modcloth/javascript
Money Advice Service: moneyadviceservice/javascript
Muber: muber
National Geographic: natgeo
Nimbl3: nimbl3/javascript
NullDev: NullDevCo/JavaScript-Styleguide
Nulogy: nulogy/javascript
Orange Hill Development: orangehill/javascript
Orion Health: orionhealth/javascript
OutBoxSoft: OutBoxSoft/javascript
Peerby: Peerby/javascript
Pier 1: Pier1/javascript
Qotto: Qotto/javascript-style-guide
Razorfish: razorfish/javascript-style-guide
reddit: reddit/styleguide/javascript
React: facebook.github.io/react/contributing/how-to-contribute.html#style-guide
REI: reidev/js-style-guide
Ripple: ripple/javascript-style-guide
Sainsbury‚Äôs Supermarkets: jsainsburyplc
SeekingAlpha: seekingalpha/javascript-style-guide
Shutterfly: shutterfly/javascript
Sourcetoad: sourcetoad/javascript
Springload: springload
StratoDem Analytics: stratodem/javascript
SteelKiwi Development: steelkiwi/javascript
StudentSphere: studentsphere/javascript
SwoopApp: swoopapp/javascript
SysGarage: sysgarage/javascript-style-guide
Syzygy Warsaw: syzygypl/javascript
Target: target/javascript
Terra: terra
TheLadders: TheLadders/javascript
The Nerdery: thenerdery/javascript-standards
Tomify: tomprats
Traitify: traitify/eslint-config-traitify
T4R Technology: T4R-Technology/javascript
UrbanSim: urbansim
VoxFeed: VoxFeed/javascript-style-guide
WeBox Studio: weboxstudio/javascript
Weggo: Weggo/javascript
Zillow: zillow/javascript
ZocDoc: ZocDoc/javascript

‚¨Ü back to top
Translation
This style guide is also available in other languages:

 Brazilian Portuguese: armoucar/javascript-style-guide
 Bulgarian: borislavvv/javascript
 Catalan: fpmweb/javascript-style-guide
 Chinese (Simplified): lin-123/javascript
 Chinese (Traditional): jigsawye/javascript
 French: nmussy/javascript-style-guide
 German: timofurrer/javascript-style-guide
 Italian: sinkswim/javascript-style-guide
 Japanese: mitsuruog/javascript-style-guide
 Korean: ParkSB/javascript-style-guide
 Russian: leonidlebedev/javascript-airbnb
 Spanish: paolocarrasco/javascript-style-guide
 Thai: lvarayut/javascript-style-guide
 Turkish: eraycetinay/javascript
 Ukrainian: ivanzusko/javascript
 Vietnam: dangkyokhoang/javascript-style-guide

The JavaScript Style Guide Guide

Reference

Chat With Us About JavaScript

Find us on gitter.

Contributors

View Contributors

License
(The MIT License)
Copyright (c) 2012 Airbnb
Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
'Software'), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:
The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
‚¨Ü back to top
Amendments
We encourage you to fork this guide and change the rules to fit your team‚Äôs style guide. Below, you may list some amendments to the style guide. This allows you to periodically update your style guide without having to deal with merge conflicts.
};
",GitHub - airbnb/javascript: JavaScript Style Guide
25,JavaScript,"D3: Data-Driven Documents

D3 (or D3.js) is a JavaScript library for visualizing data using web standards. D3 helps you bring data to life using SVG, Canvas and HTML. D3 combines powerful visualization and interaction techniques with a data-driven approach to DOM manipulation, giving you the full capabilities of modern browsers and the freedom to design the right visual interface for your data.
Resources

API Reference
Release Notes
Gallery
Examples
Wiki

Installing
If you use npm, npm install d3. Otherwise, download the latest release. The released bundle supports anonymous AMD, CommonJS, and vanilla environments. You can load directly from d3js.org, CDNJS, or unpkg. For example:
<script src=""https://d3js.org/d3.v5.js""></script>
For the minified version:
<script src=""https://d3js.org/d3.v5.min.js""></script>
You can also use the standalone D3 microlibraries. For example, d3-selection:
<script src=""https://d3js.org/d3-selection.v1.js""></script>
D3 is written using ES2015 modules. Create a custom bundle using Rollup, Webpack, or your preferred bundler. To import D3 into an ES2015 application, either import specific symbols from specific D3 modules:
import {scaleLinear} from ""d3-scale"";
Or import everything into a namespace (here, d3):
import * as d3 from ""d3"";
In Node:
var d3 = require(""d3"");
You can also require individual modules and combine them into a d3 object using Object.assign:
var d3 = Object.assign({}, require(""d3-format""), require(""d3-geo""), require(""d3-geo-projection""));
","GitHub - d3/d3: Bring data to life with SVG, Canvas and HTML."
26,JavaScript,"

    React Native
  


Learn once, write anywhere:
  Build mobile apps with React.






















Getting Started
 ¬∑ 
Learn the Basics
 ¬∑ 
Showcase
 ¬∑ 
Contribute
 ¬∑ 
Community
 ¬∑ 
Support

React Native brings React's declarative UI framework to iOS and Android. With React Native, you use native UI controls and have full access to the native platform.

Declarative. React makes it painless to create interactive UIs. Declarative views make your code more predictable and easier to debug.
Component-Based. Build encapsulated components that manage their state, then compose them to make complex UIs.
Developer Velocity. See local changes in seconds. Changes to JavaScript code can be live reloaded without rebuilding the native app.
Portability. Reuse code across iOS, Android, and other platforms.

React Native is developed and supported by many companies and individual core contributors. Find out more in our ecosystem overview.
Contents

Requirements
Building your first React Native app
Documentation
Upgrading
How to Contribute
Code of Conduct
License

üìã Requirements
React Native apps may target iOS 9.0 and Android 4.1 (API 16) or newer. You may use Windows, macOS, or Linux as your development operating system, though building and running iOS apps is limited to macOS. Tools like Expo can be used to work around this.
üéâ Building your first React Native app
Follow the Getting Started guide. The recommended way to install React Native depends on your project. Here you can find short guides for the most common scenarios:

Trying out React Native
Creating a New Application
Adding React Native to an Existing Application

üìñ Documentation
The full documentation for React Native can be found on our website.
The React Native documentation discusses components, APIs, and topics that are specific to React Native. For further documentation on the React API that is shared between React Native and React DOM, refer to the React documentation.
The source for the React Native documentation and website is hosted on a separate repo, @facebook/react-native-website.
üöÄ Upgrading
Upgrading to new versions of React Native may give you access to more APIs, views, developer tools, and other goodies. See the Upgrading Guide for instructions.
React Native releases are discussed in the React Native Community, @react-native-community/react-native-releases.
üëè How to Contribute
The main purpose of this repository is to continue evolving React Native core. We want to make contributing to this project as easy and transparent as possible, and we are grateful to the community for contributing bug fixes and improvements. Read below to learn how you can take part in improving React Native.
Code of Conduct
Facebook has adopted a Code of Conduct that we expect project participants to adhere to.
Please read the full text so that you can understand what actions will and will not be tolerated.
Contributing Guide
Read our Contributing Guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to React Native.
Open Source Roadmap
You can learn more about our vision for React Native in the Roadmap.
Good First Issues
We have a list of good first issues that contain bugs which have a relatively limited scope. This is a great place to get started, gain experience, and get familiar with our contribution process.
Discussions
Larger discussions and proposals are discussed in @react-native-community/discussions-and-proposals.
üìÑ License
React Native is MIT licensed, as found in the LICENSE file.
React Native documentation is Creative Commons licensed, as found in the LICENSE-docs file.
",GitHub - facebook/react-native: A framework for building native apps with React.
27,JavaScript,"Create React App  
Create React apps with no build configuration.

Creating an App ‚Äì How to create a new app.
User Guide ‚Äì How to develop apps bootstrapped with Create React App.

Create React App works on macOS, Windows, and Linux.
If something doesn‚Äôt work, please file an issue.
If you have questions or need help, please ask in our Spectrum community.
Quick Overview
npx create-react-app my-app
cd my-app
npm start
(npx comes with npm 5.2+ and higher, see instructions for older npm versions)
Then open http://localhost:3000/ to see your app.
When you‚Äôre ready to deploy to production, create a minified bundle with npm run build.



Get Started Immediately
You don‚Äôt need to install or configure tools like Webpack or Babel.
They are preconfigured and hidden so that you can focus on the code.
Create a project, and you‚Äôre good to go.
Creating an App
You‚Äôll need to have Node 8.16.0 or Node 10.16.0 or later version on your local development machine (but it‚Äôs not required on the server). You can use nvm (macOS/Linux) or nvm-windows to switch Node versions between different projects.
To create a new app, you may choose one of the following methods:
npx
npx create-react-app my-app
(npx is a package runner tool that comes with npm 5.2+ and higher, see instructions for older npm versions)
npm
npm init react-app my-app
npm init <initializer> is available in npm 6+
Yarn
yarn create react-app my-app
yarn create is available in Yarn 0.25+
It will create a directory called my-app inside the current folder.
Inside that directory, it will generate the initial project structure and install the transitive dependencies:
my-app
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ node_modules
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ public
‚îÇ   ‚îú‚îÄ‚îÄ favicon.ico
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îî‚îÄ‚îÄ manifest.json
‚îî‚îÄ‚îÄ src
    ‚îú‚îÄ‚îÄ App.css
    ‚îú‚îÄ‚îÄ App.js
    ‚îú‚îÄ‚îÄ App.test.js
    ‚îú‚îÄ‚îÄ index.css
    ‚îú‚îÄ‚îÄ index.js
    ‚îú‚îÄ‚îÄ logo.svg
    ‚îî‚îÄ‚îÄ serviceWorker.js

No configuration or complicated folder structures, only the files you need to build your app.
Once the installation is done, you can open your project folder:
cd my-app
Inside the newly created project, you can run some built-in commands:
npm start or yarn start
Runs the app in development mode.
Open http://localhost:3000 to view it in the browser.
The page will automatically reload if you make changes to the code.
You will see the build errors and lint warnings in the console.



npm test or yarn test
Runs the test watcher in an interactive mode.
By default, runs tests related to files changed since the last commit.
Read more about testing.
npm run build or yarn build
Builds the app for production to the build folder.
It correctly bundles React in production mode and optimizes the build for the best performance.
The build is minified and the filenames include the hashes.
Your app is ready to be deployed.
User Guide
You can find detailed instructions on using Create React App and many tips in its documentation.
How to Update to New Versions?
Please refer to the User Guide for this and other information.
Philosophy


One Dependency: There is only one build dependency. It uses Webpack, Babel, ESLint, and other amazing projects, but provides a cohesive curated experience on top of them.


No Configuration Required: You don't need to configure anything. A reasonably good configuration of both development and production builds is handled for you so you can focus on writing code.


No Lock-In: You can ‚Äúeject‚Äù to a custom setup at any time. Run a single command, and all the configuration and build dependencies will be moved directly into your project, so you can pick up right where you left off.


What‚Äôs Included?
Your environment will have everything you need to build a modern single-page React app:

React, JSX, ES6, TypeScript and Flow syntax support.
Language extras beyond ES6 like the object spread operator.
Autoprefixed CSS, so you don‚Äôt need -webkit- or other prefixes.
A fast interactive unit test runner with built-in support for coverage reporting.
A live development server that warns about common mistakes.
A build script to bundle JS, CSS, and images for production, with hashes and sourcemaps.
An offline-first service worker and a web app manifest, meeting all the Progressive Web App criteria. (Note: Using the service worker is opt-in as of react-scripts@2.0.0 and higher)
Hassle-free updates for the above tools with a single dependency.

Check out this guide for an overview of how these tools fit together.
The tradeoff is that these tools are preconfigured to work in a specific way. If your project needs more customization, you can ""eject"" and customize it, but then you will need to maintain this configuration.
Popular Alternatives
Create React App is a great fit for:

Learning React in a comfortable and feature-rich development environment.
Starting new single-page React applications.
Creating examples with React for your libraries and components.

Here are a few common cases where you might want to try something else:


If you want to try React without hundreds of transitive build tool dependencies, consider using a single HTML file or an online sandbox instead.


If you need to integrate React code with a server-side template framework like Rails, Django or Symfony, or if you‚Äôre not building a single-page app, consider using nwb, or Neutrino which are more flexible. For Rails specifically, you can use Rails Webpacker. For Symfony, try Symfony's Webpack Encore.


If you need to publish a React component, nwb can also do this, as well as Neutrino's react-components preset.


If you want to do server rendering with React and Node.js, check out Next.js or Razzle. Create React App is agnostic of the backend, and only produces static HTML/JS/CSS bundles.


If your website is mostly static (for example, a portfolio or a blog), consider using Gatsby instead. Unlike Create React App, it pre-renders the website into HTML at the build time.


Finally, if you need more customization, check out Neutrino and its React preset.


All of the above tools can work with little to no configuration.
If you prefer configuring the build yourself, follow this guide.
React Native
Looking for something similar, but for React Native?
Check out Expo CLI.
Contributing
We'd love to have your helping hand on create-react-app! See CONTRIBUTING.md for more information on what we're looking for and how to get started.
Credits
This project exists thanks to all the people who contribute.

Acknowledgements
We are grateful to the authors of existing related projects for their ideas and collaboration:

@eanplatter
@insin
@mxstbr

License
Create React App is open source software licensed as MIT.
",GitHub - facebook/create-react-app: Set up a modern web app by running one command.
28,JavaScript,"axios







Promise based HTTP client for the browser and node.js
Features

Make XMLHttpRequests from the browser
Make http requests from node.js
Supports the Promise API
Intercept request and response
Transform request and response data
Cancel requests
Automatic transforms for JSON data
Client side support for protecting against XSRF

Browser Support













Latest ‚úî
Latest ‚úî
Latest ‚úî
Latest ‚úî
Latest ‚úî
11 ‚úî




Installing
Using npm:
$ npm install axios
Using bower:
$ bower install axios
Using yarn:
$ yarn add axios
Using cdn:
<script src=""https://unpkg.com/axios/dist/axios.min.js""></script>
Example
note: CommonJS usage
In order to gain the TypeScript typings (for intellisense / autocomplete) while using CommonJS imports with require() use the following approach:
const axios = require('axios').default;

// axios.<method> will now provide autocomplete and parameter typings
Performing a GET request
const axios = require('axios');

// Make a request for a user with a given ID
axios.get('/user?ID=12345')
  .then(function (response) {
    // handle success
    console.log(response);
  })
  .catch(function (error) {
    // handle error
    console.log(error);
  })
  .finally(function () {
    // always executed
  });

// Optionally the request above could also be done as
axios.get('/user', {
    params: {
      ID: 12345
    }
  })
  .then(function (response) {
    console.log(response);
  })
  .catch(function (error) {
    console.log(error);
  })
  .finally(function () {
    // always executed
  });  

// Want to use async/await? Add the `async` keyword to your outer function/method.
async function getUser() {
  try {
    const response = await axios.get('/user?ID=12345');
    console.log(response);
  } catch (error) {
    console.error(error);
  }
}

NOTE: async/await is part of ECMAScript 2017 and is not supported in Internet
Explorer and older browsers, so use with caution.

Performing a POST request
axios.post('/user', {
    firstName: 'Fred',
    lastName: 'Flintstone'
  })
  .then(function (response) {
    console.log(response);
  })
  .catch(function (error) {
    console.log(error);
  });
Performing multiple concurrent requests
function getUserAccount() {
  return axios.get('/user/12345');
}

function getUserPermissions() {
  return axios.get('/user/12345/permissions');
}

axios.all([getUserAccount(), getUserPermissions()])
  .then(axios.spread(function (acct, perms) {
    // Both requests are now complete
  }));
axios API
Requests can be made by passing the relevant config to axios.
axios(config)
// Send a POST request
axios({
  method: 'post',
  url: '/user/12345',
  data: {
    firstName: 'Fred',
    lastName: 'Flintstone'
  }
});
// GET request for remote image
axios({
  method: 'get',
  url: 'http://bit.ly/2mTM3nY',
  responseType: 'stream'
})
  .then(function (response) {
    response.data.pipe(fs.createWriteStream('ada_lovelace.jpg'))
  });
axios(url[, config])
// Send a GET request (default method)
axios('/user/12345');
Request method aliases
For convenience aliases have been provided for all supported request methods.
axios.request(config)
axios.get(url[, config])
axios.delete(url[, config])
axios.head(url[, config])
axios.options(url[, config])
axios.post(url[, data[, config]])
axios.put(url[, data[, config]])
axios.patch(url[, data[, config]])
NOTE
When using the alias methods url, method, and data properties don't need to be specified in config.
Concurrency
Helper functions for dealing with concurrent requests.
axios.all(iterable)
axios.spread(callback)
Creating an instance
You can create a new instance of axios with a custom config.
axios.create([config])
const instance = axios.create({
  baseURL: 'https://some-domain.com/api/',
  timeout: 1000,
  headers: {'X-Custom-Header': 'foobar'}
});
Instance methods
The available instance methods are listed below. The specified config will be merged with the instance config.
axios#request(config)
axios#get(url[, config])
axios#delete(url[, config])
axios#head(url[, config])
axios#options(url[, config])
axios#post(url[, data[, config]])
axios#put(url[, data[, config]])
axios#patch(url[, data[, config]])
axios#getUri([config])
Request Config
These are the available config options for making requests. Only the url is required. Requests will default to GET if method is not specified.
{
  // `url` is the server URL that will be used for the request
  url: '/user',

  // `method` is the request method to be used when making the request
  method: 'get', // default

  // `baseURL` will be prepended to `url` unless `url` is absolute.
  // It can be convenient to set `baseURL` for an instance of axios to pass relative URLs
  // to methods of that instance.
  baseURL: 'https://some-domain.com/api/',

  // `transformRequest` allows changes to the request data before it is sent to the server
  // This is only applicable for request methods 'PUT', 'POST', 'PATCH' and 'DELETE'
  // The last function in the array must return a string or an instance of Buffer, ArrayBuffer,
  // FormData or Stream
  // You may modify the headers object.
  transformRequest: [function (data, headers) {
    // Do whatever you want to transform the data

    return data;
  }],

  // `transformResponse` allows changes to the response data to be made before
  // it is passed to then/catch
  transformResponse: [function (data) {
    // Do whatever you want to transform the data

    return data;
  }],

  // `headers` are custom headers to be sent
  headers: {'X-Requested-With': 'XMLHttpRequest'},

  // `params` are the URL parameters to be sent with the request
  // Must be a plain object or a URLSearchParams object
  params: {
    ID: 12345
  },

  // `paramsSerializer` is an optional function in charge of serializing `params`
  // (e.g. https://www.npmjs.com/package/qs, http://api.jquery.com/jquery.param/)
  paramsSerializer: function (params) {
    return Qs.stringify(params, {arrayFormat: 'brackets'})
  },

  // `data` is the data to be sent as the request body
  // Only applicable for request methods 'PUT', 'POST', and 'PATCH'
  // When no `transformRequest` is set, must be of one of the following types:
  // - string, plain object, ArrayBuffer, ArrayBufferView, URLSearchParams
  // - Browser only: FormData, File, Blob
  // - Node only: Stream, Buffer
  data: {
    firstName: 'Fred'
  },
  
  // syntax alternative to send data into the body
  // method post
  // only the value is sent, not the key
  data: 'Country=Brasil&City=Belo Horizonte',

  // `timeout` specifies the number of milliseconds before the request times out.
  // If the request takes longer than `timeout`, the request will be aborted.
  timeout: 1000, // default is `0` (no timeout)

  // `withCredentials` indicates whether or not cross-site Access-Control requests
  // should be made using credentials
  withCredentials: false, // default

  // `adapter` allows custom handling of requests which makes testing easier.
  // Return a promise and supply a valid response (see lib/adapters/README.md).
  adapter: function (config) {
    /* ... */
  },

  // `auth` indicates that HTTP Basic auth should be used, and supplies credentials.
  // This will set an `Authorization` header, overwriting any existing
  // `Authorization` custom headers you have set using `headers`.
  // Please note that only HTTP Basic auth is configurable through this parameter.
  // For Bearer tokens and such, use `Authorization` custom headers instead.
  auth: {
    username: 'janedoe',
    password: 's00pers3cret'
  },

  // `responseType` indicates the type of data that the server will respond with
  // options are: 'arraybuffer', 'document', 'json', 'text', 'stream'
  //   browser only: 'blob'
  responseType: 'json', // default

  // `responseEncoding` indicates encoding to use for decoding responses
  // Note: Ignored for `responseType` of 'stream' or client-side requests
  responseEncoding: 'utf8', // default

  // `xsrfCookieName` is the name of the cookie to use as a value for xsrf token
  xsrfCookieName: 'XSRF-TOKEN', // default

  // `xsrfHeaderName` is the name of the http header that carries the xsrf token value
  xsrfHeaderName: 'X-XSRF-TOKEN', // default

  // `onUploadProgress` allows handling of progress events for uploads
  onUploadProgress: function (progressEvent) {
    // Do whatever you want with the native progress event
  },

  // `onDownloadProgress` allows handling of progress events for downloads
  onDownloadProgress: function (progressEvent) {
    // Do whatever you want with the native progress event
  },

  // `maxContentLength` defines the max size of the http response content in bytes allowed
  maxContentLength: 2000,

  // `validateStatus` defines whether to resolve or reject the promise for a given
  // HTTP response status code. If `validateStatus` returns `true` (or is set to `null`
  // or `undefined`), the promise will be resolved; otherwise, the promise will be
  // rejected.
  validateStatus: function (status) {
    return status >= 200 && status < 300; // default
  },

  // `maxRedirects` defines the maximum number of redirects to follow in node.js.
  // If set to 0, no redirects will be followed.
  maxRedirects: 5, // default

  // `socketPath` defines a UNIX Socket to be used in node.js.
  // e.g. '/var/run/docker.sock' to send requests to the docker daemon.
  // Only either `socketPath` or `proxy` can be specified.
  // If both are specified, `socketPath` is used.
  socketPath: null, // default

  // `httpAgent` and `httpsAgent` define a custom agent to be used when performing http
  // and https requests, respectively, in node.js. This allows options to be added like
  // `keepAlive` that are not enabled by default.
  httpAgent: new http.Agent({ keepAlive: true }),
  httpsAgent: new https.Agent({ keepAlive: true }),

  // 'proxy' defines the hostname and port of the proxy server.
  // You can also define your proxy using the conventional `http_proxy` and
  // `https_proxy` environment variables. If you are using environment variables
  // for your proxy configuration, you can also define a `no_proxy` environment
  // variable as a comma-separated list of domains that should not be proxied.
  // Use `false` to disable proxies, ignoring environment variables.
  // `auth` indicates that HTTP Basic auth should be used to connect to the proxy, and
  // supplies credentials.
  // This will set an `Proxy-Authorization` header, overwriting any existing
  // `Proxy-Authorization` custom headers you have set using `headers`.
  proxy: {
    host: '127.0.0.1',
    port: 9000,
    auth: {
      username: 'mikeymike',
      password: 'rapunz3l'
    }
  },

  // `cancelToken` specifies a cancel token that can be used to cancel the request
  // (see Cancellation section below for details)
  cancelToken: new CancelToken(function (cancel) {
  })
}
Response Schema
The response for a request contains the following information.
{
  // `data` is the response that was provided by the server
  data: {},

  // `status` is the HTTP status code from the server response
  status: 200,

  // `statusText` is the HTTP status message from the server response
  statusText: 'OK',

  // `headers` the headers that the server responded with
  // All header names are lower cased
  headers: {},

  // `config` is the config that was provided to `axios` for the request
  config: {},

  // `request` is the request that generated this response
  // It is the last ClientRequest instance in node.js (in redirects)
  // and an XMLHttpRequest instance in the browser
  request: {}
}
When using then, you will receive the response as follows:
axios.get('/user/12345')
  .then(function (response) {
    console.log(response.data);
    console.log(response.status);
    console.log(response.statusText);
    console.log(response.headers);
    console.log(response.config);
  });
When using catch, or passing a rejection callback as second parameter of then, the response will be available through the error object as explained in the Handling Errors section.
Config Defaults
You can specify config defaults that will be applied to every request.
Global axios defaults
axios.defaults.baseURL = 'https://api.example.com';
axios.defaults.headers.common['Authorization'] = AUTH_TOKEN;
axios.defaults.headers.post['Content-Type'] = 'application/x-www-form-urlencoded';
Custom instance defaults
// Set config defaults when creating the instance
const instance = axios.create({
  baseURL: 'https://api.example.com'
});

// Alter defaults after instance has been created
instance.defaults.headers.common['Authorization'] = AUTH_TOKEN;
Config order of precedence
Config will be merged with an order of precedence. The order is library defaults found in lib/defaults.js, then defaults property of the instance, and finally config argument for the request. The latter will take precedence over the former. Here's an example.
// Create an instance using the config defaults provided by the library
// At this point the timeout config value is `0` as is the default for the library
const instance = axios.create();

// Override timeout default for the library
// Now all requests using this instance will wait 2.5 seconds before timing out
instance.defaults.timeout = 2500;

// Override timeout for this request as it's known to take a long time
instance.get('/longRequest', {
  timeout: 5000
});
Interceptors
You can intercept requests or responses before they are handled by then or catch.
// Add a request interceptor
axios.interceptors.request.use(function (config) {
    // Do something before request is sent
    return config;
  }, function (error) {
    // Do something with request error
    return Promise.reject(error);
  });

// Add a response interceptor
axios.interceptors.response.use(function (response) {
    // Any status code that lie within the range of 2xx cause this function to trigger
    // Do something with response data
    return response;
  }, function (error) {
    // Any status codes that falls outside the range of 2xx cause this function to trigger
    // Do something with response error
    return Promise.reject(error);
  });
If you need to remove an interceptor later you can.
const myInterceptor = axios.interceptors.request.use(function () {/*...*/});
axios.interceptors.request.eject(myInterceptor);
You can add interceptors to a custom instance of axios.
const instance = axios.create();
instance.interceptors.request.use(function () {/*...*/});
Handling Errors
axios.get('/user/12345')
  .catch(function (error) {
    if (error.response) {
      // The request was made and the server responded with a status code
      // that falls out of the range of 2xx
      console.log(error.response.data);
      console.log(error.response.status);
      console.log(error.response.headers);
    } else if (error.request) {
      // The request was made but no response was received
      // `error.request` is an instance of XMLHttpRequest in the browser and an instance of
      // http.ClientRequest in node.js
      console.log(error.request);
    } else {
      // Something happened in setting up the request that triggered an Error
      console.log('Error', error.message);
    }
    console.log(error.config);
  });
Using the validateStatus config option, you can define HTTP code(s) that should throw an error.
axios.get('/user/12345', {
  validateStatus: function (status) {
    return status < 500; // Reject only if the status code is greater than or equal to 500
  }
})
Using toJSON you get an object with more information about the HTTP error.
axios.get('/user/12345')
  .catch(function (error) {
    console.log(error.toJSON());
  });
Cancellation
You can cancel a request using a cancel token.

The axios cancel token API is based on the withdrawn cancelable promises proposal.

You can create a cancel token using the CancelToken.source factory as shown below:
const CancelToken = axios.CancelToken;
const source = CancelToken.source();

axios.get('/user/12345', {
  cancelToken: source.token
}).catch(function (thrown) {
  if (axios.isCancel(thrown)) {
    console.log('Request canceled', thrown.message);
  } else {
    // handle error
  }
});

axios.post('/user/12345', {
  name: 'new name'
}, {
  cancelToken: source.token
})

// cancel the request (the message parameter is optional)
source.cancel('Operation canceled by the user.');
You can also create a cancel token by passing an executor function to the CancelToken constructor:
const CancelToken = axios.CancelToken;
let cancel;

axios.get('/user/12345', {
  cancelToken: new CancelToken(function executor(c) {
    // An executor function receives a cancel function as a parameter
    cancel = c;
  })
});

// cancel the request
cancel();

Note: you can cancel several requests with the same cancel token.

Using application/x-www-form-urlencoded format
By default, axios serializes JavaScript objects to JSON. To send data in the application/x-www-form-urlencoded format instead, you can use one of the following options.
Browser
In a browser, you can use the URLSearchParams API as follows:
const params = new URLSearchParams();
params.append('param1', 'value1');
params.append('param2', 'value2');
axios.post('/foo', params);

Note that URLSearchParams is not supported by all browsers (see caniuse.com), but there is a polyfill available (make sure to polyfill the global environment).

Alternatively, you can encode data using the qs library:
const qs = require('qs');
axios.post('/foo', qs.stringify({ 'bar': 123 }));
Or in another way (ES6),
import qs from 'qs';
const data = { 'bar': 123 };
const options = {
  method: 'POST',
  headers: { 'content-type': 'application/x-www-form-urlencoded' },
  data: qs.stringify(data),
  url,
};
axios(options);
Node.js
In node.js, you can use the querystring module as follows:
const querystring = require('querystring');
axios.post('http://something.com/', querystring.stringify({ foo: 'bar' }));
You can also use the qs library.
NOTE
The qs library is preferable if you need to stringify nested objects, as the querystring method has known issues with that use case (https://github.com/nodejs/node-v0.x-archive/issues/1665).
Semver
Until axios reaches a 1.0 release, breaking changes will be released with a new minor version. For example 0.5.1, and 0.5.4 will have the same API, but 0.6.0 will have breaking changes.
Promises
axios depends on a native ES6 Promise implementation to be supported.
If your environment doesn't support ES6 Promises, you can polyfill.
TypeScript
axios includes TypeScript definitions.
import axios from 'axios';
axios.get('/user?ID=12345');
Resources

Changelog
Upgrade Guide
Ecosystem
Contributing Guide
Code of Conduct

Credits
axios is heavily inspired by the $http service provided in Angular. Ultimately axios is an effort to provide a standalone $http-like service for use outside of Angular.
License
MIT
",GitHub - axios/axios: Promise based HTTP client for the browser and node.js
29,JavaScript,"




Node.js is a JavaScript runtime built on Chrome's V8 JavaScript engine. For
more information on using Node.js, see the Node.js Website.
The Node.js project uses an open governance model. The
OpenJS Foundation provides support for the project.
This project is bound by a Code of Conduct.
Table of Contents

Support
Release Types

Download

Current and LTS Releases
Nightly Releases
API Documentation


Verifying Binaries


Building Node.js
Security
Contributing to Node.js
Current Project Team Members

TSC (Technical Steering Committee)
Collaborators
Release Keys



Support
Looking for help? Check out the
instructions for getting support.
Release Types

Current: Under active development. Code for the Current release is in the
branch for its major version number (for example,
v10.x). Node.js releases a new
major version every 6 months, allowing for breaking changes. This happens in
April and October every year. Releases appearing each October have a support
life of 8 months. Releases appearing each April convert to LTS (see below)
each October.
LTS: Releases that receive Long-term Support, with a focus on stability
and security. Every even-numbered major version will become an LTS release.
LTS releases receive 12 months of Active LTS support and a further 18 months
of Maintenance. LTS release lines have alphabetically-ordered codenames,
beginning with v4 Argon. There are no breaking changes or feature additions,
except in some special circumstances.
Nightly: Code from the Current branch built every 24-hours when there are
changes. Use with caution.

Current and LTS releases follow Semantic Versioning. A
member of the Release Team signs each Current and LTS release.
For more information, see the
Release README.
Download
Binaries, installers, and source tarballs are available at
https://nodejs.org/en/download/.
Current and LTS Releases
https://nodejs.org/download/release/
The latest directory is an
alias for the latest Current release. The latest-codename directory is an
alias for the latest release from an LTS line. For example, the
latest-carbon directory
contains the latest Carbon (Node.js 8) release.
Nightly Releases
https://nodejs.org/download/nightly/
Each directory name and filename contains a date (in UTC time) and the commit
SHA at the HEAD of the release.
API Documentation
Documentation for the latest Current release is at https://nodejs.org/api/.
Version-specific documentation is available in each release directory in the
docs subdirectory. Version-specific documentation is also at
https://nodejs.org/download/docs/.
Verifying Binaries
Download directories contain a SHASUMS256.txt file with SHA checksums for the
files.
To download SHASUMS256.txt using curl:
$ curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt
To check that a downloaded file matches the checksum, run
it through sha256sum with a command such as:
$ grep node-vx.y.z.tar.gz SHASUMS256.txt | sha256sum -c -
For Current and LTS, the GPG detached signature of SHASUMS256.txt is in
SHASUMS256.txt.sig. You can use it with gpg to verify the integrity of
SHASUM256.txt. You will first need to import
the GPG keys of individuals authorized to create releases. To
import the keys:
$ gpg --keyserver pool.sks-keyservers.net --recv-keys DD8F2338BAE7501E3DD5AC78C273792F7D83545D
See the bottom of this README for a full script to import active release keys.
Next, download the SHASUMS256.txt.sig for the release:
$ curl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt.sig
Then use gpg --verify SHASUMS256.txt.sig SHASUMS256.txt to verify
the file's signature.
Building Node.js
See BUILDING.md for instructions on how to build Node.js from
source and a list of supported platforms.
Security
For information on reporting security vulnerabilities in Node.js, see
SECURITY.md.
Contributing to Node.js

Contributing to the project
Working Groups
Strategic Initiatives

Current Project Team Members
For information about the governance of the Node.js project, see
GOVERNANCE.md.
TSC (Technical Steering Committee)

addaleax -
Anna Henningsen <anna@addaleax.net> (she/her)
apapirovski -
Anatoli Papirovski <apapirovski@mac.com> (he/him)
BethGriggs -
Beth Griggs <Bethany.Griggs@uk.ibm.com> (she/her)
ChALkeR -
–°–∫–æ–≤–æ—Ä–æ–¥–∞ –ù–∏–∫–∏—Ç–∞ –ê–Ω–¥—Ä–µ–µ–≤–∏—á <chalkerx@gmail.com> (he/him)
cjihrig -
Colin Ihrig <cjihrig@gmail.com> (he/him)
danbev -
Daniel Bevenius <daniel.bevenius@gmail.com> (he/him)
fhinkel -
Franziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)
Fishrock123 -
Jeremiah Senkpiel <fishrock123@rocketmail.com>
gabrielschulhof -
Gabriel Schulhof <gabriel.schulhof@intel.com>
gireeshpunathil -
Gireesh Punathil <gpunathi@in.ibm.com> (he/him)
jasnell -
James M Snell <jasnell@gmail.com> (he/him)
joyeecheung -
Joyee Cheung <joyeec9h3@gmail.com> (she/her)
mcollina -
Matteo Collina <matteo.collina@gmail.com> (he/him)
mhdawson -
Michael Dawson <michael_dawson@ca.ibm.com> (he/him)
MylesBorins -
Myles Borins <myles.borins@gmail.com> (he/him)
sam-github -
Sam Roberts <vieuxtech@gmail.com>
targos -
Micha√´l Zasso <targos@protonmail.com> (he/him)
thefourtheye -
Sakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)
tniessen -
Tobias Nie√üen <tniessen@tnie.de>
Trott -
Rich Trott <rtrott@gmail.com> (he/him)

TSC Emeriti

bnoordhuis -
Ben Noordhuis <info@bnoordhuis.nl>
chrisdickinson -
Chris Dickinson <christopher.s.dickinson@gmail.com>
evanlucas -
Evan Lucas <evanlucas@me.com> (he/him)
gibfahn -
Gibson Fahnestock <gibfahn@gmail.com> (he/him)
indutny -
Fedor Indutny <fedor.indutny@gmail.com>
isaacs -
Isaac Z. Schlueter <i@izs.me>
joshgav -
Josh Gavant <josh.gavant@outlook.com>
mscdex -
Brian White <mscdex@mscdex.net>
nebrius -
Bryan Hughes <bryan@nebri.us>
ofrobots -
Ali Ijaz Sheikh <ofrobots@google.com> (he/him)
orangemocha -
Alexis Campailla <orangemocha@nodejs.org>
piscisaureus -
Bert Belder <bertbelder@gmail.com>
rvagg -
Rod Vagg <r@va.gg>
shigeki -
Shigeki Ohtsu <ohtsu@ohtsu.org> (he/him)
TimothyGu -
Tiancheng ""Timothy"" Gu <timothygu99@gmail.com> (he/him)
trevnorris -
Trevor Norris <trev.norris@gmail.com>

Collaborators

addaleax -
Anna Henningsen <anna@addaleax.net> (she/her)
ak239 -
Aleksei Koziatinskii <ak239spb@gmail.com>
AndreasMadsen -
Andreas Madsen <amwebdk@gmail.com> (he/him)
antsmartian -
Anto Aravinth <anto.aravinth.cse@gmail.com> (he/him)
apapirovski -
Anatoli Papirovski <apapirovski@mac.com> (he/him)
aqrln -
Alexey Orlenko <eaglexrlnk@gmail.com> (he/him)
bcoe -
Ben Coe <bencoe@gmail.com> (he/him)
bengl -
Bryan English <bryan@bryanenglish.com> (he/him)
benjamingr -
Benjamin Gruenbaum <benjamingr@gmail.com>
BethGriggs -
Beth Griggs <Bethany.Griggs@uk.ibm.com> (she/her)
bmeck -
Bradley Farias <bradley.meck@gmail.com>
bmeurer -
Benedikt Meurer <benedikt.meurer@gmail.com>
bnoordhuis -
Ben Noordhuis <info@bnoordhuis.nl>
boneskull -
Christopher Hiller <boneskull@boneskull.com> (he/him)
BridgeAR -
Ruben Bridgewater <ruben@bridgewater.de> (he/him)
bzoz -
Bartosz Sosnowski <bartosz@janeasystems.com>
calvinmetcalf -
Calvin Metcalf <calvin.metcalf@gmail.com>
cclauss -
Christian Clauss <cclauss@me.com> (he/him)
ChALkeR -
–°–∫–æ–≤–æ—Ä–æ–¥–∞ –ù–∏–∫–∏—Ç–∞ –ê–Ω–¥—Ä–µ–µ–≤–∏—á <chalkerx@gmail.com> (he/him)
cjihrig -
Colin Ihrig <cjihrig@gmail.com> (he/him)
claudiorodriguez -
Claudio Rodriguez <cjrodr@yahoo.com>
codebytere -
Shelley Vohr <codebytere@gmail.com> (she/her)
danbev -
Daniel Bevenius <daniel.bevenius@gmail.com> (he/him)
davisjam -
Jamie Davis <davisjam@vt.edu> (he/him)
devnexen -
David Carlier <devnexen@gmail.com>
devsnek -
Gus Caplan <me@gus.host> (he/him)
digitalinfinity -
Hitesh Kanwathirtha <digitalinfinity@gmail.com> (he/him)
edsadr -
Adrian Estrada <edsadr@gmail.com> (he/him)
eljefedelrodeodeljefe -
Robert Jefe Lindstaedt <robert.lindstaedt@gmail.com>
eugeneo -
Eugene Ostroukhov <eostroukhov@google.com>
evanlucas -
Evan Lucas <evanlucas@me.com> (he/him)
fhinkel -
Franziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)
Fishrock123 -
Jeremiah Senkpiel <fishrock123@rocketmail.com>
gabrielschulhof -
Gabriel Schulhof <gabriel.schulhof@intel.com>
gdams -
George Adams <george.adams@uk.ibm.com> (he/him)
geek -
Wyatt Preul <wpreul@gmail.com>
gengjiawen -
Jiawen Geng <technicalcute@gmail.com>
gibfahn -
Gibson Fahnestock <gibfahn@gmail.com> (he/him)
gireeshpunathil -
Gireesh Punathil <gpunathi@in.ibm.com> (he/him)
guybedford -
Guy Bedford <guybedford@gmail.com> (he/him)
hashseed -
Yang Guo <yangguo@chromium.org> (he/him)
hiroppy -
Yuta Hiroto <hello@hiroppy.me> (he/him)
iarna -
Rebecca Turner <me@re-becca.org>
indutny -
Fedor Indutny <fedor.indutny@gmail.com>
italoacasas -
Italo A. Casas <me@italoacasas.com> (he/him)
JacksonTian -
Jackson Tian <shyvo1987@gmail.com>
jasnell -
James M Snell <jasnell@gmail.com> (he/him)
jbergstroem -
Johan Bergstr√∂m <bugs@bergstroem.nu>
jdalton -
John-David Dalton <john.david.dalton@gmail.com>
jkrems -
Jan Krems <jan.krems@gmail.com> (he/him)
joaocgreis -
Jo√£o Reis <reis@janeasystems.com>
joyeecheung -
Joyee Cheung <joyeec9h3@gmail.com> (she/her)
julianduque -
Julian Duque <julianduquej@gmail.com> (he/him)
JungMinu -
Minwoo Jung <nodecorelab@gmail.com> (he/him)
kfarnung -
Kyle Farnung <kfarnung@microsoft.com> (he/him)
lance -
Lance Ball <lball@redhat.com> (he/him)
legendecas -
Chengzhong Wu <legendecas@gmail.com> (he/him)
Leko -
Shingo Inoue <leko.noor@gmail.com> (he/him)
lpinca -
Luigi Pinca <luigipinca@gmail.com> (he/him)
lundibundi -
Denys Otrishko <shishugi@gmail.com> (he/him)
maclover7 -
Jon Moss <me@jonathanmoss.me> (he/him)
mafintosh
Mathias Buus <mathiasbuus@gmail.com> (he/him)
mcollina -
Matteo Collina <matteo.collina@gmail.com> (he/him)
mhdawson -
Michael Dawson <michael_dawson@ca.ibm.com> (he/him)
misterdjules -
Julien Gilli <jgilli@nodejs.org>
mmarchini -
Matheus Marchini <mat@mmarchini.me>
MoonBall -
Chen Gang <gangc.cxy@foxmail.com>
mscdex -
Brian White <mscdex@mscdex.net>
MylesBorins -
Myles Borins <myles.borins@gmail.com> (he/him)
not-an-aardvark -
Teddy Katz <teddy.katz@gmail.com> (he/him)
ofrobots -
Ali Ijaz Sheikh <ofrobots@google.com> (he/him)
oyyd -
Ouyang Yadong <oyydoibh@gmail.com> (he/him)
princejwesley -
Prince John Wesley <princejohnwesley@gmail.com>
psmarshall -
Peter Marshall <petermarshall@chromium.org> (he/him)
Qard -
Stephen Belanger <admin@stephenbelanger.com> (he/him)
refack -
Refael Ackermann (◊®◊§◊ê◊ú ◊§◊ú◊ó◊ô) <refack@gmail.com> (he/him/◊î◊ï◊ê/◊ê◊™◊î)
richardlau -
Richard Lau <riclau@uk.ibm.com>
ronkorving -
Ron Korving <ron@ronkorving.nl>
rubys -
Sam Ruby <rubys@intertwingly.net>
rvagg -
Rod Vagg <rod@vagg.org>
ryzokuken -
Ujjwal Sharma <usharma1998@gmail.com> (he/him)
saghul -
Sa√∫l Ibarra Corretg√© <saghul@gmail.com>
sam-github -
Sam Roberts <vieuxtech@gmail.com>
santigimeno -
Santiago Gimeno <santiago.gimeno@gmail.com>
sebdeckers -
Sebastiaan Deckers <sebdeckers83@gmail.com>
seishun -
Nikolai Vavilov <vvnicholas@gmail.com>
shigeki -
Shigeki Ohtsu <ohtsu@ohtsu.org> (he/him)
shisama -
Masashi Hirano <shisama07@gmail.com> (he/him)
silverwind -
Roman Reiss <me@silverwind.io>
srl295 -
Steven R Loomis <srloomis@us.ibm.com>
starkwang -
Weijia Wang <starkwang@126.com>
targos -
Micha√´l Zasso <targos@protonmail.com> (he/him)
thefourtheye -
Sakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)
thekemkid -
Glen Keane <glenkeane.94@gmail.com> (he/him)
TimothyGu -
Tiancheng ""Timothy"" Gu <timothygu99@gmail.com> (he/him)
tniessen -
Tobias Nie√üen <tniessen@tnie.de>
trevnorris -
Trevor Norris <trev.norris@gmail.com>
trivikr -
Trivikram Kamat <trivikr.dev@gmail.com>
Trott -
Rich Trott <rtrott@gmail.com> (he/him)
vdeturckheim -
Vladimir de Turckheim <vlad2t@hotmail.com> (he/him)
vkurchatkin -
Vladimir Kurchatkin <vladimir.kurchatkin@gmail.com>
watilde -
Daijiro Wachi <daijiro.wachi@gmail.com> (he/him)
watson -
Thomas Watson <w@tson.dk>
XadillaX -
Khaidi Chu <i@2333.moe> (he/him)
yhwang -
Yihong Wang <yh.wang@ibm.com>
yorkie -
Yorkie Liu <yorkiefixer@gmail.com>
yosuke-furukawa -
Yosuke Furukawa <yosuke.furukawa@gmail.com>
ZYSzys -
Yongsheng Zhang <zyszys98@gmail.com> (he/him)

Collaborator Emeriti

andrasq -
Andras <andras@kinvey.com>
AnnaMag -
Anna M. Kedzierska <anna.m.kedzierska@gmail.com>
brendanashworth -
Brendan Ashworth <brendan.ashworth@me.com>
estliberitas -
Alexander Makarenko <estliberitas@gmail.com>
chrisdickinson -
Chris Dickinson <christopher.s.dickinson@gmail.com>
DavidCai1993 -
David Cai <davidcai1993@yahoo.com> (he/him)
firedfox -
Daniel Wang <wangyang0123@gmail.com>
imran-iq -
Imran Iqbal <imran@imraniqbal.org>
imyller -
Ilkka Myller <ilkka.myller@nodefield.com>
isaacs -
Isaac Z. Schlueter <i@izs.me>
jasongin -
Jason Ginchereau <jasongin@microsoft.com>
jhamhader -
Yuval Brik <yuval@brik.org.il>
joshgav -
Josh Gavant <josh.gavant@outlook.com>
kunalspathak -
Kunal Pathak <kunal.pathak@microsoft.com>
lucamaraschi -
Luca Maraschi <luca.maraschi@gmail.com> (he/him)
lxe -
Aleksey Smolenchuk <lxe@lxe.co>
matthewloring -
Matthew Loring <mattloring@google.com>
micnic -
Nicu Micleu»ôanu <micnic90@gmail.com> (he/him)
mikeal -
Mikeal Rogers <mikeal.rogers@gmail.com>
monsanto -
Christopher Monsanto <chris@monsan.to>
Olegas -
Oleg Elifantiev <oleg@elifantiev.ru>
orangemocha -
Alexis Campailla <orangemocha@nodejs.org>
othiym23 -
Forrest L Norvell <ogd@aoaioxxysz.net> (he/him)
petkaantonov -
Petka Antonov <petka_antonov@hotmail.com>
phillipj -
Phillip Johnsen <johphi@gmail.com>
piscisaureus -
Bert Belder <bertbelder@gmail.com>
pmq20 -
Minqi Pan <pmq2001@gmail.com>
rlidwka -
Alex Kocharin <alex@kocharin.ru>
rmg -
Ryan Graham <r.m.graham@gmail.com>
robertkowalski -
Robert Kowalski <rok@kowalski.gd>
romankl -
Roman Klauke <romaaan.git@gmail.com>
RReverser -
Ingvar Stepanyan <me@rreverser.com>
stefanmb -
Stefan Budeanu <stefan@budeanu.com>
tellnes -
Christian Tellnes <christian@tellnes.no>
thlorenz -
Thorsten Lorenz <thlorenz@gmx.de>
tunniclm -
Mike Tunnicliffe <m.j.tunnicliffe@gmail.com>
vsemozhetbyt -
Vse Mozhet Byt <vsemozhetbyt@gmail.com> (he/him)
whitlockjc -
Jeremy Whitlock <jwhitlock@apache.org>

Collaborators follow the COLLABORATOR_GUIDE.md in
maintaining the Node.js project.
Release Keys
GPG keys used to sign Node.js releases:

Beth Griggs <bethany.griggs@uk.ibm.com>
4ED778F539E3634C779C87C6D7062848A1AB005C
Colin Ihrig <cjihrig@gmail.com>
94AE36675C464D64BAFA68DD7434390BDBE9B9C5
Evan Lucas <evanlucas@me.com>
B9AE9905FFD7803F25714661B63B535A4C206CA9
Gibson Fahnestock <gibfahn@gmail.com>
77984A986EBC2AA786BC0F66B01FBB92821C587A
James M Snell <jasnell@keybase.io>
71DCFD284A79C3B38668286BC97EC7A07EDE3FC1
Jeremiah Senkpiel <fishrock@keybase.io>
FD3A5288F042B6850C66B31F09FE44734EB7990E
Micha√´l Zasso <targos@protonmail.com>
8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600
Myles Borins <myles.borins@gmail.com>
C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8
Rod Vagg <rod@vagg.org>
DD8F2338BAE7501E3DD5AC78C273792F7D83545D
Ruben Bridgewater <ruben@bridgewater.de>
A48C2BEE680E841632CD4E44F07496B3EB3C1762
Shelley Vohr <shelley.vohr@gmail.com>
B9E2F5981AA6E0CD28160D9FF13993A75599653C

To import the full set of trusted release keys:
gpg --keyserver pool.sks-keyservers.net --recv-keys 4ED778F539E3634C779C87C6D7062848A1AB005C
gpg --keyserver pool.sks-keyservers.net --recv-keys B9E2F5981AA6E0CD28160D9FF13993A75599653C
gpg --keyserver pool.sks-keyservers.net --recv-keys 94AE36675C464D64BAFA68DD7434390BDBE9B9C5
gpg --keyserver pool.sks-keyservers.net --recv-keys B9AE9905FFD7803F25714661B63B535A4C206CA9
gpg --keyserver pool.sks-keyservers.net --recv-keys 77984A986EBC2AA786BC0F66B01FBB92821C587A
gpg --keyserver pool.sks-keyservers.net --recv-keys 71DCFD284A79C3B38668286BC97EC7A07EDE3FC1
gpg --keyserver pool.sks-keyservers.net --recv-keys FD3A5288F042B6850C66B31F09FE44734EB7990E
gpg --keyserver pool.sks-keyservers.net --recv-keys 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600
gpg --keyserver pool.sks-keyservers.net --recv-keys C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8
gpg --keyserver pool.sks-keyservers.net --recv-keys DD8F2338BAE7501E3DD5AC78C273792F7D83545D
gpg --keyserver pool.sks-keyservers.net --recv-keys A48C2BEE680E841632CD4E44F07496B3EB3C1762
See the section above on Verifying Binaries for how to
use these keys to verify a downloaded file.
Other keys used to sign some previous releases:

Chris Dickinson <christopher.s.dickinson@gmail.com>
9554F04D7259F04124DE6B476D5A82AC7E37093B
Isaac Z. Schlueter <i@izs.me>
93C7E9E91B49E432C2F75674B0A78B0A6C481CF6
Italo A. Casas <me@italoacasas.com>
56730D5401028683275BD23C23EFEFE93C4CFFFE
Julien Gilli <jgilli@fastmail.fm>
114F43EE0176B71C7BC219DD50A3051F888C628D
Timothy J Fontaine <tjfontaine@gmail.com>
7937DFD2AB06298B2293C3187D33FF9D0246406D

",GitHub - nodejs/node: Node.js JavaScript runtime
30,JavaScript,"

Version 5 ‚Äì the iconic SVG, font, and CSS framework

The internet's most popular icon toolkit has been redesigned and built from
scratch. On top of this, features like icon font ligatures, an SVG framework,
official NPM packages for popular frontend libraries like React, and access to
a new CDN.
Not familiar with Font Awesome 5? Learn
more about our
successful Kickstarter and plan. You can also order Font Awesome
Pro which includes tons more icons directly
from fontawesome.com.
Documentation
Learn how to get started with Font Awesome and then dive deeper into other and advanced topics:
Using Font Awesome on the Web

With SVG with JavaScript
With web fonts with CSS
Upgrading from version 4
Installing Font Awesome with a package manager
Downloading + hosting Font Awesome yourself
Performance and security
Accessibility
Troubleshooting

Advanced Options & Techniques

Using CSS pseudo-elements
SVG sprites
The Font Awesome API
SVG symbols
SVG JavaScript Core
Server side rendering

Using Font Awesome on the Desktop

Getting started
Upgrading from version 4
Using ligatures
Using glyphs
Troubleshooting

Where did Font Awesome 4 (or 3) go?
Now that Font Awesome 5 has been released we are marking version 4 as
end-of-life. We don't plan on releasing any further versions of the 4.x or 3.x.
Documentation is still available but it's moved to
https://fontawesome.com/v4.7.0 and
https://fontawesome.com/v3.2.1.
The Git repository for
v4.7.0 and
v3.2.1 can
be found in our GitHub releases.
Change log
We'll keep track of each release in the CHANGELOG.md
Looking for older versions of Font Awesome? Check the releases.
Upgrading
From time-to-time we'll have special upgrading instructions from one version to the next.
Check out the UPGRADING.md guide when you upgrade your dependencies.
Code of conduct
We will behave ourselves if you behave yourselves. For more details see our
CODE_OF_CONDUCT.md.
Contributing
Please read through our contributing guidelines.  Included
are directions for opening issues.
Versioning
Font Awesome will be maintained under the Semantic Versioning guidelines as much as possible. Releases will be numbered
with the following format:
<major>.<minor>.<patch>
For more information on SemVer, please visit http://semver.org.
The major version ""5"" is part of an umbrella release.  It includes many different types of files and technologies. Therefore
we deviate from normal SemVer in the following ways:

Any release may update the design, look-and-feel, or branding of an existing
icon
We will never intentionally release a patch version update that breaks
backward compatibility
A minor release may include backward-incompatible changes but we will
write clear upgrading instructions in UPGRADING.md
A minor or patch release will never remove icons
Bug fixes will be addressed as patch releases unless they include backward
incompatibility then they will be minor releases

License
Font Awesome Free is free, open source, and GPL friendly. You can use it for
commercial projects, open source projects, or really almost whatever you want.

Icons ‚Äî CC BY 4.0 License

In the Font Awesome Free download, the CC BY 4.0 license applies to all icons packaged as .svg and .js files types.


Fonts ‚Äî SIL OFL 1.1 License

In the Font Awesome Free download, the SIL OLF license applies to all icons packaged as web and desktop font files.


Code ‚Äî MIT License

In the Font Awesome Free download, the MIT license applies to all non-font and non-icon files.



Attribution is required by MIT, SIL OLF, and CC BY licenses. Downloaded Font
Awesome Free files already contain embedded comments with sufficient
attribution, so you shouldn't need to do anything additional when using these
files normally.
We've kept attribution comments terse, so we ask that you do not actively work
to remove them from files, especially code. They're a great way for folks to
learn about Font Awesome.
Team

Dave Gandy
Travis Chase
Rob Madole
Brian Talbot
Jory Raphael
Mike Wilkerson
Trevor Chase
Jason Lundien
Jason Otero
Edward Emanuel
Geremia Taglialatela

","GitHub - FortAwesome/Font-Awesome: The iconic SVG, font, and CSS toolkit"
31,JavaScript,"AngularJS 
AngularJS lets you write client-side web applications as if you had a smarter browser.  It lets you
use good old HTML (or HAML, Jade/Pug and friends!) as your template language and lets you extend HTML‚Äôs
syntax to express your application‚Äôs components clearly and succinctly.  It automatically
synchronizes data from your UI (view) with your JavaScript objects (model) through 2-way data
binding. To help you structure your application better and make it easy to test, AngularJS teaches
the browser how to do dependency injection and inversion of control.
It also helps with server-side communication, taming async callbacks with promises and deferred objects,
and it makes client-side navigation and deep linking with hashbang urls or HTML5 pushState a
piece of cake. Best of all? It makes development fun!

On July 1, 2018 AngularJS entered a 3 year Long Term Support period: Find out more
Looking for the new Angular? Go here: https://github.com/angular/angular


Web site: https://angularjs.org
Tutorial: https://docs.angularjs.org/tutorial
API Docs: https://docs.angularjs.org/api
Developer Guide: https://docs.angularjs.org/guide
Contribution guidelines: CONTRIBUTING.md
Core Development: DEVELOPERS.md
Dashboard: https://dashboard.angularjs.org

Documentation
Go to https://docs.angularjs.org
Contribute
We've set up a separate document for our
contribution guidelines.
Develop
We've set up a separate document for
developers.

What to use AngularJS for and when to use it
AngularJS is the next generation framework where each component is designed to work with every other
component in an interconnected way like a well-oiled machine. AngularJS is JavaScript MVC made easy
and done right. (Well it is not really MVC, read on, to understand what this means.)
MVC, no, MV* done the right way!
MVC, short for Model-View-Controller, is a design pattern, i.e. how the code should be organized and
how the different parts of an application separated for proper readability and debugging. Model is
the data and the database. View is the user interface and what the user sees. Controller is the main
link between Model and View. These are the three pillars of major programming frameworks present on
the market today. On the other hand AngularJS works on MV*, short for Model-View-Whatever. The
Whatever is AngularJS's way of telling that you may create any kind of linking between the Model
and the View here.
Unlike other frameworks in any programming language, where MVC, the three separate components, each
one has to be written and then connected by the programmer, AngularJS helps the programmer by asking
him/her to just create these and everything else will be taken care of by AngularJS.
Interconnection with HTML at the root level
AngularJS uses HTML to define the user's interface. AngularJS also enables the programmer to write
new HTML tags (AngularJS Directives) and increase the readability and understandability of the HTML
code. Directives are AngularJS‚Äôs way of bringing additional functionality to HTML. Directives
achieve this by enabling us to invent our own HTML elements. This also helps in making the code DRY
(Don't Repeat Yourself), which means once created, a new directive can be used anywhere within the
application.
HTML is also used to determine the wiring of the app. Special attributes in the HTML determine where
to load the app, which components or controllers to use for each element, etc. We specify ""what""
gets loaded, but not ""how"". This declarative approach greatly simplifies app development in a sort
of WYSIWYG way. Rather than spending time on how the program flows and orchestrating the various
moving parts, we simply define what we want and AngularJS will take care of the dependencies.
Data Handling made simple
Data and Data Models in AngularJS are plain JavaScript objects and one can add and change properties
directly on it and loop over objects and arrays at will.
Two-way Data Binding
One of AngularJS's strongest features. Two-way Data Binding means that if something changes in the
Model, the change gets reflected in the View instantaneously, and the same happens the other way
around. This is also referred to as Reactive Programming, i.e. suppose a = b + c is being
programmed and after this, if the value of b and/or c is changed then the value of a will be
automatically updated to reflect the change. AngularJS uses its ""scopes"" as a glue between the Model
and View and makes these updates in one available for the other.
Less Written Code and Easily Maintainable Code
Everything in AngularJS is created to enable the programmer to end up writing less code that is
easily maintainable and readable by any other new person on the team. Believe it or not, one can
write a complete working two-way data binded application in less than 10 lines of code. Try and see
for yourself!
Testing Ready
AngularJS has Dependency Injection, i.e. it takes care of providing all the necessary dependencies
to its controllers and services whenever required. This helps in making the AngularJS code ready for
unit testing by making use of mock dependencies created and injected. This makes AngularJS more
modular and easily testable thus in turn helping a team create more robust applications.
",GitHub - angular/angular.js: AngularJS - HTML enhanced for web apps!
32,JavaScript,"JavaScript Algorithms and Data Structures


This repository contains JavaScript based examples of many
popular algorithms and data structures.
Each algorithm and data structure has its own separate README
with related explanations and links for further reading (including ones
to YouTube videos).
Read this in other languages:
ÁÆÄ‰Ωì‰∏≠Êñá,
ÁπÅÈ´î‰∏≠Êñá,
ÌïúÍµ≠Ïñ¥,
Êó•Êú¨Ë™û,
Polski,
Fran√ßais,
Espa√±ol,
Portugu√™s
‚òù Note that this project is meant to be used for learning and researching purposes
only and it is not meant to be used for production.
Data Structures
A data structure is a particular way of organizing and storing data in a computer so that it can
be accessed and modified efficiently. More precisely, a data structure is a collection of data
values, the relationships among them, and the functions or operations that can be applied to
the data.
B - Beginner, A - Advanced

B Linked List
B Doubly Linked List
B Queue
B Stack
B Hash Table
B Heap - max and min heap versions
B Priority Queue
A Trie
A Tree

A Binary Search Tree
A AVL Tree
A Red-Black Tree
A Segment Tree - with min/max/sum range queries examples
A Fenwick Tree (Binary Indexed Tree)


A Graph (both directed and undirected)
A Disjoint Set
A Bloom Filter

Algorithms
An algorithm is an unambiguous specification of how to solve a class of problems. It is
a set of rules that precisely define a sequence of operations.
B - Beginner, A - Advanced
Algorithms by Topic

Math

B Bit Manipulation - set/get/update/clear bits, multiplication/division by two, make negative etc.
B Factorial
B Fibonacci Number - classic and closed-form versions
B Primality Test (trial division method)
B Euclidean Algorithm - calculate the Greatest Common Divisor (GCD)
B Least Common Multiple (LCM)
B Sieve of Eratosthenes - finding all prime numbers up to any given limit
B Is Power of Two - check if the number is power of two (naive and bitwise algorithms)
B Pascal's Triangle
B Complex Number - complex numbers and basic operations with them
B Radian & Degree - radians to degree and backwards conversion
B Fast Powering
A Integer Partition
A Square Root - Newton's method
A Liu Hui œÄ Algorithm - approximate œÄ calculations based on N-gons
A Discrete Fourier Transform - decompose a function of time (a signal) into the frequencies that make it up


Sets

B Cartesian Product - product of multiple sets
B Fisher‚ÄìYates Shuffle - random permutation of a finite sequence
A Power Set - all subsets of a set (bitwise and backtracking solutions)
A Permutations (with and without repetitions)
A Combinations (with and without repetitions)
A Longest Common Subsequence (LCS)
A Longest Increasing Subsequence
A Shortest Common Supersequence (SCS)
A Knapsack Problem - ""0/1"" and ""Unbound"" ones
A Maximum Subarray - ""Brute Force"" and ""Dynamic Programming"" (Kadane's) versions
A Combination Sum - find all combinations that form specific sum


Strings

B Hamming Distance - number of positions at which the symbols are different
A Levenshtein Distance - minimum edit distance between two sequences
A Knuth‚ÄìMorris‚ÄìPratt Algorithm (KMP Algorithm) - substring search (pattern matching)
A Z Algorithm - substring search (pattern matching)
A Rabin Karp Algorithm - substring search
A Longest Common Substring
A Regular Expression Matching


Searches

B Linear Search
B Jump Search (or Block Search) - search in sorted array
B Binary Search - search in sorted array
B Interpolation Search - search in uniformly distributed sorted array


Sorting

B Bubble Sort
B Selection Sort
B Insertion Sort
B Heap Sort
B Merge Sort
B Quicksort - in-place and non-in-place implementations
B Shellsort
B Counting Sort
B Radix Sort


Linked Lists

B Straight Traversal
B Reverse Traversal


Trees

B Depth-First Search (DFS)
B Breadth-First Search (BFS)


Graphs

B Depth-First Search (DFS)
B Breadth-First Search (BFS)
B Kruskal‚Äôs Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph
A Dijkstra Algorithm - finding shortest paths to all graph vertices from single vertex
A Bellman-Ford Algorithm - finding shortest paths to all graph vertices from single vertex
A Floyd-Warshall Algorithm - find shortest paths between all pairs of vertices
A Detect Cycle - for both directed and undirected graphs (DFS and Disjoint Set based versions)
A Prim‚Äôs Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph
A Topological Sorting - DFS method
A Articulation Points - Tarjan's algorithm (DFS based)
A Bridges - DFS based algorithm
A Eulerian Path and Eulerian Circuit - Fleury's algorithm - Visit every edge exactly once
A Hamiltonian Cycle - Visit every vertex exactly once
A Strongly Connected Components - Kosaraju's algorithm
A Travelling Salesman Problem - shortest possible route that visits each city and returns to the origin city


Cryptography

B Polynomial Hash - rolling hash function based on polynomial


Machine Learning

B NanoNeuron - 7 simple JS functions that illustrate how machines can actually learn (forward/backward propagation)


Uncategorized

B Tower of Hanoi
B Square Matrix Rotation - in-place algorithm
B Jump Game - backtracking, dynamic programming (top-down + bottom-up) and greedy examples
B Unique Paths - backtracking, dynamic programming and Pascal's Triangle based examples
B Rain Terraces - trapping rain water problem (dynamic programming and brute force versions)
B Recursive Staircase - count the number of ways to reach to the top (4 solutions)
A N-Queens Problem
A Knight's Tour



Algorithms by Paradigm
An algorithmic paradigm is a generic method or approach which underlies the design of a class
of algorithms. It is an abstraction higher than the notion of an algorithm, just as an
algorithm is an abstraction higher than a computer program.

Brute Force - look at all the possibilities and selects the best solution

B Linear Search
B Rain Terraces - trapping rain water problem
B Recursive Staircase - count the number of ways to reach to the top
A Maximum Subarray
A Travelling Salesman Problem - shortest possible route that visits each city and returns to the origin city
A Discrete Fourier Transform - decompose a function of time (a signal) into the frequencies that make it up


Greedy - choose the best option at the current time, without any consideration for the future

B Jump Game
A Unbound Knapsack Problem
A Dijkstra Algorithm - finding shortest path to all graph vertices
A Prim‚Äôs Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph
A Kruskal‚Äôs Algorithm - finding Minimum Spanning Tree (MST) for weighted undirected graph


Divide and Conquer - divide the problem into smaller parts and then solve those parts

B Binary Search
B Tower of Hanoi
B Pascal's Triangle
B Euclidean Algorithm - calculate the Greatest Common Divisor (GCD)
B Merge Sort
B Quicksort
B Tree Depth-First Search (DFS)
B Graph Depth-First Search (DFS)
B Jump Game
B Fast Powering
A Permutations (with and without repetitions)
A Combinations (with and without repetitions)


Dynamic Programming - build up a solution using previously found sub-solutions

B Fibonacci Number
B Jump Game
B Unique Paths
B Rain Terraces - trapping rain water problem
B Recursive Staircase - count the number of ways to reach to the top
A Levenshtein Distance - minimum edit distance between two sequences
A Longest Common Subsequence (LCS)
A Longest Common Substring
A Longest Increasing Subsequence
A Shortest Common Supersequence
A 0/1 Knapsack Problem
A Integer Partition
A Maximum Subarray
A Bellman-Ford Algorithm - finding shortest path to all graph vertices
A Floyd-Warshall Algorithm - find shortest paths between all pairs of vertices
A Regular Expression Matching


Backtracking - similarly to brute force, try to generate all possible solutions, but each time you generate next solution you test
if it satisfies all conditions, and only then continue generating subsequent solutions. Otherwise, backtrack, and go on a
different path of finding a solution. Normally the DFS traversal of state-space is being used.

B Jump Game
B Unique Paths
B Power Set - all subsets of a set
A Hamiltonian Cycle - Visit every vertex exactly once
A N-Queens Problem
A Knight's Tour
A Combination Sum - find all combinations that form specific sum


Branch & Bound - remember the lowest-cost solution found at each stage of the backtracking
search, and use the cost of the lowest-cost solution found so far as a lower bound on the cost of
a least-cost solution to the problem, in order to discard partial solutions with costs larger than the
lowest-cost solution found so far. Normally BFS traversal in combination with DFS traversal of state-space
tree is being used.

How to use this repository
Install all dependencies
npm install

Run ESLint
You may want to run it to check code quality.
npm run lint

Run all tests
npm test

Run tests by name
npm test -- 'LinkedList'

Playground
You may play with data-structures and algorithms in ./src/playground/playground.js file and write
tests for it in ./src/playground/__test__/playground.test.js.
Then just simply run the following command to test if your playground code works as expected:
npm test -- 'playground'

Useful Information
References
‚ñ∂ Data Structures and Algorithms on YouTube
Big O Notation
Big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows.
On the chart below you may find most common orders of growth of algorithms specified in Big O notation.

Source: Big O Cheat Sheet.
Below is the list of some of the most used Big O notations and their performance comparisons against different sizes of the input data.



Big O Notation
Computations for 10 elements
Computations for 100 elements
Computations for 1000 elements




O(1)
1
1
1


O(log N)
3
6
9


O(N)
10
100
1000


O(N log N)
30
600
9000


O(N^2)
100
10000
1000000


O(2^N)
1024
1.26e+29
1.07e+301


O(N!)
3628800
9.3e+157
4.02e+2567



Data Structure Operations Complexity



Data Structure
Access
Search
Insertion
Deletion
Comments




Array
1
n
n
n



Stack
n
n
1
1



Queue
n
n
1
1



Linked List
n
n
1
n



Hash Table
-
n
n
n
In case of perfect hash function costs would be O(1)


Binary Search Tree
n
n
n
n
In case of balanced tree costs would be O(log(n))


B-Tree
log(n)
log(n)
log(n)
log(n)



Red-Black Tree
log(n)
log(n)
log(n)
log(n)



AVL Tree
log(n)
log(n)
log(n)
log(n)



Bloom Filter
-
1
1
-
False positives are possible while searching



Array Sorting Algorithms Complexity



Name
Best
Average
Worst
Memory
Stable
Comments




Bubble sort
n
n2
n2
1
Yes



Insertion sort
n
n2
n2
1
Yes



Selection sort
n2
n2
n2
1
No



Heap sort
n¬†log(n)
n¬†log(n)
n¬†log(n)
1
No



Merge sort
n¬†log(n)
n¬†log(n)
n¬†log(n)
n
Yes



Quick sort
n¬†log(n)
n¬†log(n)
n2
log(n)
No
Quicksort is usually done in-place with O(log(n)) stack space


Shell sort
n¬†log(n)
depends on gap sequence
n¬†(log(n))2
1
No



Counting sort
n + r
n + r
n + r
n + r
Yes
r - biggest number in array


Radix sort
n * k
n * k
n * k
n + k
Yes
k - length of longest key



",GitHub - trekhleb/javascript-algorithms: üìù Algorithms and data structures implemented in JavaScript with explanations and links to further readings
33,JavaScript,"three.js






JavaScript 3D library
The aim of the project is to create an easy to use, lightweight, 3D library with a default WebGL renderer. The library also provides Canvas 2D, SVG and CSS3D renderers in the examples.
Examples ‚Äî
Documentation ‚Äî
Wiki ‚Äî
Migrating ‚Äî
Questions ‚Äî
Forum ‚Äî
Gitter ‚Äî
Slack
Usage
Download the minified library and include it in your HTML, or install and import it as a module,
Alternatively, see how to build the library yourself.
<script src=""js/three.min.js""></script>
This code creates a scene, a camera, and a geometric cube, and it adds the cube to the scene. It then creates a WebGL renderer for the scene and camera, and it adds that viewport to the document.body element. Finally, it animates the cube within the scene for the camera.
var camera, scene, renderer;
var geometry, material, mesh;

init();
animate();

function init() {

	camera = new THREE.PerspectiveCamera( 70, window.innerWidth / window.innerHeight, 0.01, 10 );
	camera.position.z = 1;

	scene = new THREE.Scene();

	geometry = new THREE.BoxGeometry( 0.2, 0.2, 0.2 );
	material = new THREE.MeshNormalMaterial();

	mesh = new THREE.Mesh( geometry, material );
	scene.add( mesh );

	renderer = new THREE.WebGLRenderer( { antialias: true } );
	renderer.setSize( window.innerWidth, window.innerHeight );
	document.body.appendChild( renderer.domElement );

}

function animate() {

	requestAnimationFrame( animate );

	mesh.rotation.x += 0.01;
	mesh.rotation.y += 0.02;

	renderer.render( scene, camera );

}
If everything went well you should see this.
Change log
Releases
",GitHub - mrdoob/three.js: JavaScript 3D library.
34,JavaScript,"Puppeteer
    

API | FAQ | Contributing | Troubleshooting

Puppeteer is a Node library which provides a high-level API to control Chrome or Chromium over the DevTools Protocol. Puppeteer runs headless by default, but can be configured to run full (non-headless) Chrome or Chromium.

What can I do?
Most things that you can do manually in the browser can be done using Puppeteer! Here are a few examples to get you started:

Generate screenshots and PDFs of pages.
Crawl a SPA (Single-Page Application) and generate pre-rendered content (i.e. ""SSR"" (Server-Side Rendering)).
Automate form submission, UI testing, keyboard input, etc.
Create an up-to-date, automated testing environment. Run your tests directly in the latest version of Chrome using the latest JavaScript and browser features.
Capture a timeline trace of your site to help diagnose performance issues.
Test Chrome Extensions.

Give it a spin: https://try-puppeteer.appspot.com/
Getting Started
Installation
To use Puppeteer in your project, run:
npm i puppeteer
# or ""yarn add puppeteer""
Note: When you install Puppeteer, it downloads a recent version of Chromium (~170MB Mac, ~282MB Linux, ~280MB Win) that is guaranteed to work with the API. To skip the download, see Environment variables.
puppeteer-core
Since version 1.7.0 we publish the puppeteer-core package,
a version of Puppeteer that doesn't download Chromium by default.
npm i puppeteer-core
# or ""yarn add puppeteer-core""
puppeteer-core is intended to be a lightweight version of Puppeteer for launching an existing browser installation or for connecting to a remote one. Be sure that the version of puppeteer-core you install is compatible with the
browser you intend to connect to.
See puppeteer vs puppeteer-core.
Usage
Puppeteer follows the latest maintenance LTS version of Node.
Note: Prior to v1.18.1, Puppeteer required at least Node v6.4.0. All subsequent versions rely on
Node 8.9.0+. All examples below use async/await which is only supported in Node v7.6.0 or greater.
Puppeteer will be familiar to people using other browser testing frameworks. You create an instance
of Browser, open pages, and then manipulate them with Puppeteer's API.
Example - navigating to https://example.com and saving a screenshot as example.png:
Save file as example.js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');
  await page.screenshot({path: 'example.png'});

  await browser.close();
})();
Execute script on the command line
node example.js
Puppeteer sets an initial page size to 800√ó600px, which defines the screenshot size. The page size can be customized  with Page.setViewport().
Example - create a PDF.
Save file as hn.js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://news.ycombinator.com', {waitUntil: 'networkidle2'});
  await page.pdf({path: 'hn.pdf', format: 'A4'});

  await browser.close();
})();
Execute script on the command line
node hn.js
See Page.pdf() for more information about creating pdfs.
Example - evaluate script in the context of the page
Save file as get-dimensions.js
const puppeteer = require('puppeteer');

(async () => {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://example.com');

  // Get the ""viewport"" of the page, as reported by the page.
  const dimensions = await page.evaluate(() => {
    return {
      width: document.documentElement.clientWidth,
      height: document.documentElement.clientHeight,
      deviceScaleFactor: window.devicePixelRatio
    };
  });

  console.log('Dimensions:', dimensions);

  await browser.close();
})();
Execute script on the command line
node get-dimensions.js
See Page.evaluate() for more information on evaluate and related methods like evaluateOnNewDocument and exposeFunction.
Default runtime settings
1. Uses Headless mode
Puppeteer launches Chromium in headless mode. To launch a full version of Chromium, set the headless option when launching a browser:
const browser = await puppeteer.launch({headless: false}); // default is true
2. Runs a bundled version of Chromium
By default, Puppeteer downloads and uses a specific version of Chromium so its API
is guaranteed to work out of the box. To use Puppeteer with a different version of Chrome or Chromium,
pass in the executable's path when creating a Browser instance:
const browser = await puppeteer.launch({executablePath: '/path/to/Chrome'});
See Puppeteer.launch() for more information.
See this article for a description of the differences between Chromium and Chrome. This article describes some differences for Linux users.
3. Creates a fresh user profile
Puppeteer creates its own Chromium user profile which it cleans up on every run.
Resources

API Documentation
Examples
Community list of Puppeteer resources

Debugging tips


Turn off headless mode - sometimes it's useful to see what the browser is
displaying. Instead of launching in headless mode, launch a full version of
the browser using  headless: false:
 const browser = await puppeteer.launch({headless: false});



Slow it down - the slowMo option slows down Puppeteer operations by the
specified amount of milliseconds. It's another way to help see what's going on.
 const browser = await puppeteer.launch({
   headless: false,
   slowMo: 250 // slow down by 250ms
 });



Capture console output - You can listen for the console event.
This is also handy when debugging code in page.evaluate():
 page.on('console', msg => console.log('PAGE LOG:', msg.text()));

 await page.evaluate(() => console.log(`url is ${location.href}`));



Use debugger in application code browser
There are two execution context: node.js that is running test code, and the browser
running application code being tested. This lets you debug code in the
application code browser; ie code inside evaluate().


Use {devtools: true} when launching Puppeteer:
const browser = await puppeteer.launch({devtools: true});


Change default test timeout:
jest: jest.setTimeout(100000);
jasmine: jasmine.DEFAULT_TIMEOUT_INTERVAL = 100000;
mocha: this.timeout(100000); (don't forget to change test to use function and not '=>')


Add an evaluate statement with debugger inside / add  debugger to an existing evaluate statement:
await page.evaluate(() => {debugger;});
The test will now stop executing in the above evaluate statement, and chromium will stop in debug mode.




Use debugger in node.js
This will let you debug test code. For example, you can step over await page.click() in the node.js script and see the click happen in the application code browser.
Note that you won't be able to run await page.click() in
DevTools console due to this Chromium bug. So if
you want to try something out, you have to add it to your test file.

Add debugger; to your test, eg:
debugger;
await page.click('a[target=_blank]');


Set headless to false
Run node --inspect-brk, eg node --inspect-brk node_modules/.bin/jest tests
In Chrome open chrome://inspect/#devices and click inspect
In the newly opened test browser, type F8 to resume test execution
Now your debugger will be hit and you can debug in the test browser



Enable verbose logging - internal DevTools protocol traffic
will be logged via the debug module under the puppeteer namespace.
 # Basic verbose logging
 env DEBUG=""puppeteer:*"" node script.js

 # Protocol traffic can be rather noisy. This example filters out all Network domain messages
 env DEBUG=""puppeteer:*"" env DEBUG_COLORS=true node script.js 2>&1 | grep -v '""Network'



Debug your Puppeteer (node) code easily, using ndb




npm install -g ndb (or even better, use npx!)


add a debugger to your Puppeteer (node) code


add ndb (or npx ndb) before your test command. For example:
ndb jest or ndb mocha (or npx ndb jest / npx ndb mocha)


debug your test inside chromium like a boss!


Contributing to Puppeteer
Check out contributing guide to get an overview of Puppeteer development.
FAQ
Q: Who maintains Puppeteer?
The Chrome DevTools team maintains the library, but we'd love your help and expertise on the project!
See Contributing.
Q: What are Puppeteer‚Äôs goals and principles?
The goals of the project are:

Provide a slim, canonical library that highlights the capabilities of the DevTools Protocol.
Provide a reference implementation for similar testing libraries. Eventually, these other frameworks could adopt Puppeteer as their foundational layer.
Grow the adoption of headless/automated browser testing.
Help dogfood new DevTools Protocol features...and catch bugs!
Learn more about the pain points of automated browser testing and help fill those gaps.

We adapt Chromium principles to help us drive product decisions:

Speed: Puppeteer has almost zero performance overhead over an automated page.
Security: Puppeteer operates off-process with respect to Chromium, making it safe to automate potentially malicious pages.
Stability: Puppeteer should not be flaky and should not leak memory.
Simplicity: Puppeteer provides a high-level API that‚Äôs easy to use, understand, and debug.

Q: Is Puppeteer replacing Selenium/WebDriver?
No. Both projects are valuable for very different reasons:

Selenium/WebDriver focuses on cross-browser automation; its value proposition is a single standard API that works across all major browsers.
Puppeteer focuses on Chromium; its value proposition is richer functionality and higher reliability.

That said, you can use Puppeteer to run tests against Chromium, e.g. using the community-driven jest-puppeteer. While this probably shouldn‚Äôt be your only testing solution, it does have a few good points compared to WebDriver:

Puppeteer requires zero setup and comes bundled with the Chromium version it works best with, making it very easy to start with. At the end of the day, it‚Äôs better to have a few tests running chromium-only, than no tests at all.
Puppeteer has event-driven architecture, which removes a lot of potential flakiness. There‚Äôs no need for evil ‚Äúsleep(1000)‚Äù calls in puppeteer scripts.
Puppeteer runs headless by default, which makes it fast to run. Puppeteer v1.5.0 also exposes browser contexts, making it possible to efficiently parallelize test execution.
Puppeteer shines when it comes to debugging: flip the ‚Äúheadless‚Äù bit to false, add ‚ÄúslowMo‚Äù, and you‚Äôll see what the browser is doing. You can even open Chrome DevTools to inspect the test environment.

Q: Why doesn‚Äôt Puppeteer v.XXX work with Chromium v.YYY?
We see Puppeteer as an indivisible entity with Chromium. Each version of Puppeteer bundles a specific version of Chromium ‚Äì the only version it is guaranteed to work with.
This is not an artificial constraint: A lot of work on Puppeteer is actually taking place in the Chromium repository. Here‚Äôs a typical story:

A Puppeteer bug is reported: https://github.com/puppeteer/puppeteer/issues/2709
It turned out this is an issue with the DevTools protocol, so we‚Äôre fixing it in Chromium: https://chromium-review.googlesource.com/c/chromium/src/+/1102154
Once the upstream fix is landed, we roll updated Chromium into Puppeteer: https://github.com/puppeteer/puppeteer/pull/2769

However, oftentimes it is desirable to use Puppeteer with the official Google Chrome rather than Chromium. For this to work, you should install a puppeteer-core version that corresponds to the Chrome version.
For example, in order to drive Chrome 71 with puppeteer-core, use chrome-71 npm tag:
npm install puppeteer-core@chrome-71
Q: Which Chromium version does Puppeteer use?
Look for chromium_revision in package.json. To find the corresponding Chromium commit and version number, search for the revision prefixed by an r in OmahaProxy's ""Find Releases"" section.
Q: What‚Äôs considered a ‚ÄúNavigation‚Äù?
From Puppeteer‚Äôs standpoint, ‚Äúnavigation‚Äù is anything that changes a page‚Äôs URL.
Aside from regular navigation where the browser hits the network to fetch a new document from the web server, this includes anchor navigations and History API usage.
With this definition of ‚Äúnavigation,‚Äù Puppeteer works seamlessly with single-page applications.
Q: What‚Äôs the difference between a ‚Äútrusted"" and ""untrusted"" input event?
In browsers, input events could be divided into two big groups: trusted vs. untrusted.

Trusted events: events generated by users interacting with the page, e.g. using a mouse or keyboard.
Untrusted event: events generated by Web APIs, e.g. document.createEvent or element.click() methods.

Websites can distinguish between these two groups:

using an Event.isTrusted event flag
sniffing for accompanying events. For example, every trusted 'click' event is preceded by 'mousedown' and 'mouseup' events.

For automation purposes it‚Äôs important to generate trusted events. All input events generated with Puppeteer are trusted and fire proper accompanying events. If, for some reason, one needs an untrusted event, it‚Äôs always possible to hop into a page context with page.evaluate and generate a fake event:
await page.evaluate(() => {
  document.querySelector('button[type=submit]').click();
});
Q: What features does Puppeteer not support?
You may find that Puppeteer does not behave as expected when controlling pages that incorporate audio and video. (For example, video playback/screenshots is likely to fail.) There are two reasons for this:

Puppeteer is bundled with Chromium ‚Äî not Chrome ‚Äî and so by default, it inherits all of Chromium's media-related limitations. This means that Puppeteer does not support licensed formats such as AAC or H.264. (However, it is possible to force Puppeteer to use a separately-installed version Chrome instead of Chromium via the executablePath option to puppeteer.launch. You should only use this configuration if you need an official release of Chrome that supports these media formats.)
Since Puppeteer (in all configurations) controls a desktop version of Chromium/Chrome, features that are only supported by the mobile version of Chrome are not supported. This means that Puppeteer does not support HTTP Live Streaming (HLS).

Q: I am having trouble installing / running Puppeteer in my test environment. Where should I look for help?
We have a troubleshooting guide for various operating systems that lists the required dependencies.
Q: How do I try/test a prerelease version of Puppeteer?
You can check out this repo or install the latest prerelease from npm:
npm i --save puppeteer@next
Please note that prerelease may be unstable and contain bugs.
Q: I have more questions! Where do I ask?
There are many ways to get help on Puppeteer:

bugtracker
Stack Overflow
slack channel

Make sure to search these channels before posting your question.
",GitHub - puppeteer/puppeteer: Headless Chrome Node.js API
35,JavaScript,"
30 seconds of code

Short JavaScript code snippets for all your development needs


Visit our website to view our snippet collection.
Use the Search page to find snippets that suit your needs. You can search by name, tag, language or using a snippet's description. Just start typing a term and see what comes up.
Browse the JavaScript Snippet List to see all the snippets in this project or click individual tags at the top of the same page to narrow down your search to a specific tag.
Click on each snippet card to view the whole snippet, including code, explanation and examples.
You can use the button on the right side of a snippet card to copy the code to clipboard.
If you like the project, give it a star. It means a lot to the people maintaining it.

Want to contribute?

If you want to help us improve, take a minute to read the Contribution Guidelines first.
Use the Snippet Template to add new snippets to the collection.
If you find a problem with a specific snippet, please open an issue.
If you find a problem with the website, please report it in the web repository.

Credits & Sponsors

This repository is maintained by the 30-seconds organization on GitHub.
All snippets are licensed under the CC0-1.0 License, unless explicitly stated otherwise.
Logos, names and trademarks are not to be used without the explicit consent of the maintainers or owners of the 30 seconds GitHub organization.
Our website is powered by Netlify, Gatsby, Travis CI & GitHub.

",GitHub - 30-seconds/30-seconds-of-code: Short JavaScript code snippets for all your development needs
36,JavaScript,"


Material-UI

React components that implement Google's Material Design.















Installation
Material-UI is available as an npm package.
Stable channel v4
// with npm
npm install @material-ui/core

// with yarn
yarn add @material-ui/core
v3.x (Migration from v3 to v4)
v0.x (Migration to v1)
Please note that @next will only point to pre-releases; to get the latest stable release use @latest instead.
Who sponsors Material-UI?
Diamond üíé
3/3 slots available
Diamond Sponsors are those who have pledged $2,000/month and more to Material-UI.
Please contact us at diamond@material-ui.com to subscribe to this tier.
Gold üèÜ
via Patreon




via OpenCollective



Gold Sponsors are those who have pledged $500/month and more to Material-UI.
There is more!
See the full list of our backers.
Usage
Here is a quick example to get you started, it's all you need:
import React from 'react';
import ReactDOM from 'react-dom';
import Button from '@material-ui/core/Button';

function App() {
  return (
    <Button variant=""contained"" color=""primary"">
      Hello World
    </Button>
  );
}

ReactDOM.render(<App />, document.querySelector('#app'));
Yes, it's really all you need to get started as you can see in this live and interactive demo:

Questions
For how-to questions and other non-issues,
please use StackOverflow instead of Github issues.
There is a StackOverflow tag called ""material-ui"" that you can use to tag your questions.
Examples
Are you looking for an example project to get started?
We host some.
Documentation
Check out our documentation website.
Premium Themes
You can find complete templates & themes in our premium themes section.
Contributing
Read our contributing guide to learn about our development process, how to propose bugfixes and improvements, and how to build and test your changes to Material-UI.
Notice that contributions go far beyond pull requests and commits.
Although we love giving you the opportunity to put your stamp on Material-UI, we also are thrilled to receive a variety of other contributions.
Changelog
Recently Updated?
Please read the changelog.
Roadmap
The future plans and high priority features and enhancements can be found in the roadmap file.
License
This project is licensed under the terms of the
MIT license.
","GitHub - mui-org/material-ui: React components for faster and easier web development. Build your own design system, or start with Material Design."
37,JavaScript,"jQuery ‚Äî New Wave JavaScript


Contribution Guides
In the spirit of open source software development, jQuery always encourages community code contribution. To help you get started and before you jump into writing code, be sure to read these important contribution guidelines thoroughly:

Getting Involved
Core Style Guide
Writing Code for jQuery Foundation Projects

Environments in which to use jQuery

Browser support
jQuery also supports Node, browser extensions, and other non-browser environments.

What you need to build your own jQuery
To build jQuery, you need to have the latest Node.js/npm and git 1.7 or later. Earlier versions might work, but are not supported.
For Windows, you have to download and install git and Node.js.
macOS users should install Homebrew. Once Homebrew is installed, run brew install git to install git,
and brew install node to install Node.js.
Linux/BSD users should use their appropriate package managers to install git and Node.js, or build from source
if you swing that way. Easy-peasy.
How to build your own jQuery
Clone a copy of the main jQuery git repo by running:
git clone git://github.com/jquery/jquery.git
Enter the jquery directory and run the build script:
cd jquery && npm run build
The built version of jQuery will be put in the dist/ subdirectory, along with the minified copy and associated map file.
If you want to create custom build or help with jQuery development, it would be better to install grunt command line interface as a global package:
npm install -g grunt-cli

Make sure you have grunt installed by testing:
grunt -V

Now by running the grunt command, in the jquery directory, you can build a full version of jQuery, just like with an npm run build command:
grunt

There are many other tasks available for jQuery Core:
grunt -help

Modules
Special builds can be created that exclude subsets of jQuery functionality.
This allows for smaller custom builds when the builder is certain that those parts of jQuery are not being used.
For example, an app that only used JSONP for $.ajax() and did not need to calculate offsets or positions of elements could exclude the offset and ajax/xhr modules.
Any module may be excluded except for core, and selector. To exclude a module, pass its path relative to the src folder (without the .js extension).
Some example modules that can be excluded are:

ajax: All AJAX functionality: $.ajax(), $.get(), $.post(), $.ajaxSetup(), .load(), transports, and ajax event shorthands such as .ajaxStart().
ajax/xhr: The XMLHTTPRequest AJAX transport only.
ajax/script: The <script> AJAX transport only; used to retrieve scripts.
ajax/jsonp: The JSONP AJAX transport only; depends on the ajax/script transport.
css: The .css() method. Also removes all modules depending on css (including effects, dimensions, and offset).
css/showHide:  Non-animated .show(), .hide() and .toggle(); can be excluded if you use classes or explicit .css() calls to set the display property. Also removes the effects module.
deprecated: Methods documented as deprecated but not yet removed.
dimensions: The .width() and .height() methods, including inner- and outer- variations.
effects: The .animate() method and its shorthands such as .slideUp() or .hide(""slow"").
event: The .on() and .off() methods and all event functionality. Also removes event/alias.
event/alias: All event attaching/triggering shorthands like .click() or .mouseover().
event/trigger: The .trigger() and .triggerHandler() methods. Used by the alias module.
offset: The .offset(), .position(), .offsetParent(), .scrollLeft(), and .scrollTop() methods.
wrap: The .wrap(), .wrapAll(), .wrapInner(), and .unwrap() methods.
core/ready: Exclude the ready module if you place your scripts at the end of the body. Any ready callbacks bound with jQuery() will simply be called immediately. However, jQuery(document).ready() will not be a function and .on(""ready"", ...) or similar will not be triggered.
deferred: Exclude jQuery.Deferred. This also removes jQuery.Callbacks. Note that modules that depend on jQuery.Deferred(AJAX, effects, core/ready) will not be removed and will still expect jQuery.Deferred to be there. Include your own jQuery.Deferred implementation or exclude those modules as well (grunt custom:-deferred,-ajax,-effects,-core/ready).
exports/global: Exclude the attachment of global jQuery variables ($ and jQuery) to the window.
exports/amd: Exclude the AMD definition.

The build process shows a message for each dependent module it excludes or includes.
AMD name
As an option, you can set the module name for jQuery's AMD definition. By default, it is set to ""jquery"", which plays nicely with plugins and third-party libraries, but there may be cases where you'd like to change this. Simply set the ""amd"" option:
grunt custom --amd=""custom-name""
Or, to define anonymously, set the name to an empty string.
grunt custom --amd=""""
Custom Build Examples
To create a custom build, first check out the version:
git pull; git checkout VERSION
Where VERSION is the version you want to customize. Then, make sure all Node dependencies are installed:
npm install
Create the custom build using the grunt custom option, listing the modules to be excluded.
Exclude all ajax functionality:
grunt custom:-ajax
Excluding css removes modules depending on CSS: effects, offset, dimensions.
grunt custom:-css
Exclude a bunch of modules:
grunt custom:-ajax,-css,-deprecated,-dimensions,-effects,-event/alias,-offset,-wrap
For questions or requests regarding custom builds, please start a thread on the Developing jQuery Core section of the forum. Due to the combinatorics and custom nature of these builds, they are not regularly tested in jQuery's unit test process.
Running the Unit Tests
Make sure you have the necessary dependencies:
npm install
Start grunt watch or npm start to auto-build jQuery as you work:
grunt watch
Run the unit tests with a local server that supports PHP. Ensure that you run the site from the root directory, not the ""test"" directory. No database is required. Pre-configured php local servers are available for Windows and Mac. Here are some options:

Windows: WAMP download
Mac: MAMP download
Linux: Setting up LAMP
Mongoose (most platforms)

Building to a different directory
To copy the built jQuery files from /dist to another directory:
grunt && grunt dist:/path/to/special/location/
With this example, the output files would be:
/path/to/special/location/jquery.js
/path/to/special/location/jquery.min.js
To add a permanent copy destination, create a file in dist/ called "".destination.json"". Inside the file, paste and customize the following:
{
  ""/Absolute/path/to/other/destination"": true
}
Additionally, both methods can be combined.
Essential Git
As the source code is handled by the Git version control system, it's useful to know some features used.
Cleaning
If you want to purge your working directory back to the status of upstream, the following commands can be used (remember everything you've worked on is gone after these):
git reset --hard upstream/master
git clean -fdx
Rebasing
For feature/topic branches, you should always use the --rebase flag to git pull, or if you are usually handling many temporary ""to be in a github pull request"" branches, run the following to automate this:
git config branch.autosetuprebase local
(see man git-config for more information)
Handling merge conflicts
If you're getting merge conflicts when merging, instead of editing the conflicted files manually, you can use the feature
git mergetool. Even though the default tool xxdiff looks awful/old, it's rather useful.
The following are some commands that can be used there:

Ctrl + Alt + M - automerge as much as possible
b - jump to next merge conflict
s - change the order of the conflicted lines
u - undo a merge
left mouse button - mark a block to be the winner
middle mouse button - mark a line to be the winner
Ctrl + S - save
Ctrl + Q - quit

QUnit Reference
Test methods
expect( numAssertions );
stop();
start();
Note: QUnit's eventual addition of an argument to stop/start is ignored in this test suite so that start and stop can be passed as callbacks without worrying about their parameters.
Test assertions
ok( value, [message] );
equal( actual, expected, [message] );
notEqual( actual, expected, [message] );
deepEqual( actual, expected, [message] );
notDeepEqual( actual, expected, [message] );
strictEqual( actual, expected, [message] );
notStrictEqual( actual, expected, [message] );
throws( block, [expected], [message] );
Test Suite Convenience Methods Reference (See test/data/testinit.js)
Returns an array of elements with the given IDs
q( ... );
Example:
q(""main"", ""foo"", ""bar"");

=> [ div#main, span#foo, input#bar ]
Asserts that a selection matches the given IDs
t( testName, selector, [ ""array"", ""of"", ""ids"" ] );
Example:
t(""Check for something"", ""//[a]"", [""foo"", ""bar""]);
Fires a native DOM event without going through jQuery
fireNative( node, eventType )
Example:
fireNative( jQuery(""#elem"")[0], ""click"" );
Add random number to url to stop caching
url( ""some/url"" );
Example:
url(""index.html"");

=> ""data/index.html?10538358428943""


url(""mock.php?foo=bar"");

=> ""data/mock.php?foo=bar&10538358345554""
Run tests in an iframe
Some tests may require a document other than the standard test fixture, and
these can be run in a separate iframe. The actual test code and assertions
remain in jQuery's main test files; only the minimal test fixture markup
and setup code should be placed in the iframe file.
testIframe( testName, fileName,
  function testCallback(
      assert, jQuery, window, document,
	  [ additional args ] ) {
	...
  } );
This loads a page, constructing a url with fileName ""./data/"" + fileName.
The iframed page determines when the callback occurs in the test by
including the ""/test/data/iframeTest.js"" script and calling
startIframeTest( [ additional args ] ) when appropriate. Often this
will be after either document ready or window.onload fires.
The testCallback receives the QUnit assert object created by testIframe
for this test, followed by the global jQuery, window, and document from
the iframe. If the iframe code passes any arguments to startIframeTest,
they follow the document argument.
Questions?
If you have any questions, please feel free to ask on the
Developing jQuery Core forum or in #jquery on irc.freenode.net.
",GitHub - jquery/jquery: jQuery JavaScript Library
38,JavaScript,"




































webpack

    webpack is a module bundler. Its main purpose is to bundle JavaScript files for usage in a browser, yet it is also capable of transforming, bundling, or packaging just about any resource or asset.
  

Table of Contents

Install
Introduction
Concepts
Contributing
Support
Core Team
Sponsoring
Premium Partners
Other Backers and Sponsors
Gold Sponsors
Silver Sponsors
Bronze Sponsors
Backers
Special Thanks

Install
Install with npm:
npm install --save-dev webpack
Install with yarn:
yarn add webpack --dev
Introduction
webpack is a bundler for modules. The main purpose is to bundle JavaScript
files for usage in a browser, yet it is also capable of transforming, bundling,
or packaging just about any resource or asset.
TL;DR

Bundles ES Modules, CommonJS, and AMD modules (even combined).
Can create a single bundle or multiple chunks that are asynchronously loaded at runtime (to reduce initial loading time).
Dependencies are resolved during compilation, reducing the runtime size.
Loaders can preprocess files while compiling, e.g. TypeScript to JavaScript, Handlebars strings to compiled functions, images to Base64, etc.
Highly modular plugin system to do whatever else your application requires.

Get Started
Check out webpack's quick Get Started guide and the other guides.
Browser Compatibility
webpack supports all browsers that are ES5-compliant (IE8 and below are not supported).
webpack also needs Promise for import() and require.ensure(). If you want to support older browsers, you will need to load a polyfill before using these expressions.
Concepts
Plugins
webpack has a rich plugin
interface. Most of the features
within webpack itself use this plugin interface. This makes webpack very
flexible.



Name
Status
Install Size
Description




mini-css-extract-plugin


Extracts CSS into separate files. It creates a CSS file per JS file which contains CSS.


compression-webpack-plugin


Prepares compressed versions of assets to serve them with Content-Encoding


i18n-webpack-plugin


Adds i18n support to your bundles


html-webpack-plugin


Simplifies creation of HTML files (index.html) to serve your bundles


extract-text-webpack-plugin


Extract text from a bundle, or bundles, into a separate file



Loaders
webpack enables the use of loaders to preprocess files. This allows you to bundle
any static resource way beyond JavaScript. You can easily write your own
loaders using Node.js.
Loaders are activated by using loadername! prefixes in require() statements,
or are automatically applied via regex from your webpack configuration.
Files



Name
Status
Install Size
Description




raw-loader


Loads raw content of a file (utf-8)


val-loader


Executes code as module and considers exports as JS code


url-loader


Works like the file loader, but can return a Data Url if the file is smaller than a limit


file-loader


Emits the file into the output folder and returns the (relative) url



JSON



Name
Status
Install Size
Description







Loads a JSON file (included by default)





Loads and transpiles a JSON 5 file





Loads and transpiles a CSON file



Transpiling



Name
Status
Install Size
Description




<script>


Executes a JavaScript file once in global context (like in script tag), require()s are not parsed





Loads ES2015+ code and transpiles to ES5 using Babel





Loads ES2015+ code and transpiles to ES5 using Traceur





Loads TypeScript like JavaScript


awesome-typescript-loader


Awesome TypeScript loader for webpack





Loads CoffeeScript like JavaScript



Templating



Name
Status
Install Size
Description







Exports HTML as string, requires references to static resources





Loads Pug templates and returns a function





Compiles Markdown to HTML





Loads and transforms a HTML file using PostHTML





Compiles Handlebars to HTML



Styling



Name
Status
Install Size
Description




<style>


Add exports of a module as style to DOM





Loads CSS file with resolved imports and returns CSS code





Loads and compiles a LESS file





Loads and compiles a Sass/SCSS file





Loads and compiles a Stylus file





Loads and transforms a CSS/SSS file using PostCSS



Linting & Testing



Name
Status
Install Size
Description







Tests with mocha (Browser/NodeJS)





PreLoader for linting code using ESLint





PreLoader for linting code using JSHint



Frameworks



Name
Status
Install Size
Description







Loads and compiles Vue Components





Process HTML & CSS with preprocessor of choice and require() Web Components like first-class modules





Loads and compiles Angular 2 Components





Riot official webpack loader



Performance
webpack uses async I/O and has multiple caching levels. This makes webpack fast
and incredibly fast on incremental compilations.
Module Formats
webpack supports ES2015+, CommonJS and AMD modules out of the box. It performs clever static
analysis on the AST of your code. It even has an evaluation engine to evaluate
simple expressions. This allows you to support most existing libraries out of the box.
Code Splitting
webpack allows you to split your codebase into multiple chunks. Chunks are
loaded asynchronously at runtime. This reduces the initial loading time.
Optimizations
webpack can do many optimizations to reduce the output size of your
JavaScript by deduplicating frequently used modules, minifying, and giving
you full control of what is loaded initially and what is loaded at runtime
through code splitting. It can also make your code chunks cache
friendly by using hashes.
Contributing
We want contributing to webpack to be fun, enjoyable, and educational for anyone, and everyone. We have a vibrant ecosystem that spans beyond this single repo. We welcome you to check out any of the repositories in our organization or webpack-contrib organization which houses all of our loaders and plugins.
Contributions go far beyond pull requests and commits. Although we love giving you the opportunity to put your stamp on webpack, we also are thrilled to receive a variety of other contributions including:

Documentation updates, enhancements, designs, or bugfixes
Spelling or grammar fixes
README.md corrections or redesigns
Adding unit, or functional tests
Triaging GitHub issues -- especially determining whether an issue still persists or is reproducible.
Searching #webpack on twitter and helping someone else who needs help
Teaching others how to contribute to one of the many webpack's repos!
Blogging, speaking about, or creating tutorials about one of webpack's many features.
Helping others in our webpack gitter channel.

If you are worried or don't know where to start, you can always reach out to Sean Larkin (@TheLarkInn) on Twitter or simply submit an issue and a maintainer can help give you guidance!
We have also started a series on our Medium Publication called The Contributor's Guide to webpack. We welcome you to read it and post any questions or responses if you still need help.
Looking to speak about webpack? We'd love to review your talk abstract/CFP! You can email it to webpack [at] opencollective [dot] com and we can give pointers or tips!!!
Creating your own plugins and loaders
If you create a loader or plugin, we would <3 for you to open source it, and put it on npm. We follow the x-loader, x-webpack-plugin naming convention.
Support
We consider webpack to be a low-level tool used not only individually but also layered beneath other awesome tools. Because of its flexibility, webpack isn't always the easiest entry-level solution, however we do believe it is the most powerful. That said, we're always looking for ways to improve and simplify the tool without compromising functionality. If you have any ideas on ways to accomplish this, we're all ears!
If you're just getting started, take a look at our new docs and concepts page. This has a high level overview that is great for beginners!!
Looking for webpack 1 docs? Please check out the old wiki, but note that this deprecated version is no longer supported.
If you want to discuss something or just need help, here is our Gitter room where there are always individuals looking to help out!
If you are still having difficulty, we would love for you to post
a question to StackOverflow with the webpack tag. It is much easier to answer questions that include your webpack.config.js and relevant files! So if you can provide them, we'd be extremely grateful (and more likely to help you find the answer!)
If you are twitter savvy you can tweet #webpack with your question and someone should be able to reach out and help also.
If you have discovered a üêú or have a feature suggestion, feel free to create an issue on Github.
License

Core Team






Tobias Koppers
Core

Founder of webpack




Johannes Ewald
Loaders & Plugins

Early adopter of webpack




Sean T. Larkin
Public Relations

Founder of the core team




Kees Kluskens
Development

Sponsor








Sponsoring
Most of the core team members, webpack contributors and contributors in the ecosystem do this open source work in their free time. If you use webpack for a serious task, and you'd like us to invest more time on it, please donate. This project increases your income/productivity too. It makes development and applications faster and it reduces the required bandwidth.
This is how we use the donations:

Allow the core team to work on webpack
Thank contributors if they invested a large amount of time in contributing
Support projects in the ecosystem that are of great value for users
Support projects that are voted most (work in progress)
Infrastructure cost
Fees for money handling

Premium Partners




Other Backers and Sponsors
Before we started using OpenCollective, donations were made anonymously. Now that we have made the switch, we would like to acknowledge these sponsors (and the ones who continue to donate using OpenCollective). If we've missed someone, please send us a PR, and we'll add you to this list.

Google Angular Team, Architects.io,



Gold Sponsors
Become a gold sponsor and get your logo on our README on Github with a link to your site.
































Silver Sponsors
Become a silver sponsor and get your logo on our README on Github with a link to your site.
































Bronze Sponsors
Become a bronze sponsor and get your logo on our README on Github with a link to your site.







































































































Backers
Become a backer and get your image on our README on Github with a link to your site.





































































































Special Thanks to
(In chronological order)

@google for Google Web Toolkit (GWT), which aims to compile Java to JavaScript. It features a similar Code Splitting as webpack.
@medikoo for modules-webmake, which is a similar project. webpack was born because I wanted Code Splitting for modules-webmake. Interestingly the Code Splitting issue is still open (thanks also to @Phoscur for the discussion).
@substack for browserify, which is a similar project and source for many ideas.
@jrburke for require.js, which is a similar project and source for many ideas.
@defunctzombie for the browser-field spec, which makes modules available for node.js, browserify and webpack.
Every early webpack user, which contributed to webpack by writing issues or PRs. You influenced the direction...
@shama, @jhnns and @sokra for maintaining this project
Everyone who has written a loader for webpack. You are the ecosystem...
Everyone I forgot to mention here, but also influenced webpack.

","GitHub - webpack/webpack: A bundler for javascript and friends. Packs many modules into a few bundled assets. Code Splitting allows for loading parts of the application on demand. Through ""loaders"", modules can be CommonJs, AMD, ES6 modules, CSS, Images, JSON, Coffeescript, LESS, ... and your custom stuff."
39,JavaScript,"Atom



Atom is a hackable text editor for the 21st century, built on Electron, and based on everything we love about our favorite editors. We designed it to be deeply customizable, but still approachable using the default configuration.


Visit atom.io to learn more or visit the Atom forum.
Follow @AtomEditor on Twitter for important
announcements.
This project adheres to the Contributor Covenant code of conduct.
By participating, you are expected to uphold this code. Please report unacceptable behavior to atom@github.com.
Documentation
If you want to read about using Atom or developing packages in Atom, the Atom Flight Manual is free and available online. You can find the source to the manual in atom/flight-manual.atom.io.
The API reference for developing packages is also documented on Atom.io.
Installing
Prerequisites

Git

macOS
Download the latest Atom release.
Atom will automatically update when a new release is available.
Windows
Download the latest Atom installer. AtomSetup.exe is 32-bit. For 64-bit systems, download AtomSetup-x64.exe.
Atom will automatically update when a new release is available.
You can also download atom-windows.zip (32-bit) or atom-x64-windows.zip (64-bit) from the releases page.
The .zip version will not automatically update.
Using Chocolatey? Run cinst Atom to install the latest version of Atom.
Linux
Atom is only available for 64-bit Linux systems.
Configure your distribution's package manager to install and update Atom by following the Linux installation instructions in the Flight Manual.  You will also find instructions on how to install Atom's official Linux packages without using a package repository, though you will not get automatic updates after installing Atom this way.
Archive extraction
An archive is available for people who don't want to install atom as root.
This version enables you to install multiple Atom versions in parallel. It has been built on Ubuntu 64-bit,
but should be compatible with other Linux distributions.

Install dependencies (on Ubuntu): sudo apt install git gconf2 gconf-service libgtk2.0-0 libudev1 libgcrypt20 libnotify4 libxtst6 libnss3 python gvfs-bin xdg-utils libcap2
Download atom-amd64.tar.gz from the Atom releases page.
Run tar xf atom-amd64.tar.gz in the directory where you want to extract the Atom folder.
Launch Atom using the installed atom command from the newly extracted directory.

The Linux version does not currently automatically update so you will need to
repeat these steps to upgrade to future releases.
Building

Linux
macOS
Windows

Discussion

Discuss Atom on our forums
Chat about Atom on our Slack team -- instructions for joining

License
MIT
When using the Atom or other GitHub logos, be sure to follow the GitHub logo guidelines.
",GitHub - atom/atom: The hackable text editor
40,PHP,"






About Laravel
Laravel is a web application framework with expressive, elegant syntax. We believe development must be an enjoyable and creative experience to be truly fulfilling. Laravel takes the pain out of development by easing common tasks used in many web projects, such as:

Simple, fast routing engine.
Powerful dependency injection container.
Multiple back-ends for session and cache storage.
Expressive, intuitive database ORM.
Database agnostic schema migrations.
Robust background job processing.
Real-time event broadcasting.

Laravel is accessible, powerful, and provides tools required for large, robust applications.
Learning Laravel
Laravel has the most extensive and thorough documentation and video tutorial library of all modern web application frameworks, making it a breeze to get started with the framework.
If you don't feel like reading, Laracasts can help. Laracasts contains over 1500 video tutorials on a range of topics including Laravel, modern PHP, unit testing, and JavaScript. Boost your skills by digging into our comprehensive video library.
Laravel Sponsors
We would like to extend our thanks to the following sponsors for funding Laravel development. If you are interested in becoming a sponsor, please visit the Laravel Patreon page.

Vehikl
Tighten Co.
Kirschbaum Development Group
64 Robots
Cubet Techno Labs
Cyber-Duck
British Software Development
Webdock, Fast VPS Hosting
DevSquad
UserInsights
Fragrantica
SOFTonSOFA
User10
Soumettre.fr
CodeBrisk
1Forge
TECPRESSO
Runtime Converter
WebL'Agence
Invoice Ninja
iMi digital
Earthlink
Steadfast Collective
We Are The Robots Inc.
Understand.io
Abdel Elrafa
Hyper Host
Appoly
OP.GG

Contributing
Thank you for considering contributing to the Laravel framework! The contribution guide can be found in the Laravel documentation.
Code of Conduct
In order to ensure that the Laravel community is welcoming to all, please review and abide by the Code of Conduct.
Security Vulnerabilities
If you discover a security vulnerability within Laravel, please send an e-mail to Taylor Otwell via taylor@laravel.com. All security vulnerabilities will be promptly addressed.
License
The Laravel framework is open-sourced software licensed under the MIT license.
",GitHub - laravel/laravel: A PHP framework for web artisans
41,PHP,"jQuery File Upload
Contents

Description
Demo
Features
Security
Setup
Requirements

Mandatory requirements
Optional requirements
Cross-domain requirements


Browsers

Desktop browsers
Mobile browsers
Extended browser support information


Testing
Support
License

Description

File Upload widget with multiple file selection, drag&drop support,
progress bars, validation and preview images, audio and video for jQuery.
Supports cross-domain, chunked and resumable file uploads and client-side
image resizing.
Works with any server-side platform (PHP, Python, Ruby on Rails, Java,
Node.js, Go etc.) that supports standard HTML form file uploads.

Demo
Demo File Upload
Features

Multiple file upload:
Allows to select multiple files at once and upload them simultaneously.
Drag & Drop support:
Allows to upload files by dragging them from your desktop or file manager and
dropping them on your browser window.
Upload progress bar:
Shows a progress bar indicating the upload progress for individual files and
for all uploads combined.
Cancelable uploads:
Individual file uploads can be canceled to stop the upload progress.
Resumable uploads:
Aborted uploads can be resumed with browsers supporting the Blob API.
Chunked uploads:
Large files can be uploaded in smaller chunks with browsers supporting the
Blob API.
Client-side image resizing:
Images can be automatically resized on client-side with browsers supporting
the required JS APIs.
Preview images, audio and video:
A preview of image, audio and video files can be displayed before uploading
with browsers supporting the required APIs.
No browser plugins (e.g. Adobe Flash) required:
The implementation is based on open standards like HTML5 and JavaScript and
requires no additional browser plugins.
Graceful fallback for legacy browsers:
Uploads files via XMLHttpRequests if supported and uses iframes as fallback
for legacy browsers.
HTML file upload form fallback:
Allows progressive enhancement by using a standard HTML file upload form as
widget element.
Cross-site file uploads:
Supports uploading files to a different domain with cross-site XMLHttpRequests
or iframe redirects.
Multiple plugin instances:
Allows to use multiple plugin instances on the same webpage.
Customizable and extensible:
Provides an API to set individual options and define callback methods for
various upload events.
Multipart and file contents stream uploads:
Files can be uploaded as standard ""multipart/form-data"" or file contents
stream (HTTP PUT file upload).
Compatible with any server-side application platform:
Works with any server-side platform (PHP, Python, Ruby on Rails, Java,
Node.js, Go etc.) that supports standard HTML form file uploads.

Security
‚ö†Ô∏è Please read the VULNERABILITIES document for a list of
fixed vulnerabilities
Please also read the SECURITY document for instructions on how to
securely configure your Webserver for file uploads.
Setup
jQuery File Upload can be installed via NPM:
npm install blueimp-file-upload
This allows you to include jquery.fileupload.js and
its extensions via node_modules, e.g:
<script src=""node_modules/blueimp-file-upload/js/jquery.fileupload.js""></script>
The widget can then be initialized on a file upload form the following way:
$('#fileupload').fileupload();
For further information, please refer to the following guides:

Main documentation page
List of all available Options
The plugin API
How to setup the plugin on your website
How to use only the basic plugin.

Requirements
Mandatory requirements

jQuery v1.6+
jQuery UI widget factory v1.9+
(included): Required for the basic File Upload plugin, but very lightweight
without any other dependencies from the jQuery UI suite.
jQuery Iframe Transport plugin
(included): Required for
browsers without XHR file upload support.

Optional requirements

JavaScript Templates engine
v3+: Used to render the selected and uploaded files for the Basic Plus UI and
jQuery UI versions.
JavaScript Load Image library
v2+: Required for the image previews and resizing functionality.
JavaScript Canvas to Blob polyfill
v3+:Required for the image previews and resizing functionality.
blueimp Gallery v2+: Used to display the
uploaded images in a lightbox.
Bootstrap v3+: Used for the demo design.
Glyphicons Icon set used by Bootstrap.

Cross-domain requirements
Cross-domain File Uploads
using the
Iframe Transport plugin
require a redirect back to the origin server to retrieve the upload results. The
example implementation
makes use of
result.html
as a static redirect page for the origin server.
The repository also includes the
jQuery XDomainRequest Transport plugin,
which enables limited cross-domain AJAX requests in Microsoft Internet Explorer
8 and 9 (IE 10 supports cross-domain XHR requests).
The XDomainRequest object allows GET and POST requests only and doesn't support
file uploads. It is used on the
Demo to delete uploaded files
from the cross-domain demo file upload service.
Browsers
Desktop browsers
The File Upload plugin is regularly tested with the latest browser versions and
supports the following minimal versions:

Google Chrome
Apple Safari 4.0+
Mozilla Firefox 3.0+
Opera 11.0+
Microsoft Internet Explorer 6.0+

Mobile browsers
The File Upload plugin has been tested with and supports the following mobile
browsers:

Apple Safari on iOS 6.0+
Google Chrome on iOS 6.0+
Google Chrome on Android 4.0+
Default Browser on Android 2.3+
Opera Mobile 12.0+

Extended browser support information
For a detailed overview of the features supported by each browser version and
known operating system / browser bugs, please have a look at the
Extended browser support information.
Testing
The project comes with three sets of tests:

Code linting using ESLint.
Unit tests using Mocha.
End-to-end tests using blueimp/wdio.

To run the tests, follow these steps:

Start Docker.
Install development dependencies:
npm install

Run the tests:
npm test


Support
This project is actively maintained, but there is no official support channel.
If you have a question that another developer might help you with, please post
to
Stack Overflow
and tag your question with blueimp jquery file upload.
License
Released under the MIT license.
","GitHub - blueimp/jQuery-File-Upload: File Upload widget with multiple file selection, drag&drop support, progress bar, validation and preview images, audio and video for jQuery. Supports cross-domain, chunked and resumable file uploads. Works with any server-side platform (Google App Engine, PHP, Python, Ruby on Rails, Java, etc.) that supports standard HTML form file uploads."
42,PHP,"Faker

Faker is a PHP library that generates fake data for you. Whether you need to bootstrap your database, create good-looking XML documents, fill-in your persistence to stress test it, or anonymize data taken from a production service, Faker is for you.
Faker is heavily inspired by Perl's Data::Faker, and by ruby's Faker.
Faker requires PHP >= 5.3.3.
  
Table of Contents

Installation
Basic Usage
Formatters

Base
Lorem Ipsum Text
Person
Address
Phone Number
Company
Real Text
Date and Time
Internet
User Agent
Payment
Color
File
Image
Uuid
Barcode
Miscellaneous
Biased
Html Lorem


Modifiers
Localization
Populating Entities Using an ORM or an ODM
Seeding the Generator
Faker Internals: Understanding Providers
Real Life Usage
Language specific formatters
Third-Party Libraries Extending/Based On Faker
License

Installation
composer require fzaninotto/faker
Basic Usage
Autoloading
Faker supports both PSR-0 as PSR-4 autoloaders.
<?php
# When installed via composer
require_once 'vendor/autoload.php';
You can also load Fakers shipped PSR-0 autoloader
<?php
# Load Fakers own autoloader
require_once '/path/to/Faker/src/autoload.php';
alternatively, you can use any another PSR-4 compliant autoloader
Create fake data
Use Faker\Factory::create() to create and initialize a faker generator, which can generate data by accessing properties named after the type of data you want.
<?php
// use the factory to create a Faker\Generator instance
$faker = Faker\Factory::create();

// generate data by accessing properties
echo $faker->name;
  // 'Lucy Cechtelar';
echo $faker->address;
  // ""426 Jordy Lodge
  // Cartwrightshire, SC 88120-6700""
echo $faker->text;
  // Dolores sit sint laboriosam dolorem culpa et autem. Beatae nam sunt fugit
  // et sit et mollitia sed.
  // Fuga deserunt tempora facere magni omnis. Omnis quia temporibus laudantium
  // sit minima sint.
Even if this example shows a property access, each call to $faker->name yields a different (random) result. This is because Faker uses __get() magic, and forwards Faker\Generator->$property calls to Faker\Generator->format($property).
<?php
for ($i = 0; $i < 10; $i++) {
  echo $faker->name, ""\n"";
}
  // Adaline Reichel
  // Dr. Santa Prosacco DVM
  // Noemy Vandervort V
  // Lexi O'Conner
  // Gracie Weber
  // Roscoe Johns
  // Emmett Lebsack
  // Keegan Thiel
  // Wellington Koelpin II
  // Ms. Karley Kiehn V
Tip: For a quick generation of fake data, you can also use Faker as a command line tool thanks to faker-cli.
Formatters
Each of the generator properties (like name, address, and lorem) are called ""formatters"". A faker generator has many of them, packaged in ""providers"". Here is a list of the bundled formatters in the default locale.
Faker\Provider\Base
randomDigit             // 7
randomDigitNot(5)       // 0, 1, 2, 3, 4, 6, 7, 8, or 9
randomDigitNotNull      // 5
randomNumber($nbDigits = NULL, $strict = false) // 79907610
randomFloat($nbMaxDecimals = NULL, $min = 0, $max = NULL) // 48.8932
numberBetween($min = 1000, $max = 9000) // 8567
randomLetter            // 'b'
// returns randomly ordered subsequence of a provided array
randomElements($array = array ('a','b','c'), $count = 1) // array('c')
randomElement($array = array ('a','b','c')) // 'b'
shuffle('hello, world') // 'rlo,h eoldlw'
shuffle(array(1, 2, 3)) // array(2, 1, 3)
numerify('Hello ###') // 'Hello 609'
lexify('Hello ???') // 'Hello wgt'
bothify('Hello ##??') // 'Hello 42jz'
asciify('Hello ***') // 'Hello R6+'
regexify('[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'); // sm0@y8k96a.ej

Faker\Provider\Lorem
word                                             // 'aut'
words($nb = 3, $asText = false)                  // array('porro', 'sed', 'magni')
sentence($nbWords = 6, $variableNbWords = true)  // 'Sit vitae voluptas sint non voluptates.'
sentences($nb = 3, $asText = false)              // array('Optio quos qui illo error.', 'Laborum vero a officia id corporis.', 'Saepe provident esse hic eligendi.')
paragraph($nbSentences = 3, $variableNbSentences = true) // 'Ut ab voluptas sed a nam. Sint autem inventore aut officia aut aut blanditiis. Ducimus eos odit amet et est ut eum.'
paragraphs($nb = 3, $asText = false)             // array('Quidem ut sunt et quidem est accusamus aut. Fuga est placeat rerum ut. Enim ex eveniet facere sunt.', 'Aut nam et eum architecto fugit repellendus illo. Qui ex esse veritatis.', 'Possimus omnis aut incidunt sunt. Asperiores incidunt iure sequi cum culpa rem. Rerum exercitationem est rem.')
text($maxNbChars = 200)                          // 'Fuga totam reiciendis qui architecto fugiat nemo. Consequatur recusandae qui cupiditate eos quod.'

Faker\Provider\en_US\Person
title($gender = null|'male'|'female')     // 'Ms.'
titleMale                                 // 'Mr.'
titleFemale                               // 'Ms.'
suffix                                    // 'Jr.'
name($gender = null|'male'|'female')      // 'Dr. Zane Stroman'
firstName($gender = null|'male'|'female') // 'Maynard'
firstNameMale                             // 'Maynard'
firstNameFemale                           // 'Rachel'
lastName                                  // 'Zulauf'

Faker\Provider\en_US\Address
cityPrefix                          // 'Lake'
secondaryAddress                    // 'Suite 961'
state                               // 'NewMexico'
stateAbbr                           // 'OH'
citySuffix                          // 'borough'
streetSuffix                        // 'Keys'
buildingNumber                      // '484'
city                                // 'West Judge'
streetName                          // 'Keegan Trail'
streetAddress                       // '439 Karley Loaf Suite 897'
postcode                            // '17916'
address                             // '8888 Cummings Vista Apt. 101, Susanbury, NY 95473'
country                             // 'Falkland Islands (Malvinas)'
latitude($min = -90, $max = 90)     // 77.147489
longitude($min = -180, $max = 180)  // 86.211205

Faker\Provider\en_US\PhoneNumber
phoneNumber             // '201-886-0269 x3767'
tollFreePhoneNumber     // '(888) 937-7238'
e164PhoneNumber     // '+27113456789'

Faker\Provider\en_US\Company
catchPhrase             // 'Monitored regional contingency'
bs                      // 'e-enable robust architectures'
company                 // 'Bogan-Treutel'
companySuffix           // 'and Sons'
jobTitle                // 'Cashier'

Faker\Provider\en_US\Text
realText($maxNbChars = 200, $indexSize = 2) // ""And yet I wish you could manage it?) 'And what are they made of?' Alice asked in a shrill, passionate voice. 'Would YOU like cats if you were never even spoke to Time!' 'Perhaps not,' Alice replied.""

Faker\Provider\DateTime
unixTime($max = 'now')                // 58781813
dateTime($max = 'now', $timezone = null) // DateTime('2008-04-25 08:37:17', 'UTC')
dateTimeAD($max = 'now', $timezone = null) // DateTime('1800-04-29 20:38:49', 'Europe/Paris')
iso8601($max = 'now')                 // '1978-12-09T10:10:29+0000'
date($format = 'Y-m-d', $max = 'now') // '1979-06-09'
time($format = 'H:i:s', $max = 'now') // '20:49:42'
dateTimeBetween($startDate = '-30 years', $endDate = 'now', $timezone = null) // DateTime('2003-03-15 02:00:49', 'Africa/Lagos')
dateTimeInInterval($startDate = '-30 years', $interval = '+ 5 days', $timezone = null) // DateTime('2003-03-15 02:00:49', 'Antartica/Vostok')
dateTimeThisCentury($max = 'now', $timezone = null)     // DateTime('1915-05-30 19:28:21', 'UTC')
dateTimeThisDecade($max = 'now', $timezone = null)      // DateTime('2007-05-29 22:30:48', 'Europe/Paris')
dateTimeThisYear($max = 'now', $timezone = null)        // DateTime('2011-02-27 20:52:14', 'Africa/Lagos')
dateTimeThisMonth($max = 'now', $timezone = null)       // DateTime('2011-10-23 13:46:23', 'Antarctica/Vostok')
amPm($max = 'now')                    // 'pm'
dayOfMonth($max = 'now')              // '04'
dayOfWeek($max = 'now')               // 'Friday'
month($max = 'now')                   // '06'
monthName($max = 'now')               // 'January'
year($max = 'now')                    // '1993'
century                               // 'VI'
timezone                              // 'Europe/Paris'

Methods accepting a $timezone argument default to date_default_timezone_get(). You can pass a custom timezone string to each method, or define a custom timezone for all time methods at once using $faker::setDefaultTimezone($timezone).
Faker\Provider\Internet
email                   // 'tkshlerin@collins.com'
safeEmail               // 'king.alford@example.org'
freeEmail               // 'bradley72@gmail.com'
companyEmail            // 'russel.durward@mcdermott.org'
freeEmailDomain         // 'yahoo.com'
safeEmailDomain         // 'example.org'
userName                // 'wade55'
password                // 'k&|X+a45*2['
domainName              // 'wolffdeckow.net'
domainWord              // 'feeney'
tld                     // 'biz'
url                     // 'http://www.skilesdonnelly.biz/aut-accusantium-ut-architecto-sit-et.html'
slug                    // 'aut-repellat-commodi-vel-itaque-nihil-id-saepe-nostrum'
ipv4                    // '109.133.32.252'
localIpv4               // '10.242.58.8'
ipv6                    // '8e65:933d:22ee:a232:f1c1:2741:1f10:117c'
macAddress              // '43:85:B7:08:10:CA'

Faker\Provider\UserAgent
userAgent              // 'Mozilla/5.0 (Windows CE) AppleWebKit/5350 (KHTML, like Gecko) Chrome/13.0.888.0 Safari/5350'
chrome                 // 'Mozilla/5.0 (Macintosh; PPC Mac OS X 10_6_5) AppleWebKit/5312 (KHTML, like Gecko) Chrome/14.0.894.0 Safari/5312'
firefox                // 'Mozilla/5.0 (X11; Linuxi686; rv:7.0) Gecko/20101231 Firefox/3.6'
safari                 // 'Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_7_1 rv:3.0; en-US) AppleWebKit/534.11.3 (KHTML, like Gecko) Version/4.0 Safari/534.11.3'
opera                  // 'Opera/8.25 (Windows NT 5.1; en-US) Presto/2.9.188 Version/10.00'
internetExplorer       // 'Mozilla/5.0 (compatible; MSIE 7.0; Windows 98; Win 9x 4.90; Trident/3.0)'

Faker\Provider\Payment
creditCardType          // 'MasterCard'
creditCardNumber        // '4485480221084675'
creditCardExpirationDate // 04/13
creditCardExpirationDateString // '04/13'
creditCardDetails       // array('MasterCard', '4485480221084675', 'Aleksander Nowak', '04/13')
// Generates a random IBAN. Set $countryCode to null for a random country
iban($countryCode)      // 'IT31A8497112740YZ575DJ28BP4'
swiftBicNumber          // 'RZTIAT22263'

Faker\Provider\Color
hexcolor               // '#fa3cc2'
rgbcolor               // '0,255,122'
rgbColorAsArray        // array(0,255,122)
rgbCssColor            // 'rgb(0,255,122)'
safeColorName          // 'fuchsia'
colorName              // 'Gainsbor'
hslColor               // '340,50,20'
hslColorAsArray        // array(340,50,20)

Faker\Provider\File
fileExtension          // 'avi'
mimeType               // 'video/x-msvideo'
// Copy a random file from the source to the target directory and returns the fullpath or filename
file($sourceDir = '/tmp', $targetDir = '/tmp') // '/path/to/targetDir/13b73edae8443990be1aa8f1a483bc27.jpg'
file($sourceDir, $targetDir, false) // '13b73edae8443990be1aa8f1a483bc27.jpg'

Faker\Provider\Image
// Image generation provided by LoremPixel (http://lorempixel.com/)
imageUrl($width = 640, $height = 480) // 'http://lorempixel.com/640/480/'
imageUrl($width, $height, 'cats')     // 'http://lorempixel.com/800/600/cats/'
imageUrl($width, $height, 'cats', true, 'Faker') // 'http://lorempixel.com/800/400/cats/Faker'
imageUrl($width, $height, 'cats', true, 'Faker', true) // 'http://lorempixel.com/gray/800/400/cats/Faker/' Monochrome image
image($dir = '/tmp', $width = 640, $height = 480) // '/tmp/13b73edae8443990be1aa8f1a483bc27.jpg'
image($dir, $width, $height, 'cats')  // 'tmp/13b73edae8443990be1aa8f1a483bc27.jpg' it's a cat!
image($dir, $width, $height, 'cats', false) // '13b73edae8443990be1aa8f1a483bc27.jpg' it's a filename without path
image($dir, $width, $height, 'cats', true, false) // it's a no randomize images (default: `true`)
image($dir, $width, $height, 'cats', true, true, 'Faker') // 'tmp/13b73edae8443990be1aa8f1a483bc27.jpg' it's a cat with 'Faker' text. Default, `null`.

Faker\Provider\Uuid
uuid                   // '7e57d004-2b97-0e7a-b45f-5387367791cd'

Faker\Provider\Barcode
ean13          // '4006381333931'
ean8           // '73513537'
isbn13         // '9790404436093'
isbn10         // '4881416324'

Faker\Provider\Miscellaneous
boolean // false
boolean($chanceOfGettingTrue = 50) // true
md5           // 'de99a620c50f2990e87144735cd357e7'
sha1          // 'f08e7f04ca1a413807ebc47551a40a20a0b4de5c'
sha256        // '0061e4c60dac5c1d82db0135a42e00c89ae3a333e7c26485321f24348c7e98a5'
locale        // en_UK
countryCode   // UK
languageCode  // en
currencyCode  // EUR
emoji         // üòÅ

Faker\Provider\Biased
// get a random number between 10 and 20,
// with more chances to be close to 20
biasedNumberBetween($min = 10, $max = 20, $function = 'sqrt')

Faker\Provider\HtmlLorem
//Generate HTML document which is no more than 2 levels deep, and no more than 3 elements wide at any level.
randomHtml(2,3)   // <html><head><title>Aut illo dolorem et accusantium eum.</title></head><body><form action=""example.com"" method=""POST""><label for=""username"">sequi</label><input type=""text"" id=""username""><label for=""password"">et</label><input type=""password"" id=""password""></form><b>Id aut saepe non mollitia voluptas voluptas.</b><table><thead><tr><tr>Non consequatur.</tr><tr>Incidunt est.</tr><tr>Aut voluptatem.</tr><tr>Officia voluptas rerum quo.</tr><tr>Asperiores similique.</tr></tr></thead><tbody><tr><td>Sapiente dolorum dolorem sint laboriosam commodi qui.</td><td>Commodi nihil nesciunt eveniet quo repudiandae.</td><td>Voluptates explicabo numquam distinctio necessitatibus repellat.</td><td>Provident ut doloremque nam eum modi aspernatur.</td><td>Iusto inventore.</td></tr><tr><td>Animi nihil ratione id mollitia libero ipsa quia tempore.</td><td>Velit est officia et aut tenetur dolorem sed mollitia expedita.</td><td>Modi modi repudiandae pariatur voluptas rerum ea incidunt non molestiae eligendi eos deleniti.</td><td>Exercitationem voluptatibus dolor est iste quod molestiae.</td><td>Quia reiciendis.</td></tr><tr><td>Inventore impedit exercitationem voluptatibus rerum cupiditate.</td><td>Qui.</td><td>Aliquam.</td><td>Autem nihil aut et.</td><td>Dolor ut quia error.</td></tr><tr><td>Enim facilis iusto earum et minus rerum assumenda quis quia.</td><td>Reprehenderit ut sapiente occaecati voluptatum dolor voluptatem vitae qui velit.</td><td>Quod fugiat non.</td><td>Sunt nobis totam mollitia sed nesciunt est deleniti cumque.</td><td>Repudiandae quo.</td></tr><tr><td>Modi dicta libero quisquam doloremque qui autem.</td><td>Voluptatem aliquid saepe laudantium facere eos sunt dolor.</td><td>Est eos quis laboriosam officia expedita repellendus quia natus.</td><td>Et neque delectus quod fugit enim repudiandae qui.</td><td>Fugit soluta sit facilis facere repellat culpa magni voluptatem maiores tempora.</td></tr><tr><td>Enim dolores doloremque.</td><td>Assumenda voluptatem eum perferendis exercitationem.</td><td>Quasi in fugit deserunt ea perferendis sunt nemo consequatur dolorum soluta.</td><td>Maxime repellat qui numquam voluptatem est modi.</td><td>Alias rerum rerum hic hic eveniet.</td></tr><tr><td>Tempore voluptatem.</td><td>Eaque.</td><td>Et sit quas fugit iusto.</td><td>Nemo nihil rerum dignissimos et esse.</td><td>Repudiandae ipsum numquam.</td></tr><tr><td>Nemo sunt quia.</td><td>Sint tempore est neque ducimus harum sed.</td><td>Dicta placeat atque libero nihil.</td><td>Et qui aperiam temporibus facilis eum.</td><td>Ut dolores qui enim et maiores nesciunt.</td></tr><tr><td>Dolorum totam sint debitis saepe laborum.</td><td>Quidem corrupti ea.</td><td>Cum voluptas quod.</td><td>Possimus consequatur quasi dolorem ut et.</td><td>Et velit non hic labore repudiandae quis.</td></tr></tbody></table></body></html>

Modifiers
Faker provides three special providers, unique(), optional(), and valid(), to be called before any provider.
// unique() forces providers to return unique values
$values = array();
for ($i = 0; $i < 10; $i++) {
  // get a random digit, but always a new one, to avoid duplicates
  $values []= $faker->unique()->randomDigit;
}
print_r($values); // [4, 1, 8, 5, 0, 2, 6, 9, 7, 3]

// providers with a limited range will throw an exception when no new unique value can be generated
$values = array();
try {
  for ($i = 0; $i < 10; $i++) {
    $values []= $faker->unique()->randomDigitNotNull;
  }
} catch (\OverflowException $e) {
  echo ""There are only 9 unique digits not null, Faker can't generate 10 of them!"";
}

// you can reset the unique modifier for all providers by passing true as first argument
$faker->unique($reset = true)->randomDigitNotNull; // will not throw OverflowException since unique() was reset
// tip: unique() keeps one array of values per provider

// optional() sometimes bypasses the provider to return a default value instead (which defaults to NULL)
$values = array();
for ($i = 0; $i < 10; $i++) {
  // get a random digit, but also null sometimes
  $values []= $faker->optional()->randomDigit;
}
print_r($values); // [1, 4, null, 9, 5, null, null, 4, 6, null]

// optional() accepts a weight argument to specify the probability of receiving the default value.
// 0 will always return the default value; 1 will always return the provider. Default weight is 0.5 (50% chance).
$faker->optional($weight = 0.1)->randomDigit; // 90% chance of NULL
$faker->optional($weight = 0.9)->randomDigit; // 10% chance of NULL

// optional() accepts a default argument to specify the default value to return.
// Defaults to NULL.
$faker->optional($weight = 0.5, $default = false)->randomDigit; // 50% chance of FALSE
$faker->optional($weight = 0.9, $default = 'abc')->word; // 10% chance of 'abc'

// valid() only accepts valid values according to the passed validator functions
$values = array();
$evenValidator = function($digit) {
	return $digit % 2 === 0;
};
for ($i = 0; $i < 10; $i++) {
	$values []= $faker->valid($evenValidator)->randomDigit;
}
print_r($values); // [0, 4, 8, 4, 2, 6, 0, 8, 8, 6]

// just like unique(), valid() throws an overflow exception when it can't generate a valid value
$values = array();
try {
  $faker->valid($evenValidator)->randomElement(1, 3, 5, 7, 9);
} catch (\OverflowException $e) {
  echo ""Can't pick an even number in that set!"";
}
If you would like to use a modifier with a value not generated by Faker, use the passthrough() method. passthrough() simply returns whatever value it was given.
$faker->optional()->passthrough(mt_rand(5, 15));
Localization
Faker\Factory can take a locale as an argument, to return localized data. If no localized provider is found, the factory fallbacks to the default locale (en_US).
<?php
$faker = Faker\Factory::create('fr_FR'); // create a French faker
for ($i = 0; $i < 10; $i++) {
  echo $faker->name, ""\n"";
}
  // Luce du Coulon
  // Auguste Dupont
  // Roger Le Voisin
  // Alexandre Lacroix
  // Jacques Humbert-Roy
  // Th√©r√®se Guillet-Andre
  // Gilles Gros-Bodin
  // Am√©lie Pires
  // Marcel Laporte
  // Genevi√®ve Marchal
You can check available Faker locales in the source code, under the Provider directory. The localization of Faker is an ongoing process, for which we need your help. Don't hesitate to create localized providers to your own locale and submit a PR!
Populating Entities Using an ORM or an ODM
Faker provides adapters for Object-Relational and Object-Document Mappers (currently, Propel, Doctrine2, CakePHP, Spot2, Mandango and Eloquent are supported). These adapters ease the population of databases through the Entity classes provided by an ORM library (or the population of document stores using Document classes provided by an ODM library).
To populate entities, create a new populator class (using a generator instance as parameter), then list the class and number of all the entities that must be generated. To launch the actual data population, call the execute() method.
Note that some of the populators could require additional parameters. As example the doctrine populator has an option to specify
its batchSize on how often it will flush the UnitOfWork to the database.
Here is an example showing how to populate 5 Author and 10 Book objects:
<?php
$generator = \Faker\Factory::create();
$populator = new \Faker\ORM\Propel\Populator($generator);
$populator->addEntity('Author', 5);
$populator->addEntity('Book', 10);
$insertedPKs = $populator->execute();
The populator uses name and column type guessers to populate each column with relevant data. For instance, Faker populates a column named first_name using the firstName formatter, and a column with a TIMESTAMP type using the dateTime formatter. The resulting entities are therefore coherent. If Faker misinterprets a column name, you can still specify a custom closure to be used for populating a particular column, using the third argument to addEntity():
<?php
$populator->addEntity('Book', 5, array(
  'ISBN' => function() use ($generator) { return $generator->ean13(); }
));
In this example, Faker will guess a formatter for all columns except ISBN, for which the given anonymous function will be used.
Tip: To ignore some columns, specify null for the column names in the third argument of addEntity(). This is usually necessary for columns added by a behavior:
<?php
$populator->addEntity('Book', 5, array(
  'CreatedAt' => null,
  'UpdatedAt' => null,
));
Of course, Faker does not populate autoincremented primary keys. In addition, Faker\ORM\Propel\Populator::execute() returns the list of inserted PKs, indexed by class:
<?php
print_r($insertedPKs);
// array(
//   'Author' => (34, 35, 36, 37, 38),
//   'Book'   => (456, 457, 458, 459, 470, 471, 472, 473, 474, 475)
// )
Note: Due to the fact that Faker returns all the primary keys inserted, the memory consumption will go up drastically when you do batch inserts due to the big list of data.
In the previous example, the Book and Author models share a relationship. Since Author entities are populated first, Faker is smart enough to relate the populated Book entities to one of the populated Author entities.
Lastly, if you want to execute an arbitrary function on an entity before insertion, use the fourth argument of the addEntity() method:
<?php
$populator->addEntity('Book', 5, array(), array(
  function($book) { $book->publish(); },
));
Seeding the Generator
You may want to get always the same generated data - for instance when using Faker for unit testing purposes. The generator offers a seed() method, which seeds the random number generator. Calling the same script twice with the same seed produces the same results.
<?php
$faker = Faker\Factory::create();
$faker->seed(1234);

echo $faker->name; // 'Jess Mraz I';

Tip: DateTime formatters won't reproduce the same fake data if you don't fix the $max value:
<?php
// even when seeded, this line will return different results because $max varies
$faker->dateTime(); // equivalent to $faker->dateTime($max = 'now')
// make sure you fix the $max parameter
$faker->dateTime('2014-02-25 08:37:17'); // will return always the same date when seeded
Tip: Formatters won't reproduce the same fake data if you use the rand() php function. Use $faker or mt_rand() instead:
<?php
// bad
$faker->realText(rand(10,20));
// good
$faker->realText($faker->numberBetween(10,20));

Faker Internals: Understanding Providers
A Faker\Generator alone can't do much generation. It needs Faker\Provider objects to delegate the data generation to them. Faker\Factory::create() actually creates a Faker\Generator bundled with the default providers. Here is what happens under the hood:
<?php
$faker = new Faker\Generator();
$faker->addProvider(new Faker\Provider\en_US\Person($faker));
$faker->addProvider(new Faker\Provider\en_US\Address($faker));
$faker->addProvider(new Faker\Provider\en_US\PhoneNumber($faker));
$faker->addProvider(new Faker\Provider\en_US\Company($faker));
$faker->addProvider(new Faker\Provider\Lorem($faker));
$faker->addProvider(new Faker\Provider\Internet($faker));
Whenever you try to access a property on the $faker object, the generator looks for a method with the same name in all the providers attached to it. For instance, calling $faker->name triggers a call to Faker\Provider\Person::name(). And since Faker starts with the last provider, you can easily override existing formatters: just add a provider containing methods named after the formatters you want to override.
That means that you can easily add your own providers to a Faker\Generator instance. A provider is usually a class extending \Faker\Provider\Base. This parent class allows you to use methods like lexify() or randomNumber(); it also gives you access to formatters of other providers, through the protected $generator property. The new formatters are the public methods of the provider class.
Here is an example provider for populating Book data:
<?php

namespace Faker\Provider;

class Book extends \Faker\Provider\Base
{
  public function title($nbWords = 5)
  {
    $sentence = $this->generator->sentence($nbWords);
    return substr($sentence, 0, strlen($sentence) - 1);
  }

  public function ISBN()
  {
    return $this->generator->ean13();
  }
}
To register this provider, just add a new instance of \Faker\Provider\Book to an existing generator:
<?php
$faker->addProvider(new \Faker\Provider\Book($faker));
Now you can use the two new formatters like any other Faker formatter:
<?php
$book = new Book();
$book->setTitle($faker->title);
$book->setISBN($faker->ISBN);
$book->setSummary($faker->text);
$book->setPrice($faker->randomNumber(2));
Tip: A provider can also be a Plain Old PHP Object. In that case, all the public methods of the provider become available to the generator.
Real Life Usage
The following script generates a valid XML document:
<?php
require_once '/path/to/Faker/src/autoload.php';
$faker = Faker\Factory::create();
?>
<?xml version=""1.0"" encoding=""UTF-8""?>
<contacts>
<?php for ($i = 0; $i < 10; $i++): ?>
  <contact firstName=""<?php echo $faker->firstName ?>"" lastName=""<?php echo $faker->lastName ?>"" email=""<?php echo $faker->email ?>"">
    <phone number=""<?php echo $faker->phoneNumber ?>""/>
<?php if ($faker->boolean(25)): ?>
    <birth date=""<?php echo $faker->dateTimeThisCentury->format('Y-m-d') ?>"" place=""<?php echo $faker->city ?>""/>
<?php endif; ?>
    <address>
      <street><?php echo $faker->streetAddress ?></street>
      <city><?php echo $faker->city ?></city>
      <postcode><?php echo $faker->postcode ?></postcode>
      <state><?php echo $faker->state ?></state>
    </address>
    <company name=""<?php echo $faker->company ?>"" catchPhrase=""<?php echo $faker->catchPhrase ?>"">
<?php if ($faker->boolean(33)): ?>
      <offer><?php echo $faker->bs ?></offer>
<?php endif; ?>
<?php if ($faker->boolean(33)): ?>
      <director name=""<?php echo $faker->name ?>"" />
<?php endif; ?>
    </company>
<?php if ($faker->boolean(15)): ?>
    <details>
<![CDATA[
<?php echo $faker->text(400) ?>
]]>
    </details>
<?php endif; ?>
  </contact>
<?php endfor; ?>
</contacts>
Running this script produces a document looking like:
<?xml version=""1.0"" encoding=""UTF-8""?>
<contacts>
  <contact firstName=""Ona"" lastName=""Bednar"" email=""schamberger.frank@wuckert.com"">
    <phone number=""1-265-479-1196x714""/>
    <address>
      <street>182 Harrison Cove</street>
      <city>North Lloyd</city>
      <postcode>45577</postcode>
      <state>Alabama</state>
    </address>
    <company name=""Veum, Funk and Shanahan"" catchPhrase=""Function-based stable solution"">
      <offer>orchestrate compelling web-readiness</offer>
    </company>
    <details>
<![CDATA[
Alias accusantium voluptatum autem nobis cumque neque modi. Voluptatem error molestiae consequatur alias.
Illum commodi molestiae aut repellat id. Et sit consequuntur aut et ullam asperiores. Cupiditate culpa voluptatem et mollitia dolor. Nisi praesentium qui ut.
]]>
    </details>
  </contact>
  <contact firstName=""Aurelie"" lastName=""Paucek"" email=""alfonzo55@durgan.com"">
    <phone number=""863.712.1363x9425""/>
    <address>
      <street>90111 Hegmann Inlet</street>
      <city>South Geovanymouth</city>
      <postcode>69961-9311</postcode>
      <state>Colorado</state>
    </address>
    <company name=""Krajcik-Grimes"" catchPhrase=""Switchable cohesive instructionset"">
    </company>
  </contact>
  <contact firstName=""Clifton"" lastName=""Kshlerin"" email=""kianna.wiegand@framiwyman.info"">
    <phone number=""692-194-4746""/>
    <address>
      <street>9791 Nona Corner</street>
      <city>Harberhaven</city>
      <postcode>74062-8191</postcode>
      <state>RhodeIsland</state>
    </address>
    <company name=""Rosenbaum-Aufderhar"" catchPhrase=""Realigned asynchronous encryption"">
    </company>
  </contact>
  <contact firstName=""Alexandre"" lastName=""Orn"" email=""thelma37@erdmancorwin.biz"">
    <phone number=""189.655.8677x027""/>
    <address>
      <street>11161 Schultz Via</street>
      <city>Feilstad</city>
      <postcode>98019</postcode>
      <state>NewJersey</state>
    </address>
    <company name=""O'Hara-Prosacco"" catchPhrase=""Re-engineered solution-oriented algorithm"">
      <director name=""Dr. Berenice Auer V"" />
    </company>
    <details>
<![CDATA[
Ut itaque et quaerat doloremque eum praesentium. Rerum in saepe dolorem. Explicabo qui consequuntur commodi minima rem.
Harum temporibus rerum dolores. Non molestiae id dolorem placeat.
Aut asperiores nihil eius repellendus. Vero nihil corporis voluptatem explicabo commodi. Occaecati omnis blanditiis beatae quod aspernatur eos.
]]>
    </details>
  </contact>
  <contact firstName=""Katelynn"" lastName=""Kohler"" email=""reinger.trudie@stiedemannjakubowski.com"">
    <phone number=""(665)713-1657""/>
    <address>
      <street>6106 Nader Village Suite 753</street>
      <city>McLaughlinstad</city>
      <postcode>43189-8621</postcode>
      <state>Missouri</state>
    </address>
    <company name=""Herman-Tremblay"" catchPhrase=""Object-based explicit service-desk"">
      <offer>expedite viral synergies</offer>
      <director name=""Arden Deckow"" />
    </company>
  </contact>
  <contact firstName=""Blanca"" lastName=""Stark"" email=""tad27@feest.net"">
    <phone number=""168.719.4692x87177""/>
    <address>
      <street>7546 Kuvalis Plaza</street>
      <city>South Wilfrid</city>
      <postcode>77069</postcode>
      <state>Georgia</state>
    </address>
    <company name=""Upton, Braun and Rowe"" catchPhrase=""Visionary leadingedge pricingstructure"">
    </company>
  </contact>
  <contact firstName=""Rene"" lastName=""Spencer"" email=""anibal28@armstrong.info"">
    <phone number=""715.222.0095x175""/>
    <birth date=""2008-08-07"" place=""Zulaufborough""/>
    <address>
      <street>478 Daisha Landing Apt. 510</street>
      <city>West Lizethhaven</city>
      <postcode>30566-5362</postcode>
      <state>WestVirginia</state>
    </address>
    <company name=""Wiza Inc"" catchPhrase=""Persevering reciprocal approach"">
      <offer>orchestrate dynamic networks</offer>
      <director name=""Erwin Nienow"" />
    </company>
    <details>
<![CDATA[
Dolorem consequatur voluptates unde optio unde. Accusantium dolorem est est architecto impedit. Corrupti et provident quo.
Reprehenderit dolores aut quidem suscipit repudiandae corporis error. Molestiae enim aperiam illo.
Et similique qui non expedita quia dolorum. Ex rem incidunt ea accusantium temporibus minus non.
]]>
    </details>
  </contact>
  <contact firstName=""Alessandro"" lastName=""Hagenes"" email=""tbreitenberg@oharagorczany.com"">
    <phone number=""1-284-958-6768""/>
    <address>
      <street>1251 Koelpin Mission</street>
      <city>North Revastad</city>
      <postcode>81620</postcode>
      <state>Maryland</state>
    </address>
    <company name=""Stiedemann-Bruen"" catchPhrase=""Re-engineered 24/7 success"">
    </company>
  </contact>
  <contact firstName=""Novella"" lastName=""Rutherford"" email=""claud65@bogisich.biz"">
    <phone number=""(091)825-7971""/>
    <address>
      <street>6396 Langworth Hills Apt. 446</street>
      <city>New Carlos</city>
      <postcode>89399-0268</postcode>
      <state>Wyoming</state>
    </address>
    <company name=""Stroman-Legros"" catchPhrase=""Expanded 4thgeneration moratorium"">
      <director name=""Earlene Bayer"" />
    </company>
  </contact>
  <contact firstName=""Andreane"" lastName=""Mann"" email=""meggie17@ornbaumbach.com"">
    <phone number=""941-659-9982x5689""/>
    <birth date=""1934-02-21"" place=""Stantonborough""/>
    <address>
      <street>2246 Kreiger Station Apt. 291</street>
      <city>Kaydenmouth</city>
      <postcode>11397-1072</postcode>
      <state>Wyoming</state>
    </address>
    <company name=""Lebsack, Bernhard and Kiehn"" catchPhrase=""Persevering actuating framework"">
      <offer>grow sticky portals</offer>
    </company>
    <details>
<![CDATA[
Quia dolor ut quia error libero. Enim facilis iusto earum et minus rerum assumenda. Quia doloribus et reprehenderit ut. Occaecati voluptatum dolor voluptatem vitae qui velit quia.
Fugiat non in itaque sunt nobis totam. Sed nesciunt est deleniti cumque alias. Repudiandae quo aut numquam modi dicta libero.
]]>
    </details>
  </contact>
</contacts>
Language specific formatters
Faker\Provider\ar_SA\Person
<?php

echo $faker->idNumber;      // ID number
echo $faker->nationalIdNumber // Citizen ID number
echo $faker->foreignerIdNumber // Foreigner ID number
echo $faker->companyIdNumber // Company ID number
Faker\Provider\ar_SA\Payment
<?php

echo $faker->bankAccountNumber // ""SA0218IBYZVZJSEC8536V4XC""
Faker\Provider\at_AT\Payment
<?php

echo $faker->vat;           // ""AT U12345678"" - Austrian Value Added Tax number
echo $faker->vat(false);    // ""ATU12345678"" - unspaced Austrian Value Added Tax number
Faker\Provider\bg_BG\Payment
<?php

echo $faker->vat;           // ""BG 0123456789"" - Bulgarian Value Added Tax number
echo $faker->vat(false);    // ""BG0123456789"" - unspaced Bulgarian Value Added Tax number
Faker\Provider\cs_CZ\Address
<?php

echo $faker->region; // ""Libereck√Ω kraj""
Faker\Provider\cs_CZ\Company
<?php

// Generates a valid IƒåO
echo $faker->ico; // ""69663963""
Faker\Provider\cs_CZ\DateTime
<?php

echo $faker->monthNameGenitive; // ""prosince""
echo $faker->formattedDate; // ""12. listopadu 2015""
Faker\Provider\cs_CZ\Person
<?php

echo $faker->birthNumber; // ""7304243452""
Faker\Provider\da_DK\Person
<?php

// Generates a random CPR number
echo $faker->cpr; // ""051280-2387""
Faker\Provider\da_DK\Address
<?php

// Generates a random 'kommune' name
echo $faker->kommune; // ""Frederiksberg""

// Generates a random region name
echo $faker->region; // ""Region Sj√¶lland""
Faker\Provider\da_DK\Company
<?php

// Generates a random CVR number
echo $faker->cvr; // ""32458723""

// Generates a random P number
echo $faker->p; // ""5398237590""
Faker\Provider\de_CH\Person
<?php

// Generates a random AVS13/AHV13 social security number
echo $faker->avs13; // ""756.1234.5678.97"" OR
echo $faker->ahv13; // ""756.1234.5678.97""
Faker\Provider\de_DE\Payment
<?php

echo $faker->bankAccountNumber; // ""DE41849025553661169313""
echo $faker->bank; // ""Volksbank Stuttgart""

Faker\Provider\en_HK\Address
<?php

// Generates a fake town name based on the words commonly found in Hong Kong
echo $faker->town; // ""Yuen Long""

// Generates a fake village name based on the words commonly found in Hong Kong
echo $faker->village; // ""O Tau""

// Generates a fake estate name based on the words commonly found in Hong Kong
echo $faker->estate; // ""Ching Lai Court""

Faker\Provider\en_HK\Phone
<?php

// Generates a Hong Kong mobile number (starting with 5, 6 or 9)
echo $faker->mobileNumber; // ""92150087""

// Generates a Hong Kong landline number (starting with 2 or 3)
echo $faker->landlineNumber; // ""32750132""

// Generates a Hong Kong fax number (starting with 7)
echo $faker->faxNumber; // ""71937729""

Faker\Provider\en_NG\Address
<?php

// Generates a random region name
echo $faker->region; // 'Katsina'
Faker\Provider\en_NG\Person
<?php

// Generates a random person name
echo $faker->name; // 'Oluwunmi Mayowa'
Faker\Provider\en_NZ\Phone
<?php

// Generates a cell (mobile) phone number
echo $faker->mobileNumber; // ""021 123 4567""

// Generates a toll free number
echo $faker->tollFreeNumber; // ""0800 123 456""

// Area Code
echo $faker->areaCode; // ""03""
Faker\Provider\en_US\Company
<?php

// Generate a random Employer Identification Number
echo $faker->ein; // '12-3456789'
Faker\Provider\en_US\Payment
<?php

echo $faker->bankAccountNumber;  // '51915734310'
echo $faker->bankRoutingNumber;  // '212240302'
Faker\Provider\en_US\Person
<?php

// Generates a random Social Security Number
echo $faker->ssn; // '123-45-6789'
Faker\Provider\en_ZA\Company
<?php

// Generates a random company registration number
echo $faker->companyNumber; // 1999/789634/01
Faker\Provider\en_ZA\Person
<?php

// Generates a random national identification number
echo $faker->idNumber; // 6606192211041

// Generates a random valid licence code
echo $faker->licenceCode; // EB
Faker\Provider\en_ZA\PhoneNumber
<?php

// Generates a special rate toll free phone number
echo $faker->tollFreeNumber; // 0800 555 5555

// Generates a mobile phone number
echo $faker->mobileNumber; // 082 123 5555
Faker\Provider\es_ES\Person
<?php

// Generates a Documento Nacional de Identidad (DNI) number
echo $faker->dni; // '77446565E'

// Generates a random valid licence code
echo $faker->licenceCode; // B
Faker\Provider\es_ES\Payment
<?php
// Generates a C√≥digo de identificaci√≥n Fiscal (CIF) number
echo $faker->vat;           // ""A35864370""
Faker\Provider\es_ES\PhoneNumber
<?php

// Generates a special rate toll free phone number
echo $faker->tollFreeNumber; // 900 123 456

// Generates a mobile phone number
echo $faker->mobileNumber; // +34 612 12 24
Faker\Provider\es_PE\Person
<?php

// Generates a Peruvian Documento Nacional de Identidad (DNI) number
echo $faker->dni; // '83367512'
Faker\Provider\fa_IR\Person
<?php

// Generates a valid nationalCode
echo $faker->nationalCode; // ""0078475759""
Faker\Provider\fa_IR\Address
<?php

// Generates a random building name
echo $faker->building; // ""ÿ≥ÿßÿÆÿ™ŸÖÿßŸÜ ÿ¢ŸÅÿ™ÿßÿ®""

// Returns a random city name
echo $faker->city // ""ÿßÿ≥ÿ™ÿßŸÜ ÿ≤ŸÜÿ¨ÿßŸÜ""
Faker\Provider\fa_IR\Company
<?php

// Generates a random contract type
echo $faker->contract; // ""ÿ±ÿ≥ŸÖ€å""
Faker\Provider\fi_FI\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""FI8350799879879616""
Faker\Provider\fi_FI\Person
<?php

//Generates a valid Finnish personal identity number (in Finnish - Henkil√∂tunnus)
echo $faker->personalIdentityNumber() // '170974-007J'

//Since the numbers are different for male and female persons, optionally you can specify gender.
echo $faker->personalIdentityNumber(\DateTime::createFromFormat('Y-m-d', '2015-12-14'), 'female') // '141215A520B'
Faker\Provider\fr_BE\Payment
<?php

echo $faker->vat;           // ""BE 0123456789"" - Belgian Value Added Tax number
echo $faker->vat(false);    // ""BE0123456789"" - unspaced Belgian Value Added Tax number
Faker\Provider\es_VE\Person
<?php

// Generate a C√©dula de identidad number, you can pass one argument to add separator
echo $faker->nationalId; // 'V11223344'
Faker\Provider\es_VE\Company
<?php

// Generates a R.I.F. number, you can pass one argument to add separators
echo $faker->taxpayerIdentificationNumber; // 'J1234567891'
Faker\Provider\fr_CH\Person
<?php

// Generates a random AVS13/AHV13 social security number
echo $faker->avs13; // ""756.1234.5678.97""
Faker\Provider\fr_FR\Address
<?php

// Generates a random department name
echo $faker->departmentName; // ""Haut-Rhin""

// Generates a random department number
echo $faker->departmentNumber; // ""2B""

// Generates a random department info (department number => department name)
$faker->department; // array('18' => 'Cher');

// Generates a random region
echo $faker->region; // ""Saint-Pierre-et-Miquelon""

// Generates a random appartement,stair
echo $faker->secondaryAddress; // ""Bat. 961""
Faker\Provider\fr_FR\Company
<?php

// Generates a random SIREN number
echo $faker->siren; // 082 250 104

// Generates a random SIRET number
echo $faker->siret; // 347 355 708 00224
Faker\Provider\fr_FR\Payment
<?php

// Generates a random VAT
echo $faker->vat; // FR 12 123 456 789
Faker\Provider\fr_FR\Person
<?php

// Generates a random NIR / S√©curit√© Sociale number
echo $faker->nir; // 1 88 07 35 127 571 - 19
Faker\Provider\fr_FR\PhoneNumber
<?php

// Generates phone numbers
echo $faker->phoneNumber; // +33 (0)1 67 97 01 31
echo $faker->mobileNumber; // +33 6 21 12 72 84
echo $faker->serviceNumber // 08 98 04 84 46
Faker\Provider\he_IL\Payment
<?php

echo $faker->bankAccountNumber // ""IL392237392219429527697""
Faker\Provider\hr_HR\Payment
<?php

echo $faker->bankAccountNumber // ""HR3789114847226078672""
Faker\Provider\hu_HU\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""HU09904437680048220079300783""
Faker\Provider\id_ID\Person
<?php

// Generates a random Nomor Induk Kependudukan (NIK)

// first argument is gender, either Person::GENDER_MALE or Person::GENDER_FEMALE, if none specified random gender is used
// second argument is birth date (DateTime object), if none specified, random birth date is used
echo $faker->nik(); // ""8522246001570940""
Faker\Provider\it_CH\Person
<?php

// Generates a random AVS13/AHV13 social security number
echo $faker->avs13; // ""756.1234.5678.97""
Faker\Provider\it_IT\Company
<?php

// Generates a random Vat Id
echo $faker->vatId(); // ""IT98746784967""
Faker\Provider\it_IT\Person
<?php

// Generates a random Tax Id code (Codice fiscale)
echo $faker->taxId(); // ""DIXDPZ44E08F367A""
Faker\Provider\ja_JP\Person
<?php

// Generates a 'kana' name
echo $faker->kanaName($gender = null|'male'|'female') // ""„Ç¢„Ç™„Çø „Éü„Éé„É´""

// Generates a 'kana' first name
echo $faker->firstKanaName($gender = null|'male'|'female') // ""„Éí„Éá„Ç≠""

// Generates a 'kana' first name on the male
echo $faker->firstKanaNameMale // ""„Éí„Éá„Ç≠""

// Generates a 'kana' first name on the female
echo $faker->firstKanaNameFemale // ""„Éû„Ç¢„É§""

// Generates a 'kana' last name
echo $faker->lastKanaName; // ""„Éä„Ç´„Ç∏„Éû""
Faker\Provider\ka_GE\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""GE33ZV9773853617253389""
Faker\Provider\kk_KZ\Company
<?php

// Generates an business identification number
echo $faker->businessIdentificationNumber; // ""150140000019""
Faker\Provider\kk_KZ\Payment
<?php

// Generates a random bank name
echo $faker->bank; // ""“ö–∞–∑–∫–æ–º–º–µ—Ä—Ü–±–∞–Ω–∫""

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""KZ1076321LO4H6X41I37""
Faker\Provider\kk_KZ\Person
<?php

// Generates an individual identification number
echo $faker->individualIdentificationNumber; // ""780322300455""

// Generates an individual identification number based on his/her birth date
echo $faker->individualIdentificationNumber(new \DateTime('1999-03-01')); // ""990301300455""
Faker\Provider\ko_KR\Address
<?php

// Generates a metropolitan city
echo $faker->metropolitanCity; // ""ÏÑúÏö∏ÌäπÎ≥ÑÏãú""

// Generates a borough
echo $faker->borough; // ""Í∞ïÎÇ®Íµ¨""
Faker\Provider\ko_KR\PhoneNumber
<?php

// Generates a local area phone numer
echo $faker->localAreaPhoneNumber; // ""02-1234-4567""

// Generates a cell phone number
echo $faker->cellPhoneNumber; // ""010-9876-5432""
Faker\Provider\lt_LT\Payment
<?php

echo $faker->bankAccountNumber // ""LT300848876740317118""
Faker\Provider\lv_LV\Person
<?php

// Generates a random personal identity card number
echo $faker->personalIdentityNumber; // ""140190-12301""
Faker\Provider\ms_MY\Address
<?php

// Generates a random Malaysian township
echo $faker->township; // ""Taman Bahagia""

// Generates a random Malaysian town address with matching postcode and state
echo $faker->townState; // ""55100 Bukit Bintang, Kuala Lumpur""
Faker\Provider\ms_MY\Miscellaneous
<?php

// Generates a random vehicle license plate number
echo $faker->jpjNumberPlate; // ""WPL 5169""
Faker\Provider\ms_MY\Payment
<?php

// Generates a random Malaysian bank
echo $faker->bank; // ""Maybank""

// Generates a random Malaysian bank account number (10-16 digits)
echo $faker->bankAccountNumber; // ""1234567890123456""

// Generates a random Malaysian insurance company
echo $faker->insurance; // ""AIA Malaysia""

// Generates a random Malaysian bank SWIFT Code
echo $faker->swiftCode; // ""MBBEMYKLXXX""
Faker\Provider\ms_MY\Person
<?php

// Generates a random personal identity card (myKad) number
echo $faker->myKadNumber($gender = null|'male'|'female', $hyphen = null|true|false); // ""710703471796""
Faker\Provider\ms_MY\PhoneNumber
<?php

// Generates a random Malaysian mobile number
echo $faker->mobileNumber($countryCodePrefix = null|true|false, $formatting = null|true|false); // ""+6012-705 3767""

// Generates a random Malaysian landline number
echo $faker->fixedLineNumber($countryCodePrefix = null|true|false, $formatting = null|true|false); // ""03-7112 0455""

// Generates a random Malaysian voip number
echo $faker->voipNumber($countryCodePrefix = null|true|false, $formatting = null|true|false); // ""015-458 7099""
Faker\Provider\ne_NP\Address
<?php

//Generates a Nepali district name
echo $faker->district;

//Generates a Nepali city name
echo $faker->cityName;
Faker\Provider\nl_BE\Payment
<?php

echo $faker->vat;           // ""BE 0123456789"" - Belgian Value Added Tax number
echo $faker->vat(false);    // ""BE0123456789"" - unspaced Belgian Value Added Tax number
Faker\Provider\nl_BE\Person
<?php

echo $faker->rrn();         // ""83051711784"" - Belgian Rijksregisternummer
echo $faker->rrn('female'); // ""50032089858"" - Belgian Rijksregisternummer for a female
Faker\Provider\nl_NL\Company
<?php

echo $faker->jobTitle; // ""Houtbewerker""
echo $faker->vat; // ""NL123456789B01"" - Dutch Value Added Tax number
echo $faker->btw; // ""NL123456789B01"" - Dutch Value Added Tax number (alias)
Faker\Provider\nl_NL\Person
<?php

echo $faker->idNumber; // ""111222333"" - Dutch Personal identification number (BSN)
Faker\Provider\nb_NO\MobileNumber
<?php

// Generates a random Norwegian mobile phone number
echo $faker->mobileNumber; // ""+4799988777""
echo $faker->mobileNumber; // ""999 88 777""
echo $faker->mobileNumber; // ""99988777""
Faker\Provider\nb_NO\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""NO3246764709816""
Faker\Provider\pl_PL\Person
<?php

// Generates a random PESEL number
echo $faker->pesel; // ""40061451555""
// Generates a random personal identity card number
echo $faker->personalIdentityNumber; // ""AKX383360""
// Generates a random taxpayer identification number (NIP)
echo $faker->taxpayerIdentificationNumber; // '8211575109'
Faker\Provider\pl_PL\Company
<?php

// Generates a random REGON number
echo $faker->regon; // ""714676680""
// Generates a random local REGON number
echo $faker->regonLocal; // ""15346111382836""
Faker\Provider\pl_PL\Payment
<?php

// Generates a random bank name
echo $faker->bank; // ""Narodowy Bank Polski""
// Generates a random bank account number
echo $faker->bankAccountNumber; // ""PL14968907563953822118075816""
Faker\Provider\pt_PT\Person
<?php

// Generates a random taxpayer identification number (in portuguese - N√∫mero de Identifica√ß√£o Fiscal NIF)
echo $faker->taxpayerIdentificationNumber; // '165249277'
Faker\Provider\pt_BR\Address
<?php

// Generates a random region name
echo $faker->region; // 'Nordeste'

// Generates a random region abbreviation
echo $faker->regionAbbr; // 'NE'
Faker\Provider\pt_BR\PhoneNumber
<?php

echo $faker->areaCode;  // 21
echo $faker->cellphone; // 9432-5656
echo $faker->landline;  // 2654-3445
echo $faker->phone;     // random landline, 8-digit or 9-digit cellphone number

// Using the phone functions with a false argument returns unformatted numbers
echo $faker->cellphone(false); // 74336667

// cellphone() has a special second argument to add the 9th digit. Ignored if generated a Radio number
echo $faker->cellphone(true, true); // 98983-3945 or 7343-1290

// Using the ""Number"" suffix adds area code to the phone
echo $faker->cellphoneNumber;       // (11) 98309-2935
echo $faker->landlineNumber(false); // 3522835934
echo $faker->phoneNumber;           // formatted, random landline or cellphone (obeying the 9th digit rule)
echo $faker->phoneNumberCleared;    // not formatted, random landline or cellphone (obeying the 9th digit rule)
Faker\Provider\pt_BR\Person
<?php

// The name generator may include double first or double last names, plus title and suffix
echo $faker->name; // 'Sr. Luis Adriano Sep√∫lveda Filho'

// Valid document generators have a boolean argument to remove formatting
echo $faker->cpf;        // '145.343.345-76'
echo $faker->cpf(false); // '45623467866'
echo $faker->rg;         // '84.405.736-3'
echo $faker->rg(false);  // '844057363'
Faker\Provider\pt_BR\Company
<?php

// Generates a Brazilian formatted and valid CNPJ
echo $faker->cnpj;        // '23.663.478/0001-24'
echo $faker->cnpj(false); // '23663478000124'
Faker\Provider\ro_MD\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""MD83BQW1CKMUW34HBESDP3A8""
Faker\Provider\ro_RO\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""RO55WRJE3OE8X3YQI7J26U1E""
Faker\Provider\ro_RO\Person
<?php

// Generates a random male name prefix/title
echo $faker->prefixMale; // ""ing.""
// Generates a random female name prefix/title
echo $faker->prefixFemale; // ""d-na.""
// Generates a random male first name
echo $faker->firstNameMale; // ""Adrian""
// Generates a random female first name
echo $faker->firstNameFemale; // ""Miruna""


// Generates a random Personal Numerical Code (CNP)
echo $faker->cnp; // ""2800523081231""
// Valid option values:
//    $gender: null (random), male, female
//    $dateOfBirth (1800+): null (random), Y-m-d, Y-m (random day), Y (random month and day)
//          i.e. '1981-06-16', '2015-03', '1900'
//    $county: 2 letter ISO 3166-2:RO county codes and B1, B2, B3, B4, B5, B6 for Bucharest's 6 sectors
//    $isResident true/false flag if the person resides in Romania
echo $faker->cnp($gender = null, $dateOfBirth = null, $county = null, $isResident = true);

Faker\Provider\ro_RO\PhoneNumber
<?php

// Generates a random toll-free phone number
echo $faker->tollFreePhoneNumber; // ""0800123456""
// Generates a random premium-rate phone number
echo $faker->premiumRatePhoneNumber; // ""0900123456""
Faker\Provider\ru_RU\Payment
<?php

// Generates a Russian bank name (based on list of real russian banks)
echo $faker->bank; // ""–û–¢–ü –ë–∞–Ω–∫""

//Generate a Russian Tax Payment Number for Company
echo $faker->inn; //  7813540735

//Generate a Russian Tax Code for Company
echo $faker->kpp; // 781301001
Faker\Provider\sv_SE\Payment
<?php

// Generates a random bank account number
echo $faker->bankAccountNumber; // ""SE5018548608468284909192""
Faker\Provider\sv_SE\Person
<?php

//Generates a valid Swedish personal identity number (in Swedish - Personnummer)
echo $faker->personalIdentityNumber() // '950910-0799'

//Since the numbers are different for male and female persons, optionally you can specify gender.
echo $faker->personalIdentityNumber('female') // '950910-0781'
Faker\Provider\tr_TR\Person
<?php

//Generates a valid Turkish identity number (in Turkish - T.C. Kimlik No)
echo $faker->tcNo // '55300634882'

Faker\Provider\zh_CN\Payment
<?php

// Generates a random bank name (based on list of real chinese banks)
echo $faker->bank; // '‰∏≠ÂõΩÂª∫ËÆæÈì∂Ë°å'
Faker\Provider\uk_UA\Payment
<?php

// Generates an Ukraine bank name (based on list of real Ukraine banks)
echo $faker->bank; // ""–û—â–∞–¥–±–∞–Ω–∫""
Faker\Provider\zh_TW\Person
<?php

// Generates a random personal identify number
echo $faker->personalIdentityNumber; // A223456789
Faker\Provider\zh_TW\Company
<?php

// Generates a random VAT / Company Tax number
echo $faker->VAT; //23456789
Third-Party Libraries Extending/Based On Faker

Symfony bundles:

willdurand/faker-bundle: Put the awesome Faker library into the Symfony2 DIC and populate your database with fake data.
hautelook/alice-bundle, h4cc/alice-fixtures-bundle: Bundles for using nelmio/alice and Faker with data fixtures. Able to use Doctrine ORM as well as Doctrine MongoDB ODM.


emanueleminotto/faker-service-provider: Faker Service Provider for Silex
bit3/faker-cli: Command Line Tool for the Faker PHP library
league/factory-muffin: enable the rapid creation of objects (PHP port of factory-girl)
fzaninotto/company-name-generator: Generate names for English tech companies with class
emanueleminotto/faker-placehold-it-provider: Generate images using placehold.it
spyrit/datalea A highly customizable random test data generator web app
frequenc1/newage-ipsum: A new aged ipsum provider for the faker library inspired by http://sebpearce.com/bullshit/
prewk/xml-faker: Create fake XML with Faker
denheck/faker-context: Behat context using Faker to generate testdata
swekaj/cron-expression-generator: Faker provider for generating random, valid cron expressions.
pragmafabrik/pomm-faker: Faker client for Pomm database framework (PostgreSQL)
nelmio/alice: Fixtures/object generator with a yaml DSL that can use Faker as data generator.
ravage84/cakephp-fake-seeder A CakePHP 2.x shell to seed your database with fake and/or fixed data.
bheller/images-generator: An image generator provider using GD for placeholder type pictures
pattern-lab/plugin-faker: Pattern Lab is a Styleguide, Component Library, and Prototyping tool. This creates unique content each time Pattern Lab is generated.
guidocella/eloquent-populator: Adapter for Laravel's Eloquent ORM.
tamperdata/exiges: Faker provider for generating random temperatures
jzonta/faker-restaurant: Faker for Food and Beverage names generate
aalaap/faker-youtube: Faker for YouTube URLs in various formats
pelmered/fake-car: Faker for cars and car data
bluemmb/faker-picsum-photos-provider: Generate images using picsum.photos
er1z/fakemock: Generate mocks using class-configuration and detection via Faker's guesser and Symfony asserts
xvladqt/faker-lorem-flickr: Generate images using loremflickr.com
metrakit/faker-eddy-malou: Generate French Eddy Malou sentences & paragraphs
drupol/belgian-national-number-faker: Generate fake Belgian national numbers
elgentos/masquerade: Configuration-based, platform-agnostic, locale-compatible data faker tool (out-of-the-box support for Magento 2)
ottaviano/faker-gravatar: Generate avatars using Gravatar
finwe/phpstan-faker: PHPStan extension for Faker methods

License
Faker is released under the MIT License. See the bundled LICENSE file for details.
",GitHub - fzaninotto/Faker: Faker is a PHP library that generates fake data for you
43,PHP,"


Symfony is a PHP framework for web and console applications and a set of reusable
PHP components. Symfony is used by thousands of web applications (including
BlaBlaCar.com and Spotify.com) and most of the popular PHP projects (including
Drupal and Magento).
Installation

Install Symfony with Composer (see requirements details).
Symfony follows the semantic versioning strictly, publishes ""Long Term
Support"" (LTS) versions and has a release process that is predictable and
business-friendly.

Documentation

Read the Getting Started guide if you are new to Symfony.
Try the Symfony Demo application to learn Symfony in practice.
Master Symfony with the Guides and Tutorials, the Components docs
and the Best Practices reference.

Community

Join the Symfony Community and meet other members at the Symfony events.
Get Symfony support on Stack Overflow, Slack, IRC, etc.
Follow us on GitHub, Twitter and Facebook.
Read our Code of Conduct and meet the CARE Team

Contributing
Symfony is an Open Source, community-driven project with thousands of
contributors. Join them contributing code or contributing documentation.
Security Issues
If you discover a security vulnerability within Symfony, please follow our
disclosure procedure.
About Us
Symfony development is sponsored by SensioLabs, led by the
Symfony Core Team and supported by Symfony contributors.
",GitHub - symfony/symfony: The Symfony PHP framework
44,PHP,"
About SecLists
SecLists is the security tester's companion. It's a collection of multiple types of lists used during security assessments, collected in one place. List types include usernames, passwords, URLs, sensitive data patterns, fuzzing payloads, web shells, and many more. The goal is to enable a security tester to pull this repository onto a new testing box and have access to every type of list that may be needed.
This project is maintained by Daniel Miessler, Jason Haddix, and g0tmi1k.

Install
Zip
wget -c https://github.com/danielmiessler/SecLists/archive/master.zip -O SecList.zip \
  && unzip SecList.zip \
  && rm -f SecList.zip

Git (Small)
git clone --depth 1 https://github.com/danielmiessler/SecLists.git

Git (Complete)
git clone https://github.com/danielmiessler/SecLists.git

Kali Linux (Tool Page)
apt -y install seclists


Attribution
See CONTRIBUTORS.md

Contributing
See CONTRIBUTING.md

Similar Projects

PayloadsAllTheThings
FuzzDB


Licensing
This project is licensed under the MIT license.

‚Äî
NOTE: Downloading this repository is likely to cause a false-positive alarm by your anti-virus or anti-malware software, the filepath should be whitelisted. There is nothing in SecLists that can harm your computer as-is, however it's not recommended to store these files on a server or other important system due to the risk of local file include attacks.
","GitHub - danielmiessler/SecLists: SecLists is the security tester's companion. It's a collection of multiple types of lists used during security assessments, collected in one place. List types include usernames, passwords, URLs, sensitive data patterns, fuzzing payloads, web shells, and many more."
45,PHP,"Composer - Dependency Management for PHP
Composer helps you declare, manage, and install dependencies of PHP projects.
See https://getcomposer.org/ for more information and documentation.

Installation / Usage
Download and install Composer by following the official instructions.
For usage, see the documentation.
Packages
Find packages on Packagist.
Community
IRC channels are on irc.freenode.org: #composer
for users and #composer-dev for development.
For support, Stack Overflow also offers a good collection of
Composer related questions.
Please note that this project is released with a
Contributor Code of Conduct.
By participating in this project and its community you agree to abide by those terms.
Requirements
PHP 5.3.2 or above (at least 5.3.4 recommended to avoid potential bugs)
Authors

Nils Adermann  | GitHub  | Twitter | naderman@naderman.de | naderman.de
Jordi Boggiano | GitHub | Twitter | j.boggiano@seld.be | seld.be

See also the list of contributors who participated in this project.
Security Reports
Please send any sensitive issue to security@packagist.org. Thanks!
License
Composer is licensed under the MIT License - see the LICENSE file for details
Acknowledgments

This project's Solver started out as a PHP port of openSUSE's
Libzypp satsolver.

",GitHub - composer/composer: Dependency Manager for PHP
46,PHP,"






About Laravel

Note: This repository contains the core code of the Laravel framework. If you want to build an application using Laravel, visit the main Laravel repository.

Laravel is a web application framework with expressive, elegant syntax. We believe development must be an enjoyable, creative experience to be truly fulfilling. Laravel attempts to take the pain out of development by easing common tasks used in the majority of web projects, such as:

Simple, fast routing engine.
Powerful dependency injection container.
Multiple back-ends for session and cache storage.
Database agnostic schema migrations.
Robust background job processing.
Real-time event broadcasting.

Laravel is accessible, yet powerful, providing tools needed for large, robust applications. A superb combination of simplicity, elegance, and innovation gives you a complete toolset required to build any application with which you are tasked.
Learning Laravel
Laravel has the most extensive and thorough documentation and video tutorial library of any modern web application framework. The Laravel documentation is in-depth and complete, making it a breeze to get started learning the framework.
If you're not in the mood to read, Laracasts contains over 1100 video tutorials covering a range of topics including Laravel, modern PHP, unit testing, JavaScript, and more. Boost the skill level of yourself and your entire team by digging into our comprehensive video library.
Contributing
Thank you for considering contributing to the Laravel framework! The contribution guide can be found in the Laravel documentation.
Code of Conduct
In order to ensure that the Laravel community is welcoming to all, please review and abide by the Code of Conduct.
Security Vulnerabilities
Please review our security policy on how to report security vulnerabilities.
License
The Laravel framework is open-sourced software licensed under the MIT license.
",GitHub - laravel/framework
47,PHP,"Guzzle, PHP HTTP client



Guzzle is a PHP HTTP client that makes it easy to send HTTP requests and
trivial to integrate with web services.

Simple interface for building query strings, POST requests, streaming large
uploads, streaming large downloads, using HTTP cookies, uploading JSON data,
etc...
Can send both synchronous and asynchronous requests using the same interface.
Uses PSR-7 interfaces for requests, responses, and streams. This allows you
to utilize other PSR-7 compatible libraries with Guzzle.
Abstracts away the underlying HTTP transport, allowing you to write
environment and transport agnostic code; i.e., no hard dependency on cURL,
PHP streams, sockets, or non-blocking event loops.
Middleware system allows you to augment and compose client behavior.

$client = new \GuzzleHttp\Client();
$response = $client->request('GET', 'https://api.github.com/repos/guzzle/guzzle');

echo $response->getStatusCode(); # 200
echo $response->getHeaderLine('content-type'); # 'application/json; charset=utf8'
echo $response->getBody(); # '{""id"": 1420053, ""name"": ""guzzle"", ...}'

# Send an asynchronous request.
$request = new \GuzzleHttp\Psr7\Request('GET', 'http://httpbin.org');
$promise = $client->sendAsync($request)->then(function ($response) {
    echo 'I completed! ' . $response->getBody();
});

$promise->wait();
Help and docs

Documentation
Stack Overflow
Gitter

Installing Guzzle
The recommended way to install Guzzle is through
Composer.
# Install Composer
curl -sS https://getcomposer.org/installer | php
Next, run the Composer command to install the latest stable version of Guzzle:
composer require guzzlehttp/guzzle
After installing, you need to require Composer's autoloader:
require 'vendor/autoload.php';
You can then later update Guzzle using composer:
composer update
Version Guidance



Version
Status
Packagist
Namespace
Repo
Docs
PSR-7
PHP Version




3.x
EOL
guzzle/guzzle
Guzzle
v3
v3
No
>= 5.3.3


4.x
EOL
guzzlehttp/guzzle
GuzzleHttp
v4
N/A
No
>= 5.4


5.x
EOL
guzzlehttp/guzzle
GuzzleHttp
v5
v5
No
>= 5.4


6.x
Maintained
guzzlehttp/guzzle
GuzzleHttp
v6
v6
Yes
>= 5.5


7.x
Latest
guzzlehttp/guzzle
GuzzleHttp
v7
v7
Yes
>= 7.2



","GitHub - guzzle/guzzle: Guzzle, an extensible PHP HTTP client"
48,PHP,"DesignPatternsPHP



Read the Docs of DesignPatternsPHP
or Download as PDF/Epub
This is a collection of known design patterns and some sample code how to implement them in PHP. Every pattern has a small list of examples.
I think the problem with patterns is that often people do know them but don't know when to apply which.
Installation
You should look at and run the tests to see what happens in the example.
To do this, you should install dependencies with Composer first:
$ composer install
Read more about how to install and use Composer on your local machine here.
To run the tests use phpunit:
$ ./vendor/bin/phpunit
using Docker (optional)
You can optionally build and browse the documentation using Docker for Mac, Windows or Linux.
Just run:
$ docker-compose up --build
Go to http://localhost:8080/README.html to read the generated documentation.
Patterns
The patterns can be structured in roughly three different categories. Please click on the üìì for a full explanation of the pattern on Wikipedia.
Creational

AbstractFactory üìì
Builder üìì
FactoryMethod üìì
Multiton (is considered an anti-pattern! ‚õîÔ∏è)
Pool üìì
Prototype üìì
SimpleFactory
Singleton üìì (is considered an anti-pattern! ‚õîÔ∏è)
StaticFactory

Structural

Adapter üìì
Bridge üìì
Composite üìì
DataMapper üìì
Decorator üìì
DependencyInjection üìì
Facade üìì
FluentInterface üìì
Flyweight üìì
Proxy üìì
Registry üìì

Behavioral

ChainOfResponsibilities üìì
Command üìì
Iterator üìì
Mediator üìì
Memento üìì
NullObject üìì
Observer üìì
Specification üìì
State üìì
Strategy üìì
TemplateMethod üìì
Visitor üìì

More

EAV üìì
Repository
ServiceLocator üìì (is considered an anti-pattern! ‚õîÔ∏è)

",GitHub - domnikl/DesignPatternsPHP: sample code for several design patterns in PHP
49,PHP,"What is CodeIgniter
CodeIgniter is an Application Development Framework - a toolkit - for people
who build web sites using PHP. Its goal is to enable you to develop projects
much faster than you could if you were writing code from scratch, by providing
a rich set of libraries for commonly needed tasks, as well as a simple
interface and logical structure to access these libraries. CodeIgniter lets
you creatively focus on your project by minimizing the amount of code needed
for a given task.

Release Information
This repo contains in-development code for future releases. To download the
latest stable release please visit the CodeIgniter Downloads page.

Changelog and New Features
You can find a list of all changes for each release in the user
guide change log.

Server Requirements
PHP version 5.6 or newer is recommended.
It should work on 5.4.8 as well, but we strongly advise you NOT to run
such old versions of PHP, because of potential security and performance
issues, as well as missing features.

Installation
Please see the installation section
of the CodeIgniter User Guide.

License
Please see the license
agreement.

Resources

User Guide
Language File Translations
Community Forums
Community Wiki
Community Slack Channel

Report security issues to our Security Panel
or via our page on HackerOne, thank you.

Acknowledgement
The CodeIgniter team would like to thank EllisLab, all the
contributors to the CodeIgniter project and you, the CodeIgniter user.
",GitHub - bcit-ci/CodeIgniter: Open Source PHP Framework (originally from EllisLab)
50,PHP,"Monolog - Logging for PHP 


Monolog sends your logs to files, sockets, inboxes, databases and various
web services. See the complete list of handlers below. Special handlers
allow you to build advanced logging strategies.
This library implements the PSR-3
interface that you can type-hint against in your own libraries to keep
a maximum of interoperability. You can also use it in your applications to
make sure you can always use another compatible logger at a later time.
As of 1.11.0 Monolog public APIs will also accept PSR-3 log levels.
Internally Monolog still uses its own level scheme since it predates PSR-3.
Installation
Install the latest version with
$ composer require monolog/monolog
Basic Usage
<?php

use Monolog\Logger;
use Monolog\Handler\StreamHandler;

// create a log channel
$log = new Logger('name');
$log->pushHandler(new StreamHandler('path/to/your.log', Logger::WARNING));

// add records to the log
$log->warning('Foo');
$log->error('Bar');
Documentation

Usage Instructions
Handlers, Formatters and Processors
Utility Classes
Extending Monolog
Log Record Structure

Support Monolog Financially
Get supported Monolog and help fund the project with the Tidelift Subscription or via GitHub sponsorship.
Tidelift delivers commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use.
Third Party Packages
Third party handlers, formatters and processors are
listed in the wiki. You
can also add your own there if you publish one.
About
Requirements

Monolog 2.x works with PHP 7.2 or above, use Monolog ^1.0 for PHP 5.3+ support.

Submitting bugs and feature requests
Bugs and feature request are tracked on GitHub
Framework Integrations

Frameworks and libraries using PSR-3
can be used very easily with Monolog since it implements the interface.
Symfony comes out of the box with Monolog.
Laravel comes out of the box with Monolog.
Lumen comes out of the box with Monolog.
PPI comes out of the box with Monolog.
CakePHP is usable with Monolog via the cakephp-monolog plugin.
Slim is usable with Monolog via the Slim-Monolog log writer.
XOOPS 2.6 comes out of the box with Monolog.
Aura.Web_Project comes out of the box with Monolog.
Nette Framework can be used with Monolog via Kdyby/Monolog extension.
Proton Micro Framework comes out of the box with Monolog.
FuelPHP comes out of the box with Monolog.
Equip Framework comes out of the box with Monolog.
Yii 2 is usable with Monolog via the yii2-monolog or yii2-psr-log-target plugins.
Hawkbit Micro Framework comes out of the box with Monolog.
SilverStripe 4 comes out of the box with Monolog.

Author
Jordi Boggiano - j.boggiano@seld.be - http://twitter.com/seldaek
See also the list of contributors which participated in this project.
License
Monolog is licensed under the MIT License - see the LICENSE file for details
Acknowledgements
This library is heavily inspired by Python's Logbook
library, although most concepts have been adjusted to fit to the PHP world.
","GitHub - Seldaek/monolog: Sends your logs to files, sockets, inboxes, databases and various web services"
51,PHP,"PHPUnit
PHPUnit is a programmer-oriented testing framework for PHP. It is an instance of the xUnit architecture for unit testing frameworks.





Installation
We distribute a PHP Archive (PHAR) that has all required (as well as some optional) dependencies of PHPUnit 9.0 bundled in a single file:
$ wget https://phar.phpunit.de/phpunit-nightly.phar

$ php phpunit-nightly.phar --version
Alternatively, you may use Composer to download and install PHPUnit as well as its dependencies. Please refer to the ""Getting Started"" guide for details on how to install PHPUnit.
Contribute
Please refer to CONTRIBUTING.md for information on how to contribute to PHPUnit and its related projects.
List of Contributors
Thanks to everyone who has contributed to PHPUnit! You can find a detailed list of contributors on every PHPUnit related package on GitHub. This list shows only the major components:

PHPUnit
php-code-coverage

A very special thanks to everyone who has contributed to the documentation and helps maintain the translations:

English
Spanish
French
Japanese
Brazilian Portuguese
Simplified Chinese

",GitHub - sebastianbergmann/phpunit: The PHP Unit Testing framework.
52,PHP,"
PHPMailer - A full-featured email creation and transfer class for PHP
Build status: 


    
Class Features

Probably the world's most popular code for sending email from PHP!
Used by many open-source projects: WordPress, Drupal, 1CRM, SugarCRM, Yii, Joomla! and many more
Integrated SMTP support - send without a local mail server
Send emails with multiple To, CC, BCC and Reply-to addresses
Multipart/alternative emails for mail clients that do not read HTML email
Add attachments, including inline
Support for UTF-8 content and 8bit, base64, binary, and quoted-printable encodings
SMTP authentication with LOGIN, PLAIN, CRAM-MD5, and XOAUTH2 mechanisms over SSL and SMTP+STARTTLS transports
Validates email addresses automatically
Protect against header injection attacks
Error messages in over 50 languages!
DKIM and S/MIME signing support
Compatible with PHP 5.5 and later
Namespaced to prevent name clashes
Much more!

Why you might need it
Many PHP developers need to send email from their code. The only PHP function that supports this is mail(). However, it does not provide any assistance for making use of popular features such as encryption, authentication, HTML messages, and attachments.
Formatting email correctly is surprisingly difficult. There are myriad overlapping RFCs, requiring tight adherence to horribly complicated formatting and encoding rules ‚Äì the vast majority of code that you'll find online that uses the mail() function directly is just plain wrong!
Please don't be tempted to do it yourself ‚Äì if you don't use PHPMailer, there are many other excellent libraries that you should look at before rolling your own. Try SwiftMailer, Zend/Mail, ZetaComponents etc.
The PHP mail() function usually sends via a local mail server, typically fronted by a sendmail binary on Linux, BSD, and macOS platforms, however, Windows usually doesn't include a local mail server; PHPMailer's integrated SMTP implementation allows email sending on Windows platforms without a local mail server.
License
This software is distributed under the LGPL 2.1 license, along with the GPL Cooperation Commitment. Please read LICENSE for information on the software availability and distribution.
Installation & loading
PHPMailer is available on Packagist (using semantic versioning), and installation via Composer is the recommended way to install PHPMailer. Just add this line to your composer.json file:
""phpmailer/phpmailer"": ""~6.1""
or run
composer require phpmailer/phpmailer
Note that the vendor folder and the vendor/autoload.php script are generated by Composer; they are not part of PHPMailer.
If you want to use the Gmail XOAUTH2 authentication class, you will also need to add a dependency on the league/oauth2-client package in your composer.json.
Alternatively, if you're not using Composer, copy the contents of the PHPMailer folder into one of the include_path directories specified in your PHP configuration and load each class file manually:
<?php
use PHPMailer\PHPMailer\PHPMailer;
use PHPMailer\PHPMailer\Exception;

require 'path/to/PHPMailer/src/Exception.php';
require 'path/to/PHPMailer/src/PHPMailer.php';
require 'path/to/PHPMailer/src/SMTP.php';
If you're not using the SMTP class explicitly (you're probably not), you don't need a use line for the SMTP class.
If you don't speak git or just want a tarball, click the 'zip' button on the right of the project page in GitHub, though note that docs and examples are not included in the tarball.
Legacy versions
PHPMailer 5.2 (which is compatible with PHP 5.0 - 7.0) is no longer being supported, even for security updates. You will find the latest version of 5.2 in the 5.2-stable branch. If you're using PHP 5.5 or later (which you should be), switch to the 6.x releases.
Upgrading from 5.2
The biggest changes are that source files are now in the src/ folder, and PHPMailer now declares the namespace PHPMailer\PHPMailer. This has several important effects ‚Äì read the upgrade guide for more details.
Minimal installation
While installing the entire package manually or with Composer is simple, convenient, and reliable, you may want to include only vital files in your project. At the very least you will need src/PHPMailer.php. If you're using SMTP, you'll need src/SMTP.php, and if you're using POP-before SMTP, you'll need src/POP3.php. You can skip the language folder if you're not showing errors to users and can make do with English-only errors. If you're using XOAUTH2 you will need src/OAuth.php as well as the Composer dependencies for the services you wish to authenticate with. Really, it's much easier to use Composer!
A Simple Example
<?php
// Import PHPMailer classes into the global namespace
// These must be at the top of your script, not inside a function
use PHPMailer\PHPMailer\PHPMailer;
use PHPMailer\PHPMailer\SMTP;
use PHPMailer\PHPMailer\Exception;

// Load Composer's autoloader
require 'vendor/autoload.php';

// Instantiation and passing `true` enables exceptions
$mail = new PHPMailer(true);

try {
    //Server settings
    $mail->SMTPDebug = SMTP::DEBUG_SERVER;                      // Enable verbose debug output
    $mail->isSMTP();                                            // Send using SMTP
    $mail->Host       = 'smtp1.example.com';                    // Set the SMTP server to send through
    $mail->SMTPAuth   = true;                                   // Enable SMTP authentication
    $mail->Username   = 'user@example.com';                     // SMTP username
    $mail->Password   = 'secret';                               // SMTP password
    $mail->SMTPSecure = PHPMailer::ENCRYPTION_STARTTLS;         // Enable TLS encryption; `PHPMailer::ENCRYPTION_SMTPS` also accepted
    $mail->Port       = 587;                                    // TCP port to connect to

    //Recipients
    $mail->setFrom('from@example.com', 'Mailer');
    $mail->addAddress('joe@example.net', 'Joe User');     // Add a recipient
    $mail->addAddress('ellen@example.com');               // Name is optional
    $mail->addReplyTo('info@example.com', 'Information');
    $mail->addCC('cc@example.com');
    $mail->addBCC('bcc@example.com');

    // Attachments
    $mail->addAttachment('/var/tmp/file.tar.gz');         // Add attachments
    $mail->addAttachment('/tmp/image.jpg', 'new.jpg');    // Optional name

    // Content
    $mail->isHTML(true);                                  // Set email format to HTML
    $mail->Subject = 'Here is the subject';
    $mail->Body    = 'This is the HTML message body <b>in bold!</b>';
    $mail->AltBody = 'This is the body in plain text for non-HTML mail clients';

    $mail->send();
    echo 'Message has been sent';
} catch (Exception $e) {
    echo ""Message could not be sent. Mailer Error: {$mail->ErrorInfo}"";
}
You'll find plenty more to play with in the examples folder.
If you are re-using the instance (e.g. when sending to a mailing list), you may need to clear the recipient list to avoid sending duplicate messages. See the mailing list example for further guidance.
That's it. You should now be ready to use PHPMailer!
Localization
PHPMailer defaults to English, but in the language folder you'll find many translations for PHPMailer error messages that you may encounter. Their filenames contain ISO 639-1 language code for the translations, for example fr for French. To specify a language, you need to tell PHPMailer which one to use, like this:
// To load the French version
$mail->setLanguage('fr', '/optional/path/to/language/directory/');
We welcome corrections and new languages - if you're looking for corrections to do, run the PHPMailerLangTest.php script in the tests folder and it will show any missing translations.
Documentation
Start reading at the GitHub wiki. If you're having trouble, this should be the first place you look as it's the most frequently updated.
Examples of how to use PHPMailer for common scenarios can be found in the examples folder. If you're looking for a good starting point, we recommend you start with the Gmail example.
Note that in order to reduce PHPMailer's deployed code footprint, the examples are no longer included if you load PHPMailer via Composer or via GitHub's zip file download, so you'll need to either clone the git repository or use the above links to get to the examples directly.
Complete generated API documentation is available online.
You can generate complete API-level documentation by running phpdoc in the top-level folder, and documentation will appear in the docs folder, though you'll need to have PHPDocumentor installed. You may find the unit tests a good source of how to do various operations such as encryption.
If the documentation doesn't cover what you need, search the many questions on Stack Overflow, and before you ask a question about ""SMTP Error: Could not connect to SMTP host."", read the troubleshooting guide.
Tests
There is a PHPUnit test script in the test folder. PHPMailer uses PHPUnit 4.8 - we would use 5.x but we need to run on PHP 5.5.
Build status: 
If this isn't passing, is there something you can do to help?
Security
Please disclose any vulnerabilities found responsibly - report any security problems found to the maintainers privately.
PHPMailer versions prior to 5.2.22 (released January 9th 2017) have a local file disclosure vulnerability, CVE-2017-5223. If content passed into msgHTML() is sourced from unfiltered user input, relative paths can map to absolute local file paths and added as attachments. Also note that addAttachment (just like file_get_contents, passthru, unlink, etc) should not be passed user-sourced params either! Reported by Yongxiang Li of Asiasecurity.
PHPMailer versions prior to 5.2.20 (released December 28th 2016) are vulnerable to CVE-2016-10045 a remote code execution vulnerability, responsibly reported by Dawid Golunski, and patched by Paul Buonopane (@Zenexer).
PHPMailer versions prior to 5.2.18 (released December 2016) are vulnerable to CVE-2016-10033 a critical remote code execution vulnerability, responsibly reported by Dawid Golunski.
See SECURITY for more detail on security issues.
Contributing
Please submit bug reports, suggestions and pull requests to the GitHub issue tracker.
We're particularly interested in fixing edge-cases, expanding test coverage and updating translations.
If you found a mistake in the docs, or want to add something, go ahead and amend the wiki - anyone can edit it.
If you have git clones from prior to the move to the PHPMailer GitHub organisation, you'll need to update any remote URLs referencing the old GitHub location with a command like this from within your clone:
git remote set-url upstream https://github.com/PHPMailer/PHPMailer.git
Please don't use the SourceForge or Google Code projects any more; they are obsolete and no longer maintained.
Sponsorship
Development time and resources for PHPMailer are provided by Smartmessages.net, a powerful email marketing system.

Other contributions are gladly received, whether in beer üç∫, T-shirts üëï, Amazon wishlist raids, or cold, hard cash üí∞. If you'd like to donate to say ""thank you"" to maintainers or contributors, please contact them through individual profile pages via the contributors page.
Changelog
See changelog.
History

PHPMailer was originally written in 2001 by Brent R. Matzelle as a SourceForge project.
Marcus Bointon (coolbru on SF) and Andy Prevost (codeworxtech) took over the project in 2004.
Became an Apache incubator project on Google Code in 2010, managed by Jim Jagielski.
Marcus created his fork on GitHub in 2008.
Jim and Marcus decide to join forces and use GitHub as the canonical and official repo for PHPMailer in 2013.
PHPMailer moves to the PHPMailer organisation on GitHub in 2013.

What's changed since moving from SourceForge?

Official successor to the SourceForge and Google Code projects.
Test suite.
Continuous integration with Travis-CI.
Composer support.
Public development.
Additional languages and language strings.
CRAM-MD5 authentication support.
Preserves full repo history of authors, commits and branches from the original SourceForge project.

",GitHub - PHPMailer/PHPMailer: The classic email sending library for PHP
53,PHP,"Carbon







An international PHP extension for DateTime. http://carbon.nesbot.com
use Carbon\Carbon;

printf(""Right now is %s"", Carbon::now()->toDateTimeString());
printf(""Right now in Vancouver is %s"", Carbon::now('America/Vancouver'));  //implicit __toString()
$tomorrow = Carbon::now()->addDay();
$lastWeek = Carbon::now()->subWeek();
$nextSummerOlympics = Carbon::createFromDate(2016)->addYears(4);

$officialDate = Carbon::now()->toRfc2822String();

$howOldAmI = Carbon::createFromDate(1975, 5, 21)->age;

$noonTodayLondonTime = Carbon::createFromTime(12, 0, 0, 'Europe/London');

$internetWillBlowUpOn = Carbon::create(2038, 01, 19, 3, 14, 7, 'GMT');

// Don't really want this to happen so mock now
Carbon::setTestNow(Carbon::createFromDate(2000, 1, 1));

// comparisons are always done in UTC
if (Carbon::now()->gte($internetWillBlowUpOn)) {
    die();
}

// Phew! Return to normal behaviour
Carbon::setTestNow();

if (Carbon::now()->isWeekend()) {
    echo 'Party!';
}
// Over 200 languages (and over 500 regional variants) supported:
echo Carbon::now()->subMinutes(2)->diffForHumans(); // '2 minutes ago'
echo Carbon::now()->subMinutes(2)->locale('zh_CN')->diffForHumans(); // '2ÂàÜÈíüÂâç'
echo Carbon::parse('2019-07-23 14:51')->isoFormat('LLLL'); // 'Tuesday, July 23, 2019 2:51 PM'
echo Carbon::parse('2019-07-23 14:51')->locale('fr_FR')->isoFormat('LLLL'); // 'mardi 23 juillet 2019 14:51'

// ... but also does 'from now', 'after' and 'before'
// rolling up to seconds, minutes, hours, days, months, years

$daysSinceEpoch = Carbon::createFromTimestamp(0)->diffInDays();
Get supported nesbot/carbon with the Tidelift Subscription
Installation
With Composer
$ composer require nesbot/carbon

{
    ""require"": {
        ""nesbot/carbon"": ""^2.16""
    }
}
<?php
require 'vendor/autoload.php';

use Carbon\Carbon;

printf(""Now: %s"", Carbon::now());

Without Composer
Why are you not using composer? Download the Carbon latest release and put the contents of the ZIP archive into a directory in your project. Then require the file autoload.php to get all classes and dependencies loaded on need.
<?php
require 'path-to-Carbon-directory/autoload.php';

use Carbon\Carbon;

printf(""Now: %s"", Carbon::now());
Docs
http://carbon.nesbot.com/docs
Security contact information
To report a security vulnerability, please use the
Tidelift security contact.
Tidelift will coordinate the fix and disclosure.
Credits
Contributors
This project exists thanks to all the people who contribute.

Translators
Thanks to people helping us to translate Carbon in so many languages
Backers
Thank you to all our backers! üôè [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]





",GitHub - briannesbitt/Carbon: A simple PHP API extension for DateTime.
54,PHP,"




Yii 2 is a modern framework designed to be a solid foundation for your PHP application.
It is fast, secure and efficient and works right out of the box pre-configured with reasonable defaults.
The framework is easy to adjust to meet your needs, because Yii has been designed to be flexible.






Installation

The minimum required PHP version of Yii is PHP 5.4.
It works best with PHP 7.
Follow the Definitive Guide
in order to get step by step instructions.

Documentation

A Definitive Guide and
a Class Reference cover every detail
of the framework.
There is a PDF version of the Definitive Guide
and a Definitive Guide Mirror which is updated every 15 minutes.
For Yii 1.1 users, there is Upgrading from Yii 1.1
to get an idea of what has changed in 2.0.

Community

Participate in discussions at forums.
Community Slack and Chat in IRC.
Follow us on Facebook, Twitter
and GitHub.
Check other communities.

Contributing
The framework is Open Source powered by an excellent community.
You may join us and:

Report an issue
Translate documentation or messages
Give us feedback or start a design discussion
Contribute to the core code or fix bugs
Become a sponsor

Reporting Security issues
Please refer to a special page at the website
describing proper workflow for security issue reports.
Directory Structure
build/               internally used build tools
docs/                documentation
framework/           core framework code
tests/               tests of the core framework code

Spreading the Word
Acknowledging or citing Yii 2 is as important as direct contributions.
In presentations
If you are giving a presentation or talk featuring work that makes use of Yii 2 and would like to acknowledge it,
we suggest using our logo on your title slide.
In projects
If you are using Yii 2 as part of an OpenSource project, a way to acknowledge it is to
use a special badge in your README:

If your code is hosted at GitHub, you can place the following in your README.md file to get the badge:
[![Yii2](https://img.shields.io/badge/Powered_by-Yii_Framework-green.svg?style=flat)](https://www.yiiframework.com/)

Sponsoring
Support this project by becoming a sponsor or a backer.
 
","GitHub - yiisoft/yii2: Yii 2: The Fast, Secure and Professional PHP Framework"
55,PHP,,"GitHub - WordPress/WordPress: WordPress, Git-ified. Synced via SVN every 15 minutes, including branches and tags! This repository is just a mirror of the WordPress subversion repository. Please do not send pull requests. Submit patches to https://core.trac.wordpress.org/ instead."
56,PHP,"Matomo (formerly Piwik) - matomo.org



Code Status


Description
Matomo is the leading Free/Libre open analytics platform.
Matomo is a full-featured PHP MySQL software program that you download and install on your own webserver.
At the end of the five-minute installation process, you will be given a JavaScript code.
Simply copy and paste this tag on websites you wish to track and access your analytics reports in real-time.
Matomo aims to be a Free software alternative to Google Analytics and is already used on more than 1,400,000 websites. Privacy is built-in!
Mission Statement

¬´ To create, as a community, the leading international open source digital analytics platform, that gives every user full control of their data. ¬ª

Or in short:

¬´ Liberate Web Analytics ¬ª

License
Matomo is released under the GPL v3 (or later) license, see LICENSE
Requirements

PHP 5.5.9 or greater
MySQL version 5.5 or greater, or MariaDB
PHP extension pdo and pdo_mysql, or the MySQLi extension.
Matomo is OS / server independent

See https://matomo.org/docs/requirements/
Install Matomo

Download Matomo
Upload matomo to your webserver
Point your browser to the directory
Follow the steps
Add the given javascript code to your pages
(You may also generate fake data to experiment, by enabling the plugin VisitorGenerator)

See https://matomo.org/docs/installation/
(When using Matomo for development you need to install Matomo from the Git repository.)
Free trial
If you do not have a server or don't want to host yourself you can use our Matomo Cloud partner service (30 day free trial): https://matomo.org/start-30-day-free-analytics-trial/
Online Demo
Check out the online demo for Matomo at demo.matomo.org
Changelog
For the list of all tickets closed in the current and past releases, see matomo.org/changelog/. For the list of technical changes in the Matomo platform, see developer.matomo.org/changelog.
Get involved!
We believe in liberating Web Analytics, providing a free platform for simple and advanced analytics. Matomo was built by dozens of people like you,
and we need your help to make Matomo better‚Ä¶ Why not participate in a useful project today? Learn how you can contribute to Matomo.
Quality Assurance
The Matomo project uses an ever-expanding comprehensive set of thousands of unit tests and hundreds of automated integration tests, system tests, JavaScript tests, and screenshot UI tests, running on a continuous integration server as part of its software quality assurance. Learn more
We use BrowserStack.com testing tool to help check the Matomo user interface is compatible with many browsers.
Security
Security is a top priority at Matomo. As potential issues are discovered, we validate, patch and release fixes as quickly as we can. We have a security bug bounty program in place that rewards researchers for finding security issues and disclosing them to us.
Learn more or check out our HackerOne program.
Support for Matomo
For Free support, post a message in our community forums: forum.matomo.org
For Professional paid support, purchase a Matomo On-Premises Support Plan: matomo.org/support-plans
Contact
Website: matomo.org
About us: matomo.org/team/
Contact us: matomo.org/contact/
More information
What makes Matomo unique from the competition:


You own your web analytics data: since Matomo is installed on your server, the data is stored in your own database and you can get all the statistics using the powerful Matomo Analytics API.


Matomo is a Free Software which can easily be configured to respect your visitors' privacy.


Modern, easy to use User Interface: you can fully customize your dashboard, drag and drop widgets and more.


Matomo features are built inside plugins: you can add new features and remove the ones you don‚Äôt need.
You can build your own web analytics plugins or hire a consultant to have your custom feature built-in Matomo.


A vibrant international Open community of more than 200,000 active users (tracking even more websites!)


Advanced Web Analytics capabilities such as E-commerce Tracking, Goal tracking, Campaign tracking,
Custom Variables, Email Reports, Custom Segment Editor, Geo Location, Real-time visits and maps, and a lot more!


Documentation and more info on https://matomo.org
We are together creating the best open analytics platform in the world!
","GitHub - matomo-org/matomo: Liberating Web Analytics. Star us on Github? +1. Matomo is the leading open alternative to Google Analytics that gives you full control over your data. Matomo lets you easily collect data from websites, apps & the IoT and visualise this data and extract insights. Privacy is built-in. We love Pull Requests!"
57,PHP,"

Parsedown




Better Markdown Parser in PHP - Demo.
Features

One File
No Dependencies
Super Fast
Extensible
GitHub flavored
Tested in 5.3 to 7.3
Markdown Extra extension

Installation
Install the composer package:
composer require erusev/parsedown

Or download the latest release and include Parsedown.php
Example
$Parsedown = new Parsedown();

echo $Parsedown->text('Hello _Parsedown_!'); # prints: <p>Hello <em>Parsedown</em>!</p>
You can also parse inline markdown only:
echo $Parsedown->line('Hello _Parsedown_!'); # prints: Hello <em>Parsedown</em>!
More examples in the wiki and in this video tutorial.
Security
Parsedown is capable of escaping user-input within the HTML that it generates. Additionally Parsedown will apply sanitisation to additional scripting vectors (such as scripting link destinations) that are introduced by the markdown syntax itself.
To tell Parsedown that it is processing untrusted user-input, use the following:
$Parsedown->setSafeMode(true);
If instead, you wish to allow HTML within untrusted user-input, but still want output to be free from XSS it is recommended that you make use of a HTML sanitiser that allows HTML tags to be whitelisted, like HTML Purifier.
In both cases you should strongly consider employing defence-in-depth measures, like deploying a Content-Security-Policy (a browser security feature) so that your page is likely to be safe even if an attacker finds a vulnerability in one of the first lines of defence above.
Security of Parsedown Extensions
Safe mode does not necessarily yield safe results when using extensions to Parsedown. Extensions should be evaluated on their own to determine their specific safety against XSS.
Escaping HTML

‚ö†Ô∏è¬†¬†WARNING: This method isn't safe from XSS!

If you wish to escape HTML in trusted input, you can use the following:
$Parsedown->setMarkupEscaped(true);
Beware that this still allows users to insert unsafe scripting vectors, such as links like [xss](javascript:alert%281%29).
Questions
How does Parsedown work?
It tries to read Markdown like a human. First, it looks at the lines. It‚Äôs interested in how the lines start. This helps it recognise blocks. It knows, for example, that if a line starts with a - then perhaps it belongs to a list. Once it recognises the blocks, it continues to the content. As it reads, it watches out for special characters. This helps it recognise inline elements (or inlines).
We call this approach ""line based"". We believe that Parsedown is the first Markdown parser to use it. Since the release of Parsedown, other developers have used the same approach to develop other Markdown parsers in PHP and in other languages.
Is it compliant with CommonMark?
It passes most of the CommonMark tests. Most of the tests that don't pass deal with cases that are quite uncommon. Still, as CommonMark matures, compliance should improve.
Who uses it?
Laravel Framework, Bolt CMS, Grav CMS, Herbie CMS, Kirby CMS, October CMS, Pico CMS, Statamic CMS, phpDocumentor, RaspberryPi.org, Symfony Demo and more.
How can I help?
Use it, star it, share it and if you feel generous, donate.
",GitHub - erusev/parsedown: Better Markdown Parser in PHP
58,PHP,"PHP Parser
 
This is a PHP 5.2 to PHP 7.4 parser written in PHP. Its purpose is to simplify static code analysis and
manipulation.
Documentation for version 4.x (stable; for running on PHP >= 7.0; for parsing PHP 5.2 to PHP 7.4).
Documentation for version 3.x (unsupported; for running on PHP >= 5.5; for parsing PHP 5.2 to PHP 7.2).
Features
The main features provided by this library are:

Parsing PHP 5 and PHP 7 code into an abstract syntax tree (AST).

Invalid code can be parsed into a partial AST.
The AST contains accurate location information.


Dumping the AST in human-readable form.
Converting an AST back to PHP code.

Experimental: Formatting can be preserved for partially changed ASTs.


Infrastructure to traverse and modify ASTs.
Resolution of namespaced names.
Evaluation of constant expressions.
Builders to simplify AST construction for code generation.
Converting an AST into JSON and back.

Quick Start
Install the library using composer:
php composer.phar require nikic/php-parser

Parse some PHP code into an AST and dump the result in human-readable form:
<?php
use PhpParser\Error;
use PhpParser\NodeDumper;
use PhpParser\ParserFactory;

$code = <<<'CODE'
<?php

function test($foo)
{
    var_dump($foo);
}
CODE;

$parser = (new ParserFactory)->create(ParserFactory::PREFER_PHP7);
try {
    $ast = $parser->parse($code);
} catch (Error $error) {
    echo ""Parse error: {$error->getMessage()}\n"";
    return;
}

$dumper = new NodeDumper;
echo $dumper->dump($ast) . ""\n"";
This dumps an AST looking something like this:
array(
    0: Stmt_Function(
        byRef: false
        name: Identifier(
            name: test
        )
        params: array(
            0: Param(
                type: null
                byRef: false
                variadic: false
                var: Expr_Variable(
                    name: foo
                )
                default: null
            )
        )
        returnType: null
        stmts: array(
            0: Stmt_Expression(
                expr: Expr_FuncCall(
                    name: Name(
                        parts: array(
                            0: var_dump
                        )
                    )
                    args: array(
                        0: Arg(
                            value: Expr_Variable(
                                name: foo
                            )
                            byRef: false
                            unpack: false
                        )
                    )
                )
            )
        )
    )
)

Let's traverse the AST and perform some kind of modification. For example, drop all function bodies:
use PhpParser\Node;
use PhpParser\Node\Stmt\Function_;
use PhpParser\NodeTraverser;
use PhpParser\NodeVisitorAbstract;

$traverser = new NodeTraverser();
$traverser->addVisitor(new class extends NodeVisitorAbstract {
    public function enterNode(Node $node) {
        if ($node instanceof Function_) {
            // Clean out the function body
            $node->stmts = [];
        }
    }
});

$ast = $traverser->traverse($ast);
echo $dumper->dump($ast) . ""\n"";
This gives us an AST where the Function_::$stmts are empty:
array(
    0: Stmt_Function(
        byRef: false
        name: Identifier(
            name: test
        )
        params: array(
            0: Param(
                type: null
                byRef: false
                variadic: false
                var: Expr_Variable(
                    name: foo
                )
                default: null
            )
        )
        returnType: null
        stmts: array(
        )
    )
)

Finally, we can convert the new AST back to PHP code:
use PhpParser\PrettyPrinter;

$prettyPrinter = new PrettyPrinter\Standard;
echo $prettyPrinter->prettyPrintFile($ast);
This gives us our original code, minus the var_dump() call inside the function:
<?php

function test($foo)
{
}
For a more comprehensive introduction, see the documentation.
Documentation

Introduction
Usage of basic components

Component documentation:

Walking the AST

Node visitors
Modifying the AST from a visitor
Short-circuiting traversals
Interleaved visitors
Simple node finding API
Parent and sibling references


Name resolution

Name resolver options
Name resolution context


Pretty printing

Converting AST back to PHP code
Customizing formatting
Formatting-preserving code transformations


AST builders

Fluent builders for AST nodes


Lexer

Lexer options
Token and file positions for nodes
Custom attributes


Error handling

Column information for errors
Error recovery (parsing of syntactically incorrect code)


Constant expression evaluation

Evaluating constant/property/etc initializers
Handling errors and unsupported expressions


JSON representation

JSON encoding and decoding of ASTs


Performance

Disabling XDebug
Reusing objects
Garbage collection impact


Frequently asked questions

Parent and sibling references



",GitHub - nikic/PHP-Parser: A PHP parser written in PHP
59,PHP," Grav



  
Grav is a Fast, Simple, and Flexible, file-based Web-platform.  There is Zero installation required.  Just extract the ZIP archive, and you are already up and running.  It follows similar principles to other flat-file CMS platforms, but has a different design philosophy than most. Grav comes with a powerful Package Management System to allow for simple installation and upgrading of plugins and themes, as well as simple updating of Grav itself.
The underlying architecture of Grav is designed to use well-established and best-in-class technologies to ensure that Grav is simple to use and easy to extend. Some of these key technologies include:

Twig Templating: for powerful control of the user interface
Markdown: for easy content creation
YAML: for simple configuration
Parsedown: for fast Markdown and Markdown Extra support
Doctrine Cache: layer for performance
Pimple Dependency Injection Container: for extensibility and maintainability
Symfony Event Dispatcher: for plugin event handling
Symfony Console: for CLI interface
Gregwar Image Library: for dynamic image manipulation

Requirements

PHP 7.1.3 or higher. Check the required modules list
Check the Apache or IIS requirements

QuickStart
These are the options to get Grav:
Downloading a Grav Package
You can download a ready-built package from the Downloads page on https://getgrav.org
With Composer
You can create a new project with the latest stable Grav release with the following command:
$ composer create-project getgrav/grav ~/webroot/grav

From GitHub


Clone the Grav repository from https://github.com/getgrav/grav to a folder in the webroot of your server, e.g. ~/webroot/grav. Launch a terminal or console and navigate to the webroot folder:
$ cd ~/webroot
$ git clone https://github.com/getgrav/grav.git



Install the plugin and theme dependencies by using the Grav CLI application bin/grav:
$ cd ~/webroot/grav
$ bin/grav install



Check out the install procedures for more information.
Adding Functionality
You can download plugins or themes manually from the appropriate tab on the Downloads page on https://getgrav.org, but the preferred solution is to use the Grav Package Manager or GPM:
$ bin/gpm index

This will display all the available plugins and then you can install one or more with:
$ bin/gpm install <plugin/theme>

Updating
To update Grav you should use the Grav Package Manager or GPM:
$ bin/gpm selfupgrade

To update plugins and themes:
$ bin/gpm update

Contributing
We appreciate any contribution to Grav, whether it is related to bugs, grammar, or simply a suggestion or improvement! Please refer to the Contributing guide for more guidance on this topic.
Security issues
If you discover a possible security issue related to Grav or one of its plugins, please email the core team at contact@getgrav.org and we'll address it as soon as possible.
Getting Started

What is Grav?
Install Grav in few seconds
Understand the Configuration
Take a peek at our available free Skeletons
If you have questions, jump on our Discord Chat Server!
Have fun!

Exploring More

Have a look at our Basic Tutorial
Dive into more advanced functions
Learn about the Grav CLI
Review examples in the Grav Cookbook
More Awesome Grav Stuff

Backers
Support Grav with a monthly donation to help us continue development. [Become a backer]

Sponsors
Become a sponsor and get your logo on our README on Github with a link to your site. [Become a sponsor]

License
See LICENSE
Running Tests
First install the dev dependencies by running composer update from the Grav root.
Then composer test will run the Unit Tests, which should be always executed successfully on any site.
Windows users should use the composer test-windows command.
You can also run a single unit test file, e.g. composer test tests/unit/Grav/Common/AssetsTest.php
","GitHub - getgrav/grav: Modern, Crazy Fast, Ridiculously Easy and Amazingly Powerful Flat-File CMS"
60,Shell,"




Oh My Zsh is an open source, community-driven framework for managing your zsh configuration.
Sounds boring. Let's try again.
Oh My Zsh will not make you a 10x developer...but you may feel like one.
Once installed, your terminal shell will become the talk of the town or your money back! With each keystroke in your command prompt, you'll take advantage of the hundreds of powerful plugins and beautiful themes. Strangers will come up to you in caf√©s and ask you, ""that is amazing! are you some sort of genius?""
Finally, you'll begin to get the sort of attention that you have always felt you deserved. ...or maybe you'll use the time that you're saving to start flossing more often. üò¨
To learn more, visit ohmyz.sh and follow @ohmyzsh on Twitter.
Getting Started
Prerequisites

A Unix-like operating system: macOS, Linux, BSD. On Windows: WSL is preferred, but cygwin or msys also mostly work.
Zsh should be installed (v4.3.9 or more recent). If not pre-installed (run zsh --version to confirm), check the following instructions here: Installing ZSH
curl or wget should be installed
git should be installed

Basic Installation
Oh My Zsh is installed by running one of the following commands in your terminal. You can install this via the command-line with either curl or wget.
via curl
sh -c ""$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)""
via wget
sh -c ""$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)""
Manual inspection
It's a good idea to inspect the install script from projects you don't yet know. You can do
that by downloading the install script first, looking through it so everything looks normal,
then running it:
curl -Lo install.sh https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh
sh install.sh
Using Oh My Zsh
Plugins
Oh My Zsh comes with a shitload of plugins to take advantage of. You can take a look in the plugins directory and/or the wiki to see what's currently available.
Enabling Plugins
Once you spot a plugin (or several) that you'd like to use with Oh My Zsh, you'll need to enable them in the .zshrc file. You'll find the zshrc file in your $HOME directory. Open it with your favorite text editor and you'll see a spot to list all the plugins you want to load.
vi ~/.zshrc
For example, this might begin to look like this:
plugins=(
  git
  bundler
  dotenv
  osx
  rake
  rbenv
  ruby
)
Note that the plugins are separated by whitespace. Do not use commas between them.
Using Plugins
Most plugins (should! we're working on this) include a README, which documents how to use them.
Themes
We'll admit it. Early in the Oh My Zsh world, we may have gotten a bit too theme happy. We have over one hundred themes now bundled. Most of them have screenshots on the wiki. Check them out!
Selecting a Theme
Robby's theme is the default one. It's not the fanciest one. It's not the simplest one. It's just the right one (for him).
Once you find a theme that you'd like to use, you will need to edit the ~/.zshrc file. You'll see an environment variable (all caps) in there that looks like:
ZSH_THEME=""robbyrussell""
To use a different theme, simply change the value to match the name of your desired theme. For example:
ZSH_THEME=""agnoster"" # (this is one of the fancy ones)
# see https://github.com/ohmyzsh/ohmyzsh/wiki/Themes#agnoster
Note: many themes require installing the Powerline Fonts in order to render properly.
Open up a new terminal window and your prompt should look something like this:

In case you did not find a suitable theme for your needs, please have a look at the wiki for more of them.
If you're feeling feisty, you can let the computer select one randomly for you each time you open a new terminal window.
ZSH_THEME=""random"" # (...please let it be pie... please be some pie..)
And if you want to pick random theme from a list of your favorite themes:
ZSH_THEME_RANDOM_CANDIDATES=(
  ""robbyrussell""
  ""agnoster""
)
FAQ
If you have some more questions or issues, you might find a solution in our FAQ.
Advanced Topics
If you're the type that likes to get their hands dirty, these sections might resonate.
Advanced Installation
Some users may want to manually install Oh My Zsh, or change the default path or other settings that
the installer accepts (these settings are also documented at the top of the install script).
Custom Directory
The default location is ~/.oh-my-zsh (hidden in your home directory)
If you'd like to change the install directory with the ZSH environment variable, either by running
export ZSH=/your/path before installing, or by setting it before the end of the install pipeline
like this:
ZSH=""$HOME/.dotfiles/oh-my-zsh"" sh install.sh
Unattended install
If you're running the Oh My Zsh install script as part of an automated install, you can pass the
flag --unattended to the install.sh script. This will have the effect of not trying to change
the default shell, and also won't run zsh when the installation has finished.
sh -c ""$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"" """" --unattended
Installing from a forked repository
The install script also accepts these variables to allow installation of a different repository:


REPO (default: ohmyzsh/ohmyzsh): this takes the form of owner/repository. If you set
this variable, the installer will look for a repository at https://github.com/{owner}/{repository}.


REMOTE (default: https://github.com/${REPO}.git): this is the full URL of the git repository
clone. You can use this setting if you want to install from a fork that is not on GitHub (GitLab,
Bitbucket...) or if you want to clone with SSH instead of HTTPS (git@github.com:user/project.git).
NOTE: it's incompatible with setting the REPO variable. This setting will take precedence.


BRANCH (default: master): you can use this setting if you want to change the default branch to be
checked out when cloning the repository. This might be useful for testing a Pull Request, or if you
want to use a branch other than master.


For example:
REPO=apjanke/oh-my-zsh BRANCH=edge sh install.sh
Manual Installation
1. Clone the repository:
git clone https://github.com/ohmyzsh/ohmyzsh.git ~/.oh-my-zsh
2. Optionally, backup your existing ~/.zshrc file:
cp ~/.zshrc ~/.zshrc.orig
3. Create a new zsh configuration file
You can create a new zsh config file by copying the template that we have included for you.
cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc
4. Change your default shell
chsh -s $(which zsh)
You must log out from your user session and log back in to see this change.
5. Initialize your new zsh configuration
Once you open up a new terminal window, it should load zsh with Oh My Zsh's configuration.
Installation Problems
If you have any hiccups installing, here are a few common fixes.

You might need to modify your PATH in ~/.zshrc if you're not able to find some commands after
switching to oh-my-zsh.
If you installed manually or changed the install location, check the ZSH environment variable in
~/.zshrc.

Custom Plugins and Themes
If you want to override any of the default behaviors, just add a new file (ending in .zsh) in the custom/ directory.
If you have many functions that go well together, you can put them as a XYZ.plugin.zsh file in the custom/plugins/ directory and then enable this plugin.
If you would like to override the functionality of a plugin distributed with Oh My Zsh, create a plugin of the same name in the custom/plugins/ directory and it will be loaded instead of the one in plugins/.
Getting Updates
By default, you will be prompted to check for upgrades every few weeks. If you would like oh-my-zsh to automatically upgrade itself without prompting you, set the following in your ~/.zshrc:
DISABLE_UPDATE_PROMPT=true
To disable automatic upgrades, set the following in your ~/.zshrc:
DISABLE_AUTO_UPDATE=true
Manual Updates
If you'd like to upgrade at any point in time (maybe someone just released a new plugin and you don't want to wait a week?) you just need to run:
upgrade_oh_my_zsh
Magic! üéâ
Uninstalling Oh My Zsh
Oh My Zsh isn't for everyone. We'll miss you, but we want to make this an easy breakup.
If you want to uninstall oh-my-zsh, just run uninstall_oh_my_zsh from the command-line. It will remove itself and revert your previous bash or zsh configuration.
How do I contribute to Oh My Zsh?
Before you participate in our delightful community, please read the code of conduct.
I'm far from being a Zsh expert and suspect there are many ways to improve ‚Äì if you have ideas on how to make the configuration easier to maintain (and faster), don't hesitate to fork and send pull requests!
We also need people to test out pull-requests. So take a look through the open issues and help where you can.
See Contributing for more details.
Do NOT send us themes
We have (more than) enough themes for the time being. Please add your theme to the external themes wiki page.
Contributors
Oh My Zsh has a vibrant community of happy users and delightful contributors. Without all the time and help from our contributors, it wouldn't be so awesome.
Thank you so much!
Follow Us
We're on the social media.

@ohmyzsh on Twitter. You should follow it.
Oh My Zsh on Facebook.

Merchandise
We have stickers, shirts, and coffee mugs available for you to show off your love of Oh My Zsh. Again, you will become the talk of the town!
License
Oh My Zsh is released under the MIT license.
About Planet Argon

Oh My Zsh was started by the team at Planet Argon, a Ruby on Rails development agency. Check out our other open source projects.
","GitHub - ohmyzsh/ohmyzsh: üôÉ A delightful community-driven (with nearly 1,500 contributors) framework for managing your zsh configuration. Includes 200+ optional plugins (rails, git, OSX, hub, capistrano, brew, ant, php, python, etc), over 140 themes to spice up your morning, and an auto-update tool so that makes it easy to keep up with the latest updates from the community."
61,Shell,"Node Version Manager   
Table of Contents

Installation and Update

Install & Update script

Ansible


Verify installation
Important Notes
Git install
Manual Install
Manual upgrade


Usage

Long-term support
Migrating global packages while installing
Default global packages from file while installing
io.js
System version of node
Listing versions

Suppressing colorized output


.nvmrc
Deeper Shell Integration

bash

Automatically call nvm use


zsh

Calling nvm use automatically in a directory with a .nvmrc file






License
Running tests
Bash completion

Usage


Compatibility Issues
Installing nvm on Alpine Linux
Removal

Manual Uninstall


Docker for development environment
Problems
Mac OS ""troubleshooting""

Installation and Update
Install & Update script
To install or update nvm, you should run the install script. To do that, you may either download and run the script manually, or use the following cURL or Wget command:
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.1/install.sh | bash
wget -qO- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.1/install.sh | bash
Running either of the above commands downloads a script and runs it. The script clones the nvm repository to ~/.nvm, and adds the source lines from the snippet below to your profile (~/.bash_profile, ~/.zshrc, ~/.profile, or ~/.bashrc).

export NVM_DIR=""$([ -z ""${XDG_CONFIG_HOME-}"" ] && printf %s ""${HOME}/.nvm"" || printf %s ""${XDG_CONFIG_HOME}/nvm"")""
[ -s ""$NVM_DIR/nvm.sh"" ] && \. ""$NVM_DIR/nvm.sh"" # This loads nvm
Note: If the environment variable $XDG_CONFIG_HOME is present, it will place the nvm files there.
Note: You can add --no-use to the end of the above script (...nvm.sh --no-use) to postpone using nvm until you manually use it.
You can customize the install source, directory, profile, and version using the NVM_SOURCE, NVM_DIR, PROFILE, and NODE_VERSION variables.
Eg: curl ... | NVM_DIR=""path/to/nvm"". Ensure that the NVM_DIR does not contain a trailing slash.
NB. The installer can use git, curl, or wget to download nvm, whatever is available.
Note: On Linux, after running the install script, if you get nvm: command not found or see no feedback from your terminal after you type command -v nvm, simply close your current terminal, open a new terminal, and try verifying again.
Note: Since OS X 10.9, /usr/bin/git has been preset by Xcode command line tools, which means we can't properly detect if Git is installed or not. You need to manually install the Xcode command line tools before running the install script, otherwise, it'll fail. (see #1782)
Note: On OS X, if you get nvm: command not found after running the install script, one of the following might be the reason:

Your system may not have a .bash_profile file where the command is set up. Create one with touch ~/.bash_profile and run the install script again
You might need to restart your terminal instance. Try opening a new tab/window in your terminal and retry.

If the above doesn't fix the problem, you may try the following:


Open your .bash_profile (or ~/.zshrc, ~/.profile, or ~/.bashrc) and add the following line of code: source ~/<your_profile_file>. E.g. source ~/.bashrc or source ~/.zshrc.


If the above don't work, try adding the snippet from the install section that finds the correct nvm directory and loads nvm, to your profile (~/.bash_profile, ~/.zshrc, ~/.profile, or ~/.bashrc).


For more information about this issue and possible workarounds, please refer here


Ansible
You can use a task:
- name: nvm
  shell: >
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.1/install.sh | bash
  args:
    creates: ""{{ ansible_env.HOME }}/.nvm/nvm.sh""

Verify installation
To verify that nvm has been installed, do:
command -v nvm
which should output nvm if the installation was successful. Please note that which nvm will not work, since nvm is a sourced shell function, not an executable binary.
Important Notes
If you're running a system without prepackaged binary available, which means you're going to install nodejs or io.js from its source code, you need to make sure your system has a C++ compiler. For OS X, Xcode will work, for Debian/Ubuntu based GNU/Linux, the build-essential and libssl-dev packages work.
Note: nvm does not support Windows (see #284), but may work in WSL (Windows Subsystem for Linux) depending on the version of WSL. For Windows, two alternatives exist, which are neither supported nor developed by us:

nvm-windows
nodist

Note: nvm does not support Fish either (see #303). Alternatives exist, which are neither supported nor developed by us:

bass allows you to use utilities written for Bash in fish shell
fast-nvm-fish only works with version numbers (not aliases) but doesn't significantly slow your shell startup
plugin-nvm plugin for Oh My Fish, which makes nvm and its completions available in fish shell
fnm - fisherman-based version manager for fish
fish-nvm - Wrapper around nvm for fish, delays sourcing nvm until it's actually used.

Note: We still have some problems with FreeBSD, because there is no official pre-built binary for FreeBSD, and building from source may need patches; see the issue ticket:

[#900] [Bug] nodejs on FreeBSD may need to be patched
nodejs/node#3716

Note: On OS X, if you do not have Xcode installed and you do not wish to download the ~4.3GB file, you can install the Command Line Tools. You can check out this blog post on how to just that:

How to Install Command Line Tools in OS X Mavericks & Yosemite (Without Xcode)

Note: On OS X, if you have/had a ""system"" node installed and want to install modules globally, keep in mind that:

When using nvm you do not need sudo to globally install a module with npm -g, so instead of doing sudo npm install -g grunt, do instead npm install -g grunt
If you have an ~/.npmrc file, make sure it does not contain any prefix settings (which is not compatible with nvm)
You can (but should not?) keep your previous ""system"" node install, but nvm will only be available to your user account (the one used to install nvm). This might cause version mismatches, as other users will be using /usr/local/lib/node_modules/* VS your user account using ~/.nvm/versions/node/vX.X.X/lib/node_modules/*

Homebrew installation is not supported. If you have issues with homebrew-installed nvm, please brew uninstall it, and install it using the instructions below, before filing an issue.
Note: If you're using zsh you can easily install nvm as a zsh plugin. Install zsh-nvm and run nvm upgrade to upgrade.
Note: Git versions before v1.7 may face a problem of cloning nvm source from GitHub via https protocol, and there is also different behavior of git before v1.6, and git prior to v1.17.10 can not clone tags, so the minimum required git version is v1.7.10. If you are interested in the problem we mentioned here, please refer to GitHub's HTTPS cloning errors article.
Git install
If you have git installed (requires git v1.7.10+):

clone this repo in the root of your user profile


cd ~/ from anywhere then git clone https://github.com/nvm-sh/nvm.git .nvm


cd ~/.nvm and check out the latest version with git checkout v0.35.1
activate nvm by sourcing it from your shell: . nvm.sh

Now add these lines to your ~/.bashrc, ~/.profile, or ~/.zshrc file to have it automatically sourced upon login:
(you may have to add to more than one of the above files)
export NVM_DIR=""$HOME/.nvm""
[ -s ""$NVM_DIR/nvm.sh"" ] && \. ""$NVM_DIR/nvm.sh""  # This loads nvm
[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""  # This loads nvm bash_completion
Manual Install
For a fully manual install, execute the following lines to first clone the nvm repository into $HOME/.nvm, and then load nvm:
export NVM_DIR=""$HOME/.nvm"" && (
  git clone https://github.com/nvm-sh/nvm.git ""$NVM_DIR""
  cd ""$NVM_DIR""
  git checkout `git describe --abbrev=0 --tags --match ""v[0-9]*"" $(git rev-list --tags --max-count=1)`
) && \. ""$NVM_DIR/nvm.sh""
Now add these lines to your ~/.bashrc, ~/.profile, or ~/.zshrc file to have it automatically sourced upon login:
(you may have to add to more than one of the above files)
export NVM_DIR=""$HOME/.nvm""
[ -s ""$NVM_DIR/nvm.sh"" ] && \. ""$NVM_DIR/nvm.sh"" # This loads nvm
Manual upgrade
For manual upgrade with git (requires git v1.7.10+):

change to the $NVM_DIR
pull down the latest changes
check out the latest version
activate the new version

(
  cd ""$NVM_DIR""
  git fetch --tags origin
  git checkout `git describe --abbrev=0 --tags --match ""v[0-9]*"" $(git rev-list --tags --max-count=1)`
) && \. ""$NVM_DIR/nvm.sh""
Usage
To download, compile, and install the latest release of node, do this:
nvm install node # ""node"" is an alias for the latest version
To install a specific version of node:
nvm install 6.14.4 # or 10.10.0, 8.9.1, etc
The first version installed becomes the default. New shells will start with the default version of node (e.g., nvm alias default).
You can list available versions using ls-remote:
nvm ls-remote
And then in any new shell just use the installed version:
nvm use node
Or you can just run it:
nvm run node --version
Or, you can run any arbitrary command in a subshell with the desired version of node:
nvm exec 4.2 node --version
You can also get the path to the executable to where it was installed:
nvm which 5.0
In place of a version pointer like ""0.10"" or ""5.0"" or ""4.2.1"", you can use the following special default aliases with nvm install, nvm use, nvm run, nvm exec, nvm which, etc:

node: this installs the latest version of node
iojs: this installs the latest version of io.js
stable: this alias is deprecated, and only truly applies to node v0.12 and earlier. Currently, this is an alias for node.
unstable: this alias points to node v0.11 - the last ""unstable"" node release, since post-1.0, all node versions are stable. (in SemVer, versions communicate breakage, not stability).

Long-term support
Node has a schedule for long-term support (LTS) You can reference LTS versions in aliases and .nvmrc files with the notation lts/* for the latest LTS, and lts/argon for LTS releases from the ""argon"" line, for example. In addition, the following commands support LTS arguments:

nvm install --lts / nvm install --lts=argon / nvm install 'lts/*' / nvm install lts/argon
nvm uninstall --lts / nvm uninstall --lts=argon / nvm uninstall 'lts/*' / nvm uninstall lts/argon
nvm use --lts / nvm use --lts=argon / nvm use 'lts/*' / nvm use lts/argon
nvm exec --lts / nvm exec --lts=argon / nvm exec 'lts/*' / nvm exec lts/argon
nvm run --lts / nvm run --lts=argon / nvm run 'lts/*' / nvm run lts/argon
nvm ls-remote --lts / nvm ls-remote --lts=argon nvm ls-remote 'lts/*' / nvm ls-remote lts/argon
nvm version-remote --lts / nvm version-remote --lts=argon / nvm version-remote 'lts/*' / nvm version-remote lts/argon

Any time your local copy of nvm connects to https://nodejs.org, it will re-create the appropriate local aliases for all available LTS lines. These aliases (stored under $NVM_DIR/alias/lts), are managed by nvm, and you should not modify, remove, or create these files - expect your changes to be undone, and expect meddling with these files to cause bugs that will likely not be supported.
Migrating global packages while installing
If you want to install a new version of Node.js and migrate npm packages from a previous version:
nvm install node --reinstall-packages-from=node
This will first use ""nvm version node"" to identify the current version you're migrating packages from. Then it resolves the new version to install from the remote server and installs it. Lastly, it runs ""nvm reinstall-packages"" to reinstall the npm packages from your prior version of Node to the new one.
You can also install and migrate npm packages from specific versions of Node like this:
nvm install 6 --reinstall-packages-from=5
nvm install v4.2 --reinstall-packages-from=iojs
Note that reinstalling packages explicitly does not update the npm version ‚Äî this is to ensure that npm isn't accidentally upgraded to a broken version for the new node version.
To update npm at the same time add the --latest-npm flag, like this:
nvm install lts/* --reinstall-packages-from=default --latest-npm
or, you can at any time run the following command to get the latest supported npm version on the current node version:
nvm install-latest-npm
If you've already gotten an error to the effect of ""npm does not support Node.js"", you'll need to (1) revert to a previous node version (nvm ls & nvm use <your latest _working_ version from the ls>, (2) delete the newly created node version (nvm uninstall <your _broken_ version of node from the ls>), then (3) rerun your nvm install with the --latest-npm flag.
Default global packages from file while installing
If you have a list of default packages you want installed every time you install a new version, we support that too -- just add the package names, one per line, to the file $NVM_DIR/default-packages. You can add anything npm would accept as a package argument on the command line.
# $NVM_DIR/default-packages

rimraf
object-inspect@1.0.2
stevemao/left-pad
io.js
If you want to install io.js:
nvm install iojs
If you want to install a new version of io.js and migrate npm packages from a previous version:
nvm install iojs --reinstall-packages-from=iojs
The same guidelines mentioned for migrating npm packages in node are applicable to io.js.
System version of node
If you want to use the system-installed version of node, you can use the special default alias ""system"":
nvm use system
nvm run system --version
Listing versions
If you want to see what versions are installed:
nvm ls
If you want to see what versions are available to install:
nvm ls-remote
Suppressing colorized output
nvm ls, nvm ls-remote and nvm alias usually produce colorized output. You can disable colors with the --no-colors option (or by setting the environment variable TERM=dumb):
nvm ls --no-colors
TERM=dumb nvm ls
To restore your PATH, you can deactivate it:
nvm deactivate
To set a default Node version to be used in any new shell, use the alias 'default':
nvm alias default node
To use a mirror of the node binaries, set $NVM_NODEJS_ORG_MIRROR:
export NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist
nvm install node

NVM_NODEJS_ORG_MIRROR=https://nodejs.org/dist nvm install 4.2
To use a mirror of the io.js binaries, set $NVM_IOJS_ORG_MIRROR:
export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist
nvm install iojs-v1.0.3

NVM_IOJS_ORG_MIRROR=https://iojs.org/dist nvm install iojs-v1.0.3
nvm use will not, by default, create a ""current"" symlink. Set $NVM_SYMLINK_CURRENT to ""true"" to enable this behavior, which is sometimes useful for IDEs. Note that using nvm in multiple shell tabs with this environment variable enabled can cause race conditions.
.nvmrc
You can create a .nvmrc file containing a node version number (or any other string that nvm understands; see nvm --help for details) in the project root directory (or any parent directory).
Afterwards, nvm use, nvm install, nvm exec, nvm run, and nvm which will use the version specified in the .nvmrc file if no version is supplied on the command line.
For example, to make nvm default to the latest 5.9 release, the latest LTS version, or the latest node version for the current directory:
$ echo ""5.9"" > .nvmrc

$ echo ""lts/*"" > .nvmrc # to default to the latest LTS version

$ echo ""node"" > .nvmrc # to default to the latest version
Then when you run nvm:
$ nvm use
Found '/path/to/project/.nvmrc' with version <5.9>
Now using node v5.9.1 (npm v3.7.3)
nvm use et. al. will traverse directory structure upwards from the current directory looking for the .nvmrc file. In other words, running nvm use et. al. in any subdirectory of a directory with an .nvmrc will result in that .nvmrc being utilized.
The contents of a .nvmrc file must be the <version> (as described by nvm --help) followed by a newline. No trailing spaces are allowed, and the trailing newline is required.
Deeper Shell Integration
You can use avn to deeply integrate into your shell and automatically invoke nvm when changing directories. avn is not supported by the nvm development team. Please report issues to the avn team.
If you prefer a lighter-weight solution, the recipes below have been contributed by nvm users. They are not supported by the nvm development team. We are, however, accepting pull requests for more examples.
bash
Automatically call nvm use
Put the following at the end of your $HOME/.bashrc:
find-up () {
    path=$(pwd)
    while [[ ""$path"" != """" && ! -e ""$path/$1"" ]]; do
        path=${path%/*}
    done
    echo ""$path""
}

cdnvm(){
    cd ""$@"";
    nvm_path=$(find-up .nvmrc | tr -d '[:space:]')

    # If there are no .nvmrc file, use the default nvm version
    if [[ ! $nvm_path = *[^[:space:]]* ]]; then

        declare default_version;
        default_version=$(nvm version default);

        # If there is no default version, set it to `node`
        # This will use the latest version on your machine
        if [[ $default_version == ""N/A"" ]]; then
            nvm alias default node;
            default_version=$(nvm version default);
        fi

        # If the current version is not the default version, set it to use the default version
        if [[ $(nvm current) != ""$default_version"" ]]; then
            nvm use default;
        fi

        elif [[ -s $nvm_path/.nvmrc && -r $nvm_path/.nvmrc ]]; then
        declare nvm_version
        nvm_version=$(<""$nvm_path""/.nvmrc)

        declare locally_resolved_nvm_version
        # `nvm ls` will check all locally-available versions
        # If there are multiple matching versions, take the latest one
        # Remove the `->` and `*` characters and spaces
        # `locally_resolved_nvm_version` will be `N/A` if no local versions are found
        locally_resolved_nvm_version=$(nvm ls --no-colors ""$nvm_version"" | tail -1 | tr -d '\->*' | tr -d '[:space:]')

        # If it is not already installed, install it
        # `nvm install` will implicitly use the newly-installed version
        if [[ ""$locally_resolved_nvm_version"" == ""N/A"" ]]; then
            nvm install ""$nvm_version"";
        elif [[ $(nvm current) != ""$locally_resolved_nvm_version"" ]]; then
            nvm use ""$nvm_version"";
        fi
    fi
}
alias cd='cdnvm'
This alias would search 'up' from your current directory in order to detect a .nvmrc file. If it finds it, it will switch to that version; if not, it will use the default version.
zsh
Calling nvm use automatically in a directory with a .nvmrc file
Put this into your $HOME/.zshrc to call nvm use automatically whenever you enter a directory that contains an
.nvmrc file with a string telling nvm which node to use:
# place this after nvm initialization!
autoload -U add-zsh-hook
load-nvmrc() {
  local node_version=""$(nvm version)""
  local nvmrc_path=""$(nvm_find_nvmrc)""

  if [ -n ""$nvmrc_path"" ]; then
    local nvmrc_node_version=$(nvm version ""$(cat ""${nvmrc_path}"")"")

    if [ ""$nvmrc_node_version"" = ""N/A"" ]; then
      nvm install
    elif [ ""$nvmrc_node_version"" != ""$node_version"" ]; then
      nvm use
    fi
  elif [ ""$node_version"" != ""$(nvm version default)"" ]; then
    echo ""Reverting to nvm default version""
    nvm use default
  fi
}
add-zsh-hook chpwd load-nvmrc
load-nvmrc
License
nvm is released under the MIT license.
Copyright (C) 2010 Tim Caswell and Jordan Harband
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Running tests
Tests are written in Urchin. Install Urchin (and other dependencies) like so:
npm install

There are slow tests and fast tests. The slow tests do things like install node
and check that the right versions are used. The fast tests fake this to test
things like aliases and uninstalling. From the root of the nvm git repository,
run the fast tests like this:
npm run test/fast

Run the slow tests like this:
npm run test/slow

Run all of the tests like this:
npm test

Nota bene: Avoid running nvm while the tests are running.
Bash completion
To activate, you need to source bash_completion:
[[ -r $NVM_DIR/bash_completion ]] && \. $NVM_DIR/bash_completion
Put the above sourcing line just below the sourcing line for nvm in your profile (.bashrc, .bash_profile).
Usage
nvm:

$ nvm Tab

alias               deactivate          install             ls                  run                 unload
clear-cache         exec                list                ls-remote           unalias             use
current             help                list-remote         reinstall-packages  uninstall           version

nvm alias:

$ nvm alias Tab

default


$ nvm alias my_alias Tab

v0.6.21        v0.8.26       v0.10.28

nvm use:

$ nvm use Tab

my_alias        default        v0.6.21        v0.8.26       v0.10.28

nvm uninstall:

$ nvm uninstall Tab

my_alias        default        v0.6.21        v0.8.26       v0.10.28

Compatibility Issues
nvm will encounter some issues if you have some non-default settings set. (see #606)
The following are known to cause issues:
Inside ~/.npmrc:
prefix='some/path'
Environment Variables:
$NPM_CONFIG_PREFIX
$PREFIX
Shell settings:
set -e
Installing nvm on Alpine Linux
In order to provide the best performance (and other optimisations), nvm will download and install pre-compiled binaries for Node (and npm) when you run nvm install X. The Node project compiles, tests and hosts/provides these pre-compiled binaries which are built for mainstream/traditional Linux distributions (such as Debian, Ubuntu, CentOS, RedHat et al).
Alpine Linux, unlike mainstream/traditional Linux distributions, is based on BusyBox, a very compact (~5MB) Linux distribution. BusyBox (and thus Alpine Linux) uses a different C/C++ stack to most mainstream/traditional Linux distributions - musl. This makes binary programs built for such mainstream/traditional incompatible with Alpine Linux, thus we cannot simply nvm install X on Alpine Linux and expect the downloaded binary to run correctly - you'll likely see ""...does not exist"" errors if you try that.
There is a -s flag for nvm install which requests nvm download Node source and compile it locally.
If installing nvm on Alpine Linux is still what you want or need to do, you should be able to achieve this by running the following from you Alpine Linux shell:
apk add -U curl bash ca-certificates openssl ncurses coreutils python2 make gcc g++ libgcc linux-headers grep util-linux binutils findutils
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.1/install.sh | bash
The Node project has some desire but no concrete plans (due to the overheads of building, testing and support) to offer Alpine-compatible binaries.
As a potential alternative, @mhart (a Node contributor) has some Docker images for Alpine Linux with Node and optionally, npm, pre-installed.
Removal
Manual Uninstall
To remove nvm manually, execute the following:
$ rm -rf ""$NVM_DIR""
Edit ~/.bashrc (or other shell resource config) and remove the lines below:
export NVM_DIR=""$HOME/.nvm""
[ -s ""$NVM_DIR/nvm.sh"" ] && \. ""$NVM_DIR/nvm.sh"" # This loads nvm
[[ -r $NVM_DIR/bash_completion ]] && \. $NVM_DIR/bash_completion
Docker for development environment
To make the development and testing work easier, we have a Dockerfile for development usage, which is based on Ubuntu 14.04 base image, prepared with essential and useful tools for nvm development, to build the docker image of the environment, run the docker command at the root of nvm repository:
$ docker build -t nvm-dev .
This will package your current nvm repository with our pre-defined development environment into a docker image named nvm-dev, once it's built with success, validate your image via docker images:
$ docker images

REPOSITORY         TAG                 IMAGE ID            CREATED             SIZE
nvm-dev            latest              9ca4c57a97d8        7 days ago          650 MB
If you got no error message, now you can easily involve in:
$ docker run -h nvm-dev -it nvm-dev

nvm@nvm-dev:~/.nvm$
Please note that it'll take about 8 minutes to build the image and the image size would be about 650MB, so it's not suitable for production usage.
For more information and documentation about docker, please refer to its official website:

https://www.docker.com/
https://docs.docker.com/

Problems


If you try to install a node version and the installation fails, be sure to run nvm cache clear to delete cached node downloads, or you might get an error like the following:
curl: (33) HTTP server doesn't seem to support byte ranges. Cannot resume.


Where's my sudo node? Check out #43


After the v0.8.6 release of node, nvm tries to install from binary packages. But in some systems, the official binary packages don't work due to incompatibility of shared libs. In such cases, use -s option to force install from source:


nvm install -s 0.8.6

If setting the default alias does not establish the node version in new shells (i.e. nvm current yields system), ensure that the system's node PATH is set before the nvm.sh source line in your shell profile (see #658)

Mac OS ""troubleshooting""
nvm node version not found in vim shell
If you set node version to a version other than your system node version nvm use 6.2.1 and open vim and run :!node -v you should see v6.2.1 if you see your system version v0.12.7. You need to run:
sudo chmod ugo-x /usr/libexec/path_helper
More on this issue in dotphiles/dotzsh.
",GitHub - nvm-sh/nvm: Node Version Manager - POSIX-compliant bash script to manage multiple active node.js versions
62,Shell,"
The Open Guide to Amazon Web Services
 ‚á¶ Join us!
Credits ‚àô Contributing guidelines
Table of Contents
Purpose

Why an Open Guide?
Scope
Legend

AWS in General

General Information
Learning and Career Development
Managing AWS
Managing Servers and Applications




Specific AWS Services
Basics
Tips
Gotchas




ALB
üìó
üìò
üìô


AMIs
üìó
üìò
üìô


API Gateway
üìó
üìò
üìô


Auto Scaling
üìó
üìò
üìô


Batch
üìó
üìò



Certificate Manager
üìó
üìò
üìô


CLB (ELB)
üìó
üìò
üìô


CloudFront
üìó
üìò
üìô


CloudFormation
üìó
üìò
üìô


CloudWatch
üìó
üìò
üìô


Device Farm
üìó
üìò
üìô


DirectConnect
üìó
üìò



DynamoDB
üìó
üìò
üìô


EBS
üìó
üìò
üìô


EC2
üìó
üìò
üìô


ECS
üìó
üìò



EKS
üìó
üìò
üìô


EFS
üìó
üìò
üìô


Elastic Beanstalk
üìó
üìò
üìô


Elastic IPs
üìó
üìò
üìô


ElastiCache
üìó
üìò
üìô


EMR
üìó
üìò
üìô


Fargate
üìó
üìò
üìô


Glacier
üìó
üìò
üìô


IoT
üìó
üìò
üìô


Kinesis Firehose


üìô


Kinesis Streams
üìó
üìò
üìô


KMS
üìó
üìò
üìô


Lambda
üìó
üìò
üìô


Load Balancers
üìó
üìò
üìô


Mobile Hub
üìó
üìò
üìô


OpsWorks
üìó
üìò
üìô


RDS
üìó
üìò
üìô


RDS Aurora
üìó
üìò
üìô


RDS Aurora MySQL
üìó
üìò
üìô


RDS Aurora PostgreSQL
üìó
üìò
üìô


RDS MySQL and MariaDB
üìó
üìò
üìô


RDS PostgreSQL
üìó
üìò
üìô


RDS SQL Server
üìó
üìò
üìô


Redshift
üìó
üìò
üìô


Route 53
üìó
üìò
üìô


S3
üìó
üìò
üìô


Security and IAM
üìó
üìò
üìô


SES
üìó
üìò
üìô


SNS
üìó
üìò
üìô


SQS
üìó
üìò
üìô


Step Functions
üìó
üìò
üìô


WAF
üìó
üìò
üìô


VPCs, Network Security, and Security Groups
üìó
üìò
üìô



Special Topics

High Availability
Billing and Cost Management
Further Reading

Legal

Disclaimer
License

Figures and Tables
 

Figure: Tools and Services Market Landscape: A selection of third-party companies/products
Figure: AWS Data Transfer Costs: Visual overview of data transfer costs
Table: Service Matrix: How AWS services compare to alternatives
Table: AWS Product Maturity and Releases: AWS product releases
Table: Storage Durability, Availability, and Price: A quantitative comparison

Why an Open Guide?
A lot of information on AWS is already written. Most people learn AWS by reading a blog or a ‚Äúgetting started guide‚Äù and referring to the standard AWS references. Nonetheless, trustworthy and practical information and recommendations aren‚Äôt easy to come by. AWS‚Äôs own documentation is a great but sprawling resource few have time to read fully, and it doesn‚Äôt include anything but official facts, so omits experiences of engineers. The information in blogs or Stack Overflow is also not consistently up to date.
This guide is by and for engineers who use AWS. It aims to be a useful, living reference that consolidates links, tips, gotchas, and best practices. It arose from discussion and editing over beers by several engineers who have used AWS extensively.
Before using the guide, please read the license and disclaimer.
Please help!
This is an early in-progress draft! It‚Äôs our first attempt at assembling this information, so is far from comprehensive still, and likely to have omissions or errors.

Please help by joining the Slack channel (we like to talk about AWS in general, even if you only have questions ‚Äî discussion helps the community and guides improvements) and contributing to the guide. This guide is open to contributions, so unlike a blog, it can keep improving. Like any open source effort, we combine efforts but also review to ensure high quality.
Scope

Currently, this guide covers selected ‚Äúcore‚Äù services, such as EC2, S3, Load Balancers, EBS, and IAM, and partial details and tips around other services. We expect it to expand.
It is not a tutorial, but rather a collection of information you can read and return to. It is for both beginners and the experienced.
The goal of this guide is to be:

Brief: Keep it dense and use links
Practical: Basic facts, concrete details, advice, gotchas, and other ‚Äúfolk knowledge‚Äù
Current: We can keep updating it, and anyone can contribute improvements
Thoughtful: The goal is to be helpful rather than present dry facts. Thoughtful opinion with rationale is welcome. Suggestions, notes, and opinions based on real experience can be extremely valuable. (We believe this is both possible with a guide of this format, unlike in some other venues.)


This guide is not sponsored by AWS or AWS-affiliated vendors. It is written by and for engineers who use AWS.

Legend

üìí Marks standard/official AWS pages and docs
üîπ Important or often overlooked tip
‚ùó ‚ÄúSerious‚Äù gotcha (used where risks or time or resource costs are significant: critical security risks, mistakes with significant financial cost, or poor architectural choices that are fundamentally difficult to correct)
üî∏ ‚ÄúRegular‚Äù gotcha, limitation, or quirk (used where consequences are things not working, breaking, or not scaling gracefully)
üìú Undocumented feature (folklore)
üê• Relatively new (and perhaps immature) services or features
‚è± Performance discussions
‚õì Lock-in: Products or decisions that are likely to tie you to AWS in a new or significant way ‚Äî that is, later moving to a non-AWS alternative would be costly in terms of engineering effort
üö™ Alternative non-AWS options
üí∏ Cost issues, discussion, and gotchas
üïç A mild warning attached to ‚Äúfull solution‚Äù or opinionated frameworks that may take significant time to understand and/or might not fit your needs exactly; the opposite of a point solution (the cathedral is a nod to Raymond‚Äôs metaphor)
üìóüìòüìô Colors indicate basics, tips, and gotchas, respectively.
üöß Areas where correction or improvement are needed (possibly with link to an issue ‚Äî do help!)

General Information
When to Use AWS

AWS is the dominant public cloud computing provider.

In general, ‚Äúcloud computing‚Äù can refer to one of three types of cloud: ‚Äúpublic,‚Äù ‚Äúprivate,‚Äù and ‚Äúhybrid.‚Äù AWS is a public cloud provider, since anyone can use it. Private clouds are within a single (usually large) organization. Many companies use a hybrid of private and public clouds.
The core features of AWS are infrastructure-as-a-service (IaaS) ‚Äî that is, virtual machines and supporting infrastructure. Other cloud service models include platform-as-a-service (PaaS), which typically are more fully managed services that deploy customers‚Äô applications, or software-as-a-service (SaaS), which are cloud-based applications. AWS does offer a few products that fit into these other models, too.
In business terms, with infrastructure-as-a-service you have a variable cost model ‚Äî it is OpEx, not CapEx (though some pre-purchased contracts are still CapEx).


AWS‚Äôs TTM revenue was $32.5 billion as of Q3 2019 according to their earnings results (slide 14 in the linked deck), or roughly 12% of Amazon.com‚Äôs total revenue (slide 8 in the same deck) for the same TTM period.
Main reasons to use AWS:

If your company is building systems or products that may need to scale
and you have technical know-how
and you want the most flexible tools
and you‚Äôre not significantly tied into different infrastructure already
and you don‚Äôt have internal, regulatory, or compliance reasons you can‚Äôt use a public cloud-based solution
and you‚Äôre not on a Microsoft-first tech stack
and you don‚Äôt have a specific reason to use Google Cloud
and you can afford, manage, or negotiate its somewhat higher costs
... then AWS is likely a good option for your company.


Each of those reasons above might point to situations where other services are preferable. In practice, many, if not most, tech startups as well as a number of modern large companies can or already do benefit from using AWS. Many large enterprises are partly migrating internal infrastructure to Azure, Google Cloud, and AWS.
Costs: Billing and cost management are such big topics that we have an entire section on this.
üîπEC2 vs. other services: Most users of AWS are most familiar with EC2, AWS‚Äô flagship virtual server product, and possibly a few others like S3 and CLBs. But AWS products now extend far beyond basic IaaS, and often companies do not properly understand or appreciate all the many AWS services and how they can be applied, due to the sharply growing number of services, their novelty and complexity, branding confusion, and fear of ‚õìlock-in to proprietary AWS technology. Although a bit daunting, it‚Äôs important for technical decision-makers in companies to understand the breadth of the AWS services and make informed decisions. (We hope this guide will help.)
üö™AWS vs. other cloud providers: While AWS is the dominant IaaS provider (31% market share in this 2016 estimate), there is significant competition and alternatives that are better suited to some companies. This Gartner report has a good overview of the major cloud players :

Google Cloud Platform. GCP arrived later to market than AWS, but has vast resources and is now used widely by many companies, including a few large ones. It is gaining market share. Not all AWS services have similar or analogous services in GCP. And vice versa: In particular, GCP offers some more advanced machine learning-based services like the Vision, Speech, and Natural Language APIs. It‚Äôs not common to switch once you‚Äôre up and running, but it does happen: Spotify migrated from AWS to Google Cloud. There is more discussion on Quora about relative benefits. Of particular note is that VPCs in GCP are global by default with subnetworks per region, while AWS‚Äô VPCs have to live within a particular region. This gives GCP an edge if you‚Äôre designing applications with geo-replication from the beginning. It‚Äôs also possible to share one GCP VPC between multiple projects (roughly analogous to AWS accounts), while in AWS you‚Äôd have to peer them. It‚Äôs also possible to peer GCP VPCs in a similar manner to how it‚Äôs done in AWS.
Microsoft Azure is the de facto choice for companies and teams that are focused on a Microsoft stack, and it has now placed significant emphasis on Linux as well
In China, AWS‚Äô footprint is relatively small. The market is dominated by Alibaba‚Äôs Alibaba Cloud, formerly called Aliyun.
Companies at (very) large scale may want to reduce costs by managing their own infrastructure. For example, Dropbox migrated to their own infrastructure.
Other cloud providers such as Digital Ocean offer similar services, sometimes with greater ease of use, more personalized support, or lower cost. However, none of these match the breadth of products, mind-share, and market domination AWS now enjoys.
Traditional managed hosting providers such as Rackspace offer cloud solutions as well.


üö™AWS vs. PaaS: If your goal is just to put up a single service that does something relatively simple, and you‚Äôre trying to minimize time managing operations engineering, consider a platform-as-a-service such as Heroku. The AWS approach to PaaS, Elastic Beanstalk, is arguably more complex, especially for simple use cases.
üö™AWS vs. web hosting: If your main goal is to host a website or blog, and you don‚Äôt expect to be building an app or more complex service, you may wish consider one of the myriad web hosting services.
üö™AWS vs. managed hosting: Traditionally, many companies pay managed hosting providers to maintain physical servers for them, then build and deploy their software on top of the rented hardware. This makes sense for businesses who want direct control over hardware, due to legacy, performance, or special compliance constraints, but is usually considered old fashioned or unnecessary by many developer-centric startups and younger tech companies.
Complexity: AWS will let you build and scale systems to the size of the largest companies, but the complexity of the services when used at scale requires significant depth of knowledge and experience. Even very simple use cases often require more knowledge to do ‚Äúright‚Äù in AWS than in a simpler environment like Heroku or Digital Ocean. (This guide may help!)
Geographic locations: AWS has data centers in over a dozen geographic locations, known as regions, in Europe, East Asia, North and South America, and now Australia and India. It also has many more edge locations globally for reduced latency of services like CloudFront.

See the current list of regions and edge locations, including upcoming ones.
If your infrastructure needs to be in close physical proximity to another service for latency or throughput reasons (for example, latency to an ad exchange), viability of AWS may depend on the location.


‚õìLock-in: As you use AWS, it‚Äôs important to be aware when you are depending on AWS services that do not have equivalents elsewhere.

Lock-in may be completely fine for your company, or a significant risk. It‚Äôs important from a business perspective to make this choice explicitly, and consider the cost, operational, business continuity, and competitive risks of being tied to AWS. AWS is such a dominant and reliable vendor, many companies are comfortable with using AWS to its full extent. Others can tell stories about the dangers of ‚Äúcloud jail‚Äù when costs spiral.
Generally, the more AWS services you use, the more lock-in you have to AWS ‚Äî that is, the more engineering resources (time and money) it will take to change to other providers in the future.
Basic services like virtual servers and standard databases are usually easy to migrate to other providers or on premises. Others like load balancers and IAM are specific to AWS but have close equivalents from other providers. The key thing to consider is whether engineers are architecting systems around specific AWS services that are not open source or relatively interchangeable. For example, Lambda, API Gateway, Kinesis, Redshift, and DynamoDB do not have substantially equivalent open source or commercial service equivalents, while EC2, RDS (MySQL or Postgres), EMR, and ElastiCache more or less do. (See more below, where these are noted with ‚õì.)


Combining AWS and other cloud providers: Many customers combine AWS with other non-AWS services. For example, legacy systems or secure data might be in a managed hosting provider, while other systems are AWS. Or a company might only use S3 with another provider doing everything else. However small startups or projects starting fresh will typically stick to AWS or Google Cloud only.
Hybrid cloud: In larger enterprises, it is common to have hybrid deployments encompassing private cloud or on-premises servers and AWS ‚Äî or other enterprise cloud providers like IBM/Bluemix, Microsoft/Azure, NetApp, or EMC.
Major customers: Who uses AWS and Google Cloud?

AWS‚Äôs list of customers includes large numbers of mainstream online properties and major brands, such as Netflix, Pinterest, Spotify (moving to Google Cloud), Airbnb, Expedia, Yelp, Zynga, Comcast, Nokia, and Bristol-Myers Squibb.
Azure‚Äôs list of customers includes companies such as NBC Universal, 3M and Honeywell Inc.
Google Cloud‚Äôs list of customers is large as well, and includes a few mainstream sites, such as Snapchat, Best Buy, Domino‚Äôs, and Sony Music.



Which Services to Use

AWS offers a lot of different services ‚Äî about a hundred at last count.
Most customers use a few services heavily, a few services lightly, and the rest not at all. What services you‚Äôll use depends on your use cases. Choices differ substantially from company to company.
Immature and unpopular services: Just because AWS has a service that sounds promising, it doesn‚Äôt mean you should use it. Some services are very narrow in use case, not mature, are overly opinionated, or have limitations, so building your own solution may be better. We try to give a sense for this by breaking products into categories.
Must-know infrastructure: Most typical small to medium-size users will focus on the following services first. If you manage use of AWS systems, you likely need to know at least a little about all of these. (Even if you don‚Äôt use them, you should learn enough to make that choice intelligently.)

IAM: User accounts and identities (you need to think about accounts early on!)
EC2: Virtual servers and associated components, including:

AMIs: Machine Images
Load Balancers: CLBs and ALBs
Autoscaling: Capacity scaling (adding and removing servers based on load)
EBS: Network-attached disks
Elastic IPs: Assigned IP addresses


S3: Storage of files
Route 53: DNS and domain registration
VPC: Virtual networking, network security, and co-location; you automatically use
CloudFront: CDN for hosting content
CloudWatch: Alerts, paging, monitoring


Managed services: Existing software solutions you could run on your own, but with managed deployment:

RDS: Managed relational databases (managed MySQL, Postgres, and Amazon‚Äôs own Aurora database)
EMR: Managed Hadoop
Elasticsearch: Managed Elasticsearch
ElastiCache: Managed Redis and Memcached


Optional but important infrastructure: These are key and useful infrastructure components that are less widely known and used. You may have legitimate reasons to prefer alternatives, so evaluate with care to be sure they fit your needs:

‚õìLambda: Running small, fully managed tasks ‚Äúserverless‚Äù
CloudTrail: AWS API logging and audit (often neglected but important)
‚õìüïçCloudFormation: Templatized configuration of collections of AWS resources
üïçElastic Beanstalk: Fully managed (PaaS) deployment of packaged Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker applications
üê•EFS: Network filesystem compatible with NFSv4.1
‚õìüïçECS: Docker container/cluster management (note Docker can also be used directly, without ECS)
üïç EKS: Kubernetes (K8) Docker Container/Cluster management
‚õìECR: Hosted private Docker registry
üê•Config: AWS configuration inventory, history, change notifications
üê•X-Ray: Trace analysis and debugging for distributed applications such as microservices.


Special-purpose infrastructure: These services are focused on specific use cases and should be evaluated if they apply to your situation. Many also are proprietary architectures, so tend to tie you to AWS.

‚õìDynamoDB: Low-latency NoSQL key-value store
‚õìGlacier: Slow and cheap alternative to S3
‚õìKinesis: Streaming (distributed log) service
‚õìSQS: Message queueing service
‚õìRedshift: Data warehouse
üê•QuickSight: Business intelligence service
SES: Send and receive e-mail for marketing or transactions
‚õìAPI Gateway: Proxy, manage, and secure API calls
‚õìIoT: Manage bidirectional communication over HTTP, WebSockets, and MQTT between AWS and clients (often but not necessarily ‚Äúthings‚Äù like appliances or sensors)
‚õìWAF: Web firewall for CloudFront to deflect attacks
‚õìKMS: Store and manage encryption keys securely
Inspector: Security audit
Trusted Advisor: Automated tips on reducing cost or making improvements
üê•Certificate Manager: Manage SSL/TLS certificates for AWS services
üê•‚õìFargate: Docker containers management, backend for ECS and EKS


Compound services: These are similarly specific, but are full-blown services that tackle complex problems and may tie you in. Usefulness depends on your requirements. If you have large or significant need, you may have these already managed by in-house systems and engineering teams.

Machine Learning: Machine learning model training and classification
Lex: Automatic speech recognition (ASR) and natural language understanding (NLU)
Polly: Text-to-speech engine in the cloud
Rekognition: Service for image recognition
‚õìüïçData Pipeline: Managed ETL service
‚õìüïçSWF: Managed state tracker for distributed polyglot job workflow
‚õìüïçLumberyard: 3D game engine


Mobile/app development:

SNS: Manage app push notifications and other end-user notifications
‚õìüïçCognito: User authentication via Facebook, Twitter, etc.
Device Farm: Cloud-based device testing
Mobile Analytics: Analytics solution for app usage
üïçMobile Hub: Comprehensive, managed mobile app framework


Enterprise services: These are relevant if you have significant corporate cloud-based or hybrid needs. Many smaller companies and startups use other solutions, like Google Apps or Box. Larger companies may also have their own non-AWS IT solutions.

AppStream: Windows apps in the cloud, with access from many devices
Workspaces: Windows desktop in the cloud, with access from many devices
WorkDocs (formerly Zocalo): Enterprise document sharing
WorkMail: Enterprise managed e-mail and calendaring service
Directory Service: Microsoft Active Directory in the cloud
Direct Connect: Dedicated network connection between office or data center and AWS
Storage Gateway: Bridge between on-premises IT and cloud storage
Service Catalog: IT service approval and compliance


Probably-don't-need-to-know services: Bottom line, our informal polling indicates these services are just not broadly used ‚Äî and often for good reasons:

Snowball: If you want to ship petabytes of data into or out of Amazon using a physical appliance, read on.
Snowmobile: Appliances are great, but if you've got exabyte scale data to get into Amazon, nothing beats a tractor trailer full of drives.
CodeCommit: Git service. You‚Äôre probably already using GitHub or your own solution (Stackshare has informal stats).
üïçCodePipeline: Continuous integration. You likely have another solution already.
üïçCodeDeploy: Deployment of code to EC2 servers. Again, you likely have another solution.
üïçOpsWorks: Management of your deployments using Chef or (as of November 2017) Puppet Enterprise.


AWS in Plain English offers more friendly explanation of what all the other different services are.

Tools and Services Market Landscape
There are now enough cloud and ‚Äúbig data‚Äù enterprise companies and products that few can keep up with the market landscape.
We‚Äôve assembled a landscape of a few of the services. This is far from complete, but tries to emphasize services that are popular with AWS practitioners ‚Äî services that specifically help with AWS, or a complementary, or tools almost anyone using AWS must learn.

üöß Suggestions to improve this figure? Please file an issue.
Common Concepts

üìí The AWS General Reference covers a bunch of common concepts that are relevant for multiple services.
AWS allows deployments in regions, which are isolated geographic locations that help you reduce latency or offer additional redundancy. Regions contain availability zones(AZs), which are typically the first tool of choice for high availability). AZs are physically separate from one another even within the same region, and may span multiple physical data centers. While they are connected via low latency links, natural disasters afflicting one should not affect others.
Each service has API endpoints for each region. Endpoints differ from service to service and not all services are available in each region, as listed in these tables.
Amazon Resource Names (ARNs) are specially formatted identifiers for identifying resources. They start with 'arn:' and are used in many services, and in particular for IAM policies.

Service Matrix
Many services within AWS can at least be compared with Google Cloud offerings or with internal Google services. And often times you could assemble the same thing yourself with open source software. This table is an effort at listing these rough correspondences. (Remember that this table is imperfect as in almost every case there are subtle differences of features!)



Service
AWS
Google Cloud
Google Internal
Microsoft Azure
Other providers
Open source ‚Äúbuild your own‚Äù
Openstack




Virtual server
EC2
Compute Engine (GCE)

Virtual Machine
DigitalOcean
OpenStack
Nova


PaaS
Elastic Beanstalk
App Engine
App Engine
Web Apps
Heroku, AppFog, OpenShift
Meteor, AppScale, Cloud Foundry, Convox



Serverless, microservices
Lambda, API Gateway
Functions

Function Apps
PubNub Blocks, Auth0 Webtask
Kong, Tyk
Qinling


Container, cluster manager
ECS, EKS, Fargate
Container Engine, Kubernetes
Borg or Omega
Container Service

Kubernetes, Mesos, Aurora
Zun


Object storage
S3
Cloud Storage
GFS
Storage Account
DigitalOcean Spaces
Swift, HDFS, Minio
Swift


Block storage
EBS
Persistent Disk

Storage Account
DigitalOcean Volumes
NFS
Cinder


SQL datastore
RDS
Cloud SQL

SQL Database

MySQL, PostgreSQL
Trove (stores NoSQL as well)


Sharded RDBMS

Cloud Spanner
F1, Spanner
Azure Database for PostgreSQL - Hyperscale (Citus)

Crate.io, CockroachDB



Bigtable

Cloud Bigtable
Bigtable


HBase



Key-value store, column store
DynamoDB
Cloud Datastore
Megastore
Tables, DocumentDB

Cassandra, CouchDB, RethinkDB, Redis



Memory cache
ElastiCache
App Engine Memcache

Redis Cache

Memcached, Redis



Search
CloudSearch, Elasticsearch (managed)


Search
Algolia, QBox, Elastic Cloud
Elasticsearch, Solr



Data warehouse
Redshift
BigQuery
Dremel
SQL Data Warehouse
Oracle, IBM, SAP, HP, many others
Greenplum



Business intelligence
QuickSight
Data Studio 360

Power BI
Tableau




Lock manager
DynamoDB (weak)

Chubby
Lease blobs in Storage Account

ZooKeeper, Etcd, Consul



Message broker
SQS, SNS, IoT
Pub/Sub
PubSub2
Service Bus

RabbitMQ, Kafka, 0MQ



Streaming, distributed log
Kinesis
Dataflow
PubSub2
Event Hubs

Kafka Streams, Apex, Flink, Spark Streaming, Storm



MapReduce
EMR
Dataproc
MapReduce
HDInsight, DataLake Analytics
Qubole
Hadoop



Monitoring
CloudWatch
Stackdriver Monitoring
Borgmon
Monitor

Prometheus(?)



Tracing
X-Ray
Stackdriver Trace

Monitor (Application Insights)
DataDog, New Relic, Epsagon
Zipkin, Jaeger, Appdash



Metric management


Borgmon, TSDB
Application Insights

Graphite, InfluxDB, OpenTSDB, Grafana, Riemann, Prometheus



CDN
CloudFront
Cloud CDN

CDN
Akamai, Fastly, Cloudflare, Limelight Networks
Apache Traffic Server



Load balancer
CLB/ALB
Load Balancing
GFE
Load Balancer, Application Gateway

nginx, HAProxy, Apache Traffic Server



DNS
Route53
DNS

DNS

bind



Email
SES



Sendgrid, Mandrill, Postmark




Git hosting
CodeCommit
Cloud Source Repositories

Visual Studio Team Services
GitHub, BitBucket
GitLab



User authentication
Cognito
Firebase Authentication

Azure Active Directory

oauth.io



Mobile app analytics
Mobile Analytics
Firebase Analytics

HockeyApp
Mixpanel




Mobile app testing
Device Farm
Firebase Test Lab

Xamarin Test Cloud
BrowserStack, Sauce Labs, Testdroid




Managing SSL/TLS certificates
Certificate Manager



Let's Encrypt, Comodo, Symantec, GlobalSign




Automatic speech recognition and natural language understanding
Transcribe (ASR), Lex (NLU)
Cloud Speech API, Natural Language API

Cognitive services
AYLIEN Text Analysis API, Ambiverse Natural Language Understanding API
Stanford's Core NLP Suite, Apache OpenNLP, Apache UIMA, spaCy



Text-to-speech engine in the cloud
Polly



Nuance, Vocalware, IBM
Mimic, eSpeak, MaryTTS



Image recognition
Rekognition
Vision API

Cognitive services
IBM Watson, Clarifai
TensorFlow, OpenCV



OCR (Text recognition)
Textract (documents), Rekognition (photographs)
Cloud Vision API

Computer Vision API

Tesseract



Language Translation
Translate
Translate

Translator Text API

Apertium



File Share and Sync
WorkDocs
Google Docs

OneDrive
Dropbox, Box, Citrix File Share
ownCloud



Machine Learning
SageMaker, DeepLens, ML
ML Engine, Auto ML

ML Studio
Watson ML




Data Loss Prevention
Macie
Cloud Data Loss Prevention

Azure Information Protection






üöß Please help fill this table in.
Selected resources with more detail on this chart:

Google internal: MapReduce, Bigtable, Spanner, F1 vs Spanner, Bigtable vs Megastore

AWS Product Maturity and Releases
It‚Äôs important to know the maturity of each AWS product. Here is a mostly complete list of first release date, with links to the release notes. Most recently released services are first. Not all services are available in all regions; see this table.



Service
Original release
Availability
CLI Support
HIPAA Compliant
PCI-DSS Compliant




üê•X-Ray
2016-12
General
‚úì
‚úì
‚úì


üê•Lex
2016-11
Preview





üê•Polly
2016-11
General
‚úì
‚úì
‚úì


üê•Rekognition
2016-11
General
‚úì
‚úì
‚úì


üê•Athena
2016-11
General
‚úì
‚úì
‚úì


üê•Batch
2016-11
General
‚úì
‚úì
‚úì


üê•Database Migration Service
2016-03
General

‚úì
‚úì


üê•Certificate Manager
2016-01
General
‚úì
‚úì
‚úì


üê•IoT
2015-08
General
‚úì
‚úì
‚úì13


üê•WAF
2015-10
General
‚úì
‚úì
‚úì


üê•Data Pipeline
2015-10
General
‚úì




üê•Elasticsearch
2015-10
General
‚úì
‚úì
‚úì


üê•Aurora
2015-07
General
‚úì
‚úì3
‚úì3


üê•Service Catalog
2015-07
General
‚úì
‚úì
‚úì


üê•Device Farm
2015-07
General
‚úì




üê•CodePipeline
2015-07
General
‚úì
‚úì



üê•CodeCommit
2015-07
General
‚úì
‚úì
‚úì


üê•API Gateway
2015-07
General
‚úì
‚úì1
‚úì


üê•Config
2015-06
General
‚úì
‚úì
‚úì


üê•EFS
2015-05
General
‚úì
‚úì
‚úì


üê•Machine Learning
2015-04
General
‚úì




Lambda
2014-11
General
‚úì
‚úì
‚úì


ECS
2014-11
General
‚úì
‚úì
‚úì


EKS
2018-06
General
‚úì12
‚úì
‚úì


KMS
2014-11
General
‚úì
‚úì
‚úì


CodeDeploy
2014-11
General
‚úì
‚úì



Kinesis
2013-12
General
‚úì
‚úì
‚úì11


CloudTrail
2013-11
General
‚úì
‚úì
‚úì


AppStream
2013-11
Preview

‚úì



CloudHSM
2013-03
General
‚úì
‚úì
‚úì


Silk
2013-03
Obsolete?





OpsWorks
2013-02
General
‚úì
‚úì
‚úì


Redshift
2013-02
General
‚úì
‚úì
‚úì


Elastic Transcoder
2013-01
General
‚úì




Glacier
2012-08
General
‚úì
‚úì
‚úì


CloudSearch
2012-04
General
‚úì




SWF
2012-02
General
‚úì
‚úì
‚úì


Storage Gateway
2012-01
General
‚úì
‚úì
‚úì


DynamoDB
2012-01
General
‚úì
‚úì
‚úì


DirectConnect
2011-08
General
‚úì
‚úì
‚úì


ElastiCache
2011-08
General
‚úì
‚úì14
‚úì14


CloudFormation
2011-04
General
‚úì
‚úì
‚úì


SES
2011-01
General
‚úì
‚úì



Elastic Beanstalk
2010-12
General
‚úì
‚úì
‚úì


Route 53
2010-10
General
‚úì
‚úì
‚úì


IAM
2010-09
General
‚úì

‚úì


SNS
2010-04
General
‚úì
‚úì
‚úì


EMR
2010-04
General
‚úì
‚úì
‚úì


RDS
2009-12
General
‚úì
‚úì2
‚úì9


VPC
2009-08
General
‚úì
‚úì
‚úì


Snowball
2015-10
General
‚úì
‚úì
‚úì15


Snowmobile
2016-11
General

‚úì
‚úì


CloudWatch
2009-05
General
‚úì
‚úì
‚úì


CloudFront
2008-11
General
‚úì
‚úì4
‚úì


Fulfillment Web Service
2008-03
Obsolete?





SimpleDB
2007-12
‚ùóNearly obsolete
‚úì

‚úì


DevPay
2007-12
General





Flexible Payments Service
2007-08
Retired





EC2
2006-08
General
‚úì
‚úì5,6,7
‚úì6,7,10


SQS
2006-07
General
‚úì
‚úì
‚úì


S3
2006-03
General
‚úì
‚úì8
‚úì


Alexa Top Sites
2006-01
General ‚ùóHTTP-only





Alexa Web Information Service
2005-10
General ‚ùóHTTP-only






Footnotes
1: Excludes use of Amazon API Gateway caching
2: RDS MySQL, Oracle, and PostgreSQL engines only
3: MySQL-compatible Aurora edition only
4: Excludes Lambda@Edge
5: Includes EC2 Systems Manager
6: Includes Elastic Block Storage (EBS)
7: Includes Elastic Load Balancing
8: Includes S3 Transfer Acceleration
9: Includes RDS MySQL, Oracle, PostgreSQL, SQL Server, and MariaDB
10: Includes Auto-Scaling
11: Data Analytics, Streams, Video Streams and Firehose
12: Kubernetes uses a custom CLI for Pod/Service management called kubectl. AWS CLI only handles Kubernetes Master concerns
13: IoT Core (includes Device Management) and Greengrass
14: ElastiCache for Redis only
15: Snowball and Snowball Edge
Compliance

Many applications have strict requirements around reliability, security, or data privacy. The AWS Compliance page has details about AWS‚Äôs certifications, which include PCI DSS Level 1, SOC 1,2, and 3, HIPAA, and ISO 9001.
Security in the cloud is a complex topic, based on a shared responsibility model, where some elements of compliance are provided by AWS, and some are provided by your company.
Several third-party vendors offer assistance with compliance, security, and auditing on AWS. If you have substantial needs in these areas, assistance is a good idea.
From inside China, AWS services outside China are generally accessible, though there are at times breakages in service. There are also AWS services inside China.

Getting Help and Support

Forums: For many problems, it‚Äôs worth searching or asking for help in the discussion forums to see if it‚Äôs a known issue.
Premium support: AWS offers several levels of premium support.

The first tier, called ""Developer support"" lets you file support tickets with 12 to 24 hour turnaround time, it starts at $29 but once your monthly spend reaches around $1000 it changes to a 3% surcharge on your bill.
The higher-level support services are quite expensive ‚Äî and increase your bill by up to 10%. Many large and effective companies never pay for this level of support. They are usually more helpful for midsize or larger companies needing rapid turnaround on deeper or more perplexing problems.
Keep in mind, a flexible architecture can reduce need for support. You shouldn‚Äôt be relying on AWS to solve your problems often. For example, if you can easily re-provision a new server, it may not be urgent to solve a rare kernel-level issue unique to one EC2 instance. If your EBS volumes have recent snapshots, you may be able to restore a volume before support can rectify the issue with the old volume. If your services have an issue in one availability zone, you should in any case be able to rely on a redundant zone or migrate services to another zone.
Larger customers also get access to AWS Enterprise support, with dedicated technical account managers (TAMs) and shorter response time SLAs.
There is definitely some controversy about how useful the paid support is. The support staff don‚Äôt always seem to have the information and authority to solve the problems that are brought to their attention. Often your ability to have a problem solved may depend on your relationship with your account rep.


Account manager: If you are at significant levels of spend (thousands of US dollars plus per month), you may be assigned (or may wish to ask for) a dedicated account manager.

These are a great resource, even if you‚Äôre not paying for premium support. Build a good relationship with them and make use of them, for questions, problems, and guidance.
Assign a single point of contact on your company‚Äôs side, to avoid confusing or overwhelming them.


Contact: The main web contact point for AWS is here. Many technical requests can be made via these channels.
Consulting and managed services: For more hands-on assistance, AWS has established relationships with many consulting partners and managed service partners. The big consultants won‚Äôt be cheap, but depending on your needs, may save you costs long term by helping you set up your architecture more effectively, or offering specific expertise, e.g. security. Managed service providers provide longer-term full-service management of cloud resources.
AWS Professional Services: AWS provides consulting services alone or in combination with partners.

Restrictions and Other Notes

üî∏Lots of resources in Amazon have limits on them. This is actually helpful, so you don‚Äôt incur large costs accidentally. You have to request that quotas be increased by opening support tickets. Some limits are easy to raise, and some are not. (Some of these are noted in sections below.) Additionally, not all service limits are published.

Obtaining Current Limits and Usage: Limit information for a service may be available from the service API, Trusted Advisor, both or neither (in which case you'll need to contact Support). This page from the awslimitchecker tool's documentation provides a nice summary of available retrieval options for each limit. The tool itself is also valuable for automating limit checks.


üî∏AWS terms of service are extensive. Much is expected boilerplate, but it does contain important notes and restrictions on each service. In particular, there are restrictions against using many AWS services in safety-critical systems. (Those appreciative of legal humor may wish to review clause 57.10.)

Related Topics

OpenStack is a private cloud alternative to AWS used by large companies that wish to avoid public cloud offerings.

Learning and Career Development
Certifications

Certifications: AWS offers certifications for IT professionals who want to demonstrate their knowledge.
Certified Cloud Practitioner
Certified Solutions Architect Associate
Certified Developer Associate
Certified SysOps Administrator Associate
Certified Solutions Architect Professional
Certified DevOps Engineer Professional
Certified Security ‚Äì Specialty
Certified Big Data ‚Äì Specialty
Certified Advanced Networking ‚Äì Specialty
Certified Machine Learning ‚Äì Specialty
Certified Alexa Skill Builder ‚Äì Specialty

Associate level certifications were once required as pre-requisites to taking the Professional examinations - this is no longer the case.

Getting certified: If you‚Äôre interested in studying for and getting certifications, this practical overview tells you a lot of what you need to know. The official page is here and there is an FAQ.
Training for certifications: Training is offered by AWS themselves (mainly instructor-led and on-site) and various third-party companies (usually as video-based training) such as A Cloud Guru, CloudAcademy and Linux Academy.
Do you need a certification? Especially in consulting companies or when working in key tech roles in large non-tech companies, certifications are important credentials. In others, including in many tech companies and startups, certifications are not common or considered necessary. (In fact, fairly or not, some Silicon Valley hiring managers and engineers see them as a ‚Äúnegative‚Äù signal on a resume.)

Certifications are required to access certificate lounges at official AWS events such as Summits and re:Invent. Lounges typically provide power charging points, seats and relatively better coffee.
Managing AWS
Managing Infrastructure State and Change
A great challenge in using AWS to build complex systems (and with DevOps in general) is to manage infrastructure state effectively over time. In general, this boils down to three broad goals for the state of your infrastructure:

Visibility: Do you know the state of your infrastructure (what services you are using, and exactly how)? Do you also know when you ‚Äî and anyone on your team ‚Äî make changes? Can you detect misconfigurations, problems, and incidents with your service?
Automation: Can you reconfigure your infrastructure to reproduce past configurations or scale up existing ones without a lot of extra manual work, or requiring knowledge that‚Äôs only in someone‚Äôs head? Can you respond to incidents easily or automatically?
Flexibility: Can you improve your configurations and scale up in new ways without significant effort? Can you add more complexity using the same tools? Do you share, review, and improve your configurations within your team?

Much of what we discuss below is really about how to improve the answers to these questions.
There are several approaches to deploying infrastructure with AWS, from the console to complex automation tools, to third-party services, all of which attempt to help achieve visibility, automation, and flexibility.
AWS Configuration Management
The first way most people experiment with AWS is via its web interface, the AWS Console. But using the Console is a highly manual process, and often works against automation or flexibility.
So if you‚Äôre not going to manage your AWS configurations manually, what should you do? Sadly, there are no simple, universal answers ‚Äî each approach has pros and cons, and the approaches taken by different companies vary widely, and include directly using APIs (and building tooling on top yourself), using command-line tools, and using third-party tools and services.
AWS Console

The AWS Console lets you control much (but not all) functionality of AWS via a web interface.
Ideally, you should only use the AWS Console in a few specific situations:

It‚Äôs great for read-only usage. If you‚Äôre trying to understand the state of your system, logging in and browsing it is very helpful.
It is also reasonably workable for very small systems and teams (for example, one engineer setting up one server that doesn‚Äôt change often).
It can be useful for operations you‚Äôre only going to do rarely, like less than once a month (for example, a one-time VPC setup you probably won‚Äôt revisit for a year). In this case using the console can be the simplest approach.


‚ùóThink before you use the console: The AWS Console is convenient, but also the enemy of automation, reproducibility, and team communication. If you‚Äôre likely to be making the same change multiple times, avoid the console. Favor some sort of automation, or at least have a path toward automation, as discussed next. Not only does using the console preclude automation, which wastes time later, but it prevents documentation, clarity, and standardization around processes for yourself and your team.

Command-Line tools

The aws command-line interface (CLI), used via the aws command, is the most basic way to save and automate AWS operations.
Don‚Äôt underestimate its power. It also has the advantage of being well-maintained ‚Äî it covers a large proportion of all AWS services, and is up to date.
In general, whenever you can, prefer the command line to the AWS Console for performing operations.
üîπEven in the absence of fancier tools, you can write simple Bash scripts that invoke aws with specific arguments, and check these into Git. This is a primitive but effective way to document operations you‚Äôve performed. It improves automation, allows code review and sharing on a team, and gives others a starting point for future work.
üîπFor use that is primarily interactive (not scripted), consider instead using the aws-shell tool from AWS. It is easier to use, with auto-completion and a colorful UI, but still works on the command line. If you‚Äôre using SAWS, a previous version of the program, you should migrate to aws-shell.

APIs and SDKs

SDKs for using AWS APIs are available in most major languages, with Go, iOS, Java, JavaScript, Python, Ruby, and PHP being most heavily used. AWS maintains a short list, but the awesome-aws list is the most comprehensive and current. Note support for C++ is still new.
Retry logic: An important aspect to consider whenever using SDKs is error handling; under heavy use, a wide variety of failures, from programming errors to throttling to AWS-related outages or failures, can be expected to occur. SDKs typically implement exponential backoff to address this, but this may need to be understood and adjusted over time for some applications. For example, it is often helpful to alert on some error codes and not on others.
‚ùóDon‚Äôt use APIs directly. Although AWS documentation includes lots of API details, it‚Äôs better to use the SDKs for your preferred language to access APIs. SDKs are more mature, robust, and well-maintained than something you‚Äôd write yourself.

Boto

A good way to automate operations in a custom way is Boto3, also known as the Amazon SDK for Python. Boto2, the previous version of this library, has been in wide use for years, but now there is a newer version with official support from Amazon, so prefer Boto3 for new projects.
Boto3 contains a variety of APIs that operate at either a high level or a low level, here some explanation of both:

The low level APIs (Client APIs) are mapped to AWS Cloud service-specific APIs, and all service operations are supported by clients. Clients are generated from a JSON service definition file.
The high level option, Resource APIs, allows you to avoid calling the network at the low level and instead provide an object-oriented way to interact with AWS Cloud services.


Boto3 has a lot of helpful features like waiters, which provide a structure that allows for code to wait for changes to occur in the cloud, for example, when you are creating an EC2 instance and need wait until the instance is running in order to perform another task.
If you find yourself writing a Bash script with more than one or two CLI commands, you‚Äôre probably doing it wrong. Stop, and consider writing a Boto script instead. This has the advantages that you can:

Check return codes easily so success of each step depends on success of past steps.
Grab interesting bits of data from responses, like instance ids or DNS names.
Add useful environment information (for example, tag your instances with git revisions, or inject the latest build identifier into your initialization script).



General Visibility

üîπTagging resources is an essential practice, especially as organizations grow, to better understand your resource usage. For example, through automation or convention, you can add tags:

For the org or developer that ‚Äúowns‚Äù that resource
For the product that resource supports
To label lifecycles, such as temporary resources or one that should be deprovisioned in the future
To distinguish production-critical infrastructure (e.g. serving systems vs backend pipelines)
To distinguish resources with special security or compliance requirements
To (once enabled) allocate cost. Note that cost allocation tags only apply on a forward-looking basis; you can't retroactively apply them to items already billed.
For many years, there was a notorious 10 tag limit per resource, which could not be raised and caused many companies significant pain. As of 2016, this was raised to 50 tags per resource.
üîπIn 2017, AWS introduced the ability to enforce tagging on instance and volume creation, deprecating portions of third party tools such as Cloud Custodian.
üî∏ Tags are case sensitive; 'environment' and 'Environment' are two different tags. Automation in setting tags is likely the only sensible option at significant scale.
üî∏ There is a bug in the ASG console where spaces after tag names are preserved. So if you type ""Name "" with a space at the end you will not get the expected behavior. This is probably true in other locations and SDKs also. Be sure you do not add trailing spaces to tag keys unless you really mean it. (As of Jul 2018)



Managing Servers and Applications
AWS vs Server Configuration
This guide is about AWS, not DevOps or server configuration management in general. But before getting into AWS in detail, it‚Äôs worth noting that in addition to the configuration management for your AWS resources, there is the long-standing problem of configuration management for servers themselves.
Philosophy

Heroku‚Äôs Twelve-Factor App principles list some established general best practices for deploying applications.
Pets vs cattle: Treat servers like cattle, not pets. That is, design systems so infrastructure is disposable. It should be minimally worrisome if a server is unexpectedly destroyed.
The concept of immutable infrastructure is an extension of this idea.
Minimize application state on EC2 instances. In general, instances should be able to be killed or die unexpectedly with minimal impact. State that is in your application should quickly move to RDS, S3, DynamoDB, EFS, or other data stores not on that instance. EBS is also an option, though it generally should not be the bootable volume, and EBS will require manual or automated re-mounting.

Server Configuration Management

There is a large set of open source tools for managing configuration of server instances.
These are generally not dependent on any particular cloud infrastructure, and work with any variety of Linux (or in many cases, a variety of operating systems).
Leading configuration management tools are Puppet, Chef, Ansible, and Saltstack. These aren‚Äôt the focus of this guide, but we may mention them as they relate to AWS.

Containers and AWS

Docker and the containerization trend are changing the way many servers and services are deployed in general.
Containers are designed as a way to package up your application(s) and all of their dependencies in a known way. When you build a container, you are including every library or binary your application needs, outside of the kernel. A big advantage of this approach is that it‚Äôs easy to test and validate a container locally without worrying about some difference between your computer and the servers you deploy on.
A consequence of this is that you need fewer AMIs and boot scripts; for most deployments, the only boot script you need is a template that fetches an exported docker image and runs it.
Companies that are embracing microservice architectures will often turn to container-based deployments.
AWS launched ECS as a service to manage clusters via Docker in late 2014, though many people still deploy Docker directly themselves. See the ECS section for more details.
AWS launched EKS as a service to manage Kubernetes Clusters mid 2018, though many people still deploy ECS or use Docker directly themselves. See the EKS section for more details.

Visibility

Store and track instance metadata (such as instance id, availability zone, etc.) and deployment info (application build id, Git revision, etc.) in your logs or reports. The instance metadata service can help collect some of the AWS data you‚Äôll need.
Use log management services: Be sure to set up a way to view and manage logs externally from servers.

Cloud-based services such as Sumo Logic, Splunk Cloud, Scalyr, LogDNA, and Loggly are the easiest to set up and use (and also the most expensive, which may be a factor depending on how much log data you have).
Major open source alternatives include Elasticsearch, Logstash, and Kibana (the ‚ÄúElastic Stack‚Äù) and Graylog.
If you can afford it (you have little data or lots of money) and don‚Äôt have special needs, it makes sense to use hosted services whenever possible, since setting up your own scalable log processing systems is notoriously time consuming.


Track and graph metrics: The AWS Console can show you simple graphs from CloudWatch, you typically will want to track and graph many kinds of metrics, from CloudWatch and your applications. Collect and export helpful metrics everywhere you can (and as long as volume is manageable enough you can afford it).

Services like Librato, KeenIO, and Datadog have fancier features or better user interfaces that can save a lot of time. (A more detailed comparison is here.)
Use Prometheus or Graphite as timeseries databases for your metrics (both are open source).
Grafana can visualize with dashboards the stored metrics of both timeseries databases (also open source).



Tips for Managing Servers

‚ùóTimezone settings on servers: unless absolutely necessary, always set the timezone on servers to UTC (see instructions for your distribution, such as Ubuntu, CentOS or Amazon Linux). Numerous distributed systems rely on time for synchronization and coordination and UTC provides the universal reference plane: it is not subject to  daylight savings changes and adjustments in local time. It will also save you a lot of headache debugging elusive timezone issues and provide coherent timeline of events in your logging and audit systems.
NTP and accurate time: If you are not using Amazon Linux (which comes preconfigured), you should confirm your servers configure NTP correctly, to avoid insidious time drift (which can then cause all sorts of issues, from breaking API calls to misleading logs). This should be part of your automatic configuration for every server. If time has already drifted substantially (generally >1000 seconds), remember NTP won‚Äôt shift it back, so you may need to remediate manually (for example, like this on Ubuntu).
Testing immutable infrastructure: If you want to be proactive about testing your service‚Äôs ability to cope with instance termination or failure, it can be helpful to introduce random instance termination during business hours, which will expose any such issues at a time when engineers are available to identify and fix them. Netflix‚Äôs Simian Army (specifically, Chaos Monkey) is a popular tool for this. Alternatively, chaos-lambda by the BBC is a lightweight option which runs on AWS Lambda.

Security and IAM
We cover security basics first, since configuring user accounts is something you usually have to do early on when setting up your system.
Security and IAM Basics

üìí IAM Homepage ‚àô User guide ‚àô FAQ
The AWS Security Blog is one of the best sources of news and information on AWS security.
IAM is the service you use to manage accounts and permissioning for AWS.
Managing security and access control with AWS is critical, so every AWS administrator needs to use and understand IAM, at least at a basic level.
IAM identities include users (people or services that are using AWS), groups (containers for sets of users and their permissions), and roles (containers for permissions assigned to AWS service instances). Permissions for these identities are governed by policies You can use AWS pre-defined policies or custom policies that you create.
IAM manages various kinds of authentication, for both users and for software services that may need to authenticate with AWS, including:

Passwords to log into the console. These are a username and password for real users.
Access keys, which you may use with command-line tools. These are two strings, one the ‚Äúid‚Äù, which is an upper-case alphabetic string of the form 'AXXXXXXXXXXXXXXXXXXX', and the other is the secret, which is a 40-character mixed-case base64-style string. These are often set up for services, not just users.

üìú Access keys that start with AKIA are normal keys. Access keys that start with ASIA are session/temporary keys from STS, and will require an additional ""SessionToken"" parameter to be sent along with the id and secret. See the documentation for a complete list of access key prefixes.


Multi-factor authentication (MFA), which is the highly recommended practice of using a keychain fob or smartphone app as a second layer of protection for user authentication.


IAM allows complex and fine-grained control of permissions, dividing users into groups, assigning permissions to roles, and so on. There is a policy language that can be used to customize security policies in a fine-grained way.

An excellent high level overview of IAM policy concepts lives at IAM Policies In A Nutshell.
üî∏The policy language has a complex and error-prone JSON syntax that‚Äôs quite confusing, so unless you are an expert, it is wise to base yours off trusted examples or AWS‚Äô own pre-defined managed policies.


At the beginning, IAM policy may be very simple, but for large systems, it will grow in complexity, and need to be managed with care.

üîπMake sure one person (perhaps with a backup) in your organization is formally assigned ownership of managing IAM policies, make sure every administrator works with that person to have changes reviewed. This goes a long way to avoiding accidental and serious misconfigurations.


It is best to give each user or service the minimum privileges needed to perform their duties. This is the principle of least privilege, one of the foundations of good security. Organize all IAM users and groups according to levels of access they need.
IAM has the permission hierarchy of:

Explicit deny: The most restrictive policy wins.
Explicit allow: Access permissions to any resource has to be explicitly given.
Implicit deny: All permissions are implicitly denied by default.


You can test policy permissions via the AWS IAM policy simulator tool tool. This is particularly useful if you write custom policies.

Security and IAM Tips

üîπUse IAM to create individual user accounts and use IAM accounts for all users from the beginning. This is slightly more work, but not that much.

That way, you define different users, and groups with different levels of privilege (if you want, choose from Amazon‚Äôs default suggestions, of administrator, power user, etc.).
This allows credential revocation, which is critical in some situations. If an employee leaves, or a key is compromised, you can revoke credentials with little effort.
You can set up Active Directory federation to use organizational accounts in AWS.


‚ùóEnable MFA on your account.

You should always use MFA, and the sooner the better ‚Äî enabling it when you already have many users is extra work.
Unfortunately it can‚Äôt be enforced in software, so an administrative policy has to be established.
Most users can use the Google Authenticator app (on iOS or Android) to support two-factor authentication. For the root account, consider a hardware fob.


‚ùóRestrict use of significant IAM credentials as much as possible. Remember that in the cloud, loss of a highly capable IAM credential could essentially mean ‚Äúgame over,‚Äù for your deployment, your users, or your whole company.

Do NOT use the Root User account other than when you initially create your account.  Create custom IAM users and/or roles and use those for your applications instead.

Lock up access and use of the root credentials as much as possible. Ideally they should be effectively ‚Äúoffline.‚Äù For critical deployments, this means attached to an actual MFA device, physically secured and rarely used.




‚ùóTurn on CloudTrail: One of the first things you should do is enable CloudTrail. Even if you are not a security hawk, there is little reason not to do this from the beginning, so you have data on what has been happening in your AWS account should you need that information. You‚Äôll likely also want to set up a log management service to search and access these logs.
üîπUse IAM roles for EC2: Rather than assign IAM users to applications like services and then sharing the sensitive credentials, define and assign roles to EC2 instances and have applications retrieve credentials from the instance metadata.
Assign IAM roles by realm ‚Äî for example, to development, staging, and production. If you‚Äôre setting up a role, it should be tied to a specific realm so you have clean separation. This prevents, for example, a development instance from connecting to a production database.
Best practices: AWS‚Äô list of best practices is worth reading in full up front.
IAM Reference: This interactive reference for all IAM actions, effects, and resources is great to have open while writing new or trying to understand existing IAM policies.
Multiple accounts: Decide on whether you want to use multiple AWS accounts and research how to organize access across them. Factors to consider:

Number of users
Importance of isolation

Resource Limits
Permission granularity
Security
API Limits


Regulatory issues
Workload
Size of infrastructure
Cost of multi-account ‚Äúoverhead‚Äù: Internal AWS service management tools may need to be custom built or adapted.
üîπIt can help to use separate AWS accounts for independent parts of your infrastructure if you expect a high rate of AWS API calls, since AWS throttles calls at the AWS account level.


Inspector is an automated security assessment service from AWS that helps identify common security risks. This allows validation that you adhere to certain security practices and may help with compliance.
Trusted Advisor addresses a variety of best practices, but also offers some basic security checks around IAM usage, security group configurations, and MFA. At paid support tiers, Trusted Advisor exposes additional checks around other areas, such as reserved instance optimization.
Use KMS for managing keys: AWS offers KMS for securely managing encryption keys, which is usually a far better option than handling key security yourself. See below.
AWS WAF is a web application firewall to help you protect your applications from common attack patterns.
Security auditing:

Security Monkey is an open source tool that is designed to assist with security audits.
Scout2 is an open source tool that uses AWS APIs to assess an environment‚Äôs security posture. Scout2 is stable and actively maintained.
üîπExport and audit security settings: You can audit security policies simply by exporting settings using AWS APIs, e.g. using a Boto script like SecConfig.py (from this 2013 talk) and then reviewing and monitoring changes manually or automatically.



Security and IAM Gotchas and Limitations

‚ùóDon‚Äôt share user credentials: It‚Äôs remarkably common for first-time AWS users to create one account and one set of credentials (access key or password), and then use them for a while, sharing among engineers and others within a company. This is easy. But don‚Äôt do this. This is an insecure practice for many reasons, but in particular, if you do, you will have reduced ability to revoke credentials on a per-user or per-service basis (for example, if an employee leaves or a key is compromised), which can lead to serious complications.
‚ùóInstance metadata throttling: The instance metadata service has rate limiting on API calls. If you deploy IAM roles widely (as you should!) and have lots of services, you may hit global account limits easily.

One solution is to have code or scripts cache and reuse the credentials locally for a short period (say 2 minutes). For example, they can be put into the ~/.aws/credentials file but must also be refreshed automatically.
But be careful not to cache credentials for too long, as they expire. (Note the other dynamic metadata also changes over time and should not be cached a long time, either.)


üî∏Some IAM operations are slower than other API calls (many seconds), since AWS needs to propagate these globally across regions.
‚ùóThe uptime of IAM‚Äôs API has historically been lower than that of the instance metadata API. Be wary of incorporating a dependency on IAM‚Äôs API into critical paths or subsystems ‚Äî for example, if you validate a user‚Äôs IAM group membership when they log into an instance and aren‚Äôt careful about precaching group membership or maintaining a back door, you might end up locking users out altogether when the API isn‚Äôt available.
‚ùóDon't check in AWS credentials or secrets to a git repository. There are bots that scan GitHub looking for credentials. Use scripts or tools, such as git-secrets to prevent anyone on your team from checking in sensitive information to your git repositories.

S3
S3 Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
S3 (Simple Storage Service) is AWS‚Äô standard cloud storage service, offering file (opaque ‚Äúblob‚Äù) storage of arbitrary numbers of files of almost any size, from 0 to 5TB. (Prior to 2011 the maximum size was 5 GB; larger sizes are now well supported via multipart support.)
Items, or objects, are placed into named buckets stored with names which are usually called keys. The main content is the value.
Objects are created, deleted, or updated. Large objects can be streamed, but you cannot modify parts of a value; you need to update the whole object. Partial data access can work via S3 Select.
Every object also has metadata, which includes arbitrary key-value pairs, and is used in a way similar to HTTP headers. Some metadata is system-defined, some are significant when serving HTTP content from buckets or CloudFront, and you can also define arbitrary metadata for your own use.
S3 URIs: Although often bucket and key names are provided in APIs individually, it‚Äôs also common practice to write an S3 location in the form 's3://bucket-name/path/to/key' (where the key here is 'path/to/key'). (You‚Äôll also see 's3n://' and 's3a://' prefixes in Hadoop systems.)
S3 vs Glacier, EBS, and EFS: AWS offers many storage services, and several besides S3 offer file-type abstractions. Glacier is for cheaper and infrequently accessed archival storage. EBS, unlike S3, allows random access to file contents via a traditional filesystem, but can only be attached to one EC2 instance at a time. EFS is a network filesystem many instances can connect to, but at higher cost. See the comparison table.

S3 Tips

For most practical purposes, you can consider S3 capacity unlimited, both in total size of files and number of objects. The number of objects in a bucket is essentially also unlimited. Customers routinely have millions of objects.
‚ùóPermissions:

üî∏If you're storing business data on Amazon S3, it‚Äôs important to manage permissions sensibly. In 2017 companies like Dow Jones and Verizon saw data breaches due to poorly-chosen S3 configuration for sensitive data. Fixing this later can be a difficult task if you have a lot of assets and internal users.
üî∏There are 3 different ways to grant permissions to access Amazon S3 content in your buckets.

IAM policies use the familiar Identity and Authentication Management permission scheme to control access to specific operations.
Bucket policies grant or deny permissions to an entire bucket. You might use this when hosting a website in S3, to make the bucket publicly readable, or to restrict access to a bucket by IP address. Amazon's sample bucket policies show a number of use cases where these policies come in handy.
Access Control Lists (ACLs) can also be applied to every bucket and object stored in S3. ACLs grant additional permissions beyond those specified in IAM or bucket policies. ACLs can be used to grant access to another AWS user, or to predefined groups like the general public. This is powerful but can be dangerous, because you need to inspect every object to see who has access.


üî∏AWS' predefined access control groups allow access that may not be what you'd expect from their names:

""All Users"", or ""Everyone"", grants permission to the general public, not only to users defined in your own AWS account. If an object is available to All Users, then it can be retrieved with a simple HTTP request of the form http://s3.amazonaws.com/bucket-name/filename. No authorization or signature is required to access data in this category.
""Authenticated Users"" grants permissions to anyone with an AWS account, again not limited to your own users. Because anyone can sign up for AWS, for all intents and purposes this is also open to the general public.
""Log Delivery"" group is used by AWS to write logs to buckets and should be safe to enable on the buckets that need it.
A typical use case of this ACL is used in conjunction with the requester pays functionality of S3.


‚ùó Bucket permissions and object permissions are two different things and independent of each other. A private object in a public bucket can be seen when listing the bucket, but not downloaded. At the same time, a public object in a private bucket won't be seen because the bucket contents can't be listed, but can still be downloaded by anyone who knows its exact key. Users that don't have access to set bucket permissions can still make objects public if they have s3:PutObjectAcl or s3:PutObjectVersionAcl permissions.
üê•In August 2017, AWS added AWS Config rules to ensure your S3 buckets are secure.

‚ùóThese AWS Config rules only check the security of your bucket policy and bucket-level ACLs. You can still create object ACLs that grant additional permissions, including opening files to the whole world.


üîπDo create new buckets if you have different types of data with different sensitivity levels. This is much less error prone than complex permissions rules. For example, if data is for administrators only, like log data, put it in a new bucket that only administrators can access.
For more guidance, see:

How to Secure an Amazon S3 Bucket
Deep dive into S3 access controls.
How do S3 permissions work?.




Bucket naming: Buckets are chosen from a global namespace (across all regions, even though S3 itself stores data in whichever S3 region you select), so you‚Äôll find many bucket names are already taken. Creating a bucket means taking ownership of the name until you delete it. Bucket names have a few restrictions on them.

Bucket names can be used as part of the hostname when accessing the bucket or its contents, like <bucket_name>.s3-us-east-1.amazonaws.com, as long as the name is DNS compliant.
A common practice is to use the company name acronym or abbreviation to prefix (or suffix, if you prefer DNS-style hierarchy) all bucket names (but please, don‚Äôt use a check on this as a security measure ‚Äî this is highly insecure and easily circumvented!).
üî∏Bucket names with '.' (periods) in them can cause certificate mismatches when used with SSL. Use '-' instead, since this then conforms with both SSL expectations and is DNS compliant.


Versioning: S3 has optional versioning support, so that all versions of objects are preserved on a bucket. This is mostly useful if you want an archive of changes or the ability to back out mistakes (caution: it lacks the featureset of full version control systems like Git).
Durability: Durability of S3 is extremely high, since internally it keeps several replicas. If you don‚Äôt delete it by accident, you can count on S3 not losing your data. (AWS offers the seemingly improbable durability rate of 99.999999999%, but this is a mathematical calculation based on independent failure rates and levels of replication ‚Äî not a true probability estimate. Either way, S3 has had a very good record of durability.) Note this is much higher durability than EBS!
üí∏S3 pricing depends on storage, requests, and transfer.

For transfer, putting data into AWS is free, but you‚Äôll pay on the way out. Transfer from S3 to EC2 in the same region is free. Transfer to other regions or the Internet in general is not free.
Deletes are free.


S3 Reduced Redundancy and Infrequent Access: Most people use the Standard storage class in S3, but there are other storage classes with lower cost:

üî∏Reduced Redundancy Storage (RRS) has been effectively deprecated, and has lower durability (99.99%, so just four nines) than standard S3. Note that it no longer participates in S3 price reductions, so it offers worse redundancy for more money than standard S3. As a result, there's no reason to use it.
Infrequent Access (IA) lets you get cheaper storage in exchange for more expensive access. This is great for archives like logs you already processed, but might want to look at later. To get an idea of the cost savings when using Infrequent Access (IA), you can use this S3 Infrequent Access Calculator.
S3 - Intelligent Tiering storage class is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.
S3 - One Zone - IA is for data that is accessed less frequently, but requires rapid access when needed. Unlike other S3 Storage Classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA.
Glacier is a third alternative discussed as a separate product.
See the comparison table.


‚è±Performance: Maximizing S3 performance means improving overall throughput in terms of bandwidth and number of operations per second.

S3 is highly scalable, so in principle you can get arbitrarily high throughput. (A good example of this is S3DistCp.)
But usually you are constrained by the pipe between the source and S3 and/or the level of concurrency of operations.
Throughput is of course highest from within AWS to S3, and between EC2 instances and S3 buckets that are in the same region.
Bandwidth from EC2 depends on instance type. See the ‚ÄúNetwork Performance‚Äù column at ec2instances.info.
Throughput of many objects is extremely high when data is accessed in a distributed way, from many EC2 instances. It‚Äôs possible to read or write objects from S3 from hundreds or thousands of instances at once.
However, throughput is very limited when objects accessed sequentially from a single instance. Individual operations take many milliseconds, and bandwidth to and from instances is limited.
Therefore, to perform large numbers of operations, it‚Äôs necessary to use multiple worker threads and connections on individual instances, and for larger jobs, multiple EC2 instances as well.
Multi-part uploads: For large objects you want to take advantage of the multi-part uploading capabilities (starting with minimum chunk sizes of 5 MB).
Large downloads: Also you can download chunks of a single large object in parallel by exploiting the HTTP GET range-header capability.
üî∏List pagination: Listing contents happens at 1000 responses per request, so for buckets with many millions of objects listings will take time.
‚ùóKey prefixes: Previously randomness in the beginning of key names was necessary in order to avoid hot spots, but that is no longer necessary as of July, 2018.
For data outside AWS, DirectConnect and S3 Transfer Acceleration can help. For S3 Transfer Acceleration, you pay about the equivalent of 1-2 months of storage for the transfer in either direction for using nearer endpoints.


Command-line applications: There are a few ways to use S3 from the command line:

Originally, s3cmd was the best tool for the job. It‚Äôs still used heavily by many.
The regular aws command-line interface now supports S3 well, and is useful for most situations.
s4cmd is a replacement, with greater emphasis on performance via multi-threading, which is helpful for large files and large sets of files, and also offers Unix-like globbing support.


GUI applications: You may prefer a GUI, or wish to support GUI access for less technical users. Some options:

The AWS Console does offer a graphical way to use S3. Use caution telling non-technical people to use it, however, since without tight permissions, it offers access to many other AWS features.
Transmit is a good option on macOS for most use cases.
Cyberduck is a good option on macOS and Windows with support for multipart uploads, ACLs, versioning, lifecycle configuration, storage classes and server side encryption (SSE-S3 and SSE-KMS).


S3 and CloudFront: S3 is tightly integrated with the CloudFront CDN. See the CloudFront section for more information, as well as S3 transfer acceleration.
Static website hosting:

S3 has a static website hosting option that is simply a setting that enables configurable HTTP index and error pages and HTTP redirect support to public content in S3. It‚Äôs a simple way to host static assets or a fully static website.
Consider using CloudFront in front of most or all assets:

Like any CDN, CloudFront improves performance significantly.
üî∏SSL is only supported on the built-in amazonaws.com domain for S3. S3 supports serving these sites through a custom domain, but not over SSL on a custom domain. However, CloudFront allows you to serve a custom domain over https. Amazon provides free SNI SSL/TLS certificates via Amazon Certificate Manager. SNI does not work on very outdated browsers/operating systems. Alternatively, you can provide your own certificate to use on CloudFront to support all browsers/operating systems for a fee.
üî∏If you are including resources across domains, such as fonts inside CSS files, you may need to configure CORS for the bucket serving those resources.
Since pretty much everything is moving to SSL nowadays, and you likely want control over the domain, you probably want to set up CloudFront with your own certificate in front of S3 (and to ignore the AWS example on this as it is non-SSL only).
That said, if you do, you‚Äôll need to think through invalidation or updates on CloudFront. You may wish to include versions or hashes in filenames so invalidation is not necessary.




Data lifecycles:

When managing data, the understanding the lifecycle of the data is as important as understanding the data itself. When putting data into a bucket, think about its lifecycle ‚Äî its end of life, not just its beginning.
üîπIn general, data with different expiration policies should be stored under separate prefixes at the top level. For example, some voluminous logs might need to be deleted automatically monthly, while other data is critical and should never be deleted. Having the former in a separate bucket or at least a separate folder is wise.
üî∏Thinking about this up front will save you pain. It‚Äôs very hard to clean up large collections of files created by many engineers with varying lifecycles and no coherent organization.
Alternatively you can set a lifecycle policy to archive old data to Glacier. Be careful with archiving large numbers of small objects to Glacier, since it may actually cost more.
There is also a storage class called Infrequent Access that has the same durability as Standard S3, but is discounted per GB. It is suitable for objects that are infrequently accessed.


Data consistency: Understanding data consistency is critical for any use of S3 where there are multiple producers and consumers of data.

Creation and updates to individual objects in S3 are atomic, in that you‚Äôll never upload a new object or change an object and have another client see only part half the change.
The uncertainty lies with when your clients and other clients see updates.
New objects: If you create a new object, you‚Äôll be able to read it instantly, which is called read-after-write consistency.

Well, with the additional caveat that if you do a read on an object before it exists, then create it, you get eventual consistency (not read-after-write).
This does not apply to any list operations; newly created objects are not guaranteed to appear in a list operation right away


Updates to objects: If you overwrite or delete an object, you‚Äôre only guaranteed eventual consistency, i.e. the change will happen but you have no guarantee of when.
üîπFor many use cases, treating S3 objects as immutable (i.e. deciding by convention they will be created or deleted but not updated) can greatly simplify the code that uses them, avoiding complex state management.
üîπNote that until 2015, 'us-standard' region had had a weaker eventual consistency model, and the other (newer) regions were read-after-write. This was finally corrected ‚Äî but watch for many old blogs mentioning this!
Slow updates: In practice, ‚Äúeventual consistency‚Äù usually means within seconds, but expect rare cases of minutes or hours.


S3 as a filesystem:

In general S3‚Äôs APIs have inherent limitations that make S3 hard to use directly as a POSIX-style filesystem while still preserving S3‚Äôs own object format. For example, appending to a file requires rewriting, which cripples performance, and atomic rename of directories, mutual exclusion on opening files, and hardlinks are impossible.
s3fs is a FUSE filesystem that goes ahead and tries anyway, but it has performance limitations and surprises for these reasons.
Riofs (C) and Goofys (Go) are more recent efforts that attempt adopt a different data storage format to address those issues, and so are likely improvements on s3fs.
S3QL (discussion) is a Python implementation that offers data de-duplication, snap-shotting, and encryption, but only one client at a time.
ObjectiveFS (discussion) is a commercial solution that supports filesystem features and concurrent clients.


If you are primarily using a VPC, consider setting up a VPC Endpoint for S3 in order to allow your VPC-hosted resources to easily access it without the need for extra network configuration or hops.
Cross-region replication: S3 has a feature for replicating a bucket between one region and another. Note that S3 is already highly replicated within one region, so usually this isn‚Äôt necessary for durability, but it could be useful for compliance (geographically distributed data storage), lower latency, or as a strategy to reduce region-to-region bandwidth costs by mirroring heavily used data in a second region.
IPv4 vs IPv6: For a long time S3 only supported IPv4 at the default endpoint https://BUCKET.s3.amazonaws.com. However, as of Aug 11, 2016 it now supports both IPv4 & IPv6! To use both, you have to enable dualstack either in your preferred API client or by directly using this url scheme https://BUCKET.s3.dualstack.REGION.amazonaws.com. This extends to S3 Transfer Acceleration as well.
S3 event notifications: S3 can be configured to send an SNS notification, SQS message, or AWS Lambda function on bucket events.
üí∏Limit your individual users (or IAM roles) to the minimal required S3 locations, and catalog the ‚Äúapproved‚Äù locations. Otherwise, S3 tends to become the dumping ground where people put data to random locations that are not cleaned up for years, costing you big bucks.
If a bucket is deleted in S3, it can take up to 10 hours before a bucket with the same name can be created again. (discussion)

S3 Gotchas and Limitations

‚ùóS3 buckets sit outside the VPC and can be accessed from anywhere in the world if bucket policies are not set to deny it. Read the permissions section above carefully, there are countless cases of buckets exposed to the public.
üî∏For many years, there was a notorious 100-bucket limit per account, which could not be raised and caused many companies significant pain. As of 2015, you can request increases. You can ask to increase the limit, but it will still be capped (generally below ~1000 per account).
üî∏Be careful not to make implicit assumptions about transactionality or sequencing of updates to objects. Never assume that if you modify a sequence of objects, the clients will see the same modifications in the same sequence, or if you upload a whole bunch of files, that they will all appear at once to all clients.
üî∏S3 has an SLA with 99.9% uptime. If you use S3 heavily, you‚Äôll inevitably see occasional error accessing or storing data as disks or other infrastructure fail. Availability is usually restored in seconds or minutes. Although availability is not extremely high, as mentioned above, durability is excellent.
üî∏After uploading, any change that you make to the object causes a full rewrite of the object, so avoid appending-like behavior with regular files.
üî∏Eventual data consistency, as discussed above, can be surprising sometimes. If S3 suffers from internal replication issues, an object may be visible from a subset of the machines, depending on which S3 endpoint they hit. Those usually resolve within seconds; however, we‚Äôve seen isolated cases when the issue lingered for 20-30 hours.
üî∏MD5s and multi-part uploads: In S3, the ETag header in S3 is a hash on the object. And in many cases, it is the MD5 hash. However, this is not the case in general when you use multi-part uploads. One workaround is to compute MD5s yourself and put them in a custom header (such as is done by s4cmd).
üî∏Incomplete multi-part upload costs: Incomplete multi-part uploads accrue storage charges even if the upload fails and no S3 object is created. Amazon (and others) recommend using a lifecycle policy to clean up incomplete uploads and save on storage costs. Note that if you have many of these, it may be worth investigating whatever's failing regularly.
üî∏US Standard region: Previously, the us-east-1 region (also known as the US Standard region) was replicated across coasts, which led to greater variability of latency. Effective Jun 19, 2015 this is no longer the case. All Amazon S3 regions now support read-after-write consistency. Amazon S3 also renamed the US Standard region to the US East (N. Virginia) region to be consistent with AWS regional naming conventions.
üî∏S3 authentication versions and regions: In newer regions, S3 only supports the latest authentication. If an S3 file operation using CLI or SDK doesn't work in one region, but works correctly in another region, make sure you are using the latest authentication signature.

Storage Durability, Availability, and Price
As an illustration of comparative features and price, the table below gives S3 Standard, RRS, IA, in comparison with Glacier, EBS, EFS, and EC2 d2.xlarge instance store using Virginia region as of Sept 2017.




Durability (per year)
Availability ‚Äúdesigned‚Äù
Availability SLA
Storage (per TB per month)
GET or retrieve (per million)
Write or archive (per million)




Glacier
Eleven 9s
Sloooow
‚Äì
$4
$50
$50


S3 IA
Eleven 9s
99.9%
99%
$12.50
$1
$10


S3 RRS
99.99%
99.99%
99.9%
$24 (first TB)
$0.40
$5


S3 Standard
Eleven 9s
99.99%
99.9%
$23
$0.40
$5


EBS
99.8%
Unstated
99.99%
$25/$45/$100/$125+ (sc1/st1/gp2/io1)




EFS
‚ÄúHigh‚Äù
‚ÄúHigh‚Äù
‚Äì
$300




EC2 d2.xlarge instance store
Unstated
Unstated
‚Äì
$25.44
$0
$0



Especially notable items are in boldface. Sources: S3 pricing, S3 SLA, S3 FAQ, RRS info (note that this is considered deprecated), Glacier pricing, EBS availability and durability, EBS pricing, EFS pricing, EC2 SLA
EC2
EC2 Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing (see also ec2instances.info)
EC2 (Elastic Compute Cloud) is AWS‚Äô offering of the most fundamental piece of cloud computing: A virtual private server. These ‚Äúinstances‚Äù can run most Linux, BSD, and Windows operating systems. Internally, they've used a heavily modified Xen virtualization. That said, new instance classes are being introduced with a KVM derived hypervisor instead, called Nitro. So far, this is limited to the C5 and M5 instance types. Lastly, there's a ""bare metal hypervisor"" available for i3.metal instances
The term ‚ÄúEC2‚Äù is sometimes used to refer to the servers themselves, but technically refers more broadly to a whole collection of supporting services, too, like load balancing (CLBs/ALBs/NLBs), IP addresses (EIPs), bootable images (AMIs), security groups, and network drives (EBS) (which we discuss individually in this guide).
üí∏EC2 pricing and cost management is a complicated topic. It can range from free (on the AWS free tier) to a lot, depending on your usage. Pricing is by instance type, by second or hour, and changes depending on AWS region and whether you are purchasing your instances On-Demand, on the Spot market or pre-purchasing (Reserved Instances).
Network Performance: For some instance types, AWS uses general terms like Low, Medium, and High to refer to network performance. Users have done benchmarking to provide expectations for what these terms can mean.

EC2 Alternatives and Lock-In

Running EC2 is akin to running a set of physical servers, as long as you don‚Äôt do automatic scaling or tooled cluster setup. If you just run a set of static instances, migrating to another VPS or dedicated server provider should not be too hard.
üö™Alternatives to EC2: The direct alternatives are Google Cloud, Microsoft Azure, Rackspace, DigitalOcean, AWS's own Lightsail offering, and other VPS providers, some of which offer similar APIs for setting up and removing instances. (See the comparisons above.)
Should you use Amazon Linux? AWS encourages use of their own Amazon Linux, which is evolved from Red Hat Enterprise Linux (RHEL) and CentOS. It‚Äôs used by many, but others are skeptical. Whatever you do, think this decision through carefully. It‚Äôs true Amazon Linux is heavily tested and better supported in the unlikely event you have deeper issues with OS and virtualization on EC2. But in general, many companies do just fine using a standard, non-Amazon Linux distribution, such as Ubuntu or CentOS. Using a standard Linux distribution means you have an exactly replicable environment should you use another hosting provider instead of (or in addition to) AWS. It‚Äôs also helpful if you wish to test deployments on local developer machines running the same standard Linux distribution (a practice that‚Äôs getting more common with Docker, too. Amazon now supports an official Amazon Linux Docker image, aimed at assisting with local development on a comparable environment, though this is new enough that it should be considered experimental). Note that the currently-in-testing Amazon Linux 2 supports on-premise deployments explicitly.
EC2 costs: See the section on this.

EC2 Tips


üîπPicking regions: When you first set up, consider which regions you want to use first. Many people in North America just automatically set up in the us-east-1 (N. Virginia) region, which is the default, but it‚Äôs worth considering if this is best up front. You'll want to evaluate service availability (some services are not available in all regions), costing (baseline costs also vary by region by up to 10-30% (generally lowest in us-east-1 for comparison purposes)), and compliance (various countries have differing regulations with regard to data privacy, for example).


Instance types: EC2 instances come in many types, corresponding to the capabilities of the virtual machine in CPU architecture and speed, RAM, disk sizes and types (SSD or magnetic), and network bandwidth.

Selecting instance types is complex since there are so many types. Additionally there are different generations, released over the years.
üîπUse the list at ec2instances.info to review costs and features. Amazon‚Äôs own list of instance types is hard to use, and doesn‚Äôt list features and price together, which makes it doubly difficult.
Prices vary a lot, so use ec2instances.info to determine the set of machines that meet your needs and ec2price.com to find the cheapest type in the region you‚Äôre working in. Depending on the timing and region, it might be much cheaper to rent an instance with more memory or CPU than the bare minimum.



Turn off your instances when they aren‚Äôt in use. For many situations such as testing or staging resources, you may not need your instances on 24/7, and you won‚Äôt need to pay EC2 running costs when they are suspended. Given that costs are calculated based on usage, this is a simple mechanism for cost savings. This can be achieved using Lambda and CloudWatch, an open source option like cloudcycler, or a SaaS provider like GorillaStack. (Note: if you turn off instances with an ephemeral root volume, any state will be lost when the instance is turned off. Therefore, for stateful applications it is safer to turn off EBS backed instances).


Dedicated instances and dedicated hosts are assigned hardware, instead of usual virtual instances. They are more expensive than virtual instances but can be preferable for performance, compliance, financial modeling, or licensing reasons.


32 bit vs 64 bit: A few micro, small, and medium instances are still available to use as 32-bit architecture. You‚Äôll be using 64-bit EC2 (‚Äúamd64‚Äù) instances nowadays, though smaller instances still support 32 bit (‚Äúi386‚Äù). Use 64 bit unless you have legacy constraints or other good reasons to use 32.


HVM vs PV: There are two kinds of virtualization technology used by EC2, hardware virtual machine (HVM) and paravirtual (PV). Historically, PV was the usual type, but now HVM is becoming the standard. If you want to use the newest instance types, you must use HVM. See the instance type matrix for details.


Operating system: To use EC2, you‚Äôll need to pick a base operating system. It can be Windows or Linux, such as Ubuntu or Amazon Linux. You do this with AMIs, which are covered in more detail in their own section below.


Limits: You can‚Äôt create arbitrary numbers of instances. Default limits on numbers of EC2 instances per account vary by instance type, as described in this list.


‚ùóUse termination protection: For any instances that are important and long-lived (in particular, aren't part of auto-scaling), enable termination protection. This is an important line of defense against user mistakes, such as accidentally terminating many instances instead of just one due to human error.


SSH key management:

When you start an instance, you need to have at least one ssh key pair set up, to bootstrap, i.e., allow you to ssh in the first time.
Aside from bootstrapping, you should manage keys yourself on the instances, assigning individual keys to individual users or services as appropriate.
Avoid reusing the original boot keys except by administrators when creating new instances.
Avoid sharing keys and add individual ssh keys for individual users.



GPU support: You can rent GPU-enabled instances on EC2 for use in machine learning or graphics rendering workloads.

There are three types of GPU-enabled instances currently available:

The P3 series offers NVIDIA Tesla V100 GPUs in 1, 4 and 8 GPU configurations targeting machine learning, scientific workloads, and other high performance computing applications.
The P2 series offers NVIDIA Tesla K80 GPUs in 1, 8 and 16 GPU configurations targeting machine learning, scientific workloads, and other high performance computing applications.
The G3 series offers NVIDIA Tesla M60 GPUs in 1, 2, or 4 GPU configurations targeting graphics and video encoding.


AWS offers two different AMIs that are targeted to GPU applications. In particular, they target deep learning workloads, but also provide access to more stripped-down driver-only base images.

AWS offers both an Amazon Linux Deep Learning AMI (based on Amazon Linux) as well as an Ubuntu Deep Learning AMI. Both come with most NVIDIA drivers and ancillary software (CUDA, CUBLAS, CuDNN, TensorFlow, PyTorch, etc.) installed to lower the barrier to usage.
‚õì Note that using these AMIs can lead to lock in due to the fact that you have no direct access to software configuration or versioning.
üî∏ The compendium of frameworks included can lead to long instance startup times and difficult-to-reason-about environments.


üîπAs with any expensive EC2 instance types, Spot instances can offer significant savings with GPU workloads when interruptions are tolerable.



All current EC2 instance types can take advantage of IPv6 addressing, so long as they are launched in a subnet with an allocated CIDR range in an IPv6-enabled VPC.


EC2 Gotchas and Limitations

‚ùóNever use ssh passwords. Just don‚Äôt do it; they are too insecure, and consequences of compromise too severe. Use keys instead. Read up on this and fully disable ssh password access to your ssh server by making sure 'PasswordAuthentication no' is in your /etc/ssh/sshd_config file. If you‚Äôre careful about managing ssh private keys everywhere they are stored, it is a major improvement on security over password-based authentication.
üî∏For all newer instance types, when selecting the AMI to use, be sure you select the HVM AMI, or it just won‚Äôt work.
‚ùóWhen creating an instance and using a new ssh key pair, make sure the ssh key permissions are correct.
üî∏Sometimes certain EC2 instances can get scheduled for retirement by AWS due to ‚Äúdetected degradation of the underlying hardware,‚Äù in which case you are given a couple of weeks to migrate to a new instance

If your instance root device is an EBS volume, you can typically stop and then start the instance which moves it to healthy host hardware, giving you control over timing of this event. Note however that you will lose any instance store volume data (ephemeral drives) if your instance type has instance store volumes.
The instance public IP (if it has one) will likely change unless you're using Elastic IPs. This could be a problem if other systems depend on the IP address.


üî∏Periodically you may find that your server or load balancer is receiving traffic for (presumably) a previous EC2 server that was running at the same IP address that you are handed out now (this may not matter, or it can be fixed by migrating to another new instance).
‚ùóIf the EC2 API itself is a critical dependency of your infrastructure (e.g. for automated server replacement, custom scaling algorithms, etc.) and you are running at a large scale or making many EC2 API calls, make sure that you understand when they might fail (calls to it are rate limited and the limits are not published and subject to change) and code and test against that possibility.
‚ùóMany newer EC2 instance types are either EBS-only, or backed by local NVMe disks assigned to the instance. Make sure to factor in EBS performance and costs when planning to use them.
‚ùóIf you're operating at significant scale, you may wish to break apart API calls that enumerate all of your resources, and instead operate either on individual resources, or a subset of the entire list. EC2 APIs will time out! Consider using filters to restrict what gets returned.
‚ùó‚è± Instances come in two types: Fixed Performance Instances (e.g. M3, C3, and R3) and Burstable Performance Instances (e.g. T2). A T2 instance receives CPU credits continuously, the rate of which depends on the instance size. T2 instances accrue CPU credits when they are idle, and use CPU credits when they are active. However, once an instance runs out of credits, you'll notice a severe degradation in performance. If you need consistently high CPU performance for applications such as video encoding, high volume websites or HPC applications, it is recommended to use Fixed Performance Instances.
Instance user-data is limited to 16 KB. (This limit applies to the data in raw form, not base64-encoded form.) If more data is needed, it can be downloaded from S3 by a user-data script.
Very new accounts may not be able to launch some instance types, such as GPU instances, because of an initially imposed ‚Äúsoft limit‚Äù of zero. This limit can be raised by making a support request. See AWS Service Limits for the method to make the support request. Note that this limit of zero is not currently documented.
Since multiple AWS instances all run on the same physical hardware, early cloud adopters encountered what became known as the Noisy Neighbor problem. This feeling of not getting what you are paying for led to user frustration, however ""steal"" may not be the best word to describe what's actually happening based on a detailed explanation of how the kernel determine steal time. Avoiding having CPU steal affect your application in the cloud may be best handled by properly designing your cloud architecture.
AWS introduced Dedicated Tenancy in 2011. This allows customers to have all resources from a single server. Some saw this as a way to solve the noisy neighbor problem since only that customer uses the CPU. This approach comes with a significant risk if that physical system needed any type of maintenance. If a customer had 20 instances running using shared tenancy and one underlying server needed maintenance, only the instance on that server would go offline. If that customer had 20 instances running using dedicated tenancy, when the underlying server needs maintenance, all 20 instances would go offline.
üî∏Only i3.metal type instances providing an ability to run Android x86 emulators on AWS at the moment.

CloudWatch
CloudWatch Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing
CloudWatch monitors resources and applications, captures logs, and sends events.
CloudWatch monitoring is the standard mechanism for keeping tabs on AWS resources. A wide range of  metrics and dimensions are available via CloudWatch, allowing you to create time based graphs, alarms, and dashboards.

Alarms are the most practical use of CloudWatch, allowing you to trigger notifications from any given metric.
Alarms can trigger SNS notifications, Auto Scaling actions, or EC2 actions.
Alarms also support alerting when any M out of N datapoints cross the alarm threshold.
Publish and share graphs of metrics by creating customizable dashboard views.

Monitor and report on EC2 instance system check failure alarms.




Using CloudWatch Events:

Events create a mechanism to automate actions in various services on AWS. You can create event rules from instance states, AWS APIs, Auto Scaling, Run commands, deployments or time-based schedules (think Cron).
Triggered events can invoke Lambda functions, send SNS/SQS/Kinesis messages, or perform instance actions (terminate, restart, stop, or snapshot volumes).
Custom payloads can be sent to targets in JSON format, this is especially useful when triggering Lambdas.


Using CloudWatch Logs:

CloudWatch Logs is a streaming log storage system. By storing logs within AWS you have access to unlimited paid storage, but you also have the option of streaming logs directly to ElasticSearch or custom Lambdas.
A log agent installed on your servers will process logs over time and send them to CloudWatch Logs.
You can export logged data to S3 or stream results to other AWS services.
CloudWatch Logs can be encrypted using keys managed through KMS.


Detailed monitoring: Detailed monitoring for EC2 instances must be enabled to get granular metrics, and is billed under CloudWatch.

CloudWatch Alternatives and Lock-In

CloudWatch offers fairly basic functionality that doesn't create significant (additional) AWS lock-in. Most of the metrics provided by the service can be obtained through APIs that can be imported into other aggregation or visualization tools or services (many specifically provide CloudWatch data import services).
üö™ Alternatives to CloudWatch monitoring services include NewRelic, Datadog, Sumo Logic, Zabbix, Nagios, Ruxit, Elastic Stack, open source options such as StatsD or collectd with Graphite, and many others.
üö™ CloudWatch Log alternatives include Splunk, Sumo Logic, Loggly, LogDNA, Logstash, Papertrail, Elastic Stack, and other centralized logging solutions.

CloudWatch Tips

Some very common use cases for CloudWatch are billing alarms, instance or load balancer up/down alarms, and disk usage alerts.
You can use EC2Config to monitor watch memory and disk metrics on Windows platform instances. For Linux, there are example scripts that do the same thing.
You can publish your own metrics using the AWS API. Incurs additional cost.
You can stream directly from CloudWatch Logs to a Lambda or ElasticSearch cluster by creating subscriptions on Log Groups.
Don't forget to take advantage of the CloudWatch non-expiring free tier.

CloudWatch Gotchas and Limitations

üî∏Metrics in CloudWatch originate on the hypervisor. The hypervisor doesn't have access to OS information, so certain metrics (most notably memory utilization) are not available unless pushed to CloudWatch from inside the instance.
üî∏You can not use more than one metric for an alarm.
üî∏Notifications you receive from alarms will not have any contextual detail; they have only the specifics of the threshold, alarm state, and timing.
üî∏By default, CloudWatch metric resolution is 1 minute. If you send multiple values of a metric within the same minute, they will be aggregated into minimum, maximum, average and total (sum) per minute.
üê•In July 2017, a new high-resolution option was added for CloudWatch metrics and alarms. This feature allows you to record metrics with 1-second resolution, and to evaluate CloudWatch alarms every 10 seconds.

The blog post introducing this feature describes how to publish a high-resolution metric to CloudWatch. Note that when calling the PutMetricData API, StorageResolution is an attribute of each item you send in the MetricData array, not a direct parameter of the PutMetricData API call.


üî∏Data about metrics is kept in CloudWatch for 15 months, starting November 2016 (used to be 14 days). Minimum granularity increases after 15 days.

AMIs
AMI Basics

üìí User guide
AMIs (Amazon Machine Images) are immutable images that are used to launch preconfigured EC2 instances. They come in both public and private flavors. Access to public AMIs is either freely available (shared/community AMIs) or bought and sold in the AWS Marketplace.
Many operating system vendors publish ready-to-use base AMIs. For Ubuntu, see the Ubuntu AMI Finder. Amazon of course has AMIs for Amazon Linux.

AMI Tips

AMIs are built independently based on how they will be deployed. You must select AMIs that match your deployment when using them or creating them:

EBS or instance store
PV or HVM virtualization types
32 bit (‚Äúi386‚Äù) vs 64 bit (‚Äúamd64‚Äù) architecture


As discussed above, modern deployments will usually be with 64-bit EBS-backed HVM.
You can create your own custom AMI by snapshotting the state of an EC2 instance that you have modified.
AMIs backed by EBS storage have the necessary image data loaded into the EBS volume itself and don‚Äôt require an extra pull from S3, which results in EBS-backed instances coming up much faster than instance storage-backed ones.
AMIs are per region, so you must look up AMIs in your region, or copy your AMIs between regions with the AMI Copy feature.
As with other AWS resources, it‚Äôs wise to use tags to version AMIs and manage their lifecycle.
If you create your own AMIs, there is always some tension in choosing how much installation and configuration you want to ‚Äúbake‚Äù into them.

Baking less into your AMIs (for example, just a configuration management client that downloads, installs, and configures software on new EC2 instances when they are launched) allows you to minimize time spent automating AMI creation and managing the AMI lifecycle (you will likely be able to use fewer AMIs and will probably not need to update them as frequently), but results in longer waits before new instances are ready for use and results in a higher chance of launch-time installation or configuration failures.
Baking more into your AMIs (for example, pre-installing but not fully configuring common software along with a configuration management client that loads configuration settings at launch time) results in a faster launch time and fewer opportunities for your software installation and configuration to break at instance launch time but increases the need for you to create and manage a robust AMI creation pipeline.
Baking even more into your AMIs (for example, installing all required software as well and potentially also environment-specific configuration information) results in fast launch times and a much lower chance of instance launch-time failures but (without additional re-deployment and re-configuration considerations) can require time consuming AMI updates in order to update software or configuration as well as more complex AMI creation automation processes.


Which option you favor depends on how quickly you need to scale up capacity, and size and maturity of your team and product.

When instances boot fast, auto-scaled services require less spare capacity built in and can more quickly scale up in response to sudden increases in load. When setting up a service with autoscaling, consider baking more into your AMIs and backing them with the EBS storage option.
As systems become larger, it common to have more complex AMI management, such as a multi-stage AMI creation process in which few (ideally one) common base AMIs are infrequently regenerated when components that are common to all deployed services are updated and then a more frequently run ‚Äúservice-level‚Äù AMI generation process that includes installation and possibly configuration of application-specific software.


More thinking on AMI creation strategies here.
Use tools like Packer to simplify and automate AMI creation.
If you use RHEL instances and happen to have existing RHEL on-premise Red Hat subscriptions, then you could leverage Red Hat's Cloud Access program to migrate a portion of your subscriptions to AWS, and thereby not having AWS charge you for RHEL subscriptions a second time. You can either use your own self-created RHEL AMI's or Red Hat provided Gold Images that will be added to your private AMI's once you sign up for Red Hat Cloud Access.

AMI Gotchas and Limitations


üî∏Amazon Linux package versions: By default, instances based on Amazon Linux AMIs are configured point to the latest versions of packages in Amazon‚Äôs package repository. This means that the package versions that get installed are not locked and it is possible for changes, including breaking ones, to appear when applying updates in the future. If you bake your AMIs with updates already applied, this is unlikely to cause problems in running services whose instances are based on those AMIs ‚Äì breaks will appear at the earlier AMI-baking stage of your build process, and will need to be fixed or worked around before new AMIs can be generated. There is a ‚Äúlock on launch‚Äù feature that allows you to configure Amazon Linux instances to target the repository of a particular major version of the Amazon Linux AMI, reducing the likelihood that breaks caused by Amazon-initiated package version changes will occur at package install time but at the cost of not having updated packages get automatically installed by future update runs. Pairing use of the ‚Äúlock on launch‚Äù feature with a process to advance the Amazon Linux AMI at your discretion can give you tighter control over update behaviors and timings.


Cloud-Init Defaults: Oftentimes users create AMIs after performing customizations (albeit manually or via some tool such as Packer or Ansible).  If you're not careful to alter cloud-init settings that correspond to the system service (e.g. sshd, etc.) you've customized, you may find that your changes are no longer in effect after booting your new AMI for the first time, as cloud-init has overwritten them.
Some distros have different files than others, but all are generally located in /etc/cloud/, regardless of distro.  You will want to review these files carefully for your chosen distro before rolling your own AMIs.  A complete reference to cloud-init is available on the cloud-init site.  This is an advanced configuration mechanism, so test any changes made to these files in a sandbox prior to any serious usage.


Auto Scaling
Auto Scaling Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing at no additional charge
Auto Scaling Groups (ASGs) are used to control the number of instances in a service, reducing manual effort to provision or deprovision EC2 instances.
They can be configured through Scaling Policies to automatically increase or decrease instance counts based on metrics like CPU utilization, or based on a schedule.
There are three common ways of using ASGs - dynamic (automatically adjust instance count based on metrics for things like CPU utilization), static (maintain a specific instance count at all times), scheduled (maintain different instance counts at different times of day or on days of the week).
üí∏ASGs have no additional charge themselves; you pay for underlying EC2 and CloudWatch services.

Auto Scaling Tips

üí∏ Better matching your cluster size to your current resource requirements through use of ASGs can result in significant cost savings for many types of workloads.
Pairing ASGs with CLBs is a common pattern used to deal with changes in the amount of traffic a service receives.
Dynamic Auto Scaling is easiest to use with stateless, horizontally scalable services.
Even if you are not using ASGs to dynamically increase or decrease instance counts, you should seriously consider maintaining all instances inside of ASGs ‚Äì given a target instance count, the ASG will work to ensure that number of instances running is equal to that target, replacing instances for you if they die or are marked as being unhealthy. This results in consistent capacity and better stability for your service.
Autoscalers can be configured to terminate instances that a CLB or ALB has marked as being unhealthy.

Auto Scaling Gotchas and Limitations

üî∏ReplaceUnhealthy setting: By default, ASGs will kill instances that the EC2 instance manager considers to be unresponsive. It is possible for instances whose CPU is completely saturated for minutes at a time to appear to be unresponsive, causing an ASG with the default ReplaceUnhealthy setting turned on to replace them. When instances that are managed by ASGs are expected to consistently run with very high CPU, consider deactivating this setting. If you do so, however, detecting and killing unhealthy nodes will become your responsibility.

EBS
EBS Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
EBS (Elastic Block Store) provides block level storage. That is, it offers storage volumes that can be attached as filesystems, like traditional network drives.
EBS volumes can only be attached to one EC2 instance at a time. In contrast, EFS can be shared but has a much higher price point (a comparison).

EBS Tips

‚è±RAID: Use RAID drives for increased performance.
‚è±A worthy read is AWS‚Äô post on EBS IO characteristics as well as their performance tips.
‚è±One can provision IOPS (that is, pay for a specific level of I/O operations per second) to ensure a particular level of performance for a disk.
‚è±A single gp2 EBS volume allows 16k IOPS max To get the maximum performance out of a gp2 EBS volume, it has to be of a maximum size and attached to an EBS-optimized EC2 instance.
üí∏Standard and gp2 EBS volumes improve IOPS with size. It may make sense for you to simply enlarge a volume instead of paying for better performance explicitly. This can in many cases reduce costs by 2/3.
A standard block size for an EBS volume is 16kb.

EBS Gotchas and Limitations

‚ùóEBS durability is reasonably good for a regular hardware drive (annual failure rate of between 0.1% - 0.2%). On the other hand, that is very poor if you don‚Äôt have backups! By contrast, S3 durability is extremely high. If you care about your data, back it up to S3 with snapshots.
üî∏EBS has an SLA with 99.99% uptime. See notes on high availability below.
‚ùóEBS volumes have a volume type indicating the physical storage type. The types called ‚Äústandard‚Äù (st1 or sc1) are actually old spinning-platter disks, which deliver only hundreds of IOPS ‚Äî not what you want unless you‚Äôre really trying to cut costs. Modern SSD-based gp2 or io1 are typically the options you want.
‚ùóWhen restoring a snapshot to create an EBS volume, blocks are lazily read from S3 the first time they're referenced. To avoid an initial period of high latency, you may wish to use dd or fio as per the official documentation.

EFS
EFS Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
üê•EFS is Amazon‚Äôs network filesystem. It‚Äôs presented as an NFSv4.1 server. Any compatible NFSv4 client can mount it.
It is designed to be highly available and durable and each EFS file system object is redundantly stored across multiple availability zones.
EFS is designed to be used as a shared network drive and it can automatically scale up to petabytes of stored data and thousands of instances attached to it.
EFS can offer higher throughput (multiple gigabytes per second) and better durability and availability than EBS (see the comparison table), but with higher latency.
EFS is priced based on the volume of data stored, and costs much more than EBS; it's in the ballpark of three times as much compared to general purpose gp2 EBS volumes.
‚è± Performance is dependent on the volume of data stored, as is the price:

Like EBS, EFS uses a credit based system. Credits are earned at a rate of 50 KiB/s per GiB of storage and consumed in bursts during reading/writing files or metadata. Unlike EBS, operations on metadata (file size, owner, date, etc.) also consume credits. The BurstCreditBalance metric in CloudWatch should be monitored to make sure the file system doesn't run out of credits.
Throughput capacity during bursts is also dependent on size. Under 1 TiB, throughput can go up to 100 MiB/s. Above that, 100 MiB/s is added for each stored TiB. For instance, a file system storing 5 TiB would be able to burst at a rate of 500 MiB/s. Maximum throughput per EC2 instance is 250 MiB/s.
EFS has two performance modes that can only be set when a file system is created. One is ""General Purpose"", the other is ""Max I/O"". Max I/O scales higher, but at the cost of higher latency. When in doubt, use General Purpose, which is also the default. If the PercentIOLimit metric in CloudWatch hovers around 100%, Max I/O is recommended. Changing performance mode means creating a new EFS and migrating data.


High availability is achieved by having mount targets in different subnets / availability zones.

EFS Tips


With EFS being based on NFSv4.1, any directory on the EFS can be mounted directly, it doesn't have to be the root directory. One application could mount fs-12345678:/prog1, another fs-12345678:/prog2.


User and group level permissions can be used to control access to certain directories on the EFS file system.


‚è± Sharing EFS filesystems: One EFS filesystem can be used for multiple applications or services, but it should be considered carefully:
Pros:

Because performance is based on total size of stored files, having everything on one drive will increase performance for everyone. One application consuming credits faster than it can accumulate might be offset by another application that just stores files on EFS and rarely accesses them.

Cons:

Since credits are shared, if one application over-consumes them, it will affect the others.
A compromise is made with regards to security: all clients will have to have network access to the drive. Someone with root access on one client instance can mount any directory on the EFS and they have read-write access to all files on the drive, even if they don't have access to the applications hosted on other clients. There isn't a no-root-squash equivalent for EFS.



EFS Gotchas and Limitations

üî∏ A number of NFSv4.1 features are not supported and there are some limits to the service.
üî∏ As of 2017-08, EFS offers disk level encryption for new drives. For file systems created before that date, encryption can only be achieved by moving the data to a new EFS volume.
üî∏ An EFS file system can be mounted on premises over Direct Connect.
üî∏ An EFS file system can NOT be mounted over VPC peering or VPN, even if the VPN is running on top of Direct Connect.
üî∏ Using an EFS volume on Windows is not supported.
‚è± When a file is uploaded to EFS, it can take hours for EFS to update the details for billing and burst credit purposes.
üî∏‚è±  Metadata operations can be costly in terms of burst credit consumption. Recursively traversing a tree containing thousands of files can easily ramp up to tens or even hundreds of megabytes of burst credits being consumed, even if no file is being touched. Commands like find or chown -R can have an adverse impact on performance.

Load Balancers
Load Balancer Basics

AWS has 3 load balancing products - ‚ÄúClassic Load Balancers‚Äù (CLBs), ‚ÄúApplication Load Balancers‚Äù (ALBs), and ""Network Load Balancers"" (NLB).
Before the introduction of ALBs, ‚ÄúClassic Load Balancers‚Äù were known as ‚ÄúElastic Load Balancers‚Äù (ELBs), so older documentation, tooling, and blog posts may still reference ‚ÄúELBs‚Äù.
CLBs have been around since 2009, ALBs in 2016, NLBs were added in 2017 to AWS.
CLBs support TCP and HTTP load balancing. ALBs support HTTP load balancing only. NLBs support TCP layer 4 load balancing.
CLBs and ALBs can optionally handle termination for a single SSL certificate.
All can optionally perform active health checks of instances and remove them from the destination pool if they become unhealthy.
CLBs don't support complex / rule-based routing. ALBs support a (currently small) set of rule-based routing features. NLBs have most extensive routing options.
CLBs can only forward traffic to a single globally configured port on destination instances, while ALBs can forward to ports that are configured on a per-instance basis, better supporting routing to services on shared clusters with dynamic port assignment (like ECS or Mesos). NLBs support multiple ports on same IP; registering targets by IP address, including targets outside the VPC for the load balancer; ECS can select unused port for scheduling a task then register a target group using this port.
CLBs are supported in EC2 Classic as well as in VPCs while ALBs are supported in VPCs only.
ALBs can target groups of instances and IP based targets in the RFC1918 ranges allowing you to use on premise destinations via VPN or Direct Connect.

Load Balancer Tips

If you don‚Äôt have opinions on your load balancing up front, and don‚Äôt have complex load balancing needs like application-specific routing of requests, it‚Äôs reasonable just to use a CLB or ALB for load balancing instead.
Even if you don‚Äôt want to think about load balancing at all, because your architecture is so simple (say, just one server), put a load balancer in front of it anyway. This gives you more flexibility when upgrading, since you won‚Äôt have to change any DNS settings that will be slow to propagate, and also it lets you do a few things like terminate SSL more easily.
CLBs and ALBs have many IPs: Internally, an AWS load balancer is simply a collection of individual software load balancers hosted within EC2, with DNS load balancing traffic among them. The pool can contain many IPs, at least one per availability zone, and depending on traffic levels. They also support SSL termination, which is very convenient.
Scaling: CLBs and ALBs can scale to very high throughput, but scaling up is not instantaneous. If you‚Äôre expecting to be hit with a lot of traffic suddenly, it can make sense to load test them so they scale up in advance. You can also contact Amazon and have them ‚Äúpre-warm‚Äù the load balancer.
Client IPs: In general, if servers want to know true client IP addresses, load balancers must forward this information somehow. CLBs add the standard X-Forwarded-For header. When using a CLB as an HTTP load balancer, it‚Äôs possible to get the client‚Äôs IP address from this.
Using load balancers when deploying: One common pattern is to swap instances in the load balancer after spinning up a new stack with your latest version, keep old stack running for one or two hours, and either flip back to old stack in case of problems or tear it down.
Rotating Certificates while retaining ARN: Rotating IAM Server Certificates can be difficult as the standard practice is to upload a new one then update all resources with the new ARN. You can however retain the same ARN using the update-certificate call with the following process:


Upload a new IAM Server Certificate with a unique name (e.g fuzzy.com.new)
Rename the existing IAM Server Certificate (e.g fuzzy.com to fuzzy.com.expired)
Rename the new IAM Server Certificate to the name of the previously existing certificate (e.g fuzzy.com.new to fuzzy.com)
Jiggle the CLB/ALB Listener to pick up the change:

ALB: Invoke modify-listener with the existing details for the ALB Listener
CLB: Invoke create-load-balancer-listeners with the existing details for the CLB listener



Load Balancer Gotchas and Limitations

‚ùóCLBs and ALBs have no fixed external IP that all clients see. For most consumer apps this doesn‚Äôt matter, but enterprise customers of yours may want this. IPs will be different for each user, and will vary unpredictably for a single client over time (within the standard EC2 IP ranges). And similarly, never resolve a CLB name to an IP and put it as the value of an A record ‚Äî it will work for a while, then break!
‚ùóSome web clients or reverse proxies cache DNS lookups for a long time, which is problematic for CLBs and ALBs, since they change their IPs. This means after a few minutes, hours, or days, your client will stop working, unless you disable DNS caching. Watch out for Java‚Äôs settings and be sure to adjust them properly. Another example is nginx as a reverse proxy, which normally resolves backends only at start-up (although there is a way to get around this).
‚ùóIt‚Äôs not unheard of for IPs to be recycled between customers without a long cool-off period. So as a client, if you cache an IP and are not using SSL (to verify the server), you might get not just errors, but responses from completely different services or companies!
üî∏As an operator of a service behind a CLB or ALB, the latter phenomenon means you can also see puzzling or erroneous requests by clients of other companies. This is most common with clients using back-end APIs (since web browsers typically cache for a limited period).
‚ùóCLBs and ALBs take time to scale up, it does not handle sudden spikes in traffic well. Therefore, if you anticipate a spike, you need to ‚Äúpre-warm‚Äù the load balancer by gradually sending an increasing amount of traffic.
‚ùóTune your healthchecks carefully ‚Äî if you are too aggressive about deciding when to remove an instance and conservative about adding it back into the pool, the service that your load balancer is fronting may become inaccessible for seconds or minutes at a time. Be extra careful about this when an autoscaler is configured to terminate instances that are marked as being unhealthy by a managed load balancer.
‚ùóCLB HTTPS listeners don't support Server Name Indication (SNI). If you need SNI, you can work around this limitation by either providing a certificate with Subject Alternative Names (SANs) or by using TCP listeners and terminating SSL at your backend.
üî∏ There is a limit on the number of ALBs, CLBs and NLBs per region (separately). As of late 2017, the default limit for each is 20 per region. These limits can be easily raised for ALB and CLB, but AWS is quite reluctant to raise the limit on NLBs.
üî∏ If using a Network Load Balancer (NLB) then EC2 clients cannot connect to an NLB that resides in another VPC (VPC Peering) or AWS managed VPN unless the EC2 client is a C5, i3.metal or M5 instance type. For VPC peering, both VPCs must be in the same region. See Troubleshooting.

CLB
CLB Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
Classic Load Balancers, formerly known as Elastic Load Balancers, are HTTP and TCP load balancers that are managed and scaled for you by Amazon.

CLB Tips

Best practices: This article is a must-read if you use CLBs heavily, and has a lot more detail.

CLB Gotchas and Limitations

In general, CLBs are not as ‚Äúsmart‚Äù as some load balancers, and don‚Äôt have fancy features or fine-grained control a traditional hardware load balancer would offer. For most common cases involving sessionless apps or cookie-based sessions over HTTP, or SSL termination, they work well.
üî∏By default, CLBs will refuse to route traffic from a load balancer in one Availability Zone (AZ) to a backend instance in another. This will cause 503s if the last instance in an AZ becomes unavailable, even if there are healthy instances in other zones. If you‚Äôre running fewer than two backend instances per AZ, you almost certainly want to enable cross-zone load balancing.
üî∏Complex rules for directing traffic are not supported. For example, you can‚Äôt direct traffic based on a regular expression in the URL, like HAProxy offers.
Apex DNS names: Once upon a time, you couldn‚Äôt assign a CLB to an apex DNS record (i.e. example.com instead of foo.example.com) because it needed to be an A record instead of a CNAME. This is now possible with a Route 53 alias record directly pointing to the load balancer.
üî∏CLBs use HTTP keep-alives on the internal side. This can cause an unexpected side effect: Requests from different clients, each in their own TCP connection on the external side, can end up on the same TCP connection on the internal side. Never assume that multiple requests on the same TCP connection are from the same client!
üî∏ Traffic between CLBs and back-end instances in the same subnet will have Network ACL rules evaluated (EC2 to EC2 traffic in the same subnet would not have Network ACL rules evaluated). If the default '0.0.0.0/0 ALLOW' rule is removed from the Network ACL applied to the subnet, a rule that allows traffic on both the health check port and any listener port must be added.
As of December 2016, CLBs launched in VPCs do not support IPv6 addressing. CLBs launched in EC2-Classic support both IPv4 and IPv6 with the ""dualstack"" DNS name.

ALB
ALB Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
üê•Websockets and HTTP/2 are now supported.
üê•Internet Protocol Version 6 (IPv6) is now supported.
üê•Load Balancing via IP is now supported.
Prior to the Application Load Balancer, you were advised to use TCP instead of HTTP as the protocol to make it work (as described here) and use the obscure but useful Proxy Protocol (more on this) to pass client IPs over a TCP load balancer.

ALB Tips

Use ALBs to route to services that are hosted on shared clusters with dynamic port assignment (like ECS or Mesos).
ALBs support HTTP host-based routing (send HTTP requests for ‚Äúapi.mydomain.com‚Äù -> {target-group-1}, ‚Äúblog.mydomain.com‚Äù -> {target group 2}) as well as HTTP path-based routing (send HTTP requests for ‚Äú/api/*‚Äù ->  {target-group-1}, ‚Äú/blog/*‚Äù -> {target group 2}).

ALB Gotchas and Limitations

üî∏ALBs only support HTTP/2 over HTTPS (no plain-text HTTP/2).
üî∏ALBs only support HTTP/2 to external clients and not to internal resources (instances/containers).
ALBs support HTTP routing but not port-based TCP routing.
Instances in the ALB‚Äôs target groups have to either have a single, fixed healthcheck port (‚ÄúEC2 instance‚Äù-level healthcheck) or the healthcheck port for a target has to be the same as its application port (‚ÄúApplication instance‚Äù-level healthcheck) - you can't configure a per-target healthcheck port that is different than the application port.
ALBs are VPC-only (they are not available in EC2 Classic)
In a target group, if there is no healthy target, all requests are routed to all targets. For example, if you point a listener at a target group containing a single service that has a long initialization phase (during which the health checks would fail), requests will reach the service while it is still starting up.
üìú Although ALBs now support SNI, they only support 25 HTTPS certificates per Load Balancer. This limitation is not described here, so it might be subject to change.

Elastic Beanstalk
Elastic Beanstalk Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
EB (Elastic Beanstalk) is a PaaS (Platform as a Service) that helps developers create, deploy and scale web applications
EB handles deployment, configuration, provisioning, load balancing, auto-scaling, monitoring, and logging
EB creates AWS resources on your behalf but you retain full access and control of the underlying resources
üí∏ There is no cost to use EB but you will still be charged the full cost of the underlying AWS resources created by EB

Elastic Beanstalk Tips

To speed up deployment before launch or in a dev stage, turn off health checks and set the Deployment policy to All at once
If you have a configuration you want to re-use for multiple EB apps, you can save the current configuration using eb config save --cfg myEBConfig
By default, EB doesn't have any alarms. You'll need to add them yourself on metrics that you're monitoring.
By default, EB doesn't enable managed platform updates. Enable them in configuration to have EB automatically apply updates during a pre-specified maintenance window

Elastic Beanstalk Gotchas and Limitations

üî∏ Don't edit [apache|nginx] conf files manually on ec2 instances as they will be re-written on each deployment (use ebextensions instead)
üî∏ After creating an EB environment, it's no longer possible to change the Name tag
üî∏ EB will sometimes quarantine instances that cause multiple deployment issues. Despite being quarantined, EB will still deploy to them on subsequent deployments. To prevent this behavior, said instances will need to be terminated (or the underlying issue fixed)
File uploads are capped at 10MB for most default eb configurations - update nginx config to change
If you edit .elasticbeanstalk/saved_configs/, be aware that this is not kept in sync with the EB environment config. You'll need to manually fetch and save for changes to take effect

Elastic IPs
Elastic IP Basics

üìí Documentation ‚àô FAQ ‚àô Pricing
Elastic IPs are static IP addresses you can rent from AWS to assign to EC2 instances.

Elastic IP Tips

üîπPrefer load balancers to elastic IPs: For single-instance deployments, you could just assign elastic IP to an instance, give that IP a DNS name, and consider that your deployment. Most of the time, you should provision a load balancer instead:

It‚Äôs easy to add and remove instances from load balancers. It‚Äôs also quicker to add or remove instances from a load balancer than to reassign an elastic IP.
It‚Äôs more convenient to point DNS records to load balancers, instead of pointing them to specific IPs you manage manually. They can also be Route 53 aliases, which are easier to change and manage.
But in some situations, you do need to manage and fix IP addresses of EC2 instances, for example if a customer needs a fixed IP. These situations require elastic IPs.


Elastic IPs are limited to 5 per account. It‚Äôs possible to request more.
If an Elastic IP is not attached to an active resource there is a small hourly fee.
Elastic IPs are no extra charge as long as you‚Äôre using them. They have a (small) cost when not in use, which is a mechanism to prevent people from squatting on excessive numbers of IP addresses.

Elastic IP Gotchas and Limitations

üî∏There is officially no way to allocate a contiguous block of IP addresses, something you may desire when giving IPs to external users. Though when allocating at once, you may get lucky and have some be part of the same CIDR block. If this is important to you, you may want to bring your own IP, which is more involved than this guide will go into.

Glacier
Glacier Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Glacier is a lower-cost alternative to S3 when data is infrequently accessed, such as for archival purposes.
It‚Äôs only useful for data that is rarely accessed. It generally takes 3-5 hours to fulfill a retrieval request.
AWS has not officially revealed the storage media used by Glacier; it may be low-spin hard drives or even tapes.
AWS has released an even more cost effective storate tier called Glacier Deep Archive that offers ~12 hour retrieval latencies, but costs roughly a thousand dollars per month per petabyte.

Glacier Tips

You can physically ship your data to Amazon to put on Glacier on a USB or eSATA HDD.

Glacier Gotchas and Limitations

üî∏Getting files off Glacier is glacially slow (typically 3-5 hours or more).
üî∏Due to a fixed overhead per file (you pay per PUT or GET operation), uploading and downloading many small files on/to Glacier might be very expensive. There is also a 32k storage overhead per file. Hence it‚Äôs a good idea is to archive files before upload.
üí∏Be aware of the per-object costs of archiving S3 data to Glacier. It costs $0.05 per 1,000 requests. If you have large numbers of S3 objects of relatively small size, it will take time to reach a break-even point (initial archiving cost versus lower storage pricing).

RDS
RDS Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing (see also ec2instances.info/rds/)
RDS is a managed relational database service, allowing you to deploy and scale databases more easily. It supports Oracle, Microsoft SQL Server, PostgreSQL, MySQL, MariaDB, and Amazon‚Äôs own Aurora.
RDS offers out of the box support for high availability and failover for your databases.

RDS Tips

If you're looking for the managed convenience of RDS for other data stores such as MongoDB or Cassandra, you may wish to consider third-party services from providers such as mLab, Compose, or InstaClustr.
üîπMake sure to create a new parameter group and option group for your database since the default parameter group does not allow dynamic configuration changes.
RDS instances start with a default timezone of UTC. If necessary, this can be changed to a different timezone.

RDS Gotchas and Limitations

‚è±RDS instances run on EBS volumes (either general-purpose or provisioned IOPS), and hence are constrained by EBS performance.
üî∏Verify what database features you need, as not everything you might want is available on RDS. For example, if you are using Postgres, check the list of supported features and extensions. If the features you need aren't supported by RDS, you'll have to deploy your database yourself.
üî∏If you use the failover support offered by RDS, keep in mind that it is based on DNS changes, and make sure that your client reacts to these changes appropriately. This is particularly important for Java, given how its DNS resolver‚Äôs TTL is configured by default.
üî∏DB migration to RDS: While importing your database into RDS ensure you take into consideration the maintenance window settings. If a backup is running at the same time, your import can take a considerably longer time than you would have expected.
Database sizes are limited to 6TB for all database engines except for SQL Server which has a 4TB limit and Aurora which supports up to 64TB databases.

RDS MySQL and MariaDB
RDS MySQL and MariaDB Basics


 RDS offers MySQL versions 5.5, 5.6, 5.7 and 5.8.



 RDS offers MariaDB versions 10.0, 10.1, 10.2 and 10.3.



RDS MySQL and MariaDB Tips

MySQL RDS allows access to binary logs.
Multi-AZ instances of MySQL transparently replicate data across AZs using DRBD. Automated backups of multi-AZ instances run off the backup instance to reduce latency spikes on the primary.
üî∏Performance Schema: While Performance Schema is enabled by default in MySQL 5.6.6 and later, it is disabled by default in all versions of RDS. If you wish to enable Performance Schema, a reboot of the RDS instance will be required.
üî∏MySQL vs MariaDB vs Aurora: If you prefer a MySQL-style database but are starting something new, you probably should consider Aurora and MariaDB as well. Aurora has increased availability and is the next-generation solution. That said, Aurora may not be that much faster than MySQL for certain workloads. MariaDB, the modern community fork of MySQL, likely now has the edge over MySQL for many purposes and is supported by RDS.

RDS MySQL and MariaDB Gotchas and Limitations

üî∏No SUPER privileges. RDS provides some stored procedures to perform some tasks that require SUPER privileges such as starting or stopping replication.
üî∏You can replicate to non-RDS instances of MySQL, but replication to these instances will break during AZ failovers.
üî∏There is no ability to manually CHANGE MASTER on replicas, so they must all be rebuilt after a failover of the master.
üî∏Most global options are exposed only via DB parameter groups. Some variables that were introduced in later MySQL dot releases such as avoid_temporal_upgrade in MySQL 5.6.24 are not made available in RDS's 5.6.x parameter group and making use of them requires an upgrade to MySQL 5.7.x.
üî∏RDS features such as Point-In-Time restore and snapshot restore are not supported on MyISAM tables. Ensure you lock and flush each MyISAM table before executing a snapshot or backup operation to ensure consistency.

RDS PostgreSQL
RDS PostgreSQL Basics

RDS offers PostgreSQL 9.3, 9.4, 9.5, 9.6, and 10.

RDS PostgreSQL Tips

Recently Logical Replication is being supported, both as subscriber and publisher.
Supports a relatively large range of native extensions.
RDS PostgreSQL 10 Supports native partitioning and most of the major features and tunables.
Supports connections over SSL.
Supports multi A-Z and Point-in-time recovery.

RDS PostgreSQL Gotchas and Limitations

No superuser privileges. RDS provides a role rds_superuser that can do most of the needed operations but there are some limitations.
Some major features are delayed compared to open source PostgreSQL.
By default RDS is spec‚Äôd with general purpose SSD , if you need better performance you have to spec provisioned IOPS SSD.
You can't use RDS as a replica outside RDS without using logical replication.
There are settings that cannot be changed and most of the settings that can change can only be changed using database parameter groups.
It‚Äôs harder to troubleshoot performance problems since you have no access to the host.
Be sure to verify that all the extensions you need are available. If you are using an extension not listed there, you will need to come up with a work around, or deploy your own database in EC2.
Many Postgres utilities and maintenance items expect command line access, that can usually be satisfied by using an external ec2 server.

RDS SQL Server
RDS SQL Server Basics

RDS offers SQL Server 2008 R2, 2012, 2014, 2016 and 2017 including Express, Web, Standard and Enterprise.

RDS SQL Server Tips

Recently added support for backup and restore to/from S3 which may make it an attractive DR option for on-premises installations.

RDS SQL Server Gotchas and Limitations

üî∏The user is granted only db_owner privileges for each database on the instance.
üî∏Storage cannot be expanded for existing databases. If you need more space, you must restore your database on a new instance with larger storage.
üî∏There is a 16TB database size limit for non-Express editions. There is also a minimum storage size, 20GB for Web and Express, 200GB for Standard and Enterprise.
üî∏Limited to 30 databases per instance

RDS Aurora
RDS Aurora Basics
Aurora is a cloud only database service designed to provide a distributed, fault-tolerant relational database with self-healing storage and auto-scaling up to 64TB per instance.  It currently comes in two versions, a MySQL compatible system, and a PostgreSQL compatible system.
RDS Aurora MySQL
RDS Aurora MySQL Basics

Amazon‚Äôs proprietary fork of MySQL intended to scale up for high concurrency workloads. Generally speaking, individual query performance under Aurora is not expected to improve significantly relative to MySQL or MariaDB, but Aurora is intended to maintain performance while executing many more queries concurrently than an equivalent MySQL or MariaDB server could handle.
Notable new features include:

Log-structured storage instead of B-trees to improve write performance.
Out-of-process buffer pool so that databases instances can be restarted without clearing the buffer pool.
The underlying physical storage is a specialized SSD array that automatically maintains 6 copies of your data across 3 AZs.
Aurora read replicas share the storage layer with the write master which significantly reduces replica lag, eliminates the need for the master to write and distribute the binary log for replication, and allows for zero-data-loss failovers from the master to a replica. The master and all the read replicas that share storage are known collectively as an Aurora cluster. Read replicas can span up to 5 regions.



RDS Aurora MySQL Tips

In order to take advantage of Aurora‚Äôs higher concurrency, applications should be configured with large database connection pools and should execute as many queries concurrently as possible. For example, Aurora servers have been tested to produce increasing performance on some OLTP workloads with up to 5,000 connections.
Aurora scales well with multiple CPUs and may require a large instance class for optimal performance.
The easiest migration path to Aurora is restoring a database snapshot from MySQL 5.6 or 5.7. The next easiest method is restoring a dump from a MySQL-compatible database such as MariaDB. For low-downtime migrations from other MySQL-compatible databases, you can set up an Aurora instance as a replica of your existing database. If none of those methods are options, Amazon offers a fee-based data migration service.
You can replicate from an Aurora cluster to MySQL or to another Aurora cluster. This requires binary logging to be enabled and is not as performant as native Aurora replication.
Because Aurora read replicas are the equivalent of a multi-AZ backup and they can be configured as zero-data-loss failover targets, there are fewer scenarios in which the creation of a multi-AZ Aurora instance is required.

RDS Aurora MySQL Gotchas and Limitations

üî∏Aurora 1.x is based on MySQL 5.6.x with some cherry-picking of later MySQL features. It is missing most 5.7 features as well as some online DDL features introduced in 5.6.17.
üî∏Aurora 2.x is based on MySQL 5.7.x
Aurora does not support GTID transactions in either the 5.6/Aurora 1.x or the 5.7/Aurora 2.x release lines.
Aurora maximum cluster size is 64 TB

RDS Aurora PostgreSQL
RDS Aurora PostgreSQL Basics

Amazon‚Äôs proprietary fork of PostgreSQL, intended to scale up for high concurrency workloads while maintaining ease of use. Currently based on PostgreSQL 9.6.
Higher throughput (up to 3x with similar hardware).
Automatic storage scale in 10GB increments up to 64TB.
Low latency read replicas that share the storage layer with the master which significantly reduces replica lag.
Point in time recovery.
Fast database snapshots.

RDS Aurora PostgreSQL Tips

Aurora Postgres by default is supposed to utilize high connection rates and for this reason connection pooling must be configured accordingly.
Because Aurora is based on PostgreSQL 9.6, it lacks features like declarative partitioning or logical replication.

RDS Aurora PostgreSQL Gotchas and Limitations

Aurora PostgreSQL falls behind normal RDS when it comes to available versions, so if you need features from the latest PostgreSQL version you might be better off with plain RDS.
Patching and bug fixing is separate from open source PostgreSQL.

ElastiCache
ElastiCache Basics

üìí Homepage ‚àô User
guide for Redis ‚àô User
guide for Memcached ‚àô
FAQ ‚àô
Pricing
ElastiCache is a managed in-memory cache service, that can be used to
store temporary data in a fast in-memory cache, typically in order to avoid
repeating the same computation multiple times when it could be reused.
It supports both the Memcached and
Redis open source in-memory cache software and exposes
them both using their native access APIs.
The main benefit is that AWS takes care of running, patching and optimizing
the cache nodes for you, so you just need to launch a cluster and configure
its endpoint in your application, while AWS will take of most of the operational
work of running the cache nodes.

ElastiCache Tips

Choose the
engine,
clustering configuration and instance
type
carefully based on your application needs. The documentation explains in
detail the pros, cons and limitations of each engine in order to help you
choose the best fit for your application. In a nutshell, Redis is
preferable for storing more complex data structures, while Memcached is just a
plain key/value store. The simplicity of Memcached allows it to be slightly
faster and allows it to scale out if needed, but Redis has more features which
you may use in your application.
For Memcached AWS provides enhanced SDKs for certain programming languages
which implement
auto-discovery,
a feature not available in the normal memcached client libraries.

ElastiCache Gotchas and Limitations

Since in some cases changing the cache clusters may have some restrictions,
like for
scaling
purposes, it may become a problem if they were launched using CloudFormation
in a stack that also contains other resources and you really need to change
the cache. In order to avoid getting your CloudFormation stacks in a
non-updateable state, it is recommended to launch ElastiCache clusters (just
like any other resource with similar constraints) in dedicated stacks which
can be replaced entirely with new stacks having the desired configuration.

DynamoDB
DynamoDB Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
DynamoDB is a NoSQL database with focuses on speed, flexibility, and scalability.
DynamoDB is priced on a combination of throughput and storage.

DynamoDB Alternatives and Lock-in

‚õì Unlike the technologies behind many other Amazon products, DynamoDB is a proprietary AWS product with no interface-compatible alternative available as an open source project. If you tightly couple your application to its API and featureset, it will take significant effort to replace.
The most commonly used alternative to DynamoDB is Cassandra.

DynamoDB Tips

There is a local version of DynamoDB provided for developer use.
DynamoDB Streams provides an ordered stream of changes to a table. Use it to replicate, back up, or drive events off of data
DynamoDB can be used as a simple locking service.
DynamoDB indexing can include primary keys, which can either be a single-attribute hash key or a composite hash-key range. You can also query non-primary key attributes using secondary indexes.
Data Types: DynamoDB supports three data types ‚Äì number, string, and binary ‚Äì in both scalar and multi-valued sets. DynamoDB can also support JSON.
As of late 2017, DynamoDB supports both global tables and backup / restore functionality.

DynamoDB Gotchas and Limitations

üî∏ DynamoDB doesn‚Äôt provide an easy way to bulk-load data (it is possible through Data Pipeline) and this has some unfortunate consequences. Since you need to use the regular service APIs to update existing or create new rows, it is common to temporarily turn up a destination table‚Äôs write throughput to speed import. But when the table‚Äôs write capacity is increased, DynamoDB may do an irreversible split of the partitions underlying the table, spreading the total table capacity evenly across the new generation of tables. Later, if the capacity is reduced, the capacity for each partition is also reduced but the total number of partitions is not, leaving less capacity for each partition. This leaves the table in a state where it much easier for hotspots to overwhelm individual partitions.
üî∏ It is important to make sure that DynamoDB resource limits are compatible with your dataset and workload. For example, the maximum size value that can be added to a DynamoDB table is 400 KB (larger items can be stored in S3 and a URL stored in DynamoDB).
üî∏ Dealing with time series data in DynamoDB can be challenging. A global secondary index together with down sampling timestamps can be a possible solution as explained here.
üî∏ DynamoDB does not allow an empty string as a valid attribute value. The most common work-around is to use a substitute value instead of leaving the field empty.
üî∏ When setting up fine grained policies for access to DynamoDB tables, be sure to include their secondary indices in the policy document as well.

ECS
ECS Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
ECS (EC2 Container Service) is a relatively new service (launched end of 2014) that manages clusters of services deployed via Docker.
See the Containers and AWS section for more context on containers.
ECS is growing in adoption, especially for companies that embrace microservices.
Deploying Docker directly in EC2 yourself is another common approach to using Docker on AWS. Using ECS is not required, and ECS does not (yet) seem to be the predominant way many companies are using Docker on AWS.
It‚Äôs also possible to use Elastic Beanstalk with Docker, which is reasonable if you‚Äôre already using Elastic Beanstalk.
Using Docker may change the way your services are deployed within EC2 or Elastic Beanstalk, but it does not radically change how most other services are used.
ECR (EC2 Container Registry) is Amazon‚Äôs managed Docker registry service. While simpler than running your own registry, it is missing some features that might be desired by some users:

Doesn‚Äôt support cross-region replication of images.

If you want fast fleet-wide pulls of large images, you‚Äôll need to push your image into a region-local registry.


Doesn‚Äôt support custom domains / certificates.


A container‚Äôs health is monitored via CLB or ALB. Those can also be used to address a containerized service. When using an ALB you do not need to handle port contention (i.e. services exposing the same port on the same host) since an ALB‚Äôs target groups can be associated with ECS-based services directly.
The Hitchhikers Guide to AWS ECS and Docker by J. Cole Morrison is an excellent article for Introduction to AWS ECS concepts.

ECS Tips

Log drivers: ECS supports multiple log drivers (awslogs, splunk, fluentd, syslog, json, ... ). Use awslogs for CloudWatch (make sure a group is made for the logs first). Drivers such as fluentd are not enabled by default. You can, install the agent and enable the driver by adding ECS_AVAILABLE_LOGGING_DRIVERS='[""awslogs"",""fluentd""]' to /etc/ecs/ecs.config.
This blog from Convox (and commentary) lists a number of common challenges with ECS as of early 2016.
It is possible to optimize disk clean up on ECS. By default, the unused containers are deleted after 3 hours and the unused images after 30 minutes. These settings can be changed by adding ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=10m and ECS_IMAGE_CLEANUP_INTERVAL=10m to /etc/ecs/ecs.config. More information on optimizing ECS disk cleanup.

ECS Alternatives and Lock-in

Kubernetes: Extensive container platform. Available as a hosted solution on Google Cloud (https://cloud.google.com/container-engine/) and AWS (https://tectonic.com/). AWS has a Kubernetes Quickstart (https://aws.amazon.com/quickstart/architecture/heptio-kubernetes/) developed in collaboration with Heptio.
Nomad: Orchestrator/Scheduler, tightly integrated in the Hashicorp stack (Consul, Vault, etc).

üöß Please help expand this incomplete section.
EKS
EKS Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
EKS (Elastic Kubernetes Service) is a new service (launched June 2018) that provides managed Kubernetes Masters in a Highly Available pair to deploy K8s Services and Pods on top of EC2 based Kubernetes nodes.
See the Containers and AWS section for more context on containers.
EKS is AWS's solution to hosting Kubernetes natively on AWS. It is not a replacement for ECS directly but is in response to the large market dominance of Kubernetes.
EKS does not launch EC2 nodes and would have to be configured and setup either manually or via Cloudformation (or other automation solution)
EKS management is done through a utility called kubectl, and with Kube configuration files. These files will need to be configured to speak with the K8s Master with a certificate and URL. The AWS CLI can autogenerate the configuration file that kubect requires for communicating with the cluster.1
EKS authentication is integrated with IAM roles/permissions. The AWS CLI has an integrated sub-command for generating authentication tokens.2 This was formerly done via a custom plugin for kubectl called aws-iam-authenticator (formerly heptio-authenticator-aws).
EKS provides Calico from Tigera for securing workloads within a cluster using Kubernetes network policy.

EKS Tips

Multiple clusters can be supported by using different kubeconfig files.
AWS has a Kubernetes Quickstart developed in collaboration with Heptio.

EKS Alternatives and Lock-in

ECS: Amazon's native Container Scheduled platform released in 2014.  If you don't utilise containers today and are looking to get started, ECS is an excellent product.
Kubernetes: Extensive container platform. Available as a hosted solution on Google Cloud, AWS, Digital Ocean and Azure.
Nomad: Orchestrator/Scheduler, tightly integrated in the Hashicorp stack (Consul, Vault, etc).

EKS Gotchas and Limitations

Pods and Service configurations can rapidly consume IP addresses inside a VPC.  Proper care and maintenance should be applied to ensure IP exhaustion does not occur.
There is currently no integrated monitoring in CloudWatch for EKS pods or services, you will need to deploy a monitoring system that supports Kubernetes such as Prometheus.
Autoscaling based off CPU/Memory of a node is limited as you will not be aware of pending Services/Pods that cannot start. Using cluster-autoscaler can be useful for scaling based on Node resource usage and unschedulable Pods.
Prometheus is a very popular monitoring solution for K8s, metrics and alerts can be used to send events to Lambda, SQS or other solutions to take autoscaling actions.

Footnotes
1: https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html
2: https://aws.amazon.com/about-aws/whats-new/2019/05/amazon-eks-simplifies-kubernetes-cluster-authentication/
Fargate
Fargate Basics

üìí Homepage ‚àô FAQ ‚àô Pricing
Fargate allows you to manage and deploy containers without having to worry about running the underlying compute infrastructure
Fargate serves as a new backend (in addition to the legacy EC2 backend) on which ECS and EKS tasks can be run
Fargate and EC2 backends are called ""Launch Types""
Fargate allows you to treat containers as fundamental building blocks of your infrastructure

Fargate Tips

Fargate follows a similar mindset to Lambda, which lets you focus on applications, instead of dealing with underlying infrastructure
Fargate is supported by CloudFormation, aws-cli and ecs-cli
Fargate tasks can be launched alongside tasks that use EC2 Launch Type
üí∏Before creating a large Fargate deployment, make sure to estimate costs and compare them against alternative solution that uses traditional EC2 deployment - Fargate prices can be several times those of equivalently-sized EC2 instances. To evaluate both solutions based on potential costs, refer to pricing for EC2 and Fargate.

Fargate Alternatives and Lock-in

üö™Azure Container Instances: Available on Microsoft Azure in preview version, allows to run applications in containers without having to manage virtual machines

Fargate Gotchas and Limitations

As of April 2018, Fargate is available in multiple regions: us-east-1, us-east-2, us-west-2, and eu-west-1
As of January 2019, Fargate can only be used with ECS. Support for EKS was originally planned for 2018, but has yet to launch.
The smallest resource values that can be configured for an ECS Task that uses Fargate is 0.25 vCPU and 0.5 GB of memory
Task storage is ephemeral. After a Fargate task stops, the storage is deleted.

Lambda
Lambda Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Lambda is AWS' serverless compute offering, allowing users to define Lambda functions in a selection of runtimes that can be invoked via a variety of triggers, including SNS notifications and API Gateway invocations. Lambda is the key service that enables 'serverless' architecture on AWS, alongside AWS API Gateway, AWS Batch, and AWS DynamoDB.

Lambda Tips

The idea behind 'serverless' is that users don't manage provisioning, scaling, or maintenance of the physical machines that host their application code. With Lambda, the machine that actually executes the user-defined function is abstracted as a 'container'. When defining a Lambda function, users are able to declare the amount of memory available to the function, which directly affects the physical hardware specification of the Lambda container.
Changing the amount of memory available to your Lambda functions also affects the amount of CPU power available to it.
While AWS does not offer hard guarantees around container reuse, in general it can be expected that an unaltered Lambda function will reuse a warm (previously used) container if called shortly after another invocation. Users can use this as a way to optimize their functions by smartly caching application data on initialization.
A Lambda that hasn't been invoked in some time may not have any warm containers left. In this case, the Lambda system will have to load and initialize the Lambda code in a 'cold start' scenario, which can add significant latency to Lambda invocations.
There are a few strategies to avoiding or mitigating cold starts, including keeping containers warm by periodic triggering and favoring lightweight runtimes such as Node as opposed to Java.
Lambda is integrated with AWS CloudWatch and provides a logger at runtime that publishes CloudWatch events.
Lambda offers out-of-the-box opt-in support for AWS X-Ray. X-Ray can help users diagnose Lambda issues by offering in-depth analysis of their Lambda's execution flow. This is especially useful when investigating issues calling other AWS services as X-Ray gives you a detailed and easy-to-parse visualization of the call graph.
Using timed CloudWatch events, users can use Lambda to run periodic jobs in a cron-like manner.
Events sent to Lambda that fail processing can be managed using a Dead Letter Queue (DLQ) in SQS.
More on serverless:

Martin Fowler's thoughts.
AWS Serverless Application Model (SAM), a simplification built on top of CloudFormation that can help to define, manage, and deploy serverless applications using Lambda.
Serverless, one of the most popular frameworks for building serverless applications using AWS Lambda and other serverless compute options.
Other helpful frameworks.



Lambda Alternatives and Lock-in

üö™Other clouds offer similar services with different names, including Google Cloud Functions, Azure Functions, and IBM OpenWhisk. Also if your are running Kubernetes another Lambda alternative is OpenFaaS

Lambda Gotchas and Limitations

üî∏Testing Lambdas, locally and remotely, can be a challenge. Several tools are available to make this easier, including the officially supported SAM Local.
üî∏Managing lots of Lambda functions is a workflow challenge, and tooling to manage Lambda deployments is still immature.
üî∏AWS‚Äô official workflow around managing function versioning and aliases is painful. One option is to avoid Lambda versioning by abstracting your deployment workflow outside of Lambda. One way this can be accomplished is by deploying your application in successive stages, with a distinct AWS account per stage, where each account only needs to be aware of the latest version, and rollbacks and updates are handled by external tooling.
üî∏As of Oct 2017, the minimum charge for a Lambda invocation is 100ms, so there is no cost-benefit to reducing your run time below that.
üî∏While adding/removing S3 buckets as triggers for Lambda function, this error may occur: ""There was an error creating the trigger: Configuration is ambiguously defined. Cannot have overlapping suffixes in two rules if the prefixes are overlapping for the same event type."" In this case, you can manually remove the Lambda event in the ""Events"" tab in the ""Properties"" section of the S3 bucket.
üî∏Managing the size of your deployment artifact can be a challenge, especially if using Java. Options to mitigate this include proguard and loading dependencies at runtime into /tmp.
When using DynamoDB as a trigger for your Lambda functions, this error may occur: ""PROBLEM: internal Lambda error. Please contact Lambda customer support."" This usually just means that Lambda can't detect anything in the DynamoDB stream within the last 48 hours. If the issue persists, deleting and recreating your trigger may help.
üî∏If your lambda needs access to resources in a VPC (for example ElastiCache or RDS), it will need to be deployed within it. This will increase cold-start times as an Elastic Network Interface (ENI) will have to be registered within the VPC for each concurrent function. AWS also has a relatively low initial limit (350) on the number ENI's that can be created within an VPC, however this can be increased to the 1000s if a good case is made to AWS support.
üî∏ Lambda has several resource limits as of 2017-06:

A 6MB request or response payload size.
A 50 MB limit on the compressed .zip/.jar file deployment package size.
A 250 MB limit on the code/dependencies in the package before compression.
A 500 MB limit on local storage in /tmp.



Lambda Code Samples

Fan-out is an example of using Lambda to ‚Äúfan-out‚Äù or copy data from one service, in this case Kinesis, to multiple other AWS data services. Destinations for fan-out data in the sample include IoT, SQS and more.
This AWS limit monitor using Lambdas shows use of multiple Lambdas for monitoring.
This Lambda ECS Worker Pattern shows use of Lambda in a workflow where data from S3 is picked up by the Lambda, pushed to a queue, then sent to ECS for more processing.
The Secure Pet Store is a sample Java application which uses Lambda and API Gateway with Cognito (for user identity).
aws-lambda-list is a list of ""hopefully useful AWS lambdas and lambda-related resources"". Quite a few code samples here; as usual, not guaranteed tested. Caveat Emptor.

üöß Please help expand this incomplete section.
API Gateway
API Gateway Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
API Gateway provides a scalable, secured front-end for service APIs, and can work with Lambda, Elastic Beanstalk, or regular EC2 services.
It allows ‚Äúserverless‚Äù deployment of applications built with Lambda.
üî∏Switching over deployments after upgrades can be tricky. There are no built-in mechanisms to have a single domain name migrate from one API gateway to another one. So it may be necessary to build an additional layer in front (even another API Gateway) to allow smooth migration from one deployment to another.

API Gateway Alternatives and Lock-In

Kong is an open-source, on-premises API and microservices gateway built on nginx with Lua. Kong is extensible through ‚Äúplugins‚Äù.
Tyk is an open-source API gateway implemented in Go and available in the cloud, on-premises or hybrid.

API Gateway Tips

üîπPrior to 2016-11, you could only send and receive plain text data (so people would base64-encode binary data), but binary data is now supported.
API Gateway supports the OpenApi specification (aka Swagger). This allows you to describe your API in a language-agnostic way and use various tools to generate code supporting your API.
Generating clients is extremely easy, either through the AWS console or using the get-sdk API.
API Gateway integrates with CloudWatch out-of-the-box, allowing for easy logging of requests and responses.

Note that if your request or response are too large, CloudWatch will truncate the log. For full request/reply logging, make sure to do so in your integration (e.g. Lambda).
A good practice when calling API Gateway APIs is to log the request ID on the client. You can later refer to these request IDs in CloudWatch for easier tracing and debugging.


There are multiple ways to secure your API, including built-in support for AWS Cognito. For most use-cases, Cognito is the easiest and simplest way to authenticate users.

Although you can roll your own solution using a custom authorizer, which is basically a Lambda you define that determines if a request is acceptable or not.


While API Gateway lends itself well to REST-style development, it's perfectly reasonable to implement an RPC-style API in API Gateway as well. Depending on your use-case, this can often lead to a much simpler API structure and smoother client experience.

RPC-style APIs are particularly useful when designing services that sit deeper in the stack and don't serve content directly to users.



API Gateway Gotchas and Limitations

üî∏API Gateway only supports encrypted (https) endpoints, and does not support unencrypted HTTP. (This is probably a good thing.)
üî∏API Gateway doesn‚Äôt support multi-region deployments for high availability. It is a service that is deployed in a single region but comes with a global endpoint that is served from AWS edge locations (similar to a CloudFront distribution). You cannot have multiple API Gateways with the same hostname in different AWS regions and use Route 53 to distribute the traffic. More in this forum post.
üî∏Integration timeout: All of the various integration types (eg: Lambda, HTTP) for API Gateway have timeouts, as described here. Unlike some limits, these timeouts can't be increased.
üî∏API Gateway returns a 504 status code for any network or low level transport related issue. When this happens, you may see a message in the CloudWatch logs for the request that includes the message: Execution failed due to an internal error. One possible reason for this error is that even though your backend server is up and running, it may be doing something outside of the HTTP specification (like not sending well-formed chunked messages). You can test by hitting your backend directly with the curl --raw -S -i <backend-endpoint-url> and seeing if it complains.
üî∏AWS X-Ray support exists but cumbersome to use. If you have other AWS services calling API Gateway, your trace will seemingly end there. API Gateway will also not appear as a node in your service map. More here.
üî∏Be careful using the export feature. The resulting Swagger template is often incomplete and doesn't integrate well with the Swagger extensions for things such as CORS.
üî∏Many changes to API Gateway resources need to be 'deployed' via console or API call. Unfortunately, API Gateway is terrible about notifying the user when changes are staged for deployment and what changes require deployment. If you've changed something about your API and it's not taking effect, there's a decent chance you just need to deploy it.

In particular, when deploying an API Gateway as part of a CloudFormation stack, changes will not automatically deploy unless the deployment resource itself was changed. You can change work around this by always changing the deployment resource on a CloudFormation update, or running a custom resource that ensures the deployment is made.
Alternatively, by using the Serverless Application Model definition for an API Gateway resource, you can always expect the API to be deployed on a stack update since SAM will generate a new deployment every time.


üî∏API Gateway does not support nested query parameters on method requests.
üî∏API Gateway limits number of resources to 300, as described here. This is something to be considered when you start using API Gateway as a platform where your team/organization deploys to the same API Gateway.

üöß Please help expand this incomplete section.
Step Functions
Step Functions Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Step Functions is AWS‚Äô way to create state machines that manage a serverless workflow.

Step Functions Tips

A variety of structures are supported including branching, parallel operations and waits
Tasks represent the real work nodes and are frequently Lambda functions, but can be Activities which are externally driven tasks implemented any way you like.
State machines have data that ""flows"" through the steps and can be modified and added to as the state machine executes.
It's best if your tasks are idempotent, in part because you may want to re-run the state machine with the same input data during debugging
The AWS Console facilitates your examining the execution state at various steps.

The console lets you do this with a few steps:

select the ""input"" tab from the failed execution
copy the input data (JSON)
select the state machine name in the breadcrumbs
start a new execution, pasting the input data you copied previously





Step Functions Gotchas and Limitations

Step Functions are free tier eligible up to an initial 4000 transitions per month. Thereafter, the charge is $0.025 per 1000 state transitions.
You can have many, simultaneous, executions, but be aware of lambda throttling limits. This has been per-account, pre-region, but recently became settable per-lambda.
Step Function executions are limited to 25,000 events. Each step creates multiple events. This means that iterating a loop using Lambda is limited to an iteration count of around 3000 before needing to continue as a new execution.

Route 53
Route 53 Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Route 53 is AWS‚Äô DNS service.

Route 53 Alternatives and Lock-In

Historically, AWS was slow to penetrate the DNS market (as it is often driven by perceived reliability and long-term vendor relationships) but Route 53 has matured and is becoming the standard option for many companies. Route 53 is cheap by historic DNS standards, as it has a fairly large global network with geographic DNS and other formerly ‚Äúpremium‚Äù features. It‚Äôs convenient if you are already using AWS.
‚õìGenerally you don‚Äôt get locked into a DNS provider for simple use cases, but increasingly become tied in once you use specific features like geographic routing or Route 53‚Äôs alias records.
üö™Many alternative DNS providers exist, ranging from long-standing premium brands like UltraDNS and Dyn to less well known, more modestly priced brands like DNSMadeEasy. Most DNS experts will tell you that the market is opaque enough that reliability and performance don‚Äôt really correlate well with price.
‚è±Route 53 is usually somewhere in the middle of the pack on performance tests, e.g. the SolveDNS reports.

Route 53 Tips

üîπKnow about Route 53‚Äôs ‚Äúalias‚Äù records:

Route 53 supports all the standard DNS record types, but note that alias resource record sets are not standard part of DNS, but a specific Route 53 feature. (It‚Äôs available from other DNS providers too, but each provider has a different name for it.)
Aliases are like an internal name (a bit like a CNAME) that is resolved internally on the server side. For example, traditionally you could have a CNAME to the DNS name of a CLB or ALB, but it‚Äôs often better to make an alias to the same load balancer. The effect is the same, but in the latter case, externally, all a client sees is the target the record points to.
It‚Äôs often wise to use alias record as an alternative to CNAMEs, since they can be updated instantly with an API call, without worrying about DNS propagation.
You can use them for CLBs/ALBs or any other resource where AWS supports it.
Somewhat confusingly, you can have CNAME and A aliases, depending on the type of the target.
Because aliases are extensions to regular DNS records, if exported, the output zone file will have additional non-standard ‚ÄúALIAS‚Äù lines in it.


Latency-based routing allows users around the globe to be automatically directed to the nearest AWS region where you are running, so that latency is reduced.
Understand that domain registration and DNS management (hosted zones) are two separate Route 53 services. When you buy/transfer a domain, Route 53 automatically assigns four name servers to it (e.g. ns-2.awsdns-00.com). Route 53 also offers to automatically create a hosted zone for DNS management, but you are not required do your DNS management in the same account or even in Route 53; you just need to create an NS record pointing to the servers assigned to your domain in Route 53.
One use case would be to put your domain registration (very mission critical) in a bastion account while managing the hosted zones within another account which is accessible by your applications.

Route 53 Gotchas and Limitations

üî∏Private Hosted Zone will only respond to DNS queries that originate from within a VPC. As a result Route53 will not respond to request made via a VPN or Direct connect. To get around this you will need to implement Hybrid Cloud DNS Solutions or use the Simple AD provided IP addresses to query the hosted zone.

CloudFormation
CloudFormation Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing at no additional charge
CloudFormation allows you to manage sets of resources from other AWS services grouped into stacks. CloudFormation allows you to define these stacks in a template using JSON or YAML. CloudFormation is one of the major services underpinning AWS' infrastructure as code capabilities and is crucial in enabling repeatable and consistent deployments of infrastructure.
üí∏CloudFormation itself has no additional charge; you pay for the underlying resources.

CloudFormation Alternatives and Lock-In

Hashicorp‚Äôs Terraform is a third-party alternative that can support other cloud platforms/providers including Azure and OpenStack.
üî∏Some AWS features may not be available in Terraform (e.g. multi-AZ ElastiCache using Redis), and you may have to resort to embedded CloudFormation templates.
Pulumi enables teams to define and deliver Cloud Native Infrastructure as Code on any cloud, with any language. From containers to serverless to Kubernetes to infrastructure.

CloudFormation Tips

Validate your stack in a different AWS account! CloudFormation truly shines when making multiple deployments of the same stack to different accounts and regions. A common practice is to deploy stacks in successive stages ending in a production rollout.
Avoid potentially time-consuming syntax errors from eating into your deployment time by running validate-template.
CloudFormation is sometimes slow to update what resources (and new features on old services) a user is able to define in the template. If you need to deploy a resource or feature that isn't supported by the template, CloudFormation allows running arbitrary code (using Lambda) on a stack create or update via custom resources.
Custom resources make CloudFormation into a truly powerful tool, as you can do all sorts of neat things quite easily such as sanity tests, initial configuration of Dynamo tables or S3 buckets, cleaning up old CloudWatch logs, etc.

For writing Custom Resources in Java, cfnresponse comes in very handy.
For writing Custom Resources in Javascript, AWS provides a good reference in the documentation.


CloudFormation offers a visual template designer that can be useful when getting up to speed with the template syntax.
By using StackSets, users can define and deploy an entire production application consisting of multiple stacks (one service per stack) in a single CloudFormation template.
If you're developing a serverless application (i.e., using Lambda, API Gateway) CloudFormation offers a simplified template format called SAM.
‚ùóUse a restrictive stack policy! Without one, you can inadvertently delete live production resources, probably causing a severe outage.
‚ùóTurn on termination protection on all of your stacks to avoid costly accidents!
The CloudFormation template reference is indispensable when discovering what is and isn't possible in a CloudFormation template.
Troposphere is a Python library that makes it much easier to create CloudFormation templates.

Currently supports AWS and OpenStack resource types.
Troposphere attempts to support all resources types that can be described in CloudFormation templates.
Built in error checking.
A recommended soft dependency is awacs, which allows you to generate AWS access policy in JSON by writing Python code.


stacker is a Python application that makes it easy to define, configure, orchestrate and manage dependencies for CloudFormation stacks across multiple user-defined environments.
If you are building different stacks with similar layers, it may be useful to build separate templates for each layer that you can reuse using AWS::CloudFormation::Stack.
üî∏Avoid hardcoding resource parameters that can potentially change. Use stack parameters as much as you can, and resort to default parameter values.
üîπUntil 2016, CloudFormation used only an awkward JSON format that makes both reading and debugging difficult. To use it effectively typically involved building additional tooling, including converting it to YAML, but now this is supported directly.
Wherever possible, export relevant physical IDs from your Stacks by defining Outputs in your CloudFormation Templates. These are the actual names assigned to the resources being created. Outputs can be returned from DescribeStack API calls, and get imported to other Stacks as part of the recent addition of cross-stack references.
-Note that importing outputs in a stack from another stack creates a hard dependency that is tracked by CloudFormation. You will not be able to delete the stack with the outputs until there are no importing stacks.
CloudFormation can be set up to send SNS notifications upon state changes, enabling programmatic handling of situations where stacks fail to build, or simple email alerts so the appropriate people are informed.
CloudFormation allows the use of conditionals when creating a stack.

One common way to leverage this capability is in support of multi-environment CloudFormation templates ‚Äì by configuring them to use ‚Äòif-else‚Äô statements on the value of a parameter passed in (e.g.  ‚Äúenv‚Äù), environment-specific values for things like VPC IDs, SecurityGroup IDs, and AMI names can be passed into reusable generic templates.


Version control your CloudFormation templates! In the Cloud, an application is the combination of the code written and the infrastructure it runs on. By version controlling both, it is easy to roll back to known good states.
Avoid naming your resources explicitly (e.g. DynamoDB tables). When deploying multiple stacks to the same AWS account, these names can come into conflict, potentially slowing down your testing. Prefer using resource references instead.
For things that shouldn't ever be deleted, you can set an explicit DeletionPolicy on the resource that will prevent the resource from being deleted even if the CloudFormation stack itself is deleted. This is useful for anything that can maintain expensive-to-rebuild state, such as DynamoDB tables, and things that are exposed to the outside world, such as API Gateway APIs.

CloudFormation Gotchas and Limitations

üî∏A given CloudFormation stack can end up in a wide variety of states. Error reporting is generally weak, and often times multiple observe-tweak-redeploy cycles are needed to get a working template. The internal state machine for all the varying states is extremely opaque.
üî∏Some cross-region operations are not possible in CloudFormation without using a custom resource, such as cross-region SNS subscriptions.
üî∏While having hand-made resources live alongside CloudFormation-created resources is inadvisable, it's sometimes unavoidable. If at all possible, leave ALL resource management up to a CloudFormation template and only provide read-only access to the console.
‚ùóModifications to stack resources made outside CloudFormation can potentially lead to stacks stuck in UPDATE_ROLLBACK_FAILED mode. Stacks in this state can be recovered using the continue-update-rollback command. This command can be initiated in the console or in the CLI. The --resources-to-skip parameter usable in the CLI can be useful if the continue-update-rollback command fails. New feature Drift Detection can be used to detect outside changes made to stack.
üî∏CloudFormation is useful but complex and with a variety of pain points. Many companies find alternate solutions, and many companies use it, but only with significant additional tooling.
üî∏CloudFormation can be very slow, especially for items like CloudFront distributions and Route53 CNAME entries.
üî∏It‚Äôs hard to assemble good CloudFormation configurations from existing state. AWS does offer a trick to do this, but it‚Äôs very clumsy.

CloudFormer also hasn't been updated in ages (as of Oct 2017), doesn't support templatizing many new services, and won't fully define even existing services that have since been updated. For example, Dynamo tables defined through CloudFormer won't contain TTL definitions or auto-scaling configuration. There is a third-party version of the tool with more supported resources called Former2.


üî∏Many users don‚Äôt use CloudFormation at all because of its limitations, or because they find other solutions preferable. Often there are other ways to accomplish the same goals, such as local scripts (Boto, Bash, Ansible, etc.) you manage yourself that build infrastructure, or Docker-based solutions (Convox, etc.).
üî∏Deploying large stacks (i.e., many resources) can be problematic due to unintuitive API limits. For instance, API Gateway's CreateDeployment API has a default limit of 3 requests per minute as of 1/12/2018. This limit is readily exceeded even in moderately-sized CloudFormation stacks. Creating CW alarms is another commonly seen limit (PutMetricAlarm, 3 tps as of 1/12/2018) especially when creating many autoscaling policies for DynamoDB. One way to work around this limit is to include CloudFormation 'DependsOn' clauses to artificially chain resource creation.
üî∏Creating/deleting stacks can be a little less clean than ideal. Some resources will leave behind traces in your AWS account even after deletion. E.g., Lambda will leave behind CloudWatch log groups that never expire.

VPCs, Network Security, and Security Groups
VPC Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Security groups ‚àô Pricing
VPC (Virtual Private Cloud) is the virtualized networking layer of your AWS systems.
Most AWS users should have a basic understanding of VPC concepts, but few need to get into all the details. VPC configurations can be trivial or extremely complex, depending on the extent of your network and security needs.
All modern AWS accounts (those created after 2013-12-04) are ‚ÄúEC2-VPC‚Äù accounts that support VPCs, and all instances will be in a default VPC. Older accounts may still be using ‚ÄúEC2-Classic‚Äù mode. Some features don‚Äôt work without VPCs, so you probably will want to migrate.

VPC and Network Security Tips


‚ùóSecurity groups are your first line of defense for your servers. Be extremely restrictive of what ports are open to all incoming connections. In general, if you use CLBs, ALBs or other load balancing, the only ports that need to be open to incoming traffic would be port 22 and whatever port your application uses. Security groups access policy is 'deny by default'.


Port hygiene: A good habit is to pick unique ports within an unusual range for each different kind of production service. For example, your web frontend might use 3010, your backend services 3020 and 3021, and your Postgres instances the usual 5432. Then make sure you have fine-grained security groups for each set of servers. This makes you disciplined about listing out your services, but also is more error-proof. For example, should you accidentally have an extra Apache server running on the default port 80 on a backend server, it will not be exposed.


Migrating from Classic: For migrating from older EC2-Classic deployments to modern EC2-VPC setup, this article may be of help.

You can migrate Elastic IPs between EC2-Classic and EC2-VPC.



For basic AWS use, one default VPC may be sufficient. But as you scale up, you should consider mapping out network topology more thoroughly. A good overview of best practices is here.


Consider controlling access to you private AWS resources through a VPN.

You get better visibility into and control of connection and connection attempts.
You expose a smaller surface area for attack compared to exposing separate (potentially authenticated) services over the public internet.

e.g. A bug in the YAML parser used by the Ruby on Rails admin site is much less serious when the admin site is only visible to the private network and accessed through VPN.


Another common pattern (especially as deployments get larger, security or regulatory requirements get more stringent, or team sizes increase) is to provide a bastion host behind a VPN through which all SSH connections need to transit.
For a cheap VPN to access private AWS resources, consider using a point-to-site software VPN such as OpenVPN. It can either be installed using the official AMI, though you are limited to 2 concurrent users on the free license, or it can be installed using the openvpn package on linux. The linux package allows for unlimited concurrent users but the installation is less straightforward. This OpenVPN installer script can help you install it and add client keys easily.



üîπConsider using other security groups as sources for security group rules instead of using CIDRs ‚Äî that way, all hosts in the source security group and only hosts in that security group are allowed access. This is a much more dynamic and secure way of managing security group rules.


VPC Flow Logs allow you to monitor the network traffic to, from, and within your VPC. Logs are stored in CloudWatch Logs groups, and can be used for security monitoring (with third party tools), performance evaluation, and forensic investigation.

See the VPC Flow Logs User Guide for basic information.
See the flowlogs-reader CLI tool and Python library to retrieve and work with VPC Flow Logs.



IPv6 is available in VPC. Along with this announcement came the introduction of the Egress-Only Internet Gateway. In cases where one would use NAT Gateways to enable egress-only traffic for their VPC in IPv4, one can use an Egress-Only Internet Gateway for the same purpose in IPv6.


Amazon provides an IPv6 CIDR block for your VPC at your request - at present you cannot implement your own IPv6 block if you happen to own one already.


New and existing VPCs can both use IPv6. Existing VPCs will need to be configured to have an IPv6 CIDR block associated with them, just as new VPCs do.


PrivateLink

üìíHomepage ‚àô User Guide ‚àô  Pricing
One of the uses for Private link is Interface VPC Endpoints deploys an ENI into your VPC and subnets which allows you direct access to the AWS API's as if the were accessible locally in your VPC without having to go out to the internet.
Another use case would be to expose a service of your own to other accounts in AWS through a VPC Endpoint Service

VPC and Network Security Gotchas and Limitations

üî∏VPCs are tied to one Region in one Account. Subnets are tied to one VPC and limited to one Availability Zone.
üî∏Security groups are tied to one VPC. If you are utilizing infrastructure in multiple VPCs you should make sure your configuration/deployment tools take that into account.
üî∏VPC Endpoints are currently only available for S3 and DynamoDB. If you have a security requirement to lockdown outbound traffic from your VPC you may want to use DNS filtering to control outbound traffic to other services.
‚ùóBe careful when choosing your VPC IP CIDR block: If you are going to need to make use of ClassicLink, make sure that your private IP range doesn‚Äôt overlap with that of EC2 Classic.
‚ùóIf you are going to peer VPCs, carefully consider the cost of data transfer between VPCs, since for some workloads and integrations, this can be prohibitively expensive.
‚ùóNew RDS instances require a subnet group within your VPC. If you‚Äôre using the default VPC this isn‚Äôt a concern, it will contain a subnet for each availability zone in your region. However, if you‚Äôre creating your own VPC and plan on using RDS, make sure you have at least two subnets within the VPC to act as the subnet group.
‚ùóIf you delete the default VPC, you can recreate it via the CLI or the console.
‚ùóBe careful with VPC VPN credentials! If lost or compromised, the VPN endpoint must be deleted and recreated. See the instructions for Replacing Compromised Credentials.
‚ùóSecurity Groups and Route Tables apply entries separately for IPv4 and IPv6, so one must ensure they add entries for both protocols accordingly.
üí∏Managed NAT gateways are a convenient alternative to
manually managing NAT instances, but they do come at a cost per gigabyte. Consider alternatives if you're transferring many terabytes from private subnets to the internet. If you transfer terabytes/petabytes of data from EC2 instances in private subnets to S3, avoid the NAT gateway data processing charge by setting up a Gateway Type VPC Endpoint and route the traffic to/from S3 through the VPC endpoints instead of going through the NAT gateways.

KMS
KMS Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
KMS (Key Management Service) is a secure service for creating, storing and auditing usage of cryptographic keys.
Service integration: KMS integrates with other AWS services: EBS, Elastic Transcoder, EMR, Redshift, RDS, SES, S3, WorkMail and Workspaces.
Encryption APIs: The Encrypt and Decrypt API allow you to encrypt and decrypt data on the KMS service side, never exposing the master key contents.
Data keys: The GenerateDataKey API generates a new key off of a master key. The data key contents are exposed to you so you can use it to encrypt and decrypt any size of data in your application layer. KMS does not store, manage or track data keys, you are responsible for this in your application.
üîπAuditing: Turn on CloudTrail to audit all KMS API events.
Access: Use key policies and IAM policies to grant different levels of KMS access. For example, you create an IAM policy that only allows a user to encrypt and decrypt with a specific key.

KMS Tips

üîπIt‚Äôs very common for companies to manage keys completely via home-grown mechanisms, but it‚Äôs far preferable to use a service such as KMS from the beginning, as it encourages more secure design and improves policies and processes around managing keys.
A good motivation and overview is in this AWS presentation.
The cryptographic details are in this AWS whitepaper.
This blog from Convox demonstrates why and how to use KMS for encryption at rest.

KMS Gotchas and Limitations

üî∏The Encrypt API only works with < 4KB of data. Larger data requires generating and managing a data key in your application layer.
üî∏KMS audit events are not available in the CloudTrail Lookup Events API. You need to look find them in the raw .json.gz files that CloudTrail saves in S3.
üî∏In order to encrypt a multi-part upload to S3, the KMS Key Policy needs to allow ‚Äúkms:Decrypt‚Äù and ‚Äúkms:GenerateDataKey*‚Äù in addition to ‚Äúkms:Encrypt‚Äù, otherwise the upload will fail with an ‚ÄúAccessDenied‚Äù error.
üî∏KMS keys are region specific ‚Äî they are stored and can only be used in the region in which they are created. They can't be transferred to other regions.
üî∏KMS keys have a key policy that must grant access to something to manage the key.  If you don't grant anything access to the key on creation, then you have to reach out to support to have the key policy reset Reduce the Risk of the Key Becoming Unmanagable.
üî∏If you use a key policy to grant access to IAM roles or users and then delete the user/role, recreating the user or role won't grant them permission to the key again.

CloudFront
CloudFront Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
CloudFront is AWS‚Äô content delivery network (CDN).
Its primary use is improving latency for end users through accessing cacheable content by hosting it at over 60 global edge locations.

CloudFront Alternatives and Lock-in

üö™CDNs are a highly fragmented market. CloudFront has grown to be a leader, but there are many alternatives that might better suit specific needs.

CloudFront Tips

üê•IPv6 is supported. This is a configurable setting, and is enabled by default on new CloudFront distributions. IPv6 support extends to the use of WAF with CloudFront.
üê•HTTP/2 is now supported! Clients must support TLS 1.2 and SNI.
While the most common use is for users to browse and download content (GET or HEAD methods) requests, CloudFront also supports (since 2013) uploaded data (POST, PUT, DELETE, OPTIONS, and PATCH).

You must enable this by specifying the allowed HTTP methods when you create the distribution.
Interestingly, the cost of accepting (uploaded) data is usually less than for sending (downloaded) data.


In its basic version, CloudFront supports SSL via the SNI extension to TLS, which is supported by all modern web browsers. If you need to support older browsers, you need to pay a few hundred dollars a month for dedicated IPs.

üí∏‚è±Consider invalidation needs carefully. CloudFront does support invalidation of objects from edge locations, but this typically takes many minutes to propagate to edge locations, and costs $0.005 per request after the first 1000 requests. (Some other CDNs support this better.)


Everyone should use TLS nowadays if possible. Ilya Grigorik‚Äôs table offers a good summary of features regarding TLS performance features of CloudFront.
An alternative to invalidation that is often easier to manage, and instant, is to configure the distribution to cache with query strings and then append unique query strings with versions onto assets that are updated frequently.
‚è±For good web performance, it is recommended to enable compression on CloudFront distributions if the origin is S3 or another source that does not already compress.

CloudFront Gotchas and Limitations

üî∏If using S3 as a backing store, remember that the endpoints for website hosting and for general S3 are different. Example: ‚Äúbucketname.s3.amazonaws.com‚Äù is a standard S3 serving endpoint, but to have redirect and error page support, you need to use the website hosting endpoint listed for that bucket, e.g. ‚Äúbucketname.s3-website-us-east-1.amazonaws.com‚Äù (or the appropriate region).
üî∏By default, CloudFront will not forward HTTP Host: headers through to your origin servers. This can be problematic for your origin if you run multiple sites switched with host headers. You can enable host header forwarding in the default cache behavior settings.
üî∏4096-bit SSL certificates: CloudFront do not support 4096-bit SSL certificates as of late 2016. If you are using an externally issued SSL certificate, you‚Äôll need to make sure it‚Äôs 2048 bits. See ongoing discussion.
Although connections from clients to CloudFront edge servers can make use of IPv6, connections to the origin server will continue to use IPv4.

DirectConnect
DirectConnect Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
Direct Connect is a private, dedicated connection from your network(s) to AWS.

DirectConnect Tips

If your data center has a partnering relationship with AWS, setup is streamlined.
Use for more consistent predictable network performance guarantees (1 Gbps or 10 Gbps per link).
Use to peer your colocation, corporate, or physical datacenter network with your VPC(s).

Example: Extend corporate LDAP and/or Kerberos to EC2 instances running in a VPC.
Example: Make services that are hosted outside of AWS for financial, regulatory, or legacy reasons callable from within a VPC.



Redshift
Redshift Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Redshift is AWS‚Äô managed data warehouse solution, which is massively parallel, scalable, and columnar. It is very widely used. It was built using ParAccel technology and exposes Postgres-compatible interfaces.

Redshift Alternatives and Lock-in

‚õìüö™Whatever data warehouse you select, your business will likely be locked in for a long time. Also (and not coincidentally) the data warehouse market is highly fragmented. Selecting a data warehouse is a choice to be made carefully, with research and awareness of the market landscape and what business intelligence tools you‚Äôll be using.

Redshift Tips

Although Redshift is mostly Postgres-compatible, its SQL dialect and performance profile are different.
Redshift supports only 12 primitive data types. (List of unsupported Postgres types)
It has a leader node and computation nodes (the leader node distributes queries to the computation ones). Note that some functions can be executed only on the lead node.
üîπMake sure to create a new cluster parameter group and option group for your database since the default parameter group does not allow dynamic configuration changes.
Major third-party BI tools support Redshift integration (see Quora).
Top 10 Performance Tuning Techniques for Amazon Redshift provides an excellent list of performance tuning techniques.
Amazon Redshift Utils contains useful utilities, scripts and views to simplify Redshift ops.
VACUUM regularly following a significant number of deletes or updates to reclaim space and improve query performance.
Avoid performing blanket VACUUM or ANALYZE operations at a cluster level. The checks on each table to determine whether VACUUM or ANALYZE action needs to be taken is wasteful. Only perform ANALYZE and VACUUM commands on the objects that require it. Utilize the Analyze & Vacuum Schema Utility to perform this work. The SQL to determine whether a table needs to be VACUUMed or ANALYZEd can be found in the Schema Utility README if you wish to create your own maintenance process.
Redshift provides various column compression options to optimize the stored data size. AWS strongly encourages users to use automatic compression at the COPY stage, when Redshift uses a sample of the data being ingested to analyze the column compression options. However, automatic compression can only be applied to an empty table with no data. Therefore, make sure the initial load batch is big enough to provide Redshift with a representative sample of the data (the default sample size is 100,000 rows).
Redshift uses columnar storage, hence it does not have indexing capabilities. You can, however, use distribution key and sortkey to improve performance. Redshift has two types of sort keys: compounding sort key and interleaved sort key.
A compound sort key is made up of all columns listed in the sort key definition. It is most useful when you have queries with operations using the prefix of the sortkey.
An interleaved sort key on the other hand gives equal weight to each column or a subset of columns in the sort key. So if you don't know ahead of time which column(s) you want to choose for sorting and filtering, this is a much better choice than the compound key. Here is an example using interleaved sort key.
üî∏‚è± Distribution strategies: Since data in Redshift is physically distributed among nodes, choosing the right data distribution key and distribution style is crucial for adequate query performance. There are three possible distribution style settings ‚Äî EVEN (the default), KEY, or ALL. Use KEY to collocate join key columns for tables which are joined in queries. Use ALL to place the data in small-sized tables on all cluster nodes.

Redshift Gotchas and Limitations

‚ùó‚è±While Redshift can handle heavy queries well, it does not scale horizontally, i.e. does not handle multiple queries in parallel. Therefore, if you expect a high parallel load, consider replicating or (if possible) sharding your data across multiple clusters.
üî∏ The leader node, which manages communications with client programs and all communication with compute nodes, is the single point of failure.
‚è±Although most Redshift queries parallelize well at the compute node level, certain stages are executed on the leader node, which can become the bottleneck.
üîπRedshift data commit transactions are very expensive and serialized at the cluster level. Therefore, consider grouping multiple mutation commands (COPY/INSERT/UPDATE) commands into a single transaction whenever possible.
üîπRedshift does not support multi-AZ deployments. Building multi-AZ clusters is not trivial. Here is an example using Kinesis.
üî∏Beware of storing multiple small tables in Redshift. The way Redshift tables are laid out on disk makes it impractical. The minimum space required to store a table (in MB) is nodes * slices/node * columns. For example, on a 16 node cluster an empty table with 20 columns will occupy 640MB on disk.
‚è± Query performance degrades significantly during data ingestion. WLM (Workload Management) tweaks help to some extent. However, if you need consistent read performance, consider having replica clusters (at the extra cost) and swap them during update.
‚ùó Never resize a live cluster. The resize operation can take hours depending on the dataset size. In rare cases, the operation may also get stuck and you'll end up having a non-functional cluster. The safer approach is to create a new cluster from a snapshot, resize the new cluster and shut down the old one.
üî∏Redshift has reserved keywords that are not present in Postgres (see full list here). Watch out for DELTA (Delta Encodings).
üî∏Redshift does not support many Postgres functions, most notably several date/time-related and aggregation functions. See the full list here.
üî∏ Uniqueness, primary key, and foreign key constraints on Redshift tables are informational only and are not enforced. They are, however, used by the query optimizer to generate query plans. NOT NULL column constraints are enforced. See here for more information on defining constraints.
üî∏Compression on sort key can result in significant performance impact. So if your Redshift queries involving sort key(s) are slow, you might want to consider removing compression on a sort key.
üîπ Choosing a sort key is very important since you can not change a table‚Äôs sort key after it is created. If you need to change the sort or distribution key of a table, you need to create a new table with the new key and move your data into it with a query like ‚Äúinsert into new_table select * from old_table‚Äù.
‚ùóüö™ When moving data with a query that looks like ‚Äúinsert into x select from y‚Äù, you need to have twice as much disk space available as table ‚Äúy‚Äù takes up on the cluster‚Äôs disks. Redshift first copies the data to disk and then to the new table. Here is a good article on how to this for big tables.

EMR
EMR Basics

üìí Homepage ‚àô Release guide ‚àô FAQ ‚àô Pricing
EMR (which used to stand for Elastic Map Reduce, but not anymore, since it now extends beyond map-reduce) is a service that offers managed deployment of Hadoop, HBase and Spark. It reduces the management burden of setting up and maintaining these services yourself.

EMR Alternatives and Lock-in

‚õìMost of EMR is based on open source technology that you can in principle deploy yourself. However, the job workflows and much other tooling is AWS-specific. Migrating from EMR to your own clusters is possible but not always trivial.

EMR Tips

EMR relies on many versions of Hadoop and other supporting software. Be sure to check which versions are in use.
‚è±Off-the-shelf EMR and Hadoop can have significant overhead when compared with efficient processing on a single machine. If your data is small and performance matters, you may wish to consider alternatives, as this post illustrates.
Python programmers may want to take a look at Yelp‚Äôs mrjob.
It takes time to tune performance of EMR jobs, which is why third-party services such as Qubole‚Äôs data service are gaining popularity as ways to improve performance or reduce costs.

EMR Gotchas and Limitations

üí∏‚ùóEMR costs can pile up quickly since it involves lots of instances, efficiency can be poor depending on cluster configuration and choice of workload, and accidents like hung jobs are costly. See the section on EC2 cost management, especially the tips there about Spot instances. This blog post has additional tips, but was written prior to the shift to per-second billing.
üí∏ Beware of ‚Äúdouble-dipping‚Äù. With EMR, you pay for the EC2 capacity and the service fees. In addition, EMR syncs task logs to S3, which means you pay for the storage and PUT requests at S3 standard rates. While the log files tend to be relatively small, every Hadoop job, depending on the size, generates thousands of log files that can quickly add up to thousands of dollars on the AWS bill. YARN‚Äôs log aggregation is not available on EMR.

Kinesis Streams
Kinesis Streams Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Kinesis Streams (which used to be only called Kinesis, before Kinesis Firehose and Kinesis Analytics were launched) is a service that allows you to ingest high-throughput data streams for immediate or delayed processing by other AWS services.
Kinesis Streams‚Äô subcomponents are called shards. Each shard provides 1MB/s of write capacity and 2MB/s of read capacity at a maximum of 5 reads per second. A stream can have its shards programmatically increased or decreased based on a variety of metrics.
All records entered into a Kinesis Stream are assigned a unique sequence number as they are captured. The records in a Stream are ordered by this number, so any time-ordering is preserved.
This page summarizes key terms and concepts for Kinesis Streams.

Kinesis Streams Alternatives and Lock-in

üö™ Kinesis is most closely compared to Apache Kafka, an open-source data ingestion solution. It is possible to set up a Kafka cluster hosted on EC2 instances (or any other VPS), however you are responsible for managing and maintaining both Zookeeper and the Kafka brokers in a highly available configuration. Confluent has a good blog post with their recommendations on how to do this here, which has links on the bottom to several other blogs they have written on the subject.
‚õì Kinesis uses very AWS-specific APIs, so you should be aware of the potential future costs of migrating away from it, should you choose to use it.
An application that efficiently uses Kinesis Streams will scale the number of shards up and down based on the required streaming capacity. (Note there is no direct equivalent to this with Apache Kafka.)

Kinesis Streams Tips

The KCL (Kinesis Client Library) provides a skeleton interface for Java, Node, Python, Ruby and .NET programs to easily consume data from a Kinesis Stream. In order to start consuming data from a Stream, you only need to provide a config file to point at the correct Kinesis Stream, and functions for initialising the consumer, processing the records, and shutting down the consumer within the skeletons provided.

The KCL uses a DynamoDB table to keep track of which records have been processed by the KCL. This ensures that all records are processed ‚Äúat least once‚Äù. It is up to the developer to ensure that the program can handle doubly-processed records.
The KCL also uses DynamoDB to keep track of other KCL ‚Äúworkers‚Äù. It automatically shares the available Kinesis Shards across all the workers as equally as possible.



Kinesis Streams Gotchas and Limitations

üî∏‚è±  Kinesis Streams‚Äô shards each only permit 5 reads per second. If you are evenly distributing data across many shards, your read limit for the Stream will remain at 5 reads per second on aggregate, as each consuming application will need to check every single shard for new records. This puts a hard limit on the number of different consuming applications possible per Stream for a given maximum read latency.

For example, if you have 5 consuming applications reading data from one Stream with any number of shards, they cannot read with a latency of less than one second, as each of the 5 consumers will need to poll each shard every second, reaching the cap of 5 reads per second per shard.
This blog post further discusses the performance and limitations of Kinesis in production.


üí∏ Kinesis Streams are not included in the free tier. Make sure if you do any experimentation with it on a personal account, you shut down the stream or it may run up unexpected costs (~$11 per shard-month.)

Kinesis Firehose
Kinesis Firehose Gotchas and Limitations

üî∏ üìú When delivering from Firehose to Elasticsearch, the JSON document cannot contain an ‚Äú_id‚Äù property. Firehose will not attempt to deliver those documents and won't log any error.

Device Farm
Device Farm Basics

üìí Homepage ‚àô Developer guide ‚àô FAQ ‚àô Pricing
Device Farm is an AWS service that enables mobile app testing on real devices.
Supports iOS and Android (including Kindle Fire) devices, as well as the mobile web.
Supports remote device access in order to allow for interactive testing/debugging.

Device Farm Tips

AWS Mobile blog contains several examples of Device Farm usage for testing.
Device Farm offers a free trial for users who want to evaluate their service.
Device Farm offers two pricing models: Paying per device minute is useful for small usage levels or for situations where it‚Äòs hard to predict usage amount. Unmetered plans are useful in situations where active usage is expected from the beginning.
To minimize waiting time for device availability, one approach is to create several device pools with different devices, then randomly choose one of the unused device pools on every run.

Device Farm Gotchas and Limitations

‚ùóDevices don't have a SIM card and therefore can‚Äòt be used for testing SIM card-related features.
üî∏Device Farm supports testing for most popular languages/frameworks, but not for all. An actual list of supported frameworks and languages is presented on this page.
üî∏The API and CLI for Device Farm is quite a low level and may require developing additional tools or scripts on top of it.
üî∏AWS provide several tools and plugins for Device Farm, however, it doesn‚Äòt cover all cases or platforms. It may require developing specific tools or plugins to support specific requirements.
‚ùóIn general, Device Farm doesn‚Äòt have Android devices from Chinese companies like Huawei, Meizu, Lenovo, etc. An actual list of supported devices located here.
üî∏Device availibility is uneven. It depends on several factors including device popularity. Usually, more modern devices see higher demand, thus the waiting time for them will be higher compared to relatively old devices.

Mobile Hub
Mobile Hub Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing


Mobile Hub orchestrates multiple services to create an AWS backend for mobile and web applications.
Each project in Mobile Hub has one backend made up of configurable features, plus one or more applications.
Features include Analytics, Cloud Logic, Conversational Bots, Hosting and Streaming, NoSQL Database, User Data Storage and User Sign-In. Each feature uses one or two services to deliver a chunk of functionality.
Services used include API Gateway, CloudFront, Cognito, Device Farm, DynamoDB, Lambda, Lex, Pinpoint and S3.
Application SDKs exist for Android (Java), iOS (Swift), Web (JS) and React Native (JS). There is also a CLI for JavaScript applications.

Mobile Hub Tips

The Mobile Hub console has starter kits and tutorials for various app platforms.
The CLI allows local development of Lambda code (JS by default) with awsmobile {pull|push} commands, to sync from cloud to folder, and back again.
Mobile Hub itself is free, but each of the services has its own pricing model.

Mobile Hub Gotchas and Limitations

üî∏The Cloud API feature allows importing an existing Lambda function instead of defining a new one, but there are some rough edges with the CLI. Check the GitHub issues.
‚ùóMobile Hub uses CloudFormation under the covers, and gets confused when a service is changed outside of the Mobile Hub console.

IoT
IoT Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing


IoT is a platform for allowing clients such as IoT devices or software applications (examples) to communicate with the AWS cloud.
Clients are also called devices (or things) and include a wide variety of device types.  Roughly there are three categories of device types that interact with IoT services by sending message over an IoT protocol to the IoT Pub/Sub-style message broker, which is called the IoT Device Gateway:

Send messages only: For example, the AWS IoT Button on an eddystone beacon.
Send, receive, and process messages: For example, a simple processing board, such as a Raspberry Pi (quick start guide), or an AWS device, such as Echo or Echo Dot, which are designed to work with the AWS Alexa skills kit (a programmable voice-enabled service from AWS).


AWS has a useful quick-start (using the Console) and a slide presentation on core topics.


IoT terms:

AWS IoT Things (metadata for devices in a registry) and can store device state in a JSON document, which is called a device shadow.  Device metadata can also be stored in IoT Thing Types. This aids in device metadata management by allowing for reuse of device description and configuration for more than one device.  Note that IoT Thing Types can be deprecated, but not changed ‚Äî they are immutable.
AWS IoT Certificates (device authentication) are the logical association of a unique certificate to the logical representation of a device. This association can be done in the Console.  In addition, the public key of the certificate must be copied to the physical device. This covers the authentication of devices to a particular AWS Device Gateway (or message broker). You can associate an AWS IoT certificate with an IoT device or you can register your own CA (Certificate Authority) with AWS, generate your own certificate(s) and associate those certificates with your devices via the AWS Console or cli.
AWS IoT Policies (device/topic authorization) are JSON files that are associated to one or more AWS IoT certificates. This authorizes associated devices to publish and/or subscribe to messages from one or more MQTT topics.
AWS IoT Rules are SQL-like queries which allows for reuse of some or all device message data, as described in this presentation, which summarizes design patterns with for IoT Rules.
Shown below is a diagram which summarizes the flow of messages between the AWS IoT services:




IoT Greengrass

üìí Homepage
üê•Greengrass is a software platform that extends AWS IoT capabilities allowing Lambda functions to be run directly on local devices.  It also enables IoT devices to be able to securely communicate on a local network without having to connect to the cloud.

Greengrass includes a local pub/sub message manager that can buffer messages if connectivity is lost so that inbound and outbound messages to the cloud are preserved. Locally deployed Lambda functions can be triggered by local events, messages from the cloud, or other sources.
Greengrass includes secure authentication and authorization of devices within the local network and also between the local network and the AWS cloud. It also provides secure, over-the-air software updates of Lambda functions.


Greengrass core software includes a message manager object, Lambda runtime, local copy service for IoT Thing (or device) shadows, and a deployment agent to manage Greengrass group configuration.
Greengrass groups are containers for selected IoT devices settings, subscriptions and associated Lambda functions.  In a Greengrass group a device is either a Greengrass core or an IoT device which will be connected that particular Greengrass core.
The Greengrass core SDK enables Lambda functions to interact with the AWS Greengrass core on which they run in order to publish messages, interact with the local Thing Shadows service, or invoke other deployed Lambda functions.
The AWS Greengrass Core SDK only supports sending MQTT messages with QoS = 0.
Shown below is a diagram which shows the architecture of AWS IoT Greengrass services:


IoT Alternatives and Lock-in

AWS, Microsoft and Google have all introduced IoT-specific sets of cloud services since late 2015. AWS was first, moving their IoT services to general availability in Dec 2015. Microsoft released their set of IoT services for Azure in Feb 2016.  Google has only previewed, but not released their IoT services Android Things and Weave.
Issues of lock-in center around your devices ‚Äî  protocols (for example MQTT, AMQP), message formats (such as, JSON vs. Hex...) and security (certificates).

IoT Tips

Getting started with Buttons: One way to start is to use an AWS IoT Button.  AWS provides a number of code samples for use with their IoT Button, you can use the AWS IoT console, click the ‚Äúconnect AWS IoT button‚Äù link and you'll be taken to the  AWS Lambda console.  There you fill out your button‚Äôs serial number to associate it with a Lambda. (As of this writing, AWS IoT buttons are only available for sale in the US.)
Connections and protocols: It is important to understand the details of about the devices you wish to connect to the AWS IoT service, including how you will secure the device connections, the device protocols, and more. Cloud vendors differ significantly in their support for common IoT protocols, such as MQTT, AMQP, XMPP. AWS IoT supports secure MQTT, WebSockets and HTTPS.
Support for device security via certificate processing is a key differentiator in this space.  In August 2016, AWS added just-in-time registrations for IoT devices to their services.
Combining with other services: It‚Äôs common to use other AWS services, such as AWS Lambda, Kinesis and DynamoDB, although this is by no means required.  Sample IoT application reference architectures are in this screencast.
Testing tools:

To get started, AWS includes a lightweight MQTT client in the AWS IoT console. Here you can create and test sending and receiving messages to and from various MQTT topics.
When testing locally, if using MQTT, it may be helpful to download and use the open source Mosquitto broker tool for local testing with devices and/or device simulators
Use this MQTT load simulator to test device message load throughout your IoT solution.



IoT Gotchas and Limitations

üî∏IoT protocols: It is important to verify the exact type of support for your particular IoT device message protocol. For example, one commonly used IoT protocol is MQTT. Within MQTT there are three possible levels of QoS in MQTT.  AWS IoT supports MQTT QoS 0 (fire and forget, or at most once) and QoS 1(at least once, or includes confirmation), but not QoS 2 (exactly once, requires 4-step confirmation).  This is important in understanding how much code you‚Äôll need to write for your particular application message resolution needs.  Here is a presentation about the nuances of connecting.
üî∏The ecosystems to match IAM users or roles to IoT policies and their associated authorized AWS IoT devices are immature. Custom coding to enforce your security requirements is common.
‚ùóA common mistake is to misunderstand the importance of IoT device security.  It is imperative to associate each device with a unique certificate (public key). You can generate your own certificates and upload them to AWS, or you can use AWS generated IoT device certificates. It‚Äôs best to read and understand AWS‚Äôs own guidance on this topic.
üî∏There is only one AWS IoT Gateway (endpoint) per AWS account. For production scenarios, you‚Äôll probably need to set up multiple AWS accounts in order to separate device traffic for development, test and production. It‚Äôs interesting to note that the Azure IoT Gateway supports configuration of multiple endpoints, so that a single Azure account can be used with separate pub/sub endpoints for development, testing and production
üî∏Limits: Be aware of limits, including device message size, type, frequency, and number of AWS IoT rules.

IoT Code Samples

Simple Beer Service is a surprisingly useful code example using AWS IoT, Lambda, etc.
IoT-elf offers clean Python sample using the AWS IoT SDK.
IoT Button projects on Hackster include many different code samples for projects.
5 IoT code examples: a device simulator, MQTT sample, just in time registration, truck simulator, prediction data simulator.
AWS Alexa trivia voice example is a quick-start using Alexa voice capability and Lambda.
Some Raspberry Pi examples include the Beacon project, Danbo, and GoPiGo.

SES
SES Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing
SES (or Simple Email Service) is a service that exposes SMTP endpoints for your application to directly integrate with.

SES Tips

üîπBounce Handling: Make sure you handle this early enough. Your ability to send emails can be removed if SES sees too many bounces.
üîπCredentials: Many developers get confused between SES credentials and AWS API keys. Make sure to enter SMTP credentials while using the SMTP APIs.

SES Gotchas and Limitations

üî∏Internet Access: SES SMTP endpoints are on the Internet and will not be accessible from a location without Internet access (e.g. a private subnet without NAT gateway route in the routing table). In such a case, set up an SMTP relay instance in a subnet with Internet access and configure your application to send emails to this SMTP relay instance rather than SES. The relay should have a forwarding rule to send all emails to SES). ‚ùóIf you are using a proxy instead of a NAT, confirm that your proxy service supports SMTP.

Certificate Manager
Certificate Manager Basics

üìí Homepage ‚àô User guide ‚àô FAQ ‚àô Pricing
Use the Certificate Manager to manage SSL/TLS certificates in other AWS services.
Supports importing existing certificates as well as issuing new ones.
Provides Domain Validated (DV) certificates. Validation is done by sending an email to 3 contact addresses in WHOIS and 5 common addresses for the domain, for each domain name present in the request. As of late 2017, this can also be done via DNS instead.
ACM will attempt to automatically renew a certificate issued by Amazon. It will first attempt to connect to the domain on HTTPS and check that the certificate used by the domain is the same with the certificate that it intends to renew. Failing that, it will check the DNS record used previously for validation. Failing that, ACM will attempt manual validation by sending emails to all domains in the certificate.

Certificate Manager Alternatives and Lock-in

‚õìCertificates issued by the Certificate Manager can‚Äôt be used outside of the services that support it. Imported certificates, however, can still be used elsewhere.

Certificate Manager Tips

üîπSupported services: Managed Load Balancers, CloudFront, API Gateway and Elastic Beanstalk.
üî∏During the domain validation process, if DNS validation is unsuccessful Certificate Manager will send an email to every contact address specified in the domain‚Äôs WHOIS record and up to five common administrative addresses. Some anti-spam filters can mark emails as spam because of this. You should check the spam folder of your email if you don‚Äôt receive a confirmation email.
üîπ Setting up a certificate for a test domain you don't have email set up on? You can now use DNS validation instead.
üîπRemember when requesting a wildcard domain that the request will not be valid for the level just below the wildcard, or any subdomains preceding the wildcard. Take for example an approved, issued certificate for *.bar.example.com. This would be valid for foo.bar.example.com but not bar.example.com. Likewise it would also not be valid for www.bar.foo.example.com. You would need to add each of these domains to the certificate request.

Certificate Manager Gotchas and Limitations

üî∏In order to use Certificate Manager for CloudFront distributions, the certificate must be issued or imported from us-east-1 (N. Virginia) region.
üî∏Certificates used with Elastic Load Balancers must be issued in the same region as the load balancer. Certificates can not be moved or copied between regions, as of July 2017. If a domain uses load balancers present in multiple regions, a different certificate must be requested for each region.
üî∏IoT has its own way of setting up certificates.
üî∏By default the maximum number of domains per certificate is 10. You can get this limit increased to a maximum of 100 by contacting AWS support. Note for every different domain you have on the requested cert, you'll need to press accept on an email sent to that domain. For example if you request a cert with 42 different domains or sub domains, you'll need to press accept on 42 different links.

üîπIf you request a limit increase to AWS support for this, they will respond to you asking to confirm this. Bypass this by saying in the body of your initial request:
""I acknowledge at the moment, there is no method to add or remove a name from a certificate. Instead, you must request a new certificate with the revised namelist and you must then re-approve all of the names in the certificate, even if they'd been previously approved.""


üî∏There is no way at the moment to add or remove a domain to/from an existing certificate. You must request a new certificate and re-approve it from each of the domains requested.

WAF
WAF Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing
WAF (Web Application Firewall) is used in conjunction with the CloudFront and ALB services to inspect and block/allow web requests based on user-configurable conditions.
HTTPS and HTTP requests are supported with this service.
WAF's strength is in detecting malicious activity based on pattern-matching inputs for attacks such as SQL injections, XSS, etc.
WAF supports inspection of requests received through both IPv6 and IPv4.

WAF Tips

Getting a WAF API call history can be done through CloudTrail. This is enabled through the CloudTrail console.
It's also possible to get full
logs of all the web requests inspected

WAF Gotchas and Limitations

As of May 2019, AWS WAF is  available on Amazon CloudFront and in 12 commercial AWS regions: US East (N. Virginia), US East (Ohio), US West (Oregon), US West (N. California), EU (Ireland), EU (Frankfurt), EU (London), EU (Stockholm), Asia Pacific (Tokyo), Asia Pacific (Sydney), Asia Pacific (Singapore), and Asia Pacific (Seoul).

OpsWorks
OpsWorks Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing: Stacks, Chef Automate, Puppet Enterprise
OpsWorks is a configuration management service that uses Chef or Puppet configuration management. It is broken out into three different services:

OpsWorks Stacks: The service lets you configure and launch stacks specific to your application's needs, and allows you to automate application deployments. Chef runs can be performed manually via the Execute Cookbooks command, otherwise they are only run as part of lifecycle events.

OpsWorks Stacks differs from standard configuration management services in that it also allows you to perform some infrastructure and application automation (such as creating Amazon EC2 instances and deploying applications via Chef cookbooks).


OpsWorks for Chef Automate: This service launches a dedicated Chef Automate server in your account, which can be used to associate nodes, upload coobook code, and configure systems. Automated patching, backups, OS updates, and minor Chef version upgrades are provided as part of the service. An AWS API is provided for associating/disassociating nodes. Chef runs can be scheduled on nodes using the chef-client cookbook.
OpsWorks for Puppet Enterprise: This service launches a dedicated Puppet Master in your account, which can be used to associate nodes, upload modules, and configure systems. Automated patching, backups, OS updates, and minor Puppet version upgrades are provided as part of the service. An AWS API is provided for associating/disassociating nodes. By default, the Puppet agent will run automatically every 30 minutes on associated nodes.


OpsWorks for Chef Automate and OpsWorks for Puppet Enterprise are strictly designed for configuration management, and do not provision infrastructure outside the Chef Server/Puppet Master that is created in our account.
All three OpsWorks services support managing both Amazon EC2 and on-premises infrastructure, however the implementation details differ slightly.

OpsWorks Stacks allows you to register instances and install the OpsWorks Agent to connect to your stack.
OpsWorks for Chef Automate and OpsWorks for Puppet Enterprise allow you to associate new or existing infrastructure using either the opsworks-cm:AssociateNode API action or the vendor-supported method for associating nodes to Chef Server or Puppet Enterprise.


Although OpsWorks will let you work with common Chef recipes or Puppet modules when creating your stacks, creating custom recipes will require familiarity with Chef or Puppet syntax. Chef/Puppet code is not supported as part of AWS Support.
As of December 2016, OpsWorks Stacks supports Chef versions 12, 11.10.4, 11.4.4 and 0.9.15.5.
As of December 2016, OpsWorks for Chef Automate uses Chef Server version 12.11.1 This is the current stable version of Chef.
Berkshelfcan be used  with Chef stacks of version 11.10 and later for managing cookbooks and their respective dependencies. However, on Chef 12.x stacks, Berkshelf must be installed by the stack administrator.
Running your own Chef environment may be an alternative to consider - some considerations are listed in this Bitlancer article.

OpsWorks Alternatives and Lock-in

Major competitors in Configuration Management include:

Chef
Puppet
Ansible.



OpsWorks Tips

OpsWorks Stacks and OpsWorks for Chef Automate use Chef cookbooks for configuration. Chef provides free training to learn syntax, best practices, etc. at https://learn.chef.io.
OpsWorks for Puppet Enterprise uses Puppet manifests for configuration. Puppet provides a very useful learning VM for download at https://learn.puppet.com/.

OpsWorks Gotchas and Limitations

OpsWorks Stacks is not available in the following regions:

Montreal
GovCloud
Beijing


OpsWorks for Chef Automate and OpsWorks for Puppet Enterprise are not available in the following regions:

Montreal
Sao Paulo
GovCloud
London
Paris
Seoul
Mumbai



Batch
Batch Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing
AWS Batch is a service that offers an environment to run batch computing jobs. The service dynamically provisions the optimal compute resources needed by the jobs based on their resource requirements, and can scale up to hundreds of thousands of jobs.
These batch workloads have access to all other AWS services and features.
AWS Batch, coupled with spot instances can help run the jobs when appropriate capacity is available, providing for optimal utilization of compute resources.
The batch workloads are built as a Docker Image. These images can then pushed to the EC2 Container Registry (ECR), or any private repository that can be accessed from AWS.
A Job Definition has the workload's Docker Image URI, and also lets the users specify the environment details like vCPUs, memory, volume mappings, environment variables, parameters, retry strategy, container properties, and the job's IAM role.
The Compute Environments are EC2 clusters that provide the runtime for the batch workloads to execute in.
AWS Batch provides managed, as well as unmanaged compute environments. The Managed Environments are provisioned and managed by AWS, while the Unmanaged Environments are managed by the customers.
The Job Definitions are submitted to Job Queue(s) for execution. Each queue has a priority, and has at least one Compute Environment associated with it.
AWS Batch uses ECS to execute the containerized jobs.

Batch Tips

AWS Batch supports prioritization of jobs via the Job Queue Priority. Higher the number - higher the priority.
AWS Batch supports launching the Compute Environment into specific VPC and subnets.
A Compute Environment is same as an ECS Cluster.
There is no additional cost for AWS Batch. You only pay the cost associated with the AWS Services being used - like EC2 Instances and any resources consumed by the batch jobs.
Associate IAM Roles and policies with the Compute Environment to enable the containers access to other AWS resources.
üîπ Use Unmanaged Compute Environments if you need specialized resources like Dedicated Hosts, or EFS.

SQS
SQS Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing 
SQS is a highly scalable, fully managed message queuing service from AWS.
SQS supports the pull model, where the producers queue the messages, and the consumers pull messages off the queue.
SQS provides a message visibility timeout, during which the message being processed will not be delivered to other consumers. If the consumer does not delete the message after processing, the message becomes available to other consumers upon reaching the message visibility timeout. This parameter is called VisibilityTimeout.
Each message can have up to 10 custom fields, or attributes.
SQS allows producers to set up to 15 minutes of delay before the messages are delivered to the consumers. This parameter is called DelaySeconds.
There are two types of queues supported by SQS -

Standard Queues

Guarantee at least once delivery of the messages.
Do not retain the order of delivery of the messages.


FIFO Queues

Guarantee only once delivery of the messages
Guarantee the order of the delivery of the messages




SQS supports fine grained access to various API calls and Queues via IAM policies.
The messages that fail to process can be put in a dead letter queue.

SQS Alternatives and Lock-In

Alternatives to SQS include Kafka, RabbitMQ, ActiveMQ and others.
Google Cloud Platform has Pub/Sub, and Azure has Azure Queue Service.
SQS vs SNS

SQS Tips

SNS can be used in combination of SQS to build a ‚Äúfan out‚Äù mechanism by having an SQS Queue subscribe to the SNS topic.
SQS supports encryption using AWS KMS.
Cloudwatch alarms can be creating using various SQS metrics to trigger autoscaling actions and/or notifications.

SQS Gotchas and Limitations

üî∏ SQS does not have a VPC endpoint (unlike S3 and DynamoDB), so SQS will need to be accessed using public SQS API endpoints.
üî∏ FIFO Queues are limited to 300 API calls per second.
üî∏ FIFO Queues cannot subscribe to an SNS topic.
üî∏ Standard Queues can deliver duplicate messages regardless of the visibility window. If only-once delivery is your only choice, then use FIFO queues, or build an additional layer to de-dupe the messages.
üî∏ You can send/receive messages in batch, however, there can only be maximum of 10 messages in a batch.

SNS
SNS Basics

üìí Homepage ‚àô Documentation ‚àô FAQ ‚àô Pricing
SNS (Simple Notification Service) is a pub/sub based, highly scalable, and fully managed messaging service that can also be used for mobile notifications.
SNS can push the messages down to the subscribers via SMS, Email, SQS, and HTTP/S transport protocols.
Producers publish messages to a SNS Topics, which can have many subscribers.
Each subscription has an associated protocol, which is used to notify the subscriber.
A copy of the message is sent to each subscriber using the associated protocol.
SNS can also invoke lambda functions.

SNS Alternatives and Lock-In

Popular alternatives to SNS are Kafka, Notification Hubs on Azure, and Pub/Sub on Google Cloud.
SNS vs SQS:

Both SNS and SQS are highly scalable, fully managed messaging services provided by AWS.
SQS supports a pull model, while SNS supports a push model. Consumers have to pull messages from an SQS Queue, while they're pushed the message from an SNS Topic.
An SQS message is intended to be processed by only one subscriber, while SNS topics can have many subscribers.
After processing, the SQS message is deleted from the queue by the subscriber to avoid being re-processed.
An SNS message is pushed to all subscribers of the topic at the same time, and is not available for deletion at the topic.
SNS supports multiple transport protocols of delivery of the messages to the subscribers, while SQS subscribers have to pull the messages off the queue over HTTPS.



SNS Tips

Fan-out architecture can be achieved by having multiple subscribers for a topic. This is particularly useful when events have to be fanned out to multiple, isolated systems.
SNS topics can be used to power webhooks with backoff support to subscribers over HTTP/S.
SQS queues can subscribe to SNS topics.
SNS is used to manage notifications for other AWS services like Autoscaling Groups' notifications, CloudWatch Alarms, etc.
SNS is frequently used as ‚Äúglue‚Äù between disparate systems‚Äî such as GitHub and AWS services.

SNS Gotchas and Limitations

üî∏ HTTP/S subscribers of SNS topics need to have public endpoints, as SNS does not support calling private endpoints (like those in a private subnet within a VPC).
üìú In a fan-out scenario, SSE-enabled SQS subscribers of an SNS topic will not receive the messages sent to the topic.

High Availability
This section covers tips and information on achieving high availability.
High Availability Tips

AWS offers two levels of redundancy, regions and availability zones (AZs).
When used correctly, regions and zones do allow for high availability. You may want to use non-AWS providers for larger business risk mitigation (i.e. not tying your company to one vendor), but reliability of AWS across regions is very high.
Multiple regions: Using multiple regions is complex, since it‚Äôs essentially like managing completely separate infrastructures. It is necessary for business-critical services with the highest levels of redundancy. However, for many applications (like your average consumer startup), deploying extensive redundancy across regions may be overkill.
The High Scalability Blog has a good guide to help you understand when you need to scale an application to multiple regions.
üîπMultiple AZs: Using AZs wisely is the primary tool for high availability!

A typical single-region high availability architecture would be to deploy in two or more availability zones, with load balancing in front, as in this AWS diagram.
The bulk of outages in AWS services affect one zone only. There have been rare outages affecting multiple zones simultaneously (for example, the great EBS failure of 2011) but in general most customers‚Äô outages are due to using only a single AZ for some infrastructure.
Consequently, design your architecture to minimize the impact of AZ outages, especially single-zone outages.
Deploy key infrastructure across at least two or three AZs. Replicating a single resource across more than three zones often won‚Äôt make sense if you have other backup mechanisms in place, like S3 snapshots.
A second or third AZ should significantly improve availability, but additional reliability of 4 or more AZs may not justify the costs or complexity (unless you have other reasons like capacity or Spot market prices).
üí∏Watch out for cross-AZ traffic costs. This can be an unpleasant surprise in architectures with large volume of traffic crossing AZ boundaries.
Deploy instances evenly across all available AZs, so that only a minimal fraction of your capacity is lost in case of an AZ outage.
If your architecture has single points of failure, put all of them into a single AZ. This may seem counter-intuitive, but it minimizes the likelihood of any one SPOF to go down on an outage of a single AZ.


EBS vs instance storage: For a number of years, EBSs had a poorer track record for availability than instance storage. For systems where individual instances can be killed and restarted easily, instance storage with sufficient redundancy could give higher availability overall. EBS has improved, and modern instance types (since 2015) are now EBS-only, so this approach, while helpful at one time, may be increasingly archaic.
Be sure to use and understand CLBs/ALBs appropriately. Many outages are due to not using load balancers, or misunderstanding or misconfiguring them.

High Availability Gotchas and Limitations

üî∏AZ naming differs from one customer account to the next. Your ‚Äúus-west-1a‚Äù is not the same as another customer‚Äôs ‚Äúus-west-1a‚Äù ‚Äî the letters are assigned to physical AZs randomly per account. This can also be a gotcha if you have multiple AWS accounts. Note that Zone IDs are consistent between accounts, and can be used to reliably align between AWS accounts.
üî∏üí∏Cross-AZ traffic is not free. At large scale, the costs add up to a significant amount of money. If possible, optimize your traffic to stay within the same AZ as much as possible.

Billing and Cost Management
Billing and Cost Visibility

AWS offers a free tier of service, that allows very limited usage of resources at no cost. For example, a micro instance and small amount of storage is available for no charge. Many services are only eligible for the free tier for the first twelve months that an account exists, but other services offer a free usage tier indefinitely. (If you have an old account but starting fresh, sign up for a new one to qualify for the free tier.) AWS Activate extends this to tens of thousands of dollars of free credits to startups in certain funds or accelerators.
You can set billing alerts to be notified of unexpected costs, such as costs exceeding the free tier. You can set these in a granular way.
AWS offers Cost Explorer, a tool to get better visibility into costs.
Unfortunately, the AWS console and billing tools are rarely enough to give good visibility into costs. For large accounts, the AWS billing console can time out or be too slow to use.
Tools:

üîπEnable billing reports and install an open source tool to help manage or monitor AWS resource utilization. Teevity Ice (originally written by Netflix) is probably the first one you should try. Check out docker-ice for a Dockerized version that eases installation.
üî∏One challenge with Ice is that it doesn‚Äôt cover amortized cost of reserved instances.
Other tools include Security Monkey and Cloud Custodian.
Use AWS Simple Monthly Calculator to get an estimate of usage charges for AWS services based on certain information you provide. Monthly charges will be based on your actual usage of AWS services, and may vary from the estimates the Calculator has provided.


Third-party services: Several companies offer services designed to help you gain insights into expenses or lower your AWS bill, such as Cloudability, CloudHealth Technologies, and ParkMyCloud. Some of these charge a percentage of your bill, which may be expensive. See the market landscape.
AWS‚Äôs Trusted Advisor is another service that can help with cost concerns.
Don‚Äôt be shy about asking your account manager for guidance in reducing your bill. It‚Äôs their job to keep you happily using AWS.
Tagging for cost visibility: As the infrastructure grows, a key part of managing costs is understanding where they lie. It‚Äôs strongly advisable to tag resources, and as complexity grows, group them effectively. If you set up billing allocation appropriately, you can then get visibility into expenses according to organization, product, individual engineer, or any other way that is helpful.
If you need to do custom analysis of raw billing data or want to feed it to a third party cost analysis service, enable the detailed billing report feature.
Multiple Amazon accounts can be linked for billing purposes using the Consolidated Billing feature. Large enterprises may need complex billing structures depending on ownership and approval processes.
Multiple Amazon accounts can be managed centrally using AWS Organizations.

AWS Data Transfer Costs

For deployments that involve significant network traffic, a large fraction of AWS expenses are around data transfer. Furthermore, costs of data transfer, within AZs, within regions, between regions, and into and out of AWS and the internet vary significantly depending on deployment choices.
Some of the most common gotchas:

üî∏AZ-to-AZ traffic: Note EC2 traffic between AZs is effectively the same as between regions. For example, deploying a Cassandra cluster across AZs is helpful for high availability, but can hurt on network costs.
üî∏Using public IPs when not necessary: If you use an Elastic IP or public IP address of an EC2 instance, you will incur network costs, even if it is accessed locally within the AZ.
üî∏Managed NAT Gateway data processing: Managed NAT Gateways are used to let traffic egress from private subnets--at a cost of 4.5¬¢ as a data processing fee layered on top of data transfer pricing. Past a certain point, running your own NAT instances becomes far more cost effective.
üî∏Some services do cross-AZ traffic for free: Many AWS services you'd not consider on their own merits offer a hidden value of free cross-AZ data transfer. EFS, RDS, MSK, and others are examples of this.


This figure gives an overview:


EC2 Cost Management

With EC2, there is a trade-off between engineering effort (more analysis, more tools, more complex architectures) and spend rate on AWS. If your EC2 costs are small, many of the efforts here are not worth the engineering time required to make them work. But once you know your costs will be growing in excess of an engineer‚Äôs salary, serious investment is often worthwhile.
Larger instances aren‚Äôt necessarily priced higher in the spot market ‚Äì therefore, you should look at the available options and determine which instances will be most cost effective for your jobs. See Bid Advisor.
üîπSpot instances:

EC2 Spot instances are a way to get EC2 resources at significant discount ‚Äî often many times cheaper than standard on-demand prices ‚Äî if you‚Äôre willing to accept the possibility that they be terminated with little to no warning.
Use Spot instances for potentially very significant discounts whenever you can use resources that may be restarted and don‚Äôt maintain long-term state.
The huge savings that you can get with Spot come at the cost of a significant increase in complexity when provisioning and reasoning about the availability of compute capacity.
Amazon maintains Spot prices at a market-driven fluctuating level, based on their inventory of unused capacity. Prices are typically low but can spike very high. See the price history to get a sense for this.
You set a bid price high to indicate how high you‚Äôre willing to pay, but you only pay the going rate, not the bid rate. If the market rate exceeds the bid, your instance may be terminated.
Prices are per instance type and per availability zone. The same instance type may have wildly different price in different zones at the same time. Different instance types can have very different prices, even for similarly powered instance types in the same zone.
Compare prices across instance types for better deals.
Use Spot instances whenever possible. Setting a high bid price will assure your machines stay up the vast majority of the time, at a fraction of the price of normal instances.
Get notified up to two minutes before price-triggered shutdown by polling your Spot instances‚Äô metadata, or by watching for the termination CloudWatch event.
Make sure your usage profile works well for Spot before investing heavily in tools to manage a particular configuration.


Spot fleet:

You can realize even bigger cost reductions at the same time as improvements to fleet stability relative to regular Spot usage by using Spot fleet to bid on instances across instance types, availability zones, and (through multiple Spot Fleet Requests) regions.
Spot fleet targets maintaining a specified (and weighted-by-instance-type) total capacity across a cluster of servers. If the Spot price of one instance type and availability zone combination rises above the weighted bid, it will rotate running instances out and bring up new ones of another type and location up in order to maintain the target capacity without going over target cluster cost.


Spot usage best practices:

Application profiling:

Profile your application to figure out its runtime characteristics. That would help give an understanding of the minimum cpu, memory, disk required. Having this information is critical before you try to optimize spot costs.
Once you know the minimum application requirements, instead of resorting to fixed instance types, you can bid across a variety of instance types (that gives you higher chances of getting a spot instance to run your application).E.g., If you know that 4 cpu cores are enough for your job, you can choose any instance type that is equal or above 4 cores and that has the least Spot price based on history. This helps you bid for instances with greater discount (less demand at that point).


Spot price monitoring and intelligence:

Spot Instance prices fluctuate depending on instance types, time of day, region and availability zone. The AWS CLI tools and API allow you to describe Spot price metadata given time, instance type, and region/AZ.
Based on history of Spot instance prices, you could potentially build a myriad of algorithms that would help you to pick an instance type in a way that optimizes cost, maximizes availability, or offers predictable performance.
You can also track the number of times an instance of certain type got taken away (out bid) and plot that in graphite to improve your algorithm based on time of day.


Spot machine resource utilization:

For running spiky workloads (spark, map reduce jobs) that are schedule based and where failure is non critical, Spot instances become the perfect candidates.
The time it takes to satisfy a Spot instance could vary between 2-10 mins depending on the type of instance and availability of machines in that AZ.
If you are running an infrastructure with hundreds of jobs of spiky nature, it is advisable to start pooling instances to optimize for cost, performance and most importantly time to acquire an instance.
Pooling implies creating and maintaining Spot instances so that they do not get terminated after use. This promotes re-use of Spot instances across jobs. This of course comes with the overhead of lifecycle management.
Pooling has its own set of metrics that can be tracked to optimize resource utilization, efficiency and cost.
Typical pooling implementations give anywhere between 45-60% cost optimizations and 40% reduction in spot instance creation time.
An excellent example of Pooling implementation described by Netflix (part1, part2)




Spot management gotchas

üî∏Lifetime: There is no guarantee for the lifetime of a Spot instance. It is purely based on bidding. If anyone outbids your price, the instance is taken away. Spot is not suitable for time sensitive jobs that have strong SLA. Instances will fail based on demand for Spot at that time. AWS provides a two-minute warning before Amazon EC2 must terminate your Spot instance.
üîπAPI return data: - The Spot price API returns Spot prices of varying granularity depending on the time range specified in the api call.E.g If the last 10 min worth of history is requested, the data is more fine grained. If the last 2 day worth of history is requested, the data is more coarser. Do not assume you will get all the data points. There will be skipped intervals.
‚ùóLifecycle management: Do not attempt any fancy Spot management unless absolutely necessary. If your entire usage is only a few machines and your cost is acceptable and your failure rate is lower, do not attempt to optimize. The pain for building/maintaining it is not worth just a few hundred dollar savings.


Reserved Instances: allow you to get significant discounts on EC2 compute hours in return for a commitment to pay for instance hours of a specific instance type in a specific AWS region and availability zone for a pre-established time frame (1 or 3 years). Further discounts can be realized through ‚Äúpartial‚Äù or ‚Äúall upfront‚Äù payment options.

Consider using Reserved Instances when you can predict your longer-term compute needs and need a stronger guarantee of compute availability and continuity than the (typically cheaper) Spot market can provide. However be aware that if your architecture changes your computing needs may change as well so long term contracts can seem attractive but may turn out to be cumbersome.
There are two types of Reserved Instances - Standard and Convertible. If you purchase excess Standard Reserved Instances, you may offer to ‚Äúsell back‚Äù unused Reserved Instances via the Reserved Instance Marketplace, this allows you to potentially recoup the cost of unused EC2 compute instance hours by selling them to other AWS customers.
Instance reservations are not tied to specific EC2 instances - they are applied at the billing level to eligible compute hours as they are consumed across all of the instances in an account.
üìúThere have been scattered reports of Convertible RI purchases needing to be exercised in a block-- namely, if you buy five convertible RIs in one purchase, you can't convert just two of them. Reach out to your account manager for clarification if this may impact you.


If you have multiple AWS accounts and have configured them to roll charges up to one account using the ‚ÄúConsolidated Billing‚Äù feature, you can expect unused Reserved Instance hours from one account to be applied to matching (region, availability zone, instance type) compute hours from another account.
If you have multiple AWS accounts that are linked with Consolidated Billing, plan on using reservations, and want unused reservation capacity to be able to apply to compute hours from other accounts, you‚Äôll need to create your instances in the availability zone with the same name across accounts. Keep in mind that when you have done this, your instances may not end up in the same physical data center across accounts - Amazon shuffles availability zones names across accounts in order to equalize resource utilization.
Make use of dynamic Auto Scaling, where possible, in order to better match your cluster size (and cost) to the current resource requirements of your service.
If you use RHEL instances and happen to have existing RHEL on-premise Red Hat subscriptions, then you can leverage Red Hat's Cloud Access program to migrate a portion of your on-premise subscriptions to AWS, and thereby saving on AWS charges for RHEL subscriptions. You can either use your own self-created RHEL AMI's or Red Hat provided Gold Images that will be added to your private AMI's once you sign up for Red Hat Cloud Access.

Further Reading
This section covers a few unusually useful or ‚Äúmust know about‚Äù resources or lists.

AWS

AWS In Plain English: A readable overview of all the AWS services.
Awesome AWS: A curated list of AWS tools and software.
AWS Tips I Wish I'd Known Before I Started: A list of tips from Rich Adams
AWS Whitepapers: A list of technical AWS whitepapers, covering topics such as architecture, security and economics.
Last Week in AWS: A weekly email newsletter covering the latest happenings in the AWS ecosystem.
AWS Geek: A blog by AWS Community Hero Jerry Hargrove, with notes and hand-drawn diagrams about various AWS services.


Books

Amazon Web Services in Action
AWS Lambda in Action
Serverless Architectures on AWS
Serverless Single Page Apps
The Terraform Book
AWS Scripted 2 book series
Amazon Web Services For Dummies
AWS System Administration
Python and AWS Cookbook
Resilience and Reliability on AWS
AWS documentation as Kindle ebooks


General references

AWS Well Architected Framework Guide: Amazon‚Äôs own 56 page guide to operational excellence - guidelines and checklists to validate baseline security, reliability, performance (including high availability) and cost optimization practices.
Awesome Microservices: A curated list of tools and technologies for microservice architectures. Worth browsing to learn about popular open source projects.
Is it fast yet?: Ilya Grigorik‚Äôs TLS performance overview
High Performance Browser Networking: A full, modern book on web network performance; a presentation on the HTTP/2 portion is here.



Disclaimer
The authors and contributors to this content cannot guarantee the validity of the information found here. Please make sure that you understand that the information provided here is being provided freely, and that no kind of agreement or contract is created between you and any persons associated with this content or project. The authors and contributors do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by errors or omissions in the information contained in, associated with, or linked from this content, whether such errors or omissions result from negligence, accident, or any other cause.
License

This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
",GitHub - open-guides/og-aws: üìô Amazon Web Services ‚Äî a practical guide
63,Shell,"Á®ãÂ∫èÂëòÊâæÂ∑•‰ΩúÈªëÂêçÂçï
üêåÊàë‰πüÂè™ÊòØÂõ¥ËßÇÁæ§‰ºóÔºåËÉåÂêéÁöÑÁúüÁõ∏Â§ßÂÆ∂Ëá™Â∑±Âà§Êñ≠ÔºåÊàëÂè™ÊòØÂçïÁ∫ØÁöÑÊî∂Ëóè‰∏Ä‰∫õÈìæÊé•Ôºå‰∏çÂñúÂãøÂñ∑ÔºåÊ¨¢ËøéÂ§ßÂÆ∂ÂÖ≥Ê≥®ÊàëÂæÆÂçöÂíåÊèê issues Êù•ÂèÇ‰∏éÁª¥Êä§Ëøô‰∏™ÂêçÂçï
ÊúÄËøëÁöÑ‰∫ãÊÉÖ ËÆ©ÊàëÊÑèËØÜÂà∞ Êúâ‰∫õÂêåË°å‰πüÊòØÂä£ËøπÊñëÊñëÔºåÈúÄË¶ÅËÆ∞ÂΩï‰∏Ä‰∏ã ‰ª•ÂêéÂ§ßÂÆ∂ÊãõËÅòÊ≥®ÊÑè
ÂÜôÂú®ÂâçÈù¢
Ë∞¢Ë∞¢ issues ÊèêÈÜí
È¶ñÂÖàÔºåÊØè‰∏Ä‰∏™Á®ãÂ∫èÁåøÔºåÊØè‰∏Ä‰∏™ÂÖ¨Âè∏ÈÉΩÊúâÈªëÁÇπÔºå‰ΩÜÊòØÂ¶ÇÊûúÊääÈªëÁÇπÊîæÂ§ß‰∫ÜÔºåÈªëÂêçÂçïÂ∞±Ê≤°Â≠òÂú®ÁöÑÊÑè‰πâ‰∫Ü„ÄÇÂ∞±ÊãøËøôÂá†‰∏™‰æãÂ≠êÊù•Âêß


‰∏ñÁ∫™‰Ω≥ÁºòËÆ§‰∏∫ÁôΩÂ∏ΩÂ≠êÔºåÊòØËøùÊ≥ïÁöÑ

ËôΩÁÑ∂Ëøô‰ª∂‰∫ãÊòØÊàëÊÄíÂºÄ repo ÁöÑËµ∑Âõ†Ôºå‰ΩÜÊòØ‰ªñÊ≤°ÊúâÊãñÊ¨†ÂëòÂ∑•Â∑•ËµÑÊàñ‰∏ç‰ªòÂ∑•ËµÑ



‰∏éÂàõÂßã‰∫∫ËØ¥ÁöÑËÇ°‰ªΩÔºåÈÇ£ÊòØ‰∏™‰∫∫‰∫âÂèñÁöÑ‰∫ã

‰πüÊ≤°ÊãñÊ¨†Â∑•ËµÑÊàñ‰∏ç‰ªòÂ∑•ËµÑ
Âè™ÊòØËÆ∞‰Ωè‰ª•ÂêéËÆ∞ÂæóÁïô‰∏™ÂøÉÁúºÔºåÂíåÂêà‰ºô‰∫∫‰∏çÊòØÂè™ÂÜô‰ª£Á†ÅÂ∞±ÂèØ‰ª•ÁöÑ



Âçé‰∏∫Ê∏ÖÁêÜ 34 Â≤Å‰ª•‰∏äÂëòÂ∑•Ôºå‰πüÊòØÊåâÁÖßÂêàÂêåË£ÅÂëòÔºåÁªôËµîÂÅø

Â§ßÂÖ¨Âè∏Êõ¥Êñ∞Êç¢‰ª£ÔºåË£ÅÂëòÊ≤°Êúâ‰ªª‰ΩïÈóÆÈ¢òÔºå‰∏çË£ÅÂëòÁöÑÂÖ¨Âè∏Êó©ÊôöÂÄíÈó≠
Âçé‰∏∫‰πüÊ≤°ÊúâÊãñÊ¨†Â∑•ËµÑ
Âè™ÊòØÊàë‰∏™‰∫∫ËßâÂæóÔºåÂõ†‰∏∫Ëøô‰ª∂‰∫ãÔºå‰∏çÊÉ≥ÂéªÂçé‰∏∫Â∑•‰Ωú ‰ªÖÊ≠§ËÄåÂ∑≤ÔºåÂÆ≥ÊÄïÂà∞‰∫Ü34Â∞±Ë¢´Âπ≤Êéâ



Êúâ‰∫õ‰∫ãÊÉÖÔºåÂ∫îËØ•Ë¢´ËÆ∞‰Ωè


Á®ãÂ∫èÂëòÈªëÂêçÂçï

Â¶Ç‰ΩïÁúãÂæÖÊë©ÊãúÂâçÁ´ØË¥üË¥£‰∫∫Â∞èÊò•ÔºàÂº†ËÄÄÊò•ÔºâÁñë‰ººÊÄßÈ™öÊâ∞Â•≥‰∏ãÂ±ûÔºü

ÊôÆÈÄöÁ®ãÂ∫èÂëò


Êù≠Â∑ûÊúâËµû

Â¶Ç‰ΩïÁúãÂæÖ‚ÄúÊúâËµû‚ÄùÂπ¥‰ºöÂÖ¨ÂºÄÂÆ£Â∏É996Â∑•‰ΩúÂà∂Ôºü
ÊúâËµû‰ºöÂæóÂà∞Ê≥ïÂæãÁöÑÊÉ©ÁΩöÂêóÔºü
ËÑâËÑâÂåøÂêçÂå∫ÁöÑÊúâËµûÂëòÂ∑•



‰æøÂà©ËúÇ

Âπ¥Â∫ïÁªÑÁªáÊï∞Â≠¶ËÄÉËØïÔºåËÄÉËØï‰∏çÂèäÊ†ºÂ∞±ÂºÄÈô§„ÄÇ
ËÑâËÑâÂåøÂêçÂå∫ÁöÑ‰æøÂà©ËúÇ



‰∏äÊµ∑Á¢ßÂçé‰ø°ÊÅØÊäÄÊúØÊúâÈôêÂÖ¨Âè∏

AndroidÂºÄÂèëËÄÖÁöÑËâ∞ÈöæËÆ®Ëñ™Ë∑ØÁ®ã



‰∏ñÁ∫™‰Ω≥Áºò

ÁôΩÂ∏Ω‰∫ã‰ª∂



Ë•øÂ±±Â±Ö

Â¶Ç‰ΩïÁúãÂæÖË•øÂ±±Â±ÖËøëÊúüÁöÑÂ§ßËßÑÊ®°„ÄåÂäùÈÄÄ„Äç



ÁôæÂ∫¶Êó†‰∫∫ËΩ¶

Â¶Ç‰ΩïÁúãÂæÖ‰∏∫‰∫ÜÊåëÊàòÁôæÂ∫¶Êó†‰∫∫ËΩ¶ÂÆâÂÖ®ÊÄßÔºåÊùéÂΩ¶ÂÆèËØï‰πòÊó∂ËÆ©Â∑•Á®ãÂ∏à‰∫∫‰∏∫Êã¶ËΩ¶Ôºü



Ëß¶ÊéßÁßëÊäÄ

Â¶Ç‰ΩïÁúãÂæÖËß¶Êéß‰∏éÂªñÂ§ßËß£Á∫¶Ôºü



Âçé‰∏∫

Âçé‰∏∫Ê∏ÖÁêÜ 34 Â≤Å‰ª•‰∏äÂëòÂ∑•
‰ªªÊ≠£ÈùûÂõûÂ∫îÂçé‰∏∫Ê∏ÖÁêÜËÄÅÂëòÂ∑•ÔºöÂçé‰∏∫‰∏çÂÖªÂêÉÁôΩÈ•≠ÁöÑ‰∫∫



Èü≥ÊÇ¶Âè∞

‰Ω†‰∏∫‰ªÄ‰πà‰ªéÈü≥ÊÇ¶Âè∞Á¶ªËÅåÔºü



Áè†Êµ∑Êó∂‰ª£‰∫íËÅî

Âú®Áè†Êµ∑Êó∂‰ª£‰∫íËÅî(‰ºòÂìÅÁßëÊäÄ)Â∑•‰ΩúÊòØÊÄéÊ†∑‰∏ÄÁï™‰ΩìÈ™å
Á¨¨‰∏ÄÊ¨°Âê¨ËØ¥Â∑•ËµÑÂèØ‰ª•ÊâìÂÖ´Êäò



ÈòøÈáåÈíâÈíâ

Â¶Ç‰ΩïÁúãÂæÖÈòøÈáå„ÄåÊ≠•Êï∞Âû´Â∫ïÁΩöÁ´ô‰∏ÄÂë®„Äç



4399

‰Ω†‰∏∫‰ªÄ‰πà‰ªé 4399 Á¶ªËÅåÔºü



ÈÖ∑Ê¥æ

Â¶Ç‰ΩïÁúãÂæÖÈÖ∑Ê¥æÂú®ÊØï‰∏öÂÖ•ËÅåÂâçÂ§ïÁñë‰ººËß£Á∫¶ 300 ‰ΩôÂêçÊ†°ÊãõÂ∫îÂ±äÊØï‰∏öÁîüÔºü



ËõãÂ£≥ÂÖ¨ÂØì

ÊàëÂú®ËõãÂ£≥ÂÖ¨ÂØìÁöÑÊâìÂ∑•ËÆ∞ÂΩï



ÊòéÊòüË°£Ê©±

Â¶Ç‰ΩïÁúãÂæÖÊòéÊòüË°£Ê©±ÊúâÈí±ÊêûVRËØïË°£Èó¥ÔºåÂç¥ÊãñÊ¨†ÂëòÂ∑•Â∑•ËµÑËææ1Âπ¥‰ª•‰∏äÔºü
ËÖæËÆØÊñ∞Èóª

ÈªÑÊôìÊòéÊäïÁöÑÊòéÊòüË°£Ê©±Ë¢´ÁàÜÈïøÊúüÊãñÊ¨†Ëñ™ËµÑ ÂëòÂ∑•Â∑≤Ê±ÇÂä©Ê≥ïÂæãËß£ÂÜ≥
Ê¨†Ê¨æÊ¨†Ëñ™Ê≤°ËûçËµÑÔºåÊòéÊòüË°£Ê©±ËÄÅÊùøË¶ÅÊê∫Â∞è‰∏âË∑ëË∑Ø‰∫ÜÔºü


Êñ∞Êµ™Ë¥¢Áªè

ÊòéÊòüË°£Ê©±ÁîüÊ≠ªÂ±Ä:ÊãñÊ¨†ÂëòÂ∑•Â∑•ËµÑ ËÆ§‰∏∫ËÆ®Ëñ™ÁöÑÊòØËêΩ‰∫ï‰∏ãÁü≥|ËûçËµÑ|Âº†Êô®|ÊµôÊ±üÊ∞∏Âº∫_Êñ∞Êµ™Ë¥¢Áªè_Êñ∞Êµ™ÁΩë





ÊäÄÊúØÂêà‰ºô‰∫∫

Èùí‰∫ë

Áü•‰πéÔºö‰Ω†Â¶Ç‰ΩïÁúãÈùí‰∫ëÂëòÂ∑•ÊúüÊùÉÂêàÂêåÊ¨∫ËØà‰∫ã‰ª∂Ôºü


Ë°£Ê†ºÈπø

‰Ωú‰∏∫‰∏Ä‰∏™ÊäÄÊúØÂêà‰ºô‰∫∫ÔºåÂ¶ÇÊûúÁ¢∞Âà∞‰∫ÜËøôÊ†∑ÁöÑÂÖ¨Âè∏ËøôÊ†∑ÁöÑ‰∫ãÊÉÖÔºå‰Ω†ËØ•ÊÄé‰πàÂäûÔºü
Ë¢´„ÄåÊäÄÊúØÂêà‰ºô‰∫∫„ÄçÈîÄÊØÅÂà†Èô§‰ª£Á†ÅÊï∞ÊçÆ„ÄÅÂÖ≥ÂÅúÊúçÂä°Âô®ÔºåÊíïÈÄºÂèçÂí¨ÊòØ‰∏ÄÁßç‰ªÄ‰πàÊ†∑ÁöÑ‰ΩìÈ™åÔºü


‰∏ÅÈ¶ôÂõ≠

Â¶Ç‰ΩïÁúãÂæÖÂÜØÂ§ßËæâÔºàFenngÔºâ‰ªé‰∏ÅÈ¶ôÂõ≠Á¶ªËÅåÔºü


4399

ÊõπÊîøÔºàcaozÔºâÊòØ‰∏çÊòØË¢´ 4399 ÁöÑËî°ÊñáËÉúÁöÑËôöÂÅáÊâøËØ∫Ê¨∫È™ó‰∫ÜÔºü
Â¶Ç‰ΩïÁúãÂæÖËî°ÊñáËÉúÊääÊõπÊîøÁªôÂùë‰∫ÜËøô‰ª∂‰∫ãÔºü


55Êµ∑Ê∑ò

Âú® 55 Êµ∑Ê∑òÂ∑•‰ΩúÊòØÊÄé‰πàÊ†∑‰∏ÄÁßç‰ΩìÈ™åÔºü


ÂºÇ‰π°Â•ΩÂ±Ö

Â¶Ç‰ΩïÁúãÂæÖÂºÇ‰π°Â•ΩÂ±ÖËÄÅÊùøÂ®òÊéßÂëäÁ®ãÂ∫èÂëòÂà†‰ª£Á†ÅÔºü
Êàë‰∏∫‰ªÄ‰πà‚ÄúÊòéÁõÆÂº†ËÉÜ‚ÄùÂú∞ÂíåÂºÇ‰π°Â•ΩÂ±ÖÊíïÈÄºÔºüÂõ†‰∏∫ÊàëÊòØÂçÉÂçÉ‰∏á‰∏áÊôÆÈÄöÁ®ãÂ∫èÂëòÁöÑ‰∏ÄÂêç


Âåó‰∫¨Â±ïÁ®ãÁßëÊäÄ

Â∞±ÁÆóËÄÅÂÖ¨‰∏ÄÊØõÈí±ËÇ°‰ªΩÈÉΩÊ≤°ÊãøÂà∞ÔºåÂú®ÊàëÂøÉÈáåÔºå‰ªñ‰æùÁÑ∂ÊòØÊúÄÁâõÈÄºÁöÑÂàõ‰∏öËÄÖ
Â¶Ç‰ΩïËØÑ‰ª∑„ÄäÂ∞±ÁÆóËÄÅÂÖ¨‰∏ÄÊØõÈí±ËÇ°‰ªΩÈÉΩÊ≤°ÊãøÂà∞ÔºåÂú®ÊàëÂøÉÈáåÔºå‰ªñ‰æùÁÑ∂ÊòØÊúÄÁâõÈÄºÁöÑÂàõ‰∏öËÄÖ„ÄãÈáåÈù¢Ëøô‰Ωç CEO ÁöÑÊâÄ‰ΩúÊâÄ‰∏∫Ôºü
ÂØπÈÇ£‰∏™Âàõ‰∏öËÄÖÁöÑÂ¶ªÂ≠êÊèê‰∏Ä‰∏™ÈóÆÈ¢òÔºå‰∏Ä‰∏™Âª∫ËÆÆ
‰Ωú‰∏∫Èô§ÂΩì‰∫ã‰∫∫Â§ñÊúÄÊúâÊùÉÂäõÂèëË®ÄÁöÑ‰∫∫ÔºåË∞àË∞à„ÄäÂ∞±ÁÆóËÄÅÂÖ¨‰∏ÄÊØõÈí±ËÇ°‰ªΩÈÉΩÊ≤°ÊãøÂà∞ÔºåÂú®ÊàëÂøÉÈáåÔºå‰ªñ‰æùÁÑ∂ÊòØÊúÄÁâõÈÄºÁöÑÂàõ‰∏öËÄÖ„Äã


‰πêÊõºÂ§öÁßëÊäÄ

Â¶Ç‰ΩïËØÑ‰ª∑ÊàêÈÉΩ‰πêÊõºÂ§öÂÖ¨Âè∏Êâì‰∫∫‰∫ã‰ª∂Ôºü
ÊÄé‰πàÁúãÂæÖ„ÄäË°óÁØÆ„ÄãÊâãÊ∏∏Á†îÂèëÂïÜÔºåÊàêÈÉΩ‰πêÊõºÂ§öÁßëÊäÄËøëÊó•ÂèëÁîüÁöÑÊÆ¥ÊâìÁ®ãÂ∫èÂëò‰∫ã‰ª∂Ôºü


Âåó‰∫¨ËÅöÈÅìÁßëÊäÄÊúâÈôêÂÖ¨Âè∏

Ê±â‰∏úÂÆòÂú∫ÁÆó‰ªÄ‰πàÔºåÊàëË¢´ÂàõÊäïÂúà‚ÄúÊöóÁÆó‚ÄùÁöÑÁªèÂéÜÔºåÊ≤ô‰π¶ËÆ∞ÈÉΩÊû∂‰∏ç‰Ωè



‰∏çÁ°ÆÂÆöÁöÑ

Á•ûËàüÊ∑±Âú≥ÊÄªÈÉ®

Á•ûËàüÁîµËÑëÊ∑±Âú≥ÊÄªÈÉ®ÊòØÂê¶Â∏∏Âú®Ê†°Êãõ‰πãÂêéÂá∫Áé∞Â§ßËßÑÊ®°Á¶ªËÅåÔºü‰∏∫‰ªÄ‰πàÔºüÂ¶Ç‰ΩïËØÑ‰ª∑Ôºü



ÈÄâÂÖ¨Âè∏

Âª∫ËÆÆÂ§ßÂÆ∂Âú®ÈÄâÊã©ÂÖ¨Âè∏ÁöÑÊó∂ÂÄôÊë∏Êë∏Â∫ïÂ≠ê http://tianyancha.com ÔºåÂèëÁé∞ÊúâÈóÆÈ¢òÁõ¥Êé•ÈóÆË¥üË¥£‰∫∫ÁúÅÁöÑÂêÉÂìëÂ∑¥‰∫è„ÄÇ
Áü•‰πéÂêàÈõÜÔºöÊúâÂì™‰∫õÂÖ¨Âè∏Áªô‰Ω†Áïô‰∏ã„ÄåËøòÂ•ΩÊ≤°ÂéªËøôÂÆ∂ÂÖ¨Âè∏„ÄçÁöÑÂç∞Ë±°Ôºü

ÂΩìÊäÄÊúØÂêà‰ºô‰∫∫Âª∫ËÆÆ

ÈÄâËá™ÂÜØÂ§ßËæâÁöÑÂæÆÂçö Âè™ÂÜô‰ª£Á†ÅÂíåÂΩìÂêà‰ºô‰∫∫ÔºåËøòÊòØÊúâÂæàÂ§ßÂ∑ÆÂà´ÁöÑÔºåÊàëËßâÂæó‰∏äÈù¢ÁöÑÂª∫ËÆÆÂæà‰∏≠ËÇØ

ÊúÄÂêé


ÊúÄÂêéËØ¥‰∏Ä‰∏ãÂøÉÂæóÔºåÁ´üÁÑ∂ËøòÊúâÂ∏åÊúõ GitHub Â§ÑÁêÜËøô‰∏™ repoÔºåÂéüÂõ†ÊòØÊÉ≥Áª¥Êä§githubËøô‰ªΩÁ∫ØÂáÄÔºåËøòÊää GitHub Ë¢´Â¢ôÂíåÊàëÊåÇÈí©ÔºåÊàëÁúüÊòØÈÜâ‰∫ÜÔºåÂ∞±ÂÉè‰∏ÄÁæ§ÂæÖÂÆ∞ÁöÑÁæîÁæäÔºåÂøÉÊÉ≥ÁùÄÂè™Ë¶ÅËÄÅÂÆû‰∏ÄÁÇπÔºåÂÆ∞ÁöÑÂ∞±ÊòØÂà´‰∫∫‰∏çÊòØÊàë‰∏ÄÊ†∑


ÊàëÂè™ÊòØËÆ∞ÂΩï‰∏Ä‰∏ãËá™Â∑±ÁúãÂà∞ËøáÁöÑ‰∫ã‰ª∂ÔºåÊàëËá™Â∑±‰∏ãÊ¨°Êç¢Â∑•‰ΩúÁöÑÊó∂ÂÄôÈò≤Ê≠¢Ë∏©ÂùëÔºå‰ªÖÊ≠§ËÄåÂ∑≤


Êàë‰πüÊ≤°ÊúâÈÇ£‰πàÊÑ§‰∏ñÂ´â‰øóÔºåÊØîÂ¶ÇÊàë‰ª•ÂêéÊç¢Â∑•‰ΩúÔºåËøòÊòØ‰ºöËÄÉËôëÁôæÂ∫¶ÈòøÈáåÁöÑÔºå‰ΩÜÊòØ‰ºöÈÅøÂÖç‰∏äÈù¢Âá∫Áé∞ÁöÑÂùëÔºåÊØîÂ¶ÇÁôæÂ∫¶Êó†‰∫∫ËΩ¶ÊàëÂ∞±‰∏çÂéªÂï¶ÔºåÈòøÈáåÁöÑÂÖ∂‰ªñÂæàÂ§öÈÉ®Èó®ËøòÊòØÂæàÊ£íÁöÑÔºåÂ∏åÊúõÂà∞Êó∂ÂÄô‰∏ç‰ºöÂõ†‰∏∫Ëøô‰∏™ÂêçÂçïÊääÊàë pass ÂêßüòÇ



Ê±ÇstarÔºåÊîØÊåÅÊàëÁªßÁª≠ÊääËøô‰∏™ÂêçÂçïÁª¥Êä§‰∏ãÂéª,Ë∞¢Ë∞¢ÊîØÊåÅ

",GitHub - shengxinjing/programmer-job-blacklist: Á®ãÂ∫èÂëòÊâæÂ∑•‰ΩúÈªëÂêçÂçïÔºåÊç¢Â∑•‰ΩúÂíåÂΩìÊäÄÊúØÂêà‰ºô‰∫∫ÈúÄË∞®ÊÖéÂïä Êõ¥Êñ∞ÊúâËµû
64,Shell,"tldr






A collection of simplified and community-driven man pages.
Install it with npm install -g tldr
or try the web client.
What is tldr?
New to the command-line world? Or just a little rusty?
Or perhaps you can't always remember the arguments to lsof, or tar?
Maybe it doesn't help that the first option explained in man tar is:
-b blocksize
   Specify the block size, in 512-byte records, for tape drive I/O.
   As a rule, this argument is only needed when reading from or writing to tape drives,
   and usually not even then as the default block size of 20 records (10240 bytes) is very common.

Surely people could benefit from simplified man pages
focused on practical examples.
How about:

This repository is just that: an ever-growing collection of examples
for the most common UNIX / Linux / macOS / SunOS commands.
Clients
You can access these pages on your computer using one of the following clients:

Alfred Workflow
Android clients:

tldroid, available on
Google Play (outdated)


Bash clients:

tldr
tldr-bash-client


C# client
C++ client:
brew install tldr
Chrome Extension available on
Chrome Web Store
Crystal client:
brew install porras/tap/tlcr
Dart client:
pub global activate tldr
Dash docset:
Open Preferences > Downloads > User Contributed and find tldr pages in the list
Docker images:

tldr-docker- Run the tldr command via a docker container: alias tldr='docker run --rm -it -v ~/.tldr/:/root/.tldr/ nutellinoit/tldr'


Elixir client
(binaries not yet available)
Emacs client, available on
MELPA
Go clients:

github.com/pranavraja/tldr:
go get github.com/pranavraja/tldr
(or platform binaries)
4d63.com/tldr:
go get 4d63.com/tldr or brew install 4d63/tldr/tldr
(or platform binaries)
github.com/elecprog/tldr:
go get github.com/elecprog/tldr
(or platform binaries)
github.com/isacikgoz/tldr:
go get github.com/isacikgoz/tldr
(or platform binaries)


iOS clients:

tldr-man-page, available on
App Store
tldr-pages, available on
App Store


Haskell client:
stack install tldr
Node.js client:
npm install -g tldr
OCaml client: opam install tldr
Perl5 client:
cpanm App::tldr
PHP client:
composer global require brainmaestro/tldr
Python clients:

tldr-python-client:
pip install tldr or pacman -S tldr on Arch Linux
tldr.py:
pip install tldr.py


R client:
devtools::install_github('kirillseva/tldrrr')
Ruby client:
gem install tldrb
Rust client:
cargo install tealdeer
Vim Client
Visual Studio Code extension available on Visual Studio Code Marketplace
Web clients:

tldr.jsx: http://tldr.ostera.io/
DistroWatch



There is also a comprehensive
list of clients in our Wiki.
Contributing

Your favourite command isn't covered?
You can think of more examples for an existing command?

Contributions are most welcome!
We strive to maintain a welcoming and collaborative community.
Have a look at the contributing guidelines, and go ahead!
Similar projects


Cheat
allows you to create and view interactive cheatsheets on the command-line.
It was designed to help remind *nix system administrators of options
for commands that they use frequently, but not frequently enough to remember.


Bro pages
are a highly readable supplement to man pages.
Bro pages show concise, common-case examples for Unix commands.
The examples are submitted by the user base, and can be voted up or down;
the best entries are what people see first when they look up a command.


eg
provides detailed examples with explanations on the command line.
Examples come from the repository, but eg supports displaying
custom examples and commands alongside the defaults.


What does ""tldr"" mean?
TL;DR stands for ""Too Long; Didn't Read"".
It originates in Internet slang, where it is used to indicate that a long text
(or parts of it) has been skipped as too lengthy.
Read more in Wikipedia's TL;DR article.
",GitHub - tldr-pages/tldr: üìö Simplified and community-driven man pages
65,Shell,"NEW: pure sh bible (üìñ A collection of pure POSIX sh alternatives to external processes).


pure bash bible A collection of pure bash alternatives to external
processes.
 






The goal of this book is to document commonly-known and lesser-known methods of doing various tasks using only built-in bash features. Using the snippets from this bible can help remove unneeded dependencies from scripts and in most cases make them faster. I came across these tips and discovered a few while developing neofetch, pxltrm and other smaller projects.
The snippets below are linted using shellcheck and tests have been written where applicable. Want to contribute? Read the CONTRIBUTING.md. It outlines how the unit tests work and what is required when adding snippets to the bible.
See something incorrectly described, buggy or outright wrong? Open an issue or send a pull request. If the bible is missing something, open an issue and a solution will be found.

This book is also available to purchase on leanpub. https://leanpub.com/bash
Or you can buy me a coffee.
  


Table of Contents

FOREWORD
STRINGS

Trim leading and trailing white-space from string
Trim all white-space from string and truncate spaces
Use regex on a string
Split a string on a delimiter
Change a string to lowercase
Change a string to uppercase
Reverse a string case
Trim quotes from a string
Strip all instances of pattern from string
Strip first occurrence of pattern from string
Strip pattern from start of string
Strip pattern from end of string
Percent-encode a string
Decode a percent-encoded string
Check if string contains a sub-string
Check if string starts with sub-string
Check if string ends with sub-string


ARRAYS

Reverse an array
Remove duplicate array elements
Random array element
Cycle through an array
Toggle between two values


LOOPS

Loop over a range of numbers
Loop over a variable range of numbers
Loop over an array
Loop over an array with an index
Loop over the contents of a file
Loop over files and directories


FILE HANDLING

Read a file to a string
Read a file to an array (by line)
Get the first N lines of a file
Get the last N lines of a file
Get the number of lines in a file
Count files or directories in directory
Create an empty file
Extract lines between two markers


FILE PATHS

Get the directory name of a file path
Get the base-name of a file path


VARIABLES

Assign and access a variable using a variable
Name a variable based on another variable


ESCAPE SEQUENCES

Text Colors
Text Attributes
Cursor Movement
Erasing Text


PARAMETER EXPANSION

Indirection
Replacement
Length
Expansion
Case Modification
Default Value


BRACE EXPANSION

Ranges
String Lists


CONDITIONAL EXPRESSIONS

File Conditionals
File Comparisons
Variable Conditionals
Variable Comparisons


ARITHMETIC OPERATORS

Assignment
Arithmetic
Bitwise
Logical
Miscellaneous


ARITHMETIC

Simpler syntax to set variables
Ternary Tests


TRAPS

Do something on script exit
Ignore terminal interrupt (CTRL+C, SIGINT)
React to window resize
Do something before every command
Do something when a shell function or a sourced file finishes executing


PERFORMANCE

Disable Unicode


OBSOLETE SYNTAX

Shebang
Command Substitution
Function Declaration


INTERNAL VARIABLES

Get the location to the bash binary
Get the version of the current running bash process
Open the user's preferred text editor
Get the name of the current function
Get the host-name of the system
Get the architecture of the Operating System
Get the name of the Operating System / Kernel
Get the current working directory
Get the number of seconds the script has been running
Get a pseudorandom integer


INFORMATION ABOUT THE TERMINAL

Get the terminal size in lines and columns (from a script)
Get the terminal size in pixels
Get the current cursor position


CONVERSION

Convert a hex color to RGB
Convert an RGB color to hex


CODE GOLF

Shorter for loop syntax
Shorter infinite loops
Shorter function declaration
Shorter if syntax
Simpler case statement to set variable


OTHER

Use read as an alternative to the sleep command
Check if a program is in the user's PATH
Get the current date using strftime
Get the username of the current user
Generate a UUID V4
Progress bars
Get the list of functions in a script
Bypass shell aliases
Bypass shell functions
Run a command in the background


AFTERWORD


FOREWORD
A collection of pure bash alternatives to external processes and programs. The bash scripting language is more powerful than people realise and most tasks can be accomplished without depending on external programs.
Calling an external process in bash is expensive and excessive use will cause a noticeable slowdown. Scripts and programs written using built-in methods (where applicable) will be faster, require fewer dependencies and afford a better understanding of the language itself.
The contents of this book provide a reference for solving problems encountered when writing programs and scripts in bash. Examples are in function formats showcasing how to incorporate these solutions into code.
STRINGS
Trim leading and trailing white-space from string
This is an alternative to sed, awk, perl and other tools. The
function below works by finding all leading and trailing white-space and
removing it from the start and end of the string. The : built-in is used in place of a temporary variable.
Example Function:
trim_string() {
    # Usage: trim_string ""   example   string    ""
    : ""${1#""${1%%[![:space:]]*}""}""
    : ""${_%""${_##*[![:space:]]}""}""
    printf '%s\n' ""$_""
}
Example Usage:
$ trim_string ""    Hello,  World    ""
Hello,  World

$ name=""   John Black  ""
$ trim_string ""$name""
John Black
Trim all white-space from string and truncate spaces
This is an alternative to sed, awk, perl and other tools. The
function below works by abusing word splitting to create a new string
without leading/trailing white-space and with truncated spaces.
Example Function:
# shellcheck disable=SC2086,SC2048
trim_all() {
    # Usage: trim_all ""   example   string    ""
    set -f
    set -- $*
    printf '%s\n' ""$*""
    set +f
}
Example Usage:
$ trim_all ""    Hello,    World    ""
Hello, World

$ name=""   John   Black  is     my    name.    ""
$ trim_all ""$name""
John Black is my name.
Use regex on a string
The result of bash's regex matching can be used to replace sed for a
large number of use-cases.
CAVEAT: This is one of the few platform dependent bash features.
bash will use whatever regex engine is installed on the user's system.
Stick to POSIX regex features if aiming for compatibility.
CAVEAT: This example only prints the first matching group. When using
multiple capture groups some modification is needed.
Example Function:
regex() {
    # Usage: regex ""string"" ""regex""
    [[ $1 =~ $2 ]] && printf '%s\n' ""${BASH_REMATCH[1]}""
}
Example Usage:
$ # Trim leading white-space.
$ regex '    hello' '^\s*(.*)'
hello

$ # Validate a hex color.
$ regex ""#FFFFFF"" '^(#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3}))$'
#FFFFFF

$ # Validate a hex color (invalid).
$ regex ""red"" '^(#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3}))$'
# no output (invalid)
Example Usage in script:
is_hex_color() {
    if [[ $1 =~ ^(#?([a-fA-F0-9]{6}|[a-fA-F0-9]{3}))$ ]]; then
        printf '%s\n' ""${BASH_REMATCH[1]}""
    else
        printf '%s\n' ""error: $1 is an invalid color.""
        return 1
    fi
}

read -r color
is_hex_color ""$color"" || color=""#FFFFFF""

# Do stuff.
Split a string on a delimiter
CAVEAT: Requires bash 4+
This is an alternative to cut, awk and other tools.
Example Function:
split() {
   # Usage: split ""string"" ""delimiter""
   IFS=$'\n' read -d """" -ra arr <<< ""${1//$2/$'\n'}""
   printf '%s\n' ""${arr[@]}""
}
Example Usage:
$ split ""apples,oranges,pears,grapes"" "",""
apples
oranges
pears
grapes

$ split ""1, 2, 3, 4, 5"" "", ""
1
2
3
4
5

# Multi char delimiters work too!
$ split ""hello---world---my---name---is---john"" ""---""
hello
world
my
name
is
john
Change a string to lowercase
CAVEAT: Requires bash 4+
Example Function:
lower() {
    # Usage: lower ""string""
    printf '%s\n' ""${1,,}""
}
Example Usage:
$ lower ""HELLO""
hello

$ lower ""HeLlO""
hello

$ lower ""hello""
hello
Change a string to uppercase
CAVEAT: Requires bash 4+
Example Function:
upper() {
    # Usage: upper ""string""
    printf '%s\n' ""${1^^}""
}
Example Usage:
$ upper ""hello""
HELLO

$ upper ""HeLlO""
HELLO

$ upper ""HELLO""
HELLO
Reverse a string case
CAVEAT: Requires bash 4+
Example Function:
reverse_case() {
    # Usage: reverse_case ""string""
    printf '%s\n' ""${1~~}""
}
Example Usage:
$ reverse_case ""hello""
HELLO

$ reverse_case ""HeLlO""
hElLo

$ reverse_case ""HELLO""
hello
Trim quotes from a string
Example Function:
trim_quotes() {
    # Usage: trim_quotes ""string""
    : ""${1//\'}""
    printf '%s\n' ""${_//\""}""
}
Example Usage:
$ var=""'Hello', \""World\""""
$ trim_quotes ""$var""
Hello, World
Strip all instances of pattern from string
Example Function:
strip_all() {
    # Usage: strip_all ""string"" ""pattern""
    printf '%s\n' ""${1//$2}""
}
Example Usage:
$ strip_all ""The Quick Brown Fox"" ""[aeiou]""
Th Qck Brwn Fx

$ strip_all ""The Quick Brown Fox"" ""[[:space:]]""
TheQuickBrownFox

$ strip_all ""The Quick Brown Fox"" ""Quick ""
The Brown Fox
Strip first occurrence of pattern from string
Example Function:
strip() {
    # Usage: strip ""string"" ""pattern""
    printf '%s\n' ""${1/$2}""
}
Example Usage:
$ strip ""The Quick Brown Fox"" ""[aeiou]""
Th Quick Brown Fox

$ strip ""The Quick Brown Fox"" ""[[:space:]]""
TheQuick Brown Fox
Strip pattern from start of string
Example Function:
lstrip() {
    # Usage: lstrip ""string"" ""pattern""
    printf '%s\n' ""${1##$2}""
}
Example Usage:
$ lstrip ""The Quick Brown Fox"" ""The ""
Quick Brown Fox
Strip pattern from end of string
Example Function:
rstrip() {
    # Usage: rstrip ""string"" ""pattern""
    printf '%s\n' ""${1%%$2}""
}
Example Usage:
$ rstrip ""The Quick Brown Fox"" "" Fox""
The Quick Brown
Percent-encode a string
Example Function:
urlencode() {
    # Usage: urlencode ""string""
    local LC_ALL=C
    for (( i = 0; i < ${#1}; i++ )); do
        : ""${1:i:1}""
        case ""$_"" in
            [a-zA-Z0-9.~_-])
                printf '%s' ""$_""
            ;;

            *)
                printf '%%%02X' ""'$_""
            ;;
        esac
    done
    printf '\n'
}
Example Usage:
$ urlencode ""https://github.com/dylanaraps/pure-bash-bible""
https%3A%2F%2Fgithub.com%2Fdylanaraps%2Fpure-bash-bible
Decode a percent-encoded string
Example Function:
urldecode() {
    # Usage: urldecode ""string""
    : ""${1//+/ }""
    printf '%b\n' ""${_//%/\\x}""
}
Example Usage:
$ urldecode ""https%3A%2F%2Fgithub.com%2Fdylanaraps%2Fpure-bash-bible""
https://github.com/dylanaraps/pure-bash-bible
Check if string contains a sub-string
Using a test:
if [[ $var == *sub_string* ]]; then
    printf '%s\n' ""sub_string is in var.""
fi

# Inverse (substring not in string).
if [[ $var != *sub_string* ]]; then
    printf '%s\n' ""sub_string is not in var.""
fi

# This works for arrays too!
if [[ ${arr[*]} == *sub_string* ]]; then
    printf '%s\n' ""sub_string is in array.""
fi
Using a case statement:
case ""$var"" in
    *sub_string*)
        # Do stuff
    ;;

    *sub_string2*)
        # Do more stuff
    ;;

    *)
        # Else
    ;;
esac
Check if string starts with sub-string
if [[ $var == sub_string* ]]; then
    printf '%s\n' ""var starts with sub_string.""
fi

# Inverse (var does not start with sub_string).
if [[ $var != sub_string* ]]; then
    printf '%s\n' ""var does not start with sub_string.""
fi
Check if string ends with sub-string
if [[ $var == *sub_string ]]; then
    printf '%s\n' ""var ends with sub_string.""
fi

# Inverse (var does not end with sub_string).
if [[ $var != *sub_string ]]; then
    printf '%s\n' ""var does not end with sub_string.""
fi
ARRAYS
Reverse an array
Enabling extdebug allows access to the BASH_ARGV array which stores
the current function‚Äôs arguments in reverse.
CAVEAT: Requires shopt -s compat44 in bash 5.0+.
Example Function:
reverse_array() {
    # Usage: reverse_array ""array""
    shopt -s extdebug
    f()(printf '%s\n' ""${BASH_ARGV[@]}""); f ""$@""
    shopt -u extdebug
}
Example Usage:
$ reverse_array 1 2 3 4 5
5
4
3
2
1

$ arr=(red blue green)
$ reverse_array ""${arr[@]}""
green
blue
red
Remove duplicate array elements
Create a temporary associative array. When setting associative array
values and a duplicate assignment occurs, bash overwrites the key. This
allows us to effectively remove array duplicates.
CAVEAT: Requires bash 4+
CAVEAT: List order may not stay the same.
Example Function:
remove_array_dups() {
    # Usage: remove_array_dups ""array""
    declare -A tmp_array

    for i in ""$@""; do
        [[ $i ]] && IFS="" "" tmp_array[""${i:- }""]=1
    done

    printf '%s\n' ""${!tmp_array[@]}""
}
Example Usage:
$ remove_array_dups 1 1 2 2 3 3 3 3 3 4 4 4 4 4 5 5 5 5 5 5
1
2
3
4
5

$ arr=(red red green blue blue)
$ remove_array_dups ""${arr[@]}""
red
green
blue
Random array element
Example Function:
random_array_element() {
    # Usage: random_array_element ""array""
    local arr=(""$@"")
    printf '%s\n' ""${arr[RANDOM % $#]}""
}
Example Usage:
$ array=(red green blue yellow brown)
$ random_array_element ""${array[@]}""
yellow

# Multiple arguments can also be passed.
$ random_array_element 1 2 3 4 5 6 7
3
Cycle through an array
Each time the printf is called, the next array element is printed. When
the print hits the last array element it starts from the first element
again.
arr=(a b c d)

cycle() {
    printf '%s ' ""${arr[${i:=0}]}""
    ((i=i>=${#arr[@]}-1?0:++i))
}
Toggle between two values
This works the same as above, this is just a different use case.
arr=(true false)

cycle() {
    printf '%s ' ""${arr[${i:=0}]}""
    ((i=i>=${#arr[@]}-1?0:++i))
}
LOOPS
Loop over a range of numbers
Alternative to seq.
# Loop from 0-100 (no variable support).
for i in {0..100}; do
    printf '%s\n' ""$i""
done
Loop over a variable range of numbers
Alternative to seq.
# Loop from 0-VAR.
VAR=50
for ((i=0;i<=VAR;i++)); do
    printf '%s\n' ""$i""
done
Loop over an array
arr=(apples oranges tomatoes)

# Just elements.
for element in ""${arr[@]}""; do
    printf '%s\n' ""$element""
done
Loop over an array with an index
arr=(apples oranges tomatoes)

# Elements and index.
for i in ""${!arr[@]}""; do
    printf '%s\n' ""${arr[i]}""
done

# Alternative method.
for ((i=0;i<${#arr[@]};i++)); do
    printf '%s\n' ""${arr[i]}""
done
Loop over the contents of a file
while read -r line; do
    printf '%s\n' ""$line""
done < ""file""
Loop over files and directories
Don‚Äôt use ls.
# Greedy example.
for file in *; do
    printf '%s\n' ""$file""
done

# PNG files in dir.
for file in ~/Pictures/*.png; do
    printf '%s\n' ""$file""
done

# Iterate over directories.
for dir in ~/Downloads/*/; do
    printf '%s\n' ""$dir""
done

# Brace Expansion.
for file in /path/to/parentdir/{file1,file2,subdir/file3}; do
    printf '%s\n' ""$file""
done

# Iterate recursively.
shopt -s globstar
for file in ~/Pictures/**/*; do
    printf '%s\n' ""$file""
done
shopt -u globstar
FILE HANDLING
CAVEAT: bash does not handle binary data properly in versions < 4.4.
Read a file to a string
Alternative to the cat command.
file_data=""$(<""file"")""
Read a file to an array (by line)
Alternative to the cat command.
# Bash <4 (discarding empty lines).
IFS=$'\n' read -d """" -ra file_data < ""file""

# Bash <4 (preserving empty lines).
while read -r line; do
    file_data+=(""$line"")
done < ""file""

# Bash 4+
mapfile -t file_data < ""file""
Get the first N lines of a file
Alternative to the head command.
CAVEAT: Requires bash 4+
Example Function:
head() {
    # Usage: head ""n"" ""file""
    mapfile -tn ""$1"" line < ""$2""
    printf '%s\n' ""${line[@]}""
}
Example Usage:
$ head 2 ~/.bashrc
# Prompt
PS1='‚ûú '

$ head 1 ~/.bashrc
# Prompt
Get the last N lines of a file
Alternative to the tail command.
CAVEAT: Requires bash 4+
Example Function:
tail() {
    # Usage: tail ""n"" ""file""
    mapfile -tn 0 line < ""$2""
    printf '%s\n' ""${line[@]: -$1}""
}
Example Usage:
$ tail 2 ~/.bashrc
# Enable tmux.
# [[ -z ""$TMUX""  ]] && exec tmux

$ tail 1 ~/.bashrc
# [[ -z ""$TMUX""  ]] && exec tmux
Get the number of lines in a file
Alternative to wc -l.
Example Function (bash 4):
lines() {
    # Usage: lines ""file""
    mapfile -tn 0 lines < ""$1""
    printf '%s\n' ""${#lines[@]}""
}
Example Function (bash 3):
This method uses less memory than the mapfile method and works in bash 3 but it is slower for bigger files.
lines_loop() {
    # Usage: lines_loop ""file""
    count=0
    while IFS= read -r _; do
        ((count++))
    done < ""$1""
    printf '%s\n' ""$count""
}
Example Usage:
$ lines ~/.bashrc
48

$ lines_loop ~/.bashrc
48
Count files or directories in directory
This works by passing the output of the glob to the function and then counting the number of arguments.
Example Function:
count() {
    # Usage: count /path/to/dir/*
    #        count /path/to/dir/*/
    printf '%s\n' ""$#""
}
Example Usage:
# Count all files in dir.
$ count ~/Downloads/*
232

# Count all dirs in dir.
$ count ~/Downloads/*/
45

# Count all jpg files in dir.
$ count ~/Pictures/*.jpg
64
Create an empty file
Alternative to touch.
# Shortest.
>file

# Longer alternatives:
:>file
echo -n >file
printf '' >file
Extract lines between two markers
Example Function:
extract() {
    # Usage: extract file ""opening marker"" ""closing marker""
    while IFS=$'\n' read -r line; do
        [[ $extract && $line != ""$3"" ]] &&
            printf '%s\n' ""$line""

        [[ $line == ""$2"" ]] && extract=1
        [[ $line == ""$3"" ]] && extract=
    done < ""$1""
}
Example Usage:
# Extract code blocks from MarkDown file.
$ extract ~/projects/pure-bash/README.md '```sh' '```'
# Output here...
FILE PATHS
Get the directory name of a file path
Alternative to the dirname command.
Example Function:
dirname() {
    # Usage: dirname ""path""
    local tmp=${1:-.}

    [[ $tmp != *[!/]* ]] && {
        printf '/\n'
        return
    }

    tmp=${tmp%%""${tmp##*[!/]}""}

    [[ $tmp != */* ]] && {
        printf '.\n'
        return
    }

    tmp=${tmp%/*}
    tmp=${tmp%%""${tmp##*[!/]}""}

    printf '%s\n' ""${tmp:-/}""
}
Example Usage:
$ dirname ~/Pictures/Wallpapers/1.jpg
/home/black/Pictures/Wallpapers

$ dirname ~/Pictures/Downloads/
/home/black/Pictures
Get the base-name of a file path
Alternative to the basename command.
Example Function:
basename() {
    # Usage: basename ""path"" [""suffix""]
    local tmp

    tmp=${1%""${1##*[!/]}""}
    tmp=${tmp##*/}
    tmp=${tmp%""${2/""$tmp""}""}

    printf '%s\n' ""${tmp:-/}""
}
Example Usage:
$ basename ~/Pictures/Wallpapers/1.jpg
1.jpg

$ basename ~/Pictures/Wallpapers/1.jpg .jpg
1

$ basename ~/Pictures/Downloads/
Downloads
VARIABLES
Assign and access a variable using a variable
$ hello_world=""value""

# Create the variable name.
$ var=""world""
$ ref=""hello_$var""

# Print the value of the variable name stored in 'hello_$var'.
$ printf '%s\n' ""${!ref}""
value
Alternatively, on bash 4.3+:
$ hello_world=""value""
$ var=""world""

# Declare a nameref.
$ declare -n ref=hello_$var

$ printf '%s\n' ""$ref""
value
Name a variable based on another variable
$ var=""world""
$ declare ""hello_$var=value""
$ printf '%s\n' ""$hello_world""
value
ESCAPE SEQUENCES
Contrary to popular belief, there is no issue in utilizing raw escape sequences. Using tput abstracts the same ANSI sequences as if printed manually. Worse still, tput is not actually portable. There are a number of tput variants each with different commands and syntaxes (try tput setaf 3 on a FreeBSD system). Raw sequences are fine.
Text Colors
NOTE: Sequences requiring RGB values only work in True-Color Terminal Emulators.



Sequence
What does it do?
Value




\e[38;5;<NUM>m
Set text foreground color.
0-255


\e[48;5;<NUM>m
Set text background color.
0-255


\e[38;2;<R>;<G>;<B>m
Set text foreground color to RGB color.
R, G, B


\e[48;2;<R>;<G>;<B>m
Set text background color to RGB color.
R, G, B



Text Attributes
NOTE: Prepend 2 to any code below to turn it's effect off
(examples: 21=bold text off, 22=faint text off, 23=italic text off).



Sequence
What does it do?




\e[m
Reset text formatting and colors.


\e[1m
Bold text.


\e[2m
Faint text.


\e[3m
Italic text.


\e[4m
Underline text.


\e[5m
Blinking text.


\e[7m
Highlighted text.


\e[8m
Hidden text.


\e[9m
Strike-through text.



Cursor Movement



Sequence
What does it do?
Value




\e[<LINE>;<COLUMN>H
Move cursor to absolute position.
line, column


\e[H
Move cursor to home position (0,0).



\e[<NUM>A
Move cursor up N lines.
num


\e[<NUM>B
Move cursor down N lines.
num


\e[<NUM>C
Move cursor right N columns.
num


\e[<NUM>D
Move cursor left N columns.
num


\e[s
Save cursor position.



\e[u
Restore cursor position.




Erasing Text



Sequence
What does it do?




\e[K
Erase from cursor position to end of line.


\e[1K
Erase from cursor position to start of line.


\e[2K
Erase the entire current line.


\e[J
Erase from the current line to the bottom of the screen.


\e[1J
Erase from the current line to the top of the screen.


\e[2J
Clear the screen.


\e[2J\e[H
Clear the screen and move cursor to 0,0.



PARAMETER EXPANSION
Indirection



Parameter
What does it do?




${!VAR}
Access a variable based on the value of VAR.


${!VAR*}
Expand to IFS separated list of variable names starting with VAR.


${!VAR@}
Expand to IFS separated list of variable names starting with VAR. If double-quoted, each variable name expands to a separate word.



Replacement



Parameter
What does it do?




${VAR#PATTERN}
Remove shortest match of pattern from start of string.


${VAR##PATTERN}
Remove longest match of pattern from start of string.


${VAR%PATTERN}
Remove shortest match of pattern from end of string.


${VAR%%PATTERN}
Remove longest match of pattern from end of string.


${VAR/PATTERN/REPLACE}
Replace first match with string.


${VAR//PATTERN/REPLACE}
Replace all matches with string.


${VAR/PATTERN}
Remove first match.


${VAR//PATTERN}
Remove all matches.



Length



Parameter
What does it do?




${#VAR}
Length of var in characters.


${#ARR[@]}
Length of array in elements.



Expansion



Parameter
What does it do?




${VAR:OFFSET}
Remove first N chars from variable.


${VAR:OFFSET:LENGTH}
Get substring from N character to N character.  (${VAR:10:10}: Get sub-string from char 10 to char 20)


${VAR:: OFFSET}
Get first N chars from variable.


${VAR:: -OFFSET}
Remove last N chars from variable.


${VAR: -OFFSET}
Get last N chars from variable.


${VAR:OFFSET:-OFFSET}
Cut first N chars and last N chars.



Case Modification



Parameter
What does it do?
CAVEAT




${VAR^}
Uppercase first character.
bash 4+


${VAR^^}
Uppercase all characters.
bash 4+


${VAR,}
Lowercase first character.
bash 4+


${VAR,,}
Lowercase all characters.
bash 4+


${VAR~}
Reverse case of first character.
bash 4+


${VAR~~}
Reverse case of all characters.
bash 4+



Default Value



Parameter
What does it do?




${VAR:-STRING}
If VAR is empty or unset, use STRING as its value.


${VAR-STRING}
If VAR is unset, use STRING as its value.


${VAR:=STRING}
If VAR is empty or unset, set the value of VAR to STRING.


${VAR=STRING}
If VAR is unset, set the value of VAR to STRING.


${VAR:+STRING}
If VAR is not empty, use STRING as its value.


${VAR+STRING}
If VAR is set, use STRING as its value.


${VAR:?STRING}
Display an error if empty or unset.


${VAR?STRING}
Display an error if unset.



BRACE EXPANSION
Ranges
# Syntax: {<START>..<END>}

# Print numbers 1-100.
echo {1..100}

# Print range of floats.
echo 1.{1..9}

# Print chars a-z.
echo {a..z}
echo {A..Z}

# Nesting.
echo {A..Z}{0..9}

# Print zero-padded numbers.
# CAVEAT: bash 4+
echo {01..100}

# Change increment amount.
# Syntax: {<START>..<END>..<INCREMENT>}
# CAVEAT: bash 4+
echo {1..10..2} # Increment by 2.
String Lists
echo {apples,oranges,pears,grapes}

# Example Usage:
# Remove dirs Movies, Music and ISOS from ~/Downloads/.
rm -rf ~/Downloads/{Movies,Music,ISOS}
CONDITIONAL EXPRESSIONS
File Conditionals



Expression
Value
What does it do?




-a
file
If file exists.


-b
file
If file exists and is a block special file.


-c
file
If file exists and is a character special file.


-d
file
If file exists and is a directory.


-e
file
If file exists.


-f
file
If file exists and is a regular file.


-g
file
If file exists and its set-group-id bit is set.


-h
file
If file exists and is a symbolic link.


-k
file
If file exists and its sticky-bit is set


-p
file
If file exists and is a named pipe (FIFO).


-r
file
If file exists and is readable.


-s
file
If file exists and its size is greater than zero.


-t
fd
If file descriptor is open and refers to a terminal.


-u
file
If file exists and its set-user-id bit is set.


-w
file
If file exists and is writable.


-x
file
If file exists and is executable.


-G
file
If file exists and is owned by the effective group ID.


-L
file
If file exists and is a symbolic link.


-N
file
If file exists and has been modified since last read.


-O
file
If file exists and is owned by the effective user ID.


-S
file
If file exists and is a socket.



File Comparisons



Expression
What does it do?




file -ef file2
If both files refer to the same inode and device numbers.


file -nt file2
If file is newer than file2 (uses modification time) or file exists and file2 does not.


file -ot file2
If file is older than file2 (uses modification time) or file2 exists and file does not.



Variable Conditionals



Expression
Value
What does it do?




-o
opt
If shell option is enabled.


-v
var
If variable has a value assigned.


-R
var
If variable is a name reference.


-z
var
If the length of string is zero.


-n
var
If the length of string is non-zero.



Variable Comparisons



Expression
What does it do?




var = var2
Equal to.


var == var2
Equal to (synonym for =).


var != var2
Not equal to.


var < var2
Less than (in ASCII alphabetical order.)


var > var2
Greater than (in ASCII alphabetical order.)



ARITHMETIC OPERATORS
Assignment



Operators
What does it do?




=
Initialize or change the value of a variable.



Arithmetic



Operators
What does it do?




+
Addition


-
Subtraction


*
Multiplication


/
Division


**
Exponentiation


%
Modulo


+=
Plus-Equal (Increment a variable.)


-=
Minus-Equal (Decrement a variable.)


*=
Times-Equal (Multiply a variable.)


/=
Slash-Equal (Divide a variable.)


%=
Mod-Equal (Remainder of dividing a variable.)



Bitwise



Operators
What does it do?




<<
Bitwise Left Shift


<<=
Left-Shift-Equal


>>
Bitwise Right Shift


>>=
Right-Shift-Equal


&
Bitwise AND


&=
Bitwise AND-Equal


|
Bitwise OR


|=
Bitwise OR-Equal


~
Bitwise NOT


^
Bitwise XOR


^=
Bitwise XOR-Equal



Logical



Operators
What does it do?




!
NOT


&&
AND


||
OR



Miscellaneous



Operators
What does it do?
Example




,
Comma Separator
((a=1,b=2,c=3))



ARITHMETIC
Simpler syntax to set variables
# Simple math
((var=1+2))

# Decrement/Increment variable
((var++))
((var--))
((var+=1))
((var-=1))

# Using variables
((var=var2*arr[2]))
Ternary Tests
# Set the value of var to var2 if var2 is greater than var.
# var: variable to set.
# var2>var: Condition to test.
# ?var2: If the test succeeds.
# :var: If the test fails.
((var=var2>var?var2:var))
TRAPS
Traps allow a script to execute code on various signals. In pxltrm (a pixel art editor written in bash)  traps are used to redraw the user interface on window resize. Another use case is cleaning up temporary files on script exit.
Traps should be added near the start of scripts so any early errors are also caught.
NOTE: For a full list of signals, see trap -l.
Do something on script exit
# Clear screen on script exit.
trap 'printf \\e[2J\\e[H\\e[m' EXIT
Ignore terminal interrupt (CTRL+C, SIGINT)
trap '' INT
React to window resize
# Call a function on window resize.
trap 'code_here' SIGWINCH
Do something before every command
trap 'code_here' DEBUG
Do something when a shell function or a sourced file finishes executing
trap 'code_here' RETURN
PERFORMANCE
Disable Unicode
If unicode is not required, it can be disabled for a performance increase. Results may vary however there have been noticeable improvements in neofetch and other programs.
# Disable unicode.
LC_ALL=C
LANG=C
OBSOLETE SYNTAX
Shebang
Use #!/usr/bin/env bash instead of #!/bin/bash.

The former searches the user's PATH to find the bash binary.
The latter assumes it is always installed to /bin/ which can cause issues.

NOTE: There are times when one may have a good reason for using #!/bin/bash or another direct path to the binary.
# Right:

    #!/usr/bin/env bash

# Less right:

    #!/bin/bash
Command Substitution
Use $() instead of ` `.
# Right.
var=""$(command)""

# Wrong.
var=`command`

# $() can easily be nested whereas `` cannot.
var=""$(command ""$(command)"")""
Function Declaration
Do not use the function keyword, it reduces compatibility with older versions of bash.
# Right.
do_something() {
    # ...
}

# Wrong.
function do_something() {
    # ...
}
INTERNAL VARIABLES
Get the location to the bash binary
""$BASH""
Get the version of the current running bash process
# As a string.
""$BASH_VERSION""

# As an array.
""${BASH_VERSINFO[@]}""
Open the user's preferred text editor
""$EDITOR"" ""$file""

# NOTE: This variable may be empty, set a fallback value.
""${EDITOR:-vi}"" ""$file""
Get the name of the current function
# Current function.
""${FUNCNAME[0]}""

# Parent function.
""${FUNCNAME[1]}""

# So on and so forth.
""${FUNCNAME[2]}""
""${FUNCNAME[3]}""

# All functions including parents.
""${FUNCNAME[@]}""
Get the host-name of the system
""$HOSTNAME""

# NOTE: This variable may be empty.
# Optionally set a fallback to the hostname command.
""${HOSTNAME:-$(hostname)}""
Get the architecture of the Operating System
""$HOSTTYPE""
Get the name of the Operating System / Kernel
This can be used to add conditional support for different Operating
Systems without needing to call uname.
""$OSTYPE""
Get the current working directory
This is an alternative to the pwd built-in.
""$PWD""
Get the number of seconds the script has been running
""$SECONDS""
Get a pseudorandom integer
Each time $RANDOM is used, a different integer between 0 and 32767 is returned. This variable should not be used for anything related to security (this includes encryption keys etc).
""$RANDOM""
INFORMATION ABOUT THE TERMINAL
Get the terminal size in lines and columns (from a script)
This is handy when writing scripts in pure bash and stty/tput can‚Äôt be
called.
Example Function:
get_term_size() {
    # Usage: get_term_size

    # (:;:) is a micro sleep to ensure the variables are
    # exported immediately.
    shopt -s checkwinsize; (:;:)
    printf '%s\n' ""$LINES $COLUMNS""
}
Example Usage:
# Output: LINES COLUMNS
$ get_term_size
15 55
Get the terminal size in pixels
CAVEAT: This does not work in some terminal emulators.
Example Function:
get_window_size() {
    # Usage: get_window_size
    printf '%b' ""${TMUX:+\\ePtmux;\\e}\\e[14t${TMUX:+\\e\\\\}""
    IFS=';t' read -d t -t 0.05 -sra term_size
    printf '%s\n' ""${term_size[1]}x${term_size[2]}""
}
Example Usage:
# Output: WIDTHxHEIGHT
$ get_window_size
1200x800

# Output (fail):
$ get_window_size
x
Get the current cursor position
This is useful when creating a TUI in pure bash.
Example Function:
get_cursor_pos() {
    # Usage: get_cursor_pos
    IFS='[;' read -p $'\e[6n' -d R -rs _ y x _
    printf '%s\n' ""$x $y""
}
Example Usage:
# Output: X Y
$ get_cursor_pos
1 8
CONVERSION
Convert a hex color to RGB
Example Function:
hex_to_rgb() {
    # Usage: hex_to_rgb ""#FFFFFF""
    #        hex_to_rgb ""000000""
    : ""${1/\#}""
    ((r=16#${_:0:2},g=16#${_:2:2},b=16#${_:4:2}))
    printf '%s\n' ""$r $g $b""
}
Example Usage:
$ hex_to_rgb ""#FFFFFF""
255 255 255
Convert an RGB color to hex
Example Function:
rgb_to_hex() {
    # Usage: rgb_to_hex ""r"" ""g"" ""b""
    printf '#%02x%02x%02x\n' ""$1"" ""$2"" ""$3""
}
Example Usage:
$ rgb_to_hex ""255"" ""255"" ""255""
#FFFFFF
CODE GOLF
Shorter for loop syntax
# Tiny C Style.
for((;i++<10;)){ echo ""$i"";}

# Undocumented method.
for i in {1..10};{ echo ""$i"";}

# Expansion.
for i in {1..10}; do echo ""$i""; done

# C Style.
for((i=0;i<=10;i++)); do echo ""$i""; done
Shorter infinite loops
# Normal method
while :; do echo hi; done

# Shorter
for((;;)){ echo hi;}
Shorter function declaration
# Normal method
f(){ echo hi;}

# Using a subshell
f()(echo hi)

# Using arithmetic
# This can be used to assign integer values.
# Example: f a=1
#          f a++
f()(($1))

# Using tests, loops etc.
# NOTE: ‚Äòwhile‚Äô, ‚Äòuntil‚Äô, ‚Äòcase‚Äô, ‚Äò(())‚Äô, ‚Äò[[]]‚Äô can also be used.
f()if true; then echo ""$1""; fi
f()for i in ""$@""; do echo ""$i""; done
Shorter if syntax
# One line
# Note: The 3rd statement may run when the 1st is true
[[ $var == hello ]] && echo hi || echo bye
[[ $var == hello ]] && { echo hi; echo there; } || echo bye

# Multi line (no else, single statement)
# Note: The exit status may not be the same as with an if statement
[[ $var == hello ]] &&
    echo hi

# Multi line (no else)
[[ $var == hello ]] && {
    echo hi
    # ...
}
Simpler case statement to set variable
The : built-in can be used to avoid repeating variable= in a case statement. The $_ variable stores the last argument of the last command. : always succeeds so it can be used to store the variable value.
# Modified snippet from Neofetch.
case ""$OSTYPE"" in
    ""darwin""*)
        : ""MacOS""
    ;;

    ""linux""*)
        : ""Linux""
    ;;

    *""bsd""* | ""dragonfly"" | ""bitrig"")
        : ""BSD""
    ;;

    ""cygwin"" | ""msys"" | ""win32"")
        : ""Windows""
    ;;

    *)
        printf '%s\n' ""Unknown OS detected, aborting..."" >&2
        exit 1
    ;;
esac

# Finally, set the variable.
os=""$_""
OTHER
Use read as an alternative to the sleep command
Surprisingly, sleep is an external command and not a bash built-in.
CAVEAT: Requires bash 4+
Example Function:
read_sleep() {
    # Usage: read_sleep 1
    #        read_sleep 0.2
    read -rt ""$1"" <> <(:) || :
}
Example Usage:
read_sleep 1
read_sleep 0.1
read_sleep 30
For performance-critical situations, where it is not economic to open and close an excessive number of file descriptors, the allocation of a file descriptor may be done only once for all invocations of read:
(See the generic original implementation at https://blog.dhampir.no/content/sleeping-without-a-subprocess-in-bash-and-how-to-sleep-forever)
exec {sleep_fd}<> <(:)
while some_quick_test; do
    # equivalent of sleep 0.001
    read -t 0.001 -u $sleep_fd
done
Check if a program is in the user's PATH
# There are 3 ways to do this and either one can be used.
type -p executable_name &>/dev/null
hash executable_name &>/dev/null
command -v executable_name &>/dev/null

# As a test.
if type -p executable_name &>/dev/null; then
    # Program is in PATH.
fi

# Inverse.
if ! type -p executable_name &>/dev/null; then
    # Program is not in PATH.
fi

# Example (Exit early if program is not installed).
if ! type -p convert &>/dev/null; then
    printf '%s\n' ""error: convert is not installed, exiting...""
    exit 1
fi
Get the current date using strftime
Bash‚Äôs printf has a built-in method of getting the date which can be used in place of the date command.
CAVEAT: Requires bash 4+
Example Function:
date() {
    # Usage: date ""format""
    # See: 'man strftime' for format.
    printf ""%($1)T\\n"" ""-1""
}
Example Usage:
# Using above function.
$ date ""%a %d %b  - %l:%M %p""
Fri 15 Jun  - 10:00 AM

# Using printf directly.
$ printf '%(%a %d %b  - %l:%M %p)T\n' ""-1""
Fri 15 Jun  - 10:00 AM

# Assigning a variable using printf.
$ printf -v date '%(%a %d %b  - %l:%M %p)T\n' '-1'
$ printf '%s\n' ""$date""
Fri 15 Jun  - 10:00 AM
Get the username of the current user
CAVEAT: Requires bash 4.4+
$ : \\u
# Expand the parameter as if it were a prompt string.
$ printf '%s\n' ""${_@P}""
black
Generate a UUID V4
CAVEAT: The generated value is not cryptographically secure.
Example Function:
uuid() {
    # Usage: uuid
    C=""89ab""

    for ((N=0;N<16;++N)); do
        B=""$((RANDOM%256))""

        case ""$N"" in
            6)  printf '4%x' ""$((B%16))"" ;;
            8)  printf '%c%x' ""${C:$RANDOM%${#C}:1}"" ""$((B%16))"" ;;

            3|5|7|9)
                printf '%02x-' ""$B""
            ;;

            *)
                printf '%02x' ""$B""
            ;;
        esac
    done

    printf '\n'
}
Example Usage:
$ uuid
d5b6c731-1310-4c24-9fe3-55d556d44374
Progress bars
This is a simple way of drawing progress bars without needing a for loop
in the function itself.
Example Function:
bar() {
    # Usage: bar 1 10
    #            ^----- Elapsed Percentage (0-100).
    #               ^-- Total length in chars.
    ((elapsed=$1*$2/100))

    # Create the bar with spaces.
    printf -v prog  ""%${elapsed}s""
    printf -v total ""%$(($2-elapsed))s""

    printf '%s\r' ""[${prog// /-}${total}]""
}
Example Usage:
for ((i=0;i<=100;i++)); do
    # Pure bash micro sleeps (for the example).
    (:;:) && (:;:) && (:;:) && (:;:) && (:;:)

    # Print the bar.
    bar ""$i"" ""10""
done

printf '\n'
Get the list of functions in a script
get_functions() {
    # Usage: get_functions
    IFS=$'\n' read -d """" -ra functions < <(declare -F)
    printf '%s\n' ""${functions[@]//declare -f }""
}
Bypass shell aliases
# alias
ls

# command
# shellcheck disable=SC1001
\ls
Bypass shell functions
# function
ls

# command
command ls
Run a command in the background
This will run the given command and keep it running, even after the terminal or SSH connection is terminated. All output is ignored.
bkr() {
    (nohup ""$@"" &>/dev/null &)
}

bkr ./some_script.sh # some_script.sh is now running in the background
AFTERWORD
Thanks for reading! If this bible helped you in any way and you'd like to give back, consider donating. Donations give me the time to make this the best resource possible. Can't donate? That's OK, star the repo and share it with your friends!

Rock on. ü§ò
",GitHub - dylanaraps/pure-bash-bible: üìñ A collection of pure bash alternatives to external processes.
66,Shell,"git-flow
A collection of Git extensions to provide high-level repository operations
for Vincent Driessen's branching model.
Getting started
For the best introduction to get started with git flow, please read Jeff
Kreeftmeijer's blog post:
http://jeffkreeftmeijer.com/2010/why-arent-you-using-git-flow/
Or have a look at one of these screen casts:

How to use a scalable Git branching model called git-flow (by Build a Module)
A short introduction to git-flow (by Mark Derricutt)
On the path with git-flow (by Dave Bock)

Installing git-flow
See the Wiki for up-to-date Installation Instructions.
Integration with your shell
For those who use the Bash or
ZSH shell, please check out the excellent work on the
git-flow-completion project
by bobthecow. It offers tab-completion for all
git-flow subcommands and branch names.
FAQ
See the FAQ section of the project
Wiki.
Please help out
This project is still under development. Feedback and suggestions are very
welcome and I encourage you to use the Issues
list on Github to provide that
feedback.
Feel free to fork this repo and to commit your additions. For a list of all
contributors, please see the AUTHORS file.
Any questions, tips, or general discussion can be posted to our Google group:
http://groups.google.com/group/gitflow-users
Contributing
Fork the repository.  Then, run:
git clone --recursive git@github.com:<username>/gitflow.git
cd gitflow
git branch master origin/master
git flow init -d
git flow feature start <your feature>

Then, do work and commit your changes.  Hint: export PATH=`pwd`:$PATH
from within the gitflow directory makes sure you're using the version of
gitflow you're currently developing.
git flow feature publish <your feature>

When done, open a pull request to your feature branch.
License terms
git-flow is published under the liberal terms of the BSD License, see the
LICENSE file. Although the BSD License does not require you to share
any modifications you make to the source code, you are very much encouraged and
invited to contribute back your modifications to the community, preferably
in a Github fork, of course.
Initialization
To initialize a new repo with the basic branch structure, use:
	git flow init [-d]

This will then interactively prompt you with some questions on which branches
you would like to use as development and production branches, and how you
would like your prefixes be named. You may simply press Return on any of
those questions to accept the (sane) default suggestions.
The -d flag will accept all defaults.
Creating feature/release/hotfix/support branches


To list/start/finish feature branches, use:
  git flow feature
  git flow feature start <name> [<base>]
  git flow feature finish <name>

For feature branches, the <base> arg must be a commit on develop.


To push/pull a feature branch to the remote repository, use:
  git flow feature publish <name>
    git flow feature pull <remote> <name>



To list/start/finish release branches, use:
  git flow release
  git flow release start <release> [<base>]
  git flow release finish <release>

For release branches, the <base> arg must be a commit on develop.


To list/start/finish hotfix branches, use:
  git flow hotfix
  git flow hotfix start <release> [<base>]
  git flow hotfix finish <release>

For hotfix branches, the <base> arg must be a commit on master.


To list/start support branches, use:
  git flow support
  git flow support start <release> <base>

For support branches, the <base> arg must be a commit on master.


Showing your appreciation
A few people already requested it, so now it's here: a Flattr button.
Of course, the best way to show your appreciation for the original
blog post or the git-flow tool itself remains
contributing to the community.  If you'd like to show your appreciation in
another way, however, consider Flattr'ing me:

",GitHub - nvie/gitflow: Git extensions to provide high-level repository operations for Vincent Driessen's branching model.
67,Shell,"Mathias‚Äôs dotfiles

Installation
Warning: If you want to give these dotfiles a try, you should first fork this repository, review the code, and remove things you don‚Äôt want or need. Don‚Äôt blindly use my settings unless you know what that entails. Use at your own risk!
Using Git and the bootstrap script
You can clone the repository wherever you want. (I like to keep it in ~/Projects/dotfiles, with ~/dotfiles as a symlink.) The bootstrapper script will pull in the latest version and copy the files to your home folder.
git clone https://github.com/mathiasbynens/dotfiles.git && cd dotfiles && source bootstrap.sh
To update, cd into your local dotfiles repository and then:
source bootstrap.sh
Alternatively, to update while avoiding the confirmation prompt:
set -- -f; source bootstrap.sh
Git-free install
To install these dotfiles without Git:
cd; curl -#L https://github.com/mathiasbynens/dotfiles/tarball/master | tar -xzv --strip-components 1 --exclude={README.md,bootstrap.sh,.osx,LICENSE-MIT.txt}
To update later on, just run that command again.
Specify the $PATH
If ~/.path exists, it will be sourced along with the other files, before any feature testing (such as detecting which version of ls is being used) takes place.
Here‚Äôs an example ~/.path file that adds /usr/local/bin to the $PATH:
export PATH=""/usr/local/bin:$PATH""
Add custom commands without creating a new fork
If ~/.extra exists, it will be sourced along with the other files. You can use this to add a few custom commands without the need to fork this entire repository, or to add commands you don‚Äôt want to commit to a public repository.
My ~/.extra looks something like this:
# Git credentials
# Not in the repository, to prevent people from accidentally committing under my name
GIT_AUTHOR_NAME=""Mathias Bynens""
GIT_COMMITTER_NAME=""$GIT_AUTHOR_NAME""
git config --global user.name ""$GIT_AUTHOR_NAME""
GIT_AUTHOR_EMAIL=""mathias@mailinator.com""
GIT_COMMITTER_EMAIL=""$GIT_AUTHOR_EMAIL""
git config --global user.email ""$GIT_AUTHOR_EMAIL""
You could also use ~/.extra to override settings, functions and aliases from my dotfiles repository. It‚Äôs probably better to fork this repository instead, though.
Sensible macOS defaults
When setting up a new Mac, you may want to set some sensible macOS defaults:
./.macos
Install Homebrew formulae
When setting up a new Mac, you may want to install some common Homebrew formulae (after installing Homebrew, of course):
./brew.sh
Some of the functionality of these dotfiles depends on formulae installed by brew.sh. If you don‚Äôt plan to run brew.sh, you should look carefully through the script and manually install any particularly important ones. A good example is Bash/Git completion: the dotfiles use a special version from Homebrew.
Feedback
Suggestions/improvements
welcome!
Author








Mathias Bynens



Thanks to‚Ä¶

@ptb and his macOS Setup repository
Ben Alman and his dotfiles repository
CƒÉtƒÉlin Mari»ô and his dotfiles repository
Gianni Chiappetta for sharing his amazing collection of dotfiles
Jan Moesen and his ancient .bash_profile + shiny tilde repository
Lauri ‚ÄòLri‚Äô Ranta for sharing loads of hidden preferences
Matijs Brinkhuis and his dotfiles repository
Nicolas Gallagher and his dotfiles repository
Sindre Sorhus
Tom Ryder and his dotfiles repository
Kevin Suttle and his dotfiles repository and macOS-Defaults project, which aims to provide better documentation for ~/.macos
Haralan Dobrev
Anyone who contributed a patch or made a helpful suggestion

","GitHub - mathiasbynens/dotfiles: .files, including ~/.macos ‚Äî sensible hacker defaults for macOS"
68,Shell,"Streisand




English, Fran√ßais, ÁÆÄ‰Ωì‰∏≠Êñá, –†—É—Å—Å–∫–∏–π | Mirror



Streisand
Silence censorship. Automate the effect.
The Internet can be a little unfair. It's way too easy for ISPs, telecoms, politicians, and corporations to block access to the sites and information that you care about. But breaking through these restrictions is tough. Or is it?
If you have an account with a cloud computing provider, Streisand can set up a new node with many censorship-resistant VPN services nearly automatically. You'll need a little experience with a Unix command-line. (But without Streisand, it could take days for a skilled Unix administrator to configure these services securely!) At the end, you'll have a private website with software and instructions.
Here's what a sample Streisand server looks like.
There's a list of supported cloud providers; experts may be able to use Streisand to install on many other cloud providers.
VPN services
One type of tool that people use to avoid network censorship is a Virtual Private Network (VPN). There are many kinds of VPNs.
Not all network censorship is alike; in some places, it changes from day to day. Streisand provides many different VPN services to try. (You don't have to install them all, though.)
Some Streisand services include add-ons for further censorship and throttling resistance:

OpenSSH

Tinyproxy may be used as an HTTP proxy.


OpenConnect / Cisco AnyConnect

This protocol is widely used by multi-national corporations, and might not be blocked.


OpenVPN

Stunnel add-on available.


Shadowsocks,

The V2ray-plugin is installed to provide robust traffic evasion on hostile networks (especially those implementing quality of service (QOS) throttling).


A private Tor bridge relay

Obfsproxy with obfs4 available as an add-on.


WireGuard, a modern high-performance protocol.

See also:

A more technical list of features
A more technical list of services


Cloud providers

Amazon Web Services (AWS)
Microsoft Azure
Digital Ocean
Google Compute Engine (GCE)
Linode
Rackspace

Other providers
We recommend using one of the above providers. If you are an expert and can set up a fresh Ubuntu 16.04 server elsewhere, there are ""localhost"" and ""existing remote server"" installation methods. For more information, see the advanced installation instructions.
Installation
You need command-line access to a Unix system. You can use Linux, BSD, or macOS; on Windows 10, the Windows Subsystem for Linux (WSL) counts as Linux.
Once you're ready, see the full installation instructions.
Things we want to do better
Aside from a good deal of cleanup, we could really use:

Easier setup.
Faster adoption of new censorship-avoidance tools

We're looking for help with both.
If there is something that you think Streisand should do, or if you find a bug in its documentation or execution, please file a report on the Issue Tracker.
Core Contributors

Jay Carlson (@nopdotcom)
Nick Clarke (@nickolasclarke)
Joshua Lund (@jlund)
Ali Makki (@alimakki)
Daniel McCarney (@cpu)
Corban Raun (@CorbanR)

Acknowledgements
Jason A. Donenfeld deserves a lot of credit for being brave enough to reimagine what a modern VPN should look like and for coming up with something as good as WireGuard. He has our sincere thanks for all of his patient help and high-quality feedback.
We are grateful to Trevor Smith for his massive contributions. He suggested the Gateway approach, provided tons of invaluable feedback, made everything look better, and developed the HTML template that served as the inspiration to take things to the next level before Streisand's public release.
Huge thanks to Paul Wouters of The Libreswan Project for his generous help troubleshooting the L2TP/IPsec setup.
Starcadian's 'Sunset Blood' album was played on repeat approximately 300 times during the first few months of work on the project in early 2014.
","GitHub - StreisandEffect/streisand: Streisand sets up a new server running your choice of WireGuard, OpenConnect, OpenSSH, OpenVPN, Shadowsocks, sslh, Stunnel, or a Tor bridge. It also generates custom instructions for all of these services. At the end of the run you are given an HTML file with instructions that can be shared with friends, family members, and fellow activists."
69,Shell,"

Network-wide ad blocking via your own Linux hardware

The Pi-hole¬Æ is a DNS sinkhole that protects your devices from unwanted content, without installing any client-side software.

Easy-to-install: our versatile installer walks you through the process, and takes less than ten minutes
Resolute: content is blocked in non-browser locations, such as ad-laden mobile apps and smart TVs
Responsive: seamlessly speeds up the feel of everyday browsing by caching DNS queries
Lightweight: runs smoothly with minimal hardware and software requirements
Robust: a command line interface that is quality assured for interoperability
Insightful: a beautiful responsive Web Interface dashboard to view and control your Pi-hole
Versatile: can optionally function as a DHCP server, ensuring all your devices are protected automatically
Scalable: capable of handling hundreds of millions of queries when installed on server-grade hardware
Modern: blocks ads over both IPv4 and IPv6
Free: open source software which helps ensure you are the sole person in control of your privacy





One-Step Automated Install
Those who want to get started quickly and conveniently may install Pi-hole using the following command:
curl -sSL https://install.pi-hole.net | bash
Alternative Install Methods
Piping to bash is controversial, as it prevents you from reading code that is about to run on your system. Therefore, we provide these alternative installation methods which allow code review before installation:
Method 1: Clone our repository and run
git clone --depth 1 https://github.com/pi-hole/pi-hole.git Pi-hole
cd ""Pi-hole/automated install/""
sudo bash basic-install.sh

Method 2: Manually download the installer and run
wget -O basic-install.sh https://install.pi-hole.net
sudo bash basic-install.sh

Post-install: Make your network take advantage of Pi-hole
Once the installer has been run, you will need to configure your router to have DHCP clients use Pi-hole as their DNS server which ensures that all devices connecting to your network will have content blocked without any further intervention.
If your router does not support setting the DNS server, you can use Pi-hole's built-in DHCP server; just be sure to disable DHCP on your router first (if it has that feature available).
As a last resort, you can always manually set each device to use Pi-hole as their DNS server.

Pi-hole is free, but powered by your support
There are many reoccurring costs involved with maintaining free, open source, and privacy-respecting software; expenses which our volunteer developers pitch in to cover out-of-pocket. This is just one example of how strongly we feel about our software, as well as the importance of keeping it maintained.
Make no mistake: your support is absolutely vital to help keep us innovating!
Donations
Sending a donation using our links below is extremely helpful in offsetting a portion of our monthly expenses:

 Donate via PayPal
 Bitcoin, Bitcoin Cash, Ethereum, Litecoin

Alternative support
If you'd rather not donate (which is okay!), there are other ways you can help support us:

Patreon Become a patron for rewards
Digital Ocean affiliate link
Stickermule earn a $10 credit after your first purchase
Pi-hole Swag Store affiliate link
Amazon affiliate link
DNS Made Easy affiliate link
Vultr affiliate link
Spreading the word about our software, and how you have benefited from it

Contributing via GitHub
We welcome everyone to contribute to issue reports, suggest new features, and create pull requests.
If you have something to add - anything from a typo through to a whole new feature, we're happy to check it out! Just make sure to fill out our template when submitting your request; the questions that it asks will help the volunteers quickly understand what you're aiming to achieve.
You'll find that the install script and the debug script have an abundance of comments, which will help you better understand how Pi-hole works. They're also a valuable resource to those who want to learn how to write scripts or code a program! We encourage anyone who likes to tinker to read through it and submit a pull request for us to review.
Presentations about Pi-hole
Word-of-mouth continues to help our project grow immensely, and so we are helping make this easier for people.
If you are going to be presenting Pi-hole at a conference, meetup or even a school project, get in touch with us so we can hook you up with free swag to hand out to your audience!

Getting in touch with us
While we are primarily reachable on our Discourse User Forum, we can also be found on a variety of social media outlets. Please be sure to check the FAQ's before starting a new discussion, as we do not have the spare time to reply to every request for assistance.

Frequently Asked Questions
Pi-hole Wiki
Feature Requests
Discourse User Forum
Reddit
Gitter (Real-time chat)
Twitter
YouTube
Facebook


Breakdown of Features
The Command Line Interface
The pihole command has all the functionality necessary to be able to fully administer the Pi-hole, without the need of the Web Interface. It's fast, user-friendly, and auditable by anyone with an understanding of bash.

Some notable features include:

Whitelisting, Blacklisting and Wildcards
Debugging utility
Viewing the live log file
Real-time Statistics via ssh or your TFT LCD screen
Updating Ad Lists
Querying Ad Lists for blocked domains
Enabling and Disabling Pi-hole
... and many more!

You can read our Core Feature Breakdown, as well as read up on example usage for more information.
The Web Interface Dashboard
This optional dashboard allows you to view stats, change settings, and configure your Pi-hole. It's the power of the Command Line Interface, with none of the learning curve!

Some notable features include:

Mobile friendly interface
Password protection
Detailed graphs and doughnut charts
Top lists of domains and clients
A filterable and sortable query log
Long Term Statistics to view data over user-defined time ranges
The ability to easily manage and configure Pi-hole features
... and all the main features of the Command Line Interface!

There are several ways to access the dashboard:

http://<IP_ADDPRESS_OF_YOUR_PI_HOLE>/admin/
http://pi.hole/admin/ (when using Pi-hole as your DNS server)
http://pi.hole/ (when using Pi-hole as your DNS server)

Faster-than-light Engine
FTLDNS is a lightweight, purpose-built daemon used to provide statistics needed for the Web Interface, and its API can be easily integrated into your own projects. As the name implies, FTLDNS does this all very quickly!
Some of the statistics you can integrate include:

Total number of domains being blocked
Total number of DNS queries today
Total number of ads blocked today
Percentage of ads blocked
Unique domains
Queries forwarded (to your chosen upstream DNS server)
Queries cached
Unique clients

The API can be accessed via telnet, the Web (admin/api.php) and Command Line (pihole -c -j). You can out find more details over here.

The Origin Of Pi-hole
Pi-hole being an advertising-aware DNS/Web server, makes use of the following technologies:

dnsmasq - a lightweight DNS and DHCP server
curl - A command line tool for transferring data with URL syntax
lighttpd - web server designed and optimized for high performance
php - a popular general-purpose web scripting language
AdminLTE Dashboard - premium admin control panel based on Bootstrap 3.x

While quite outdated at this point, this original blog post about Pi-hole goes into great detail about how Pi-hole was originally set up and how it works. Syntactically, it's no longer accurate, but the same basic principles and logic still apply to Pi-hole's current state.

Coverage

Lifehacker: Turn A Raspberry Pi Into An Ad Blocker With A Single Command (Feburary, 2015)
MakeUseOf: Adblock Everywhere: The Raspberry Pi-Hole Way (March, 2015)
Catchpoint: Ad-Blocking on Apple iOS9: Valuing the End User Experience (September, 2015)
Security Now Netcast: Pi-hole (October, 2015)
TekThing: Raspberry Pi-Hole Makes Ads Disappear! (December, 2015)
Foolish Tech Show (December, 2015)
Block Ads on All Home Devices for $53.18 (December, 2015)
Pi-Hole for Ubuntu 14.04 (December, 2015)
MacObserver Podcast 585 (December, 2015)
The Defrag Show: Endoscope USB Camera, The Final [HoloLens] Vote, Adblock Pi and more (January, 2016)
Adafruit: Pi-hole is a black hole for internet ads (March, 2016)
Digital Trends: 5 Fun, Easy Projects You Can Try With a $35 Raspberry Pi (March, 2016)
Adafruit: Raspberry Pi Quick Look at Pi Hole ad blocking server with Tony D (June, 2016)
Devacron: OrangePi Zero as an Ad-Block server with Pi-Hole (December, 2016)
Linux Pro: The Hole Truth (July, 2017)
Adafruit: installing Pi-hole on a Pi Zero W (August, 2017)
CryptoAUSTRALIA: How We Tried 5 Privacy Focused Raspberry Pi Projects (October, 2017)
CryptoAUSTRALIA: Pi-hole Workshop (November, 2017)
Know How 355: Killing ads with a Raspberry Pi-Hole! (November, 2017)
Hobohouse: Block Advertising on your Network with Pi-hole and Raspberry Pi (March, 2018)
Scott Helme: Securing DNS across all of my devices with Pi-Hole + DNS-over-HTTPS + 1.1.1.1 (April, 2018)
Scott Helme: Catching and dealing with naughty devices on my home network (April, 2018)
Bloomberg Business Week: Brotherhood of the Ad blockers (May, 2018)
Software Engineering Daily: Interview with the creator of Pi-hole (May, 2018)
Raspberry Pi: Block ads at home using Pi-hole and a Raspberry Pi (July, 2018)
Troy Hunt: Mmm... Pi-hole... (September, 2018)
PEBKAK Podcast: Interview With Jacob Salmela (October, 2018)


Pi-hole Projects

The Big Blocklist Collection
Pie in the Sky-Hole
Copernicus: Windows Tray Application
Magic Mirror with DNS Filtering
Windows DNS Swapper

",GitHub - pi-hole/pi-hole: A black hole for Internet advertisements
70,Shell,"Dokku









Docker powered mini-Heroku. The smallest PaaS implementation you've ever seen.
Sponsors
Become a sponsor and get your logo on our README on Github with a link to your site. [Become a sponsor]






























Backers
Support us with a monthly donation and help us continue our activities. [Become a backer]






























Requirements
A fresh VM running any of the following operating systems:

Ubuntu x64 - Any currently supported release
Debian 8.2+ x64
CentOS 7 x64 (experimental)
Arch Linux x64 (experimental)

An SSH keypair that can be used for application deployment. If this exists before installation, it will be automatically imported into dokku.
Otherwise, you will need to import the keypair manually after installation using dokku ssh-keys:add.
Installation
To install the latest stable release, run the following commands as a user who has access to sudo:
wget https://raw.githubusercontent.com/dokku/dokku/v0.19.11/bootstrap.sh
sudo DOKKU_TAG=v0.19.11 bash bootstrap.sh
You can then proceed to the ip address or domain name associated with your server to complete the web-based installation.
If you wish for a more unattended installation method, see these docs.
Upgrade
View the docs for upgrading from an older version of Dokku.
Documentation
Full documentation - including advanced installation docs - are available online at http://dokku.viewdocs.io/dokku/.
Support
You can use Github Issues, check Troubleshooting in the documentation, or join us on freenode in #dokku.
Contribution
After checking Github Issues, the Troubleshooting Guide or having a chat with us on freenode in #dokku, feel free to fork and create a Pull Request.
While we may not merge your PR as is, they serve to start conversations and improve the general Dokku experience for all users.
License
MIT License ¬© Jeff Lindsay
",GitHub - dokku/dokku: A docker-powered PaaS that helps you build and manage the lifecycle of applications
71,Shell,"Powerline fonts
This repository contains pre-patched and adjusted fonts for usage with
the Powerline statusline plugin.

Installation
Run ./install.sh to install all Powerline Fonts or see the documentation for details.

Quick installation
If you are running a Debian or Ubuntu based Linux distribution, there should
be a package available to install the Powerline Fonts with the following command:
sudo apt-get install fonts-powerline

For fedora (tested on 28) or redhat based Linux distribution, there should also be a package available to install with the following command:
sudo dnf install powerline-fonts

On other environments, you can copy and paste these commands to your terminal. Comments are fine too.
# clone
git clone https://github.com/powerline/fonts.git --depth=1
# install
cd fonts
./install.sh
# clean-up a bit
cd ..
rm -rf fonts


Uninstall
Run ./uninstall.sh to uninstall all Powerline Fonts. You can also copy
the quick installation commands changing only the line ./install.sh to
./uninstall.sh.
In both cases, please make sure you are working with the exact same version
of Powerline fonts you had checked out while installing.

Font Families


Powerline Font Family
Formerly Known As
License



3270
3270
BSD/CCAS 3.0

Anonymice Powerline
Anonymous Pro
SIL Open Font License, Version 1.1

Arimo Powerline
Arimo
Apache License, Version 2.0

Cousine Powerline
Cousine
Apache License, Version 2.0

D2Coding for Powerline
D2Coding
SIL Open Font License, Version 1.1

DejaVu Sans Mono for Powerline
DejaVu Sans Mono
DejaVu Fonts License, Version 1.0

Droid Sans Mono for Powerline
Droid Sans Mono
Apache License, Version 2.0

Droid Sans Mono Dotted for Powerline
Droid Sans Mono Dotted
Apache License, Version 2.0

Droid Sans Mono Slashed for Powerline
Droid Sans Mono Slashed
Apache License, Version 2.0

Fira Mono for Powerline
Fira Mono
SIL OPEN FONT LICENSE Version 1.1

Go Mono for Powerline
Go Mono
Go's License

Hack
Hack
SIL OFL, v1.1 + Bitstream License

Inconsolata for Powerline
Inconsolata
SIL Open Font License, Version 1.0

Inconsolata-dz for Powerline
Inconsolata-dz
SIL Open Font License, Version 1.0

Inconsolata-g for Powerline
Inconsolata-g
SIL Open Font License, Version 1.0

Input Mono
Input Mono
Input‚Äôs license

Liberation Mono Powerline
Liberation Mono
SIL Open Font License, Version 1.1

ProFontWindows
ProFont for Powerline
MIT License

Meslo for Powerline
Meslo
Apache License, Version 2.0

Source Code Pro for Powerline
Source Code Pro
SIL Open Font License, Version 1.1

Meslo Dotted for Powerline
Meslo Dotted
Apache License, Version 2.0

Meslo Slashed for Powerline
Meslo Dotted
Apache License, Version 2.0

Monofur for Powerline
Monofur
Freeware

Noto Mono for Powerline
Noto Mono
SIL Open Font License, Version 1.1

Roboto Mono for Powerline
Roboto Mono
Apache License, Version 2.0

Symbol Neu Powerline
Symbol Neu
Apache License, Version 2.0

Terminess Powerline
Terminus
SIL Open Font License, Version 1.1

Tinos Powerline
Tinos
Apache License, Version 2.0

Ubuntu Mono derivative Powerline
Ubuntu Mono
Ubuntu Font License, Version 1.0

Space Mono for Powerline
Space Mono
SIL Open Font License, Version 1.1



iTerm2 users need to set both the Regular font and the Non-ASCII Font in
""iTerm > Preferences > Profiles > Text"" to use a patched font (per this issue).

Fontconfig
In some distributions, Terminess Powerline is ignored by default and must be
explicitly allowed. A fontconfig file is provided which enables it. Copy this
file
from the fontconfig directory to your home folder under ~/.config/fontconfig/conf.d
(create it if it doesn't exist) and re-run fc-cache -vf.
",GitHub - powerline/fonts: Patched fonts for Powerline users.
72,Shell,"Simple Python Version Management: pyenv


pyenv lets you easily switch between multiple versions of Python. It's
simple, unobtrusive, and follows the UNIX tradition of single-purpose
tools that do one thing well.
This project was forked from rbenv and
ruby-build, and modified for Python.

pyenv does...

Let you change the global Python version on a per-user basis.
Provide support for per-project Python versions.
Allow you to override the Python version with an environment
variable.
Search commands from multiple versions of Python at a time.
This may be helpful to test across Python versions with tox.

In contrast with pythonbrew and pythonz, pyenv does not...

Depend on Python itself. pyenv was made from pure shell scripts.
There is no bootstrap problem of Python.
Need to be loaded into your shell. Instead, pyenv's shim
approach works by adding a directory to your $PATH.
Manage virtualenv. Of course, you can create virtualenv
yourself, or pyenv-virtualenv
to automate the process.


Table of Contents

How It Works

Understanding PATH
Understanding Shims
Choosing the Python Version
Locating the Python Installation


Installation

Basic GitHub Checkout

Upgrading
Homebrew on macOS
Advanced Configuration
Uninstalling Python Versions




Command Reference
Development

Version History
License




How It Works
At a high level, pyenv intercepts Python commands using shim
executables injected into your PATH, determines which Python version
has been specified by your application, and passes your commands along
to the correct Python installation.
Understanding PATH
When you run a command like python or pip, your operating system
searches through a list of directories to find an executable file with
that name. This list of directories lives in an environment variable
called PATH, with each directory in the list separated by a colon:
/usr/local/bin:/usr/bin:/bin

Directories in PATH are searched from left to right, so a matching
executable in a directory at the beginning of the list takes
precedence over another one at the end. In this example, the
/usr/local/bin directory will be searched first, then /usr/bin,
then /bin.
Understanding Shims
pyenv works by inserting a directory of shims at the front of your
PATH:
$(pyenv root)/shims:/usr/local/bin:/usr/bin:/bin

Through a process called rehashing, pyenv maintains shims in that
directory to match every Python command across every installed version
of Python‚Äîpython, pip, and so on.
Shims are lightweight executables that simply pass your command along
to pyenv. So with pyenv installed, when you run, say, pip, your
operating system will do the following:

Search your PATH for an executable file named pip
Find the pyenv shim named pip at the beginning of your PATH
Run the shim named pip, which in turn passes the command along to
pyenv

Choosing the Python Version
When you execute a shim, pyenv determines which Python version to use by
reading it from the following sources, in this order:


The PYENV_VERSION environment variable (if specified). You can use
the pyenv shell command to set this environment
variable in your current shell session.


The application-specific .python-version file in the current
directory (if present). You can modify the current directory's
.python-version file with the pyenv local
command.


The first .python-version file found (if any) by searching each parent
directory, until reaching the root of your filesystem.


The global $(pyenv root)/version file. You can modify this file using
the pyenv global command. If the global version
file is not present, pyenv assumes you want to use the ""system""
Python. (In other words, whatever version would run if pyenv weren't in your
PATH.)


NOTE: You can activate multiple versions at the same time, including multiple
versions of Python2 or Python3 simultaneously. This allows for parallel usage of
Python2 and Python3, and is required with tools like tox. For example, to set
your path to first use your system Python and Python3 (set to 2.7.9 and 3.4.2
in this example), but also have Python 3.3.6, 3.2, and 2.5 available on your
PATH, one would first pyenv install the missing versions, then set pyenv global system 3.3.6 3.2 2.5. At this point, one should be able to find the full
executable path to each of these using pyenv which, e.g. pyenv which python2.5
(should display $(pyenv root)/versions/2.5/bin/python2.5), or pyenv which python3.4 (should display path to system Python3). You can also specify multiple
versions in a .python-version file, separated by newlines or any whitespace.
Locating the Python Installation
Once pyenv has determined which version of Python your application has
specified, it passes the command along to the corresponding Python
installation.
Each Python version is installed into its own directory under
$(pyenv root)/versions.
For example, you might have these versions installed:

$(pyenv root)/versions/2.7.8/
$(pyenv root)/versions/3.4.2/
$(pyenv root)/versions/pypy-2.4.0/

As far as pyenv is concerned, version names are simply the directories in
$(pyenv root)/versions.
Managing Virtual Environments
There is a pyenv plugin named pyenv-virtualenv which comes with various features to help pyenv users to manage virtual environments created by virtualenv or Anaconda.
Because the activate script of those virtual environments are relying on mutating $PATH variable of user's interactive shell, it will intercept pyenv's shim style command execution hooks.
We'd recommend to install pyenv-virtualenv as well if you have some plan to play with those virtual environments.

Installation
If you're on macOS, consider installing with Homebrew.
If you're on Windows, consider using @kirankotari's pyenv-win fork. (pyenv does not work on windows outside the Windows Subsystem for Linux)
The automatic installer
Visit my other project:
https://github.com/pyenv/pyenv-installer
Basic GitHub Checkout
This will get you going with the latest version of pyenv and make it
easy to fork and contribute any changes back upstream.


Check out pyenv where you want it installed.
A good place to choose is $HOME/.pyenv (but you can install it somewhere else).
 $ git clone https://github.com/pyenv/pyenv.git ~/.pyenv



Define environment variable PYENV_ROOT to point to the path where
pyenv repo is cloned and add $PYENV_ROOT/bin to your $PATH for access
to the pyenv command-line utility.
$ echo 'export PYENV_ROOT=""$HOME/.pyenv""' >> ~/.bash_profile
$ echo 'export PATH=""$PYENV_ROOT/bin:$PATH""' >> ~/.bash_profile

Zsh note: Modify your ~/.zshenv file instead of ~/.bash_profile.
Ubuntu and Fedora note: Modify your ~/.bashrc file instead of ~/.bash_profile.
Proxy note: If you use a proxy, export http_proxy and HTTPS_PROXY too.



Add pyenv init to your shell to enable shims and autocompletion.
Please make sure eval ""$(pyenv init -)"" is placed toward the end of the shell
configuration file since it manipulates PATH during the initialization.
$ echo -e 'if command -v pyenv 1>/dev/null 2>&1; then\n  eval ""$(pyenv init -)""\nfi' >> ~/.bash_profile

Zsh note: Modify your ~/.zshenv file instead of ~/.bash_profile.
fish note: Use pyenv init - | source instead of eval (pyenv init -).
Ubuntu and Fedora note: Modify your ~/.bashrc file instead of ~/.bash_profile.

General warning: There are some systems where the BASH_ENV variable is configured
to point to .bashrc. On such systems you should almost certainly put the abovementioned line
eval ""$(pyenv init -)"" into .bash_profile, and not into .bashrc. Otherwise you
may observe strange behaviour, such as pyenv getting into an infinite loop.
See #264 for details.


Restart your shell so the path changes take effect.
You can now begin using pyenv.
$ exec ""$SHELL""


Install Python build dependencies before attempting to install a new Python version.  The
pyenv wiki provides suggested installation packages
and commands for various operating systems.


Install Python versions into $(pyenv root)/versions.
For example, to download and install Python 2.7.8, run:
$ pyenv install 2.7.8
NOTE: If you need to pass configure option to build, please use
CONFIGURE_OPTS environment variable.
NOTE: If you want to use proxy to download, please use http_proxy and https_proxy
environment variable.
NOTE: If you are having trouble installing a python version,
please visit the wiki page about
Common Build Problems


Upgrading
If you've installed pyenv using the instructions above, you can
upgrade your installation at any time using git.
To upgrade to the latest development version of pyenv, use git pull:
$ cd $(pyenv root)
$ git pull
To upgrade to a specific release of pyenv, check out the corresponding tag:
$ cd $(pyenv root)
$ git fetch
$ git tag
v0.1.0
$ git checkout v0.1.0
Uninstalling pyenv
The simplicity of pyenv makes it easy to temporarily disable it, or
uninstall from the system.

To disable pyenv managing your Python versions, simply remove the
pyenv init line from your shell startup configuration. This will
remove pyenv shims directory from PATH, and future invocations like
python will execute the system Python version, as before pyenv.

pyenv will still be accessible on the command line, but your Python
apps won't be affected by version switching.


To completely uninstall pyenv, perform step (1) and then remove
its root directory. This will delete all Python versions that were
installed under $(pyenv root)/versions/ directory:
rm -rf $(pyenv root)
If you've installed pyenv using a package manager, as a final step
perform the pyenv package removal. For instance, for Homebrew:
 brew uninstall pyenv



Homebrew on macOS
You can also install pyenv using the Homebrew
package manager for macOS.
$ brew update
$ brew install pyenv

To upgrade pyenv in the future, use upgrade instead of install.
Then follow the rest of the post-installation steps under Basic GitHub Checkout above, starting with #3 (""Add pyenv init to your shell to enable shims and autocompletion"").
Advanced Configuration
Skip this section unless you must know what every line in your shell
profile is doing.
pyenv init is the only command that crosses the line of loading
extra commands into your shell. Coming from rvm, some of you might be
opposed to this idea. Here's what pyenv init actually does:


Sets up your shims path. This is the only requirement for pyenv to
function properly. You can do this by hand by prepending
$(pyenv root)/shims to your $PATH.


Installs autocompletion. This is entirely optional but pretty
useful. Sourcing $(pyenv root)/completions/pyenv.bash will set that
up. There is also a $(pyenv root)/completions/pyenv.zsh for Zsh
users.


Rehashes shims. From time to time you'll need to rebuild your
shim files. Doing this on init makes sure everything is up to
date. You can always run pyenv rehash manually.


Installs the sh dispatcher. This bit is also optional, but allows
pyenv and plugins to change variables in your current shell, making
commands like pyenv shell possible. The sh dispatcher doesn't do
anything crazy like override cd or hack your shell prompt, but if
for some reason you need pyenv to be a real script rather than a
shell function, you can safely skip it.


To see exactly what happens under the hood for yourself, run pyenv init -.
Uninstalling Python Versions
As time goes on, you will accumulate Python versions in your
$(pyenv root)/versions directory.
To remove old Python versions, pyenv uninstall command to automate
the removal process.
Alternatively, simply rm -rf the directory of the version you want
to remove. You can find the directory of a particular Python version
with the pyenv prefix command, e.g. pyenv prefix 2.6.8.

Command Reference
See COMMANDS.md.

Environment variables
You can affect how pyenv operates with the following settings:



name
default
description




PYENV_VERSION

Specifies the Python version to be used.Also see pyenv shell


PYENV_ROOT
~/.pyenv
Defines the directory under which Python versions and shims reside.Also see pyenv root


PYENV_DEBUG

Outputs debug information.Also as: pyenv --debug <subcommand>


PYENV_HOOK_PATH
see wiki
Colon-separated list of paths searched for pyenv hooks.


PYENV_DIR
$PWD
Directory to start searching for .python-version files.


PYTHON_BUILD_ARIA2_OPTS

Used to pass additional parameters to aria2.If the aria2c binary is available on PATH, pyenv uses aria2c instead of curl or wget to download the Python Source code. If you have an unstable internet connection, you can use this variable to instruct aria2 to accelerate the download.In most cases, you will only need to use -x 10 -k 1M as value to PYTHON_BUILD_ARIA2_OPTS environment variable



Development
The pyenv source code is hosted on
GitHub.  It's clean, modular,
and easy to understand, even if you're not a shell hacker.
Tests are executed using Bats:
$ bats test
$ bats/test/<file>.bats

Please feel free to submit pull requests and file bugs on the issue
tracker.
Version History
See CHANGELOG.md.
License
The MIT License
",GitHub - pyenv/pyenv: Simple Python version management
73,Shell,"An ACME Shell script: acme.sh 
 

An ACME protocol client written purely in Shell (Unix shell) language.
Full ACME protocol implementation.
Support ACME v1 and ACME v2
Support ACME v2 wildcard certs
Simple, powerful and very easy to use. You only need 3 minutes to learn it.
Bash, dash and sh compatible.
Simplest shell script for Let's Encrypt free certificate client.
Purely written in Shell with no dependencies on python or the official Let's Encrypt client.
Just one script to issue, renew and install your certificates automatically.
DOES NOT require root/sudoer access.
Docker friendly
IPv6 support
Cron job notifications for renewal or error etc.

It's probably the easiest & smartest shell script to automatically issue & renew the free certificates from Let's Encrypt.
Wiki: https://github.com/Neilpang/acme.sh/wiki
For Docker Fans: acme.sh üíï Docker 
Twitter: @neilpangxa
‰∏≠ÊñáËØ¥Êòé
Who:

FreeBSD.org
ruby-china.org
Proxmox
pfsense
webfaction
Loadbalancer.org
discourse.org
Centminmod
splynx
archlinux
opnsense.org
CentOS Web Panel
lnmp.org
more...

Tested OS



NO
Status
Platform




1

Ubuntu


2

Debian


3

CentOS


4

Windows (cygwin with curl, openssl and crontab included)


5

FreeBSD


6

pfsense


7

openSUSE


8

Alpine Linux (with curl)


9

Archlinux


10

fedora


11

Kali Linux


12

Oracle Linux


13

Proxmox https://pve.proxmox.com/wiki/HTTPSCertificateConfiguration#Let.27s_Encrypt_using_acme.sh


14
-----
Cloud Linux  https://github.com/Neilpang/le/issues/111


15

OpenBSD


16

Mageia


17
-----
OpenWRT: Tested and working. See wiki page


18

SunOS/Solaris


19

Gentoo Linux


20

Mac OSX



For all build statuses, check our weekly build project:
https://github.com/Neilpang/acmetest
Supported CA

Letsencrypt.org CA(default)
BuyPass.com CA
Pebble strict Mode

Supported modes

Webroot mode
Standalone mode
Standalone tls-alpn mode
Apache mode
Nginx mode
DNS mode
DNS alias mode
Stateless mode

1. How to install
1. Install online
Check this project: https://github.com/Neilpang/get.acme.sh
curl https://get.acme.sh | sh
Or:
wget -O -  https://get.acme.sh | sh
2. Or, Install from git
Clone this project and launch installation:
git clone https://github.com/Neilpang/acme.sh.git
cd ./acme.sh
./acme.sh --install
You don't have to be root then, although it is recommended.
Advanced Installation: https://github.com/Neilpang/acme.sh/wiki/How-to-install
The installer will perform 3 actions:

Create and copy acme.sh to your home dir ($HOME): ~/.acme.sh/.
All certs will be placed in this folder too.
Create alias for: acme.sh=~/.acme.sh/acme.sh.
Create daily cron job to check and renew the certs if needed.

Cron entry example:
0 0 * * * ""/home/user/.acme.sh""/acme.sh --cron --home ""/home/user/.acme.sh"" > /dev/null
After the installation, you must close the current terminal and reopen it to make the alias take effect.
Ok, you are ready to issue certs now.
Show help message:
root@v1:~# acme.sh -h
2. Just issue a cert
Example 1: Single domain.
acme.sh --issue -d example.com -w /home/wwwroot/example.com
or:
acme.sh --issue -d example.com -w /home/username/public_html
or:
acme.sh --issue -d example.com -w /var/www/html
Example 2: Multiple domains in the same cert.
acme.sh --issue -d example.com -d www.example.com -d cp.example.com -w /home/wwwroot/example.com
The parameter /home/wwwroot/example.com or /home/username/public_html or /var/www/html is the web root folder where you host your website files. You MUST have write access to this folder.
Second argument ""example.com"" is the main domain you want to issue the cert for.
You must have at least one domain there.
You must point and bind all the domains to the same webroot dir: /home/wwwroot/example.com.
The certs will be placed in ~/.acme.sh/example.com/
The certs will be renewed automatically every 60 days.
More examples: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert
3. Install the cert to Apache/Nginx etc.
After the cert is generated, you probably want to install/copy the cert to your Apache/Nginx or other servers.
You MUST use this command to copy the certs to the target files, DO NOT use the certs files in ~/.acme.sh/ folder, they are for internal use only, the folder structure may change in the future.
Apache example:
acme.sh --install-cert -d example.com \
--cert-file      /path/to/certfile/in/apache/cert.pem  \
--key-file       /path/to/keyfile/in/apache/key.pem  \
--fullchain-file /path/to/fullchain/certfile/apache/fullchain.pem \
--reloadcmd     ""service apache2 force-reload""
Nginx example:
acme.sh --install-cert -d example.com \
--key-file       /path/to/keyfile/in/nginx/key.pem  \
--fullchain-file /path/to/fullchain/nginx/cert.pem \
--reloadcmd     ""service nginx force-reload""
Only the domain is required, all the other parameters are optional.
The ownership and permission info of existing files are preserved. You can pre-create the files to define the ownership and permission.
Install/copy the cert/key to the production Apache or Nginx path.
The cert will be renewed every 60 days by default (which is configurable). Once the cert is renewed, the Apache/Nginx service will be reloaded automatically by the command: service apache2 force-reload or service nginx force-reload.
Please take care:  The reloadcmd is very important. The cert can be automatically renewed, but, without a correct 'reloadcmd' the cert may not be flushed to your server(like nginx or apache), then your website will not be able to show renewed cert in 60 days.
4. Use Standalone server to issue cert
(requires you to be root/sudoer or have permission to listen on port 80 (TCP))
Port 80 (TCP) MUST be free to listen on, otherwise you will be prompted to free it and try again.
acme.sh --issue --standalone -d example.com -d www.example.com -d cp.example.com
More examples: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert
5. Use Standalone ssl server to issue cert
(requires you to be root/sudoer or have permission to listen on port 443 (TCP))
Port 443 (TCP) MUST be free to listen on, otherwise you will be prompted to free it and try again.
acme.sh --issue --alpn -d example.com -d www.example.com -d cp.example.com
More examples: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert
6. Use Apache mode
(requires you to be root/sudoer, since it is required to interact with Apache server)
If you are running a web server, Apache or Nginx, it is recommended to use the Webroot mode.
Particularly, if you are running an Apache server, you can use Apache mode instead. This mode doesn't write any files to your web root folder.
Just set string ""apache"" as the second argument and it will force use of apache plugin automatically.
acme.sh --issue --apache -d example.com -d www.example.com -d cp.example.com
This apache mode is only to issue the cert, it will not change your apache config files.
You will need to configure your website config files to use the cert by yourself.
We don't want to mess your apache server, don't worry.
More examples: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert
7. Use Nginx mode
(requires you to be root/sudoer, since it is required to interact with Nginx server)
If you are running a web server, Apache or Nginx, it is recommended to use the Webroot mode.
Particularly, if you are running an nginx server, you can use nginx mode instead. This mode doesn't write any files to your web root folder.
Just set string ""nginx"" as the second argument.
It will configure nginx server automatically to verify the domain and then restore the nginx config to the original version.
So, the config is not changed.
acme.sh --issue --nginx -d example.com -d www.example.com -d cp.example.com
This nginx mode is only to issue the cert, it will not change your nginx config files.
You will need to configure your website config files to use the cert by yourself.
We don't want to mess your nginx server, don't worry.
More examples: https://github.com/Neilpang/acme.sh/wiki/How-to-issue-a-cert
8. Automatic DNS API integration
If your DNS provider supports API access, we can use that API to automatically issue the certs.
You don't have to do anything manually!
Currently acme.sh supports most of the dns providers:
https://github.com/Neilpang/acme.sh/wiki/dnsapi
9. Use DNS manual mode:
See: https://github.com/Neilpang/acme.sh/wiki/dns-manual-mode first.
If your dns provider doesn't support any api access, you can add the txt record by your hand.
acme.sh --issue --dns -d example.com -d www.example.com -d cp.example.com
You should get an output like below:
Add the following txt record:
Domain:_acme-challenge.example.com
Txt value:9ihDbjYfTExAYeDs4DBUeuTo18KBzwvTEjUnSwd32-c

Add the following txt record:
Domain:_acme-challenge.www.example.com
Txt value:9ihDbjxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

Please add those txt records to the domains. Waiting for the dns to take effect.
Then just rerun with renew argument:
acme.sh --renew -d example.com
Ok, it's done.
Take care, this is dns manual mode, it can not be renewed automatically. you will have to add a new txt record to your domain by your hand when you renew your cert.
Please use dns api mode instead.
10. Issue ECC certificates
Let's Encrypt can now issue ECDSA certificates.
And we support them too!
Just set the keylength parameter with a prefix ec-.
For example:
Single domain ECC certificate
acme.sh --issue -w /home/wwwroot/example.com -d example.com --keylength ec-256
SAN multi domain ECC certificate
acme.sh --issue -w /home/wwwroot/example.com -d example.com -d www.example.com --keylength ec-256
Please look at the keylength parameter above.
Valid values are:

ec-256 (prime256v1, ""ECDSA P-256"")
ec-384 (secp384r1,  ""ECDSA P-384"")
ec-521 (secp521r1,  ""ECDSA P-521"", which is not supported by Let's Encrypt yet.)

11. Issue Wildcard certificates
It's simple, just give a wildcard domain as the -d parameter.
acme.sh  --issue -d example.com  -d '*.example.com'  --dns dns_cf
12. How to renew the certs
No, you don't need to renew the certs manually. All the certs will be renewed automatically every 60 days.
However, you can also force to renew a cert:
acme.sh --renew -d example.com --force
or, for ECC cert:
acme.sh --renew -d example.com --force --ecc
13. How to stop cert renewal
To stop renewal of a cert, you can execute the following to remove the cert from the renewal list:
acme.sh --remove -d example.com [--ecc]
The cert/key file is not removed from the disk.
You can remove the respective directory (e.g. ~/.acme.sh/example.com) by yourself.
14. How to upgrade acme.sh
acme.sh is in constant development, so it's strongly recommended to use the latest code.
You can update acme.sh to the latest code:
acme.sh --upgrade
You can also enable auto upgrade:
acme.sh --upgrade --auto-upgrade
Then acme.sh will be kept up to date automatically.
Disable auto upgrade:
acme.sh --upgrade --auto-upgrade 0
15. Issue a cert from an existing CSR
https://github.com/Neilpang/acme.sh/wiki/Issue-a-cert-from-existing-CSR
16. Send notifications in cronjob
https://github.com/Neilpang/acme.sh/wiki/notify
17. Under the Hood
Speak ACME language using shell, directly to ""Let's Encrypt"".
TODO:
18. Acknowledgments

Acme-tiny: https://github.com/diafygi/acme-tiny
ACME protocol: https://github.com/ietf-wg-acme/acme

Contributors
Code Contributors
This project exists thanks to all the people who contribute. [Contribute].

Financial Contributors
Become a financial contributor and help us sustain our community. [Contribute]
Individuals

Organizations
Support this project with your organization. Your logo will show up here with a link to your website. [Contribute]










19. License & Others
License is GPLv3
Please Star and Fork me.
Issues and pull requests are welcome.
20. Donate
Your donation makes acme.sh better:

PayPal/Alipay(ÊîØ‰ªòÂÆù)/Wechat(ÂæÆ‰ø°): https://donate.acme.sh/

Donate List
",GitHub - Neilpang/acme.sh: A pure Unix shell script implementing ACME client protocol
74,Shell,"iTerm Color Schemes

Intro
Installation Instructions
Contribute
Screenshots
Credits
Extra

X11 Installation
Konsole color schemes
Terminator color schemes
Mac OS Terminal color schemes
PuTTY color schemes
Xfce Terminal color schemes
FreeBSD vt(4) color schemes
Previewing color schemes
MobaXterm color schemes
LXTerminal color schemes



Intro
This is a set of color schemes for iTerm (aka iTerm2). It also includes ports to Terminal, Konsole, PuTTY, Xresources, XRDB, Remmina, Termite, XFCE, Tilda, FreeBSD VT, Terminator, Kitty, MobaXterm, LXTerminal, Microsoft's Windows Terminal, Visual Studio
Screenshots below and in the screenshots directory.
Installation Instructions
There are 2 ways to install an iTerm theme (both go to the same configuration location):


Direct way via keyboard shortcut:

Launch iTerm 2. Get the latest version at iterm2.com
Type CMD+i (‚åò+i)
Navigate to Colors tab
Click on Color Presets
Click on Import
Click on the schemes folder
Select the .itermcolors profiles you would like to import
Click on Color Presets and choose a color scheme



Via iTerm preferences

Launch iTerm 2. Get the latest version at iterm2.com
Click on iTerm2 menu title
Select Preferences... option
Select Profiles
Navigate to Colors tab
Click on Color Presets
Click on Import
Select the .itermcolors file(s) of the schemes you'd like to use
Click on Color Presets and choose a color scheme



Contribute
Have a great iTerm theme? Send it to me via a Pull Request! To export your theme settings:

Launch iTerm 2
Type CMD+i (‚åò+i)
Navigate to Colors tab
Click on Color Presets
Click on Export
Save the .itermcolors file

To include a screenshot, please generate the output using the screenshotTable.sh script in the tools directory.
For screenshot consistency, please have your font set to 13pt Monaco and no transparency on the window
It would also be very helpful if you cd tools/ and run python3 update_all.py to generate all formats of your scheme
Screenshots
3024 Day

3024 Night

AdventureTime

Afterglow

AlienBlood

Andromeda

Argonaut

Arthur

AtelierSulphurpool

Atom

Atom One Light

ayu

ayu Light

Batman

Belafonte Day

Belafonte Night

BirdsOfParadise

Blazer

Bluloco Dark

Bluloco Light

Borland

Breeze

Bright Lights

Broadcast

Brogrammer

Builtin Dark

Builtin Light

Builtin Pastel Dark

Builtin Solarized Dark

Builtin Solarized Light

Builtin Tango Dark

Builtin Tango Light

C64

Calamity

Chalk

Chalkboard

ChallengerDeep

Chester

Ciapre

CLRS

Cobalt Neon

Cobalt2

CrayonPonyFish

Cyberpunk

Dark Pastel

Dark+

Darkside

Deep

Desert

DimmedMonokai

DotGov

Dracula

Duotone Dark

Earthsong

Elemental

Elementary

ENCOM

Espresso

Espresso Libre

Fahrenheit

Fideloper

FirefoxDev

Firewatch

FishTank

Flat

Flatland

Floraverse

Forest Blue

Framer

FrontEndDelight

FunForrest

Galaxy

Github

Glacier

Grape

Grass

Gruvbox Dark

Hacktober

Hardcore

Harper

Highway

Hipster Green

Hivacruz

Homebrew

Hopscotch

Hopscotch 256

Hurtado

Hybrid

IC_Green_PPL

IC_Orange_PPL

IDEA Drak

idleToes

IR_Black

Jackie Brown

Japanesque

Jellybeans

JetBrains Darcula

Kibble

Kolorit

Lab Fox

Later This Evening

Lavandula

LiquidCarbon

LiquidCarbonTransparent

LiquidCarbonTransparentInverse

LoveLace

Man Page

Material

MaterialDark

MaterialOcean

Mathias

Medallion

Midnight In Mojave

Misterioso

Molokai

MonaLisa

Monokai Remastered

Monokai Soda

Monokai Vivid

N0tch2k

Neopolitan

Neutron

NightLion v1

NightLion v2

Night Owlish Light

Novel

Nocturnal Winter

Obsidian

Ocean

OceanicMaterial

Ollie

One Half Dark

One Half Light

Operator Mono Dark

Pandora

Parasio Dark

PaulMillr

Pencil Dark

Pencil Light

Piatto Light

Pnevma

Primary

Pro

Pro Light

Purple Peter

Purple Rain

Rebecca

Red Alert

Red Planet

Red Sands

Relaxed

Rippedcasts

Royal

Ryuuko

Seafoam Pastel

SeaShells

Seti

Shaman

Shades-Of-Purple

Slate

SleepyHollow

Snazzy

Smyck

SoftServer

Solarized Darcula (With background image)

Solarized Darcula (Without background image)

Solarized Dark - Patched
Some applications assume the ANSI color code 8 is a gray color. Solarized treats
this code as equal to the background. This theme is for people who prefer the
former. See issues #59, #62, and #63 for
more information.

Solarized Dark Higher Contrast

SpaceGray

SpaceGray Eighties

SpaceGray Eighties Dull

Spacedust

Spiderman

Spring

Square

Subliminal

Sundried

Symfonic

synthwave

Tango Adapted

Tango Half Adapted

Teerb

Terminal Basic

Thayer Bright

The Hulk

Tomorrow

Tomorrow Night

Tomorrow Night Blue

Tomorrow Night Bright

Tomorrow Night Eighties

Tomorrow Night Burns

ToyChest

Treehouse

Twilight

Ubuntu

UltraViolent

Under The Sea

Unikitty

Urple

Vaughn

VibrantInk

Violet Light

Violet Dark

WarmNeon

Wez

Whimsy

WildCherry

Wombat

Wryan

Zenburn

Credits
The schemes Novel, Espresso, Grass, Homebrew, Ocean, Pro, Man Page, Red Sands, and Terminal Basic are ports of the schemes of the same name included with the Mac Terminal application. All of Terminal's schemes have now been ported, with the exception of ""Solid Colors"" (random backgrounds, which iTerm doesn't support) and ""Aerogel"" (which is hideous).
The scheme Violet was created by ashfinal.
The scheme idleToes was inspired by the idleFingers TextMate theme and suggested for inclusion by Seth Wilson.
The scheme Zenburn was inspired by the Zenburn version created by Suraj N. Kurapati.
The scheme Symfonic was inspired by the color scheme used in the documentation for the Symfony PHP Framework.
The synthwave theme was created by brettstil
The scheme Github was inspired by the color theme palette used at Github for their UI and in some of their mascot images.
The scheme Solarized Darcula was inspired by the color theme created by @rickhanlonii. There are two screenshots below; one with a background and one without. The background image is included in the backgrounds/ directory and the image must be manually set on the profile's Preferences->Window tab. It's hard to see from the screenshot, but it looks great!
The theme Hurtado was found here.
The theme Molokai was based on the vim colorscheme of the same name.
The theme Monokai Soda was found here.
The theme Monokai Vivid was created by vitalymak.
The theme Neopolitan was created by daytonn
The theme Subliminal was created by gdsrosa
The themes Solarized Dark and Solarized Light come from the official Solarized web site
The Obsidian theme was on my hard drive and I don't recall where it came from. If anyone knows, ping me and I'll link back to the author(s).
The Spacedust theme was created by mhallendal and ported to iTerm by Couto
The theme Mathias was created by mathiasbynens
The LiquidCarbon themes were created by markcho
The NightLion themes were created by Curvve Media
The Tomorrow themes were created by chriskempson
The Tomorrow Night Burns theme were created by ashwinv11
The Twilight theme was created by stefri
The Teerb theme was created by Teerb
The IR_Black theme was found here
The Misterioso theme was created by flooose
The Hybrid theme was found here
The Thayer Bright theme was found here
The Shades Of Purple theme was found here and ported by fr3fou
The Spring theme was found here
The Smyck theme was found here
The Forest Blue theme was found here
The Cobalt2 theme was created by wesbos
The Operator Mono Dark was ported from vharadkou by dreamyguy
The WarmNeon theme was ported from PyCharm by firewut
The SpaceGray theme was created by ryentzer
The Jellybeans theme was created by qtpi
The PaulMillr theme was created by paulmillr and ported to iTerm by me
The Harper theme was created by idleberg
The CLRS theme was created by jasonlong
The Dracula theme was created by zenorocha
The themes AdventureTime, AlienBlood, BirdsOfParadise, Ciapre, CrayonPonyFish, DimmedMonokai, Earthsong, Elemental, FishTank, FrontEndDelight, FunForrest, Grape, Highway, IC_Green_PPL, IC_Orange_PPL, Lavandula, Medallion, MonaLisa, Ollie, Royal, SeaShells, Shaman, SleepyHollow, SoftServer, Sundried, ToyChest, Treehouse, and Urple were created by zdj
The Japanesque theme was created by aereal
The Seti theme was created by jesseweed and ported to iTerm2 by philduffy and slightly modified to make the ANSI blacks have a little more contrast
The Hipster Green and Jackie Brown themes were created by Unkulunkulu
The Chalk theme was created by achalv
The Pencil Dark and Pencil Light themes were created by mattly
The Flat theme was created by ahmetsulek
The Atom, Brogrammer, Glacier and Darkside themes were created by bahlo
The Afterglow theme was created by yabatadesign
The Broadcast theme was created by vinkla
The Arthur, N0tch2k, Pnevma, Square, and Wryan themes were created by baskerville
The Belafonte Day, Belafonte Night, Paraiso Dark, and C64 themes were created by idleberg
The 3024 Day and 3024 Night themes were created by 0x3024
The Argonaut theme was created by effkay
The Espresso Libre theme was created by jurgemaister
The Hardcore theme was created by hardcore
The Rippedcasts theme was created by mdw123
The Solorized Dark Higher Contrast theme was created by heisters
The VibrantInk theme was created by asanghi
The Wez theme was created by wez
The Wombat theme was created by djoyner
The WildCherry theme was created by mashaal
The Flatland theme was created by realchaseadams
The Neutron theme was created by Ch4s3
The Fideloper theme was inspired by Fideloper
The Later This Evening theme was created by vilhalmer
The Galaxy theme was created by jglovier
The Slate theme was created by deneshshan
The SpaceGray Eighties and SpaceGray Eighties Dull themes were created by mhkeller
The Borland theme was created by delip
The Cobalt Neon theme was created by vazquez
The AtelierSulphurpool theme was created by atelierbram
The Batman, Spiderman, and The Hulk themes were created by pyrophos
The ENCOM theme was created by Josh Braun
The Floraverse theme was created by papayakitty
The Material and MaterialDark themes were created by stoeffel
The OceanicMaterial theme was created by rahulpatel
The MaterialOcean theme was found here and was ported by fr3fou
The AtomOneLight theme was created by iamstarkov
The Piatto Light theme was created by kovv
The DotGov theme was inspired by the color palette used in the U.S. Web Design Standards
The DuoTone Dark theme was created by chibicode based on simurai's duotone-dark Atom theme.
The JetBrains Darcula theme was created by vitstr
The Bright Lights theme was created by tw15egan
The Ubuntu theme was inspired by default Ubuntu's console colors. Created by Maximus and ported to iTerm2 by stepin.
The Under The Sea theme was created by peterpme
The One Half themes were created by sonph, based on Atom's One with some tweakings.
The FirefoxDev theme was created by Or3stis
The Ryuuko theme was created by dylanaraps and added by iandrewt
The Firewatch theme was created by Sebastian Szturo as an atom syntax theme and ported to an iTerm scheme by Ben Styles. It was originally inspired by the Firewatch game.
The Pandora theme was created by milosmatic
The Elementary OS  theme was created by elementary.io and added by 987poiuytrewq
The Gruvbox Dark  theme was created by morhetz and added by mcchrish
The Ayu and Ayu Light  themes were created by alebcay
The Deep  theme was created by ADoyle
The Red Planet theme was created by ibrokemypie based on eliquious's Red Planet Sublime Text theme.
The Tango Adapted theme and its slightly less bright counterpart Tango Half Adapted were created by crowsonkb, based on iTerm2's Tango Light theme.
The Pro Light theme was adapted from the Pro theme by crowsonkb using her iterm_schemer tool.
The Fahrenheit theme was created by jonascarpay based on fcpg's vim-fahrenheit Vim theme.
The Calamity theme was created by Pustur
The Purple Peter theme was created by DeChamp
The Purple Rain theme was created by msorre2
The ChallengerDeep theme was found here and ported to iTerm by eendroroy
The Hopscotch theme was created by Jan T. Sott
The Hivacruz theme was created by Yann Defretin
The Nocturnal Winter theme was created by Shriram Balaji
The Rebecca theme was created vic based on his emacs rebecca-theme
The Whimsy theme was created by Rod
The Chester theme was created by KH
The Kolorit theme was created by thomaspaulmann
The Cyberpunk theme was created by Murderlon
The LoveLace theme was created by nalanbar based on a dotfile from elenapan
The Framer theme was created by markogresak based on Framer Syntax Theme provided by Framer.
The Lab Fox theme was created by theatlasroom inspired by the Gitlab design system
The Unikitty theme was created by jakebathman
The Primary theme was created by jayanthkoushik based on Google's vim colorscheme.
The Andromeda theme was created by memije based on EliverLara's Andromeda VS Code theme.
The Night Owlish Light theme was created by praveenpuglia based on sdras's Night Owl VS Code theme.
The Midnight In Mojave theme was created by OberstKrueger based on the colors outlined in the Human Interface Guidelines
The Breeze theme was based on the Breeze color scheme by KDE and ported to iTerm by nieltg
If there are other color schemes you'd like to see included, drop me a line!
Extra
X11 Installation
To install under the X Window System:


Import the .xrdb file of the scheme you'd like to use:
  #include ""/home/mbadolato/iTerm2-Color-Schemes/xrdb/Blazer.xrdb""



Use the #defines provided by the imported .xrdb file:
  Rxvt*color0:       Ansi_0_Color
  Rxvt*color1:       Ansi_1_Color
  Rxvt*color2:       Ansi_2_Color
  Rxvt*color3:       Ansi_3_Color
  Rxvt*color4:       Ansi_4_Color
  Rxvt*color5:       Ansi_5_Color
  Rxvt*color6:       Ansi_6_Color
  Rxvt*color7:       Ansi_7_Color
  Rxvt*color8:       Ansi_8_Color
  Rxvt*color9:       Ansi_9_Color
  Rxvt*color10:      Ansi_10_Color
  Rxvt*color11:      Ansi_11_Color
  Rxvt*color12:      Ansi_12_Color
  Rxvt*color13:      Ansi_13_Color
  Rxvt*color14:      Ansi_14_Color
  Rxvt*color15:      Ansi_15_Color
  Rxvt*colorBD:      Bold_Color
  Rxvt*colorIT:      Italic_Color
  Rxvt*colorUL:      Underline_Color
  Rxvt*foreground:   Foreground_Color
  Rxvt*background:   Background_Color
  Rxvt*cursorColor:  Cursor_Color

  XTerm*color0:      Ansi_0_Color
  XTerm*color1:      Ansi_1_Color
  XTerm*color2:      Ansi_2_Color
  XTerm*color3:      Ansi_3_Color
  XTerm*color4:      Ansi_4_Color
  XTerm*color5:      Ansi_5_Color
  XTerm*color6:      Ansi_6_Color
  XTerm*color7:      Ansi_7_Color
  XTerm*color8:      Ansi_8_Color
  XTerm*color9:      Ansi_9_Color
  XTerm*color10:     Ansi_10_Color
  XTerm*color11:     Ansi_11_Color
  XTerm*color12:     Ansi_12_Color
  XTerm*color13:     Ansi_13_Color
  XTerm*color14:     Ansi_14_Color
  XTerm*color15:     Ansi_15_Color
  XTerm*colorBD:     Bold_Color
  XTerm*colorIT:     Italic_Color
  XTerm*colorUL:     Underline_Color
  XTerm*foreground:  Foreground_Color
  XTerm*background:  Background_Color
  XTerm*cursorColor: Cursor_Color



Store the above snippets in a file and pass it in:
  $ xrdb -merge YOUR_FILE_CONTAINING_ABOVE_SNIPPETS



Open new XTerm or Rxvt windows to see the changes.


Adapt this procedure to other terminals as needed.


Terminator color schemes
Edit your Terminator configuration file (located in: $HOME/.config/terminator/config) and add the configurations for the theme(s) you'd like to use the [profiles] section. The terminator/ directory contains the config snippets you'll need. Just paste the configurations into the [profiles] sections, and you're good to go!
At a minimum, this is all you need. You can customize the fonts and other aspects as well, if you wish. See the Terminator documentation for more details.
An example config file that includes the code snippet for the Symfonic theme would look like this:
[global_config]
    [keybindings]
    [profiles]
      [[default]]
        palette = ""#1a1a1a:#f4005f:#98e024:#fa8419:#9d65ff:#f4005f:#58d1eb:#c4c5b5:#625e4c:#f4005f:#98e024:#e0d561:#9d65ff:#f4005f:#58d1eb:#f6f6ef""
        background_image = None
        use_system_font = False
        cursor_color = ""#f6f7ec""
        foreground_color = ""#c4c5b5""
        font = Source Code Pro Light 11
        background_color = ""#1a1a1a""
      [[Symfonic]]
        palette = ""#000000:#dc322f:#56db3a:#ff8400:#0084d4:#b729d9:#ccccff:#ffffff:#1b1d21:#dc322f:#56db3a:#ff8400:#0084d4:#b729d9:#ccccff:#ffffff""
        background_color = ""#000000""
        cursor_color = ""#dc322f""
        foreground_color = ""#ffffff""
        background_image = None
    [layouts]
      [[default]]
        [[[child1]]]
          type = Terminal
          parent = window0
        [[[window0]]]
          type = Window
          parent = """"
    [plugins]
Konsole color schemes
Copy the themes from the konsole directory to $HOME/.config/konsole (in some versions of KDE, the theme directory may be located at $HOME/.local/share/konsole), restart Konsole and choose your new theme from the profile preferences window.
If you want to make the themes available to all users, copy the .colorscheme files to /usr/share/konsole.
Terminal color schemes
Just double click on selected theme in terminal directory
PuTTY color schemes
New Session Method
This method creates a new blank session with JUST colors set properly.
Download the appropriate colorscheme.reg file and import the registry changes by right-clicking and choosing Merge. Choose ""Yes"" when prompted if you're sure. Color scheme will show up as a new PuTTY session with all defaults except entries at Window > Colours > Adjust the precise colours PuTTY displays.
Modify Session Method
This method modifies an existing session and changes JUST the color settings.
Download the appropriate colorscheme.reg file. Open the file with a text editor and change the color scheme portion (Molokai below) to match the session you want to modify:
[HKEY_CURRENT_USER\Software\SimonTatham\PuTTY\Sessions\Molokai]
- CHANGE TO (EXAMPLE) -
[HKEY_CURRENT_USER\Software\SimonTatham\PuTTY\Sessions\root@localhost]

NOTE: Some special characters will need to be changed to their Percent-encoded representation (IE, Space as %20). To quickly find the right session name view the top-level entries at HKEY_CURRENT_USER\Software\SimonTatham\PuTTY\Sessions\ with regedit.exe.
Other PuTTY Recommendations
Window > Apprearance
	Font: Consolas, bold, 14-point
	Font quality:
		( ) Antialiased     ( ) Non-Antialiased
		(O) ClearType       ( ) Default
Window > Colours
	[X] Allow terminal to specify ANSI colours
	[X] Allow terminal to use xterm 256-colour mode
	Indicate bolded text by changing:
		( ) The font   (O) The colour   ( ) Both
	[ ] Attempt to use logical palettes
	[ ] Use system colours

Xfce Terminal color schemes
Copy the colorschemes folder to ~/.local/share/xfce4/terminal/ and restart Terminal.
FreeBSD vt color schemes
Append your favourite theme from freebsd_vt/ to /boot/loader.conf
or /boot/loader.conf.local and reboot.
MobaXterm color schemes
Copy the theme content form mobaxterm/ and paste the content to your MobaXterm.ini in the corresponding place. ([Colors])
LXTerminal color schemes
Copy the theme content form lxterminal/ and paste the content to your lxterminal in the corresponding place. ([general])
Visual Studio Code (vscode) color schemes
Copy the theme content into your UserSettings.json
Previewing color schemes
preview.rb is a simple script that allows you to preview
the color schemes without having to import them. It parses .itermcolors files
and applies the colors to the current session using iTerm's proprietary
escape codes. As noted in
the linked page, it doesn't run on tmux or screen.
# Apply AdventureTime scheme to the current session
tools/preview.rb schemes/AdventureTime.itermcolors

# Apply the schemes in turn.
# - Press (almost) any key to advance; hit CTRL-C or ESC to stop
# - Press the delete key to go back
tools/preview.rb schemes/*

iTerm Color Schemes | iTerm2 Color Schemes | iTerm 2 Color Schemes | iTerm Themes | iTerm2 Themes | iTerm 2 Themes

","GitHub - mbadolato/iTerm2-Color-Schemes: Over 200 terminal color schemes/themes for iTerm/iTerm2. Includes ports to Terminal, Konsole, PuTTY, Xresources, XRDB, Remmina, Termite, XFCE, Tilda, FreeBSD VT, Terminator, Kitty, MobaXterm, LXTerminal, Microsoft's Windows Terminal, Visual Studio"
75,Shell,"fish - the friendly interactive shell 
fish is a smart and user-friendly command line shell for macOS, Linux, and the rest of the family.
fish includes features like syntax highlighting, autosuggest-as-you-type, and fancy tab completions
that just work, with no configuration required.
For more on fish's design philosophy, see the design document.
Quick Start
fish generally works like other shells, like bash or zsh. A few important differences can be found at https://fishshell.com/docs/current/tutorial.html by searching for the magic phrase ""unlike other shells"".
Detailed user documentation is available by running help within fish, and also at https://fishshell.com/docs/current/index.html
You can quickly play with fish right in your browser by clicking the button below:

Getting fish
macOS
fish can be installed:

using Homebrew: brew install fish
using MacPorts: sudo port install fish
using the installer from fishshell.com
as a standalone app from fishshell.com

Packages for Linux
Packages for Debian, Fedora, openSUSE, and Red Hat Enterprise Linux/CentOS are available from the
openSUSE Build
Service.
Packages for Ubuntu are available from the fish
PPA, and can be installed using the
following commands:
sudo apt-add-repository ppa:fish-shell/release-3
sudo apt-get update
sudo apt-get install fish

Instructions for other distributions may be found at fishshell.com.
Windows

On Windows 10, fish can be installed under the WSL Windows Subsystem for Linux with sudo apt install fish or from source with the instructions below.
Fish can also be installed on all versions of Windows using Cygwin (from the Shells category).

Building from source
If packages are not available for your platform, GPG-signed tarballs are available from
fishshell.com and fish-shell on
GitHub.  See the Building section for instructions.
Running fish
Once installed, run fish from your current shell to try fish out!
Dependencies
Running fish requires:

curses or ncurses (preinstalled on most *nix systems)
some common *nix system utilities (currently mktemp), in addition to the basic POSIX utilities (cat, cut, dirname, ls, mkdir, mkfifo, rm, sort, tee, tr, uname and sed at least, but the full coreutils plus find, sed and awk is preferred)
gettext (library and gettext command), if compiled with translation support

The following optional features also have specific requirements:

builtin commands that have the --help option or print usage messages require ul and either nroff or mandoc for display
automated completion generation from manual pages requires Python (2.7+ or 3.3+) and possibly the
backports.lzma module for Python 2.7
the fish_config web configuration tool requires Python (2.7+ or 3.3 +) and a web browser
system clipboard integration (with the default Ctrl-V and Ctrl-X bindings) require either the
xsel, xclip, wl-copy/wl-paste or pbcopy/pbpaste utilities
full completions for yarn and npm require the all-the-package-names NPM module

Switching to fish
If you wish to use fish as your default shell, use the following command:
chsh -s /usr/local/bin/fish

chsh will prompt you for your password and change your default shell. (Substitute /usr/local/bin/fish with whatever path fish was installed to, if it differs.) Log out, then log in again for the changes to take effect.
Use the following command if fish isn't already added to /etc/shells to permit fish to be your login shell:
echo /usr/local/bin/fish | sudo tee -a /etc/shells

To switch your default shell back, you can run chsh -s /bin/bash (substituting /bin/bash with /bin/tcsh or /bin/zsh as appropriate).
Building
Dependencies
Compiling fish requires:

a C++11 compiler (g++ 4.8 or later, or clang 3.3 or later)
CMake (version 3.2 or later)
a curses implementation such as ncurses (headers and libraries)
PCRE2 (headers and libraries) - a copy is included with fish
gettext (headers and libraries) - optional, for translation support

Sphinx is also optionally required to build the documentation from a cloned git repository.
Building from source (all platforms) - Makefile generator
To install into /usr/local, run:
mkdir build; cd build
cmake ..
make
sudo make install
The install directory can be changed using the -DCMAKE_INSTALL_PREFIX parameter for cmake.
Building from source (macOS) - Xcode
mkdir build; cd build
cmake .. -G Xcode
An Xcode project will now be available in the build subdirectory. You can open it with Xcode,
or run the following to build and install in /usr/local:
xcodebuild
xcodebuild -scheme install
The install directory can be changed using the -DCMAKE_INSTALL_PREFIX parameter for cmake.
Help, it didn't build!
If fish reports that it could not find curses, try installing a curses development package and build again.
On Debian or Ubuntu you want:
sudo apt-get install build-essential cmake ncurses-dev libncurses5-dev libpcre2-dev gettext

On RedHat, CentOS, or Amazon EC2:
sudo yum install ncurses-devel

Contributing Changes to the Code
See the Guide for Developers.
Contact Us
Questions, comments, rants and raves can be posted to the official fish mailing list at https://lists.sourceforge.net/lists/listinfo/fish-users or join us on our gitter.im channel. Or use the fish tag on Stackoverflow for questions related to fish script and the fish tag on Superuser for all other questions (e.g., customizing colors, changing key bindings).
Found a bug? Have an awesome idea? Please open an issue.
",GitHub - fish-shell/fish-shell: The user-friendly command line shell.
76,Shell,"
Table of Contents

Professional Programming

Contributing to this list
Must-read books
Must-read articles
Other general material and list of resources

Courses


Topics

Algorithm and data structures
API design
Attitude, habits, mindset
Automation
Biases
Career growth
Characters sets
Coding & code quality
Computer science
Configuration
Databases
Data formats
Data science
Debugging
Design (visual, UX, UI)
Design (OO modeling, architecture, patterns, anti-patterns, etc.)

Design: simplicity


Dev environment & tools
Diversity & inclusion
Documentation
Dotfiles
Editors & IDE
Engineering management
Exercises
Incident response (alerting, outages, firefighting)
Internet
Interviewing
Learning & memorizing
Low-level
Network
Problem solving
Project management
Programming languages

Python
JavaScript
FP vs. OOP


Over-engineering
Reading
Releasing & deploying
Security
Shell
System architecture

Scalability
Stability
Resiliency


Site Reliability Engineering (SRE)
Technical debt
Testing
Tools
Version control (Git)
Work ethics, productivity & work/life balance
Web development
Writing
Writing for performance


Concepts



Professional Programming

Give me six hours to chop down a tree and I will spend the first four sharpening the axe. (Abraham Lincoln)

A collection of full-stack resources for programmers.
The goal of this page is to make you a more proficient developer. You'll find only resources that I've found truly inspiring, or that have become timeless classics.
This page is not meant to be comprehensive. I am trying to keep it light and not too overwhelming. The selection of articles is opinionated.
Contributing to this list
Feel free to open a PR to contribute! I will not be adding everything: as stated above, I am trying to keep the list concise.
Must-read books
I've found these books incredibly inspiring:

The Pragmatic Programmer: From Journeyman to
Master üìñ: hands-on the most inspiring and useful book I've read about programming.
Code Complete: A Practical Handbook of Software
Construction üìñ: a nice addition to The Pragmatic Programmer, gives you the necessary framework to talk about code.
Release It! üìñ: this books goes beyond code and gives you best practices for building production-ready software. It will give you about 3 years worth of real-world experience.
Scalability Rules: 50 Principles for Scaling Web
Sites üìñ
The Linux Programming Interface: A Linux and UNIX System Programming Handbook üìñ: outside of teaching you almost everything you need to know about Linux, this book will give you insights into how software evolves, and the value of having simple & elegant interfaces.
Structure and interpretation of Computer Programs (free) üìñ: One of the most influential textbooks in Computer Science (written and used at MIT), SICP has been influential in CS education. Byte recommended SICP ""for professional programmers who are really interested in their profession"".

There are some free books available, including:

Professional software development üìñ: pretty complete and a good companion to this page. The free chapters are mostly focused on software development processes: design, testing, code writing, etc. - and not so much about tech itself.
List of free programming books

Must-read articles

Practical Advice for New Software Engineers
On Being A Senior Engineer
Lessons Learned in Software Development: one of those articles that give you years of hard-earned lessons, all in one short article. Must read.
Things I Learnt The Hard Way

Spec first, then code
Tests make better APIs
Future thinking is future trashing
Documentation is a love letter to your future self
Sometimes, it's better to let the application crash than do nothing
Understand and stay away of cargo cult
""Right tool for the job"" is just to push an agenda
Learn the basics functional programming
ALWAYS use timezones with your dates
ALWAYS use UTF-8
Create libraries
Learn to monitor
Explicit is better than implicit
Companies look for specialists but keep generalists longer
The best secure way to deal with user data is not to capture it
When it's time to stop, it's time to stop
You're responsible for the use of your code
Don't tell ""It's done"" when it's not
Pay attention on how people react to you
Beware of micro-aggressions
Keep a list of ""Things I Don't Know""


Signs that you're a good programmer
Signs that you're a bad programmer
7 absolute truths I unlearned as junior developer

Early in your career, you can learn 10x more in a supportive team in 1 year, than coding on your own
Every company has problems, every company has technical debt.
Being overly opinionated on topics you lack real-world experience with is pretty arrogant.
Many conference talks cover proof of concepts rather than real-world scenarios.
Dealing with legacy is completely normal.
Architecture is more important than nitpicking.
Focus on automation over documentation where appropriate.
Having some technical debt is healthy.
Senior engineers must develop many skills besides programming.
We‚Äôre all still junior in some areas.


How to Build Good Software

A good high-level summary of fundamental engineering practices.
The root cause of bad software has less to do with specific engineering choices, and more to do with how development projects are managed.
There is no such thing as platonically good engineering: it depends on your needs and the practical problems you encounter.
Software should be treated not as a static product, but as a living manifestation of the development team‚Äôs collective understanding.
Software projects rarely fail because they are too small; they fail because they get too big.
Beware of bureaucratic goals masquerading as problem statements. If our end goal is to make citizens‚Äô lives better, we need to explicitly acknowledge the things that are making their lives worse.
Building software is not about avoiding failure; it is about strategically failing as fast as possible to get the information you need to build something good.



Other general material and list of resources

The Imposter's Handbook - $30. From the author: ""Don't have a CS Degree? Neither do I - That's why I wrote this book.""
mr-mig/every-programmer-should-know: a collection of (mostly) technical things every software developer should know
Famous Laws Of Software Development

Courses

Google Tech Dev Guide

Topics
Algorithm and data structures

Read the CLRS. You can watch and download the course on OCW - there are newer courses as well.
Or The Algorithm Design Manual
Try out some algorithms on Project Euler

Let's be honest: algorithms can be a pretty dry topic. This quora question lists some funnier learning alternative, including:

Grokking Algorithms
Essential Algorithms

API design

Why you should use links, not keys, to represent relationships in APIs, Martin Nally, Google

""Using links instead of foreign keys to express relationships in APIs reduces the amount of information a client needs to know to use an API, and reduces the ways in which clients and servers are coupled to each other.""



Attitude, habits, mindset

Mastering Programming, Kent Beck.
The traits of a proficient programmer
The tao of programming: a set of parables about programming.
Taking Ownership Is The Most Effective Way to Get What You Want
Finding Time to Become a Better Developer
Ten minutes a day: how Alex Allain wrote a book in less than 200 hours, by writing 10 minutes every day.

Automation

Automation Should Be Like Iron Man, Not Ultron

Biases
Biases don't only apply to hiring. For instance, the fundamental attribution bias also applies when criticizing somebody's code written a long time ago, in a totally different context.

Cognitive bias cheat sheet. #hiring

Career growth

The Conjoined Triangles of Senior-Level Development looks into how to define a senior engineer.
Ten Principles for Growth as an Engineer, Dan Heller.
Don't Call Yourself a Programmer, Patrick McKenzie.
On being an Engineering Manager
The career advice I wish I had at 25

A career is a marathon, not a sprint
Most success comes from repetition, not new things
If work was really so great all the rich people would have the jobs
Management is about people, not things
Genuinely listen to others
Recognise that staff are people with finite emotional capacity
Don‚Äôt just network with people your own age
Never sacrifice personal ethics for a work reason
Recognise that failure is learning


Career advice I wish I‚Äôd been given when I was young

Don‚Äôt focus too much on long-term plans.
Find good thinkers and cold-call the ones you most admire.
Assign a high value to productivity over your whole lifespan.
Don‚Äôt over-optimise things that aren‚Äôt your top priority.
Read a lot, and read things that people around you aren‚Äôt reading.
Reflect seriously on what problem to prioritise solving.
Read more history.


Why Good Developers are Promoted into Unhappiness, Rob Walling. Or why management might not be for you.
A guide to using your career to help solve the world‚Äôs most pressing problems

Characters sets

The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)

Coding & code quality

Write code that is easy to delete, not easy to extend
The Ten Commandments of Egoless Programming
Clean Code: A Handbook of Agile Software Craftsmanship üìñ, Robert C. Martin. Describes numerous useful best practices. A bit long. There's also a clean code cheatsheet.
What Software Craftsmanship is about

We‚Äôre tired of writing crap.
We will not accept the stupid old lie about cleaning things up later.
We will not believe the claim that quick means dirty.
We will not allow anyone to force us to behave unprofessionally.



Computer science

What every computer science major should know
Teach Yourself Computer Science: an opinionated set of the best CS resources.

Configuration

The downsides of JSON for config files, Martin Tournoij.

Can't add comments
Excessive quotation and syntax noise
Using DC (declarative configuration) to control logic is often not a good idea.



Databases

A plain english introduction to CAP Theorem
NOSQL Patterns
NoSQL Databases: a Survey and Decision Guidance
Safe Operations For High Volume PostgreSQL (this is for PostgreSQL but works great for other DBs as well).
Zero downtime database migrations (code examples are using Rails but this works great for any programming language)
SQL styleguide
Algorithms Behind Modern Storage Systems, ACM Queue
Let's Build a Simple Database

Data formats

Falsehoods Programmers Believe About Phone Numbers, Google's libphonenumber.
Rules for Autocomplete: rough specifications for autocomplete fields

Data science

A dirty dozen: twelve common metric interpretation pitfalls in online controlled experiments

Debugging

Rubber Duck Problem Solving
Rubber Ducking
Five Whys
The Infinite Hows: this provides a strong criticism of the five whys method.
Linux Performance Analysis in 60,000 Milliseconds

Design (visual, UX, UI)
I highly recommend reading The Non-Designer's Design Book. This is a pretty short book that will give you some very actionable design advices.

If you're working on data, Edward Tufte's The Visual Display of Quantitative Information is considered a classic.
The Universal Principles of Design will give you enough vocabulary and concepts to describe design challenges into words.
Microsoft's Rest API guidelines
Book recommendations from HackerNews

Articles :

10 Usability Heuristics Every Designer Should Know

Visibility of System Status
The Match Between The System And The Real World
Every system should have a clear emergency exit
Don't forget that people spend 90% of their time interacting with other apps
Recognition Rather Than Recall (recognition = shallow form of retrieval from memory, e.g. a familiar person, recall = deeper retrieval)
‚ÄùPerfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.‚Äù ‚Äì Antoine de Saint-Exupery
Help Users Recognize, Diagnose, And Recover From Errors



Design (OO modeling, architecture, patterns, anti-patterns, etc.)
Here's a list of good books:

Design Patterns: Elements of Reusable Object-Oriented Software: dubbed ""the gang of four"", this is almost a required reading for any developer. A lot of those are a bit overkill for Python (because everything is an object, and dynamic typing), but the main idea (composition is better than inheritance) definitely is a good philosophy.
Patterns of Enterprise Application Architecture: learn about how database are used in real world applications. Mike Bayer's SQLAlchemy has been heavily influenced by this book.
Domain-Driven Design: Tackling Complexity in the Heart of Software üìñ, Eric Evans
Clean Architecture üìñ, Robert C. Martin. Uncle Bob proposes an architecture that leverages the Single Responsibility Principle to its fullest. A great way to start a new codebase. Also checkout the clean architecture cheatsheet.

One of the absolute references on architecture is Martin Fowler: checkout his Software Architecture Guide.
Articles:

101 Design Patterns & Tips for Developers
Python Design Patterns: For Sleek And Fashionable Code: a pretty simple introduction to common design patterns (Facade, Adapter, Decorator). A more complete list of design patterns implementation in Python on Github. Also a book here.
SourceMaking's Design Patterns seems to be a good web resource too.
O'Reilly's How to make mistakes in Python
Education of a Programmer: a developer's thoughts after 35 years in the industry. There's a particularly good section about design & complexity (see ""the end to end argument"", ""layering and componentization"").
Google's API Design Guide: a general guide to design networked API.
Domain-driven design, Wikipedia.
On the Spectrum of Abstraction üéû, Cheng Lou
The ‚ÄúBug-O‚Äù Notation, Dan Abramov

I maintain a list of antipatterns on another repo. This is a highly recommended read.

Inheritance vs. composition: a concrete example in Python. Another slightly longer one here. One last one, in Python 3.
Composition Instead Of Inheritance
Complexity and Strategy: interesting perspective on complexity and flexibility with really good examples (e.g. Google Apps Suite vs. Microsoft Office).


You can use an eraser on the drafting table or a sledge hammer on the construction site. (Frank Lloyd Wright)

Design: simplicity

Simple Made Easy üéû, Rich Hickey. This is an incredibly inspiring talk redefining simplicity, ease and complexity, and showing that solutions that look easy may actually harm your design.

Dev environment & tools

Awesome Dev Env

Tools

Glances: An eye on your system
HTTPie: a CLI, cURL-like tool for humans
jq: command-line JSON processor
tmux: terminal multiplexer
htop: an interactive process viewer for Linux

Diversity & inclusion
Check out my list of management
resources.
Documentation

Documentation-Driven Development
Writing automated tests for your documentation: this should be required, IMO. Testing code samples in your documentation ensures they never get outdated.
Documentation is king, Kenneth Reitz
Keep a Changelog

Dotfiles

Awesome dotfiles
My dotfiles

Articles

Setting Up a Mac Dev Machine From Zero to Hero With Dotfiles

Editors & IDE

Sublime Text essential plugins and resources
vim-awesome
Bram Moolenaar (Vim author), Seven habits of effective text editing (presentation). This is about Vim but it contains good lessons about why investing time in learning how to be productive with your text editors pays off.
VScode is one of the most popular text editors as of writing. Visual Studio Code Can Do That?, Smashing Magazine.

Engineering management
Checkout my list of management
resources.
Exercises
The best way to learn is to learn by doing.

danistefanovic/build-your-own-x: build your own (insert technology here)
The elevator programming game

Incident response (alerting, outages, firefighting)

Incident Response at Heroku
Blameless PostMortems and a Just Culture
My Philosophy on Alerting
A great example of a postmortem from Gitlab (01/31/2017) for an outage during which an engineer's action caused the irremediable loss of 6 hours of data.

Internet

How Does the Internet Work?
How the web works

Interviewing
Note: this is about you as an interviewee, not as an interviewer. To check out my list of resources for interviewers, go to my engineering-management repository.

System design interview for IT company
Technical Interview Megarepo: study materials for SE/CS technical interviews
How to Win the Coding Interview
I spent 3 months applying to jobs after a coding bootcamp. Here‚Äôs what I learned.
Top 10 algorithms in Interview Questions
Questions to ask your interviewer
Interactive Python coding interview challenges
Tech Interview Handbook

See also the exercises section in this document.
Learning & memorizing
Learn how to learn!

How I Rewired My Brain to Become Fluent in Math: subtitled the building blocks of understanding are memorization and repetition.
One Sure-Fire Way to Improve Your Coding: reading code!
Tips for learning programming
You can increase your intelligence: 5 ways to maximize your cognitive potential: forgive the clickbait title, it‚Äôs actually a good article.
How to ask good questions, Julia Evans.
Stop Learning Frameworks
Learning How to Learn: powerful mental tools to help you master tough subjects
Why books don‚Äôt work, Andy Matuschak.

""As a medium, books are surprisingly bad at conveying knowledge, and readers mostly don‚Äôt realize it.""
""In learning sciences, we call this model ‚Äútransmissionism.‚Äù It‚Äôs the notion that knowledge can be directly transmitted from teacher to student, like transcribing text from one page onto another. If only!""
""By re-testing yourself on material you‚Äôve learned over expanding intervals, you can cheaply and reliably commit huge volumes of information to long-term memory.""


Strategies, Tips, and Tricks for Anki: those advices work for any tool actually

Add images. Our brains are wired visually, so this helps retention.
Don't add things you don't understand.
Don't add cards memorizing entire lists.
Write it out. For wrong answers, I'll write it on paper. The act of writing is meditative. I really enjoy this.
Keep on asking yourself why? why does this work? why does it work this way? Force yourself to understand the root of a topic.
Cornell Method: when reading a topic, write out questions on the margins to quiz yourself.
Pretend you have to teach it
Use mnemonics phrases like PEMDAS for lists and other hard-to-remember topics.
Delete cards that don't make sense or you don't want to remember anymore.


Effective learning: Twenty rules of formulating knowledge

Build upon the basics
Stick to the minimum information principle: the material you learn must be formulated in as simple way as it is
Cloze deletion is easy and effective: Kaleida's mission was to create a ... It finally produced one, called Script X. But it took three years
Graphic deletion is as good as cloze deletion
Avoid sets
Avoid enumerations
Combat interference - even the simplest items can be completely intractable if they are similar to other items. Use examples, context cues, vivid illustrations, refer to emotions, and to your personal life
Personalize and provide examples - personalization might be the most effective way of building upon other memories. Your personal life is a gold mine of facts and events to refer to. As long as you build a collection for yourself, use personalization richly to build upon well established memories
Provide sources - sources help you manage the learning process, updating your knowledge, judging its reliability, or importance
Prioritize - effective learning is all about prioritizing.


How to Remember Anything You Really Want to Remember, Backed by Science

Quiz yourself
Summarize and share with someone else.
Connect what you just learned to experiences you previously had.



Richard Feynman's Learning Strategy:

Step 1: Continually ask ""Why?‚Äù
Step 2: When you learn something, learn it to where you can explain it to a child.
Step 3: Instead of arbitrarily memorizing things, look for the explanation that makes it obvious.


Most people overestimate what they can do in 1 year and underestimate what they can do in a decade.
‚Äì Bill Gates


Frankly, though, I think most people can learn a lot more than they think they can. They sell themselves short without trying.
One bit of advice: it is important to view knowledge as sort of a semantic tree‚Ää‚Äî‚Äämake sure you understand the fundamental principles, ie the trunk and big branches, before you get into the details/leaves or there is nothing for them to hang on to.
‚Äî Elon Musk


""Experience is something you don't get until just after you need it.""
‚Äï Steven Wright

Low-level

Back to Basics, Joel Spolsky. Explains why learning low level programming is important.

I think that some of the biggest mistakes people make even at the highest architectural levels come from having a weak or broken understanding of a few simple things at the very lowest levels.



Network

The Great Confusion About URIs

A URI is a string of characters that identifies a resource. Its syntax is <scheme>:<authority><path>?<query>#<fragment>, where only <scheme> and <path> are mandatory. URL and URN are URIs.
A URL is a string of characters that identifies a resource located on a computer network. Its syntax depends on its scheme. E.g. mailto:billg@microsoft.com.
A URN is a string of characters that uniquely identifies a resource. Its syntax is urn:<namespace identifier>:<namespace specific string>. E.g. urn:isbn:9780062301239



Problem solving

Dealing with Hard Problems

Project management
See Project management section on my engineering-management list of resources.
Programming languages
I would recommend learning:

JavaScript and maybe another interpreted language (Python, Ruby, etc.). Interpreted languages are useful for quick one-off automation scripts, and fastest to write for interviews. JavaScript is ubiquitous.
A compiled language (Java, C, C++...).
A more recent language to see where the industry is going (as of writing, Go, Swift, Rust, Elixir...).
A language that has first-class support for functional programming (Haskell, Scala, Clojure...).

A bit more reading:

A brief, incomplete, mostly wrong history of programming languages
Types
Resources To Help You To Create Programming Languages
Effective Programs - 10 Years of Clojure üéû, Rich Hickey. The author of Clojure reflects on his programming experience and explains the rationale behind some of Clojure's key design decisions.
Learn more programming languages, even if you won't use them, Thorsten Ball

These new perspectives, these ideas and patterns ‚Äî they linger, they stay with you, even if you end up in another language. And that is powerful enough to keep on learning new languages, because one of the best things that can happen to you when you‚Äôre trying to solve a problem is a change of perspective.




There are only two kinds of languages: the ones people complain about and the ones nobody uses.

-- Bjarne Stroustrup (C++ creator)
Python
For Python feel free to checkout my professional Python education repository.
JavaScript
JavaScript is such a pervasive language that it's almost required learning.

mbeaudru/modern-js-cheatsheet: cheatsheet for the JavaScript knowledge you will frequently encounter in modern projects.

FP vs. OOP

Jargon from the functional programming world
Goodbye, Object Oriented Programming
Functional Programming & Haskell: some good reasons to learn FP!
Functional Programming Fundamentals: short introduction to FP and its advantages.
OO vs FP, Robert C. Martin, The Clean Code Blog. A pretty interesting take on the differences between OOP and FP from an expert in OOP.

OO is not about state. Objects are bags of functions, not bags of data.
Functional Programs, like OO Programs, are composed of functions that operate on data.
FP imposes discipline upon assignment.
OO imposes discipline on function pointers.
The principles of software design still apply, regardless of your programming style. The fact that you‚Äôve decided to use a language that doesn‚Äôt have an assignment operator does not mean that you can ignore the Single Responsibility Principle.



Over-engineering

10 modern software over-engineering mistakes
A good example of over-engineering: the Juicero press (April 2017)


‚ÄúA complex system that works is invariably found to have evolved from a simple system that worked. A complex system designed from scratch never works and cannot be patched up to make it work. You have to start over, beginning with a working simple system.‚Äù

John Gall, General systemantics, an essay on how systems work, and especially how they fail..., 1975 (this quote is sometime referred as ""Galls' law"")

""Software engineering is what happens to programming
when you add time and other programmers.""

Rob Pike, Go at Google: Language Design in the Service of Software Engineering
Reading

Papers we love: papers from the computer science community to read and discuss. Can be a good source of inspiration of solving your design problems.
The morning paper: one CS research paper explained every morning.

Releasing & deploying

How We Release So Frequently
How to deploy software, Zach Holman
BlueGreenDeployment, Martin Fowler
Move fast and break nothing, Zach Holman
Flipping out, flickr. One of the first articles about feature flags.
Production Readiness Checklist, Gruntwork
Checklist: what had to be done before deploying microservices to production

Security

Penetration Testing Tools Cheat Sheet
My First 10 Minutes On a Server - Primer for Securing Ubuntu
A practical guide to securing macOS
Web Developer Security Checklist
Reckon you've seen some stupid security things?: everything not to do.

Shell

Awesome Shell
Bash Hackers Wiki
dylanaraps/pure-bash-bible: a collection of pure bash alternatives to external processes.
Master the command line, in one page must read

System architecture

High Scalability: great blog about system architecture, its weekly review article are packed with numerous insights and interesting technology reviews. Checkout the all-times favorites.
6 Rules of thumb to build blazing fast web server applications
Deep Lessons From Google And EBay On Building Ecosystems Of Microservices
Service oriented architecture: scaling the Uber engineering codebase as we grow
The twelve-factor app
Scalable Web Architecture and Distributed Systems
Introduction to Architecting Systems for Scale
A Distributed Systems Reading List
Services Engineering Reading List
System Design Cheatsheet
The Log: What every software engineer should know about real-time data's unifying abstraction: one of those classical articles that everyone should read.
Learn how to design large scale systems. Prep for the system design interview (Github repo)
Turning the database outside-out with Apache Samza
Building Microservices üìñ, Sam Newman (quite complete discussion of microservices)
Designing Data-Intensive Applications

Scalability

I already mentioned the book Scalability rules above, but there's also a presentation about it.

Stability

I already mentioned the book Release it! above. There's also a presentation from the author.

Resiliency

The Walking Dead - A Survival Guide to Resilient Applications
Defensive Programming & Resilient systems in Real World (TM)
Full Stack Fest: Architectural Patterns of Resilient Distributed Systems
Resilience Engineering Notes: comprehensive list of resources on resilience engineering

Site Reliability Engineering (SRE)
Books:

Site Reliability Engineering üìñ

Written by members of Google's SRE team, with a comprehensive analysis of the entire software lifecycle - how to build, deploy, monitor, and maintain large scale systems.



Articles:

Graduating from Bootcamp and interested in becoming a Site Reliability Engineer?: a great collection of resources to learn about SRE.
Operating a Large, Distributed System in a Reliable Way: Practices I Learned

A good summary of processes to implement.



Technical debt

TechnicalDebt, Martin Fowler.

Testing

Why bother writing tests at all?, Dave Cheney. A good intro to the topic.

Even if you don‚Äôt, someone will test your software
The majority of testing should be performed by development teams
Manual testing should not be the majority of your testing because manual testing is O(n)
Tests are the critical component that ensure you can always ship your master branch
Tests lock in behaviour
Tests give you confidence to change someone else‚Äôs code


Testing Strategies in a Microservices Architecture (Martin Fowler) is an awesome resources explaining how to test a service properly.
A Quick Puzzle to Test Your Problem Solving... and a great way to learn about confirmation bias and why you're mostly writing positive test cases.
The test pyramid
Just Say No to More End-to-End Tests
End-To-End Testing Considered Harmful
Move fast and don't break things (presentation)
Eradicating Non-Determinism in Tests, Martin Fowler
""I get paid for code that works, not for tests""
Software Testing Anti-patterns, Kostis Kapelonis.
Write tests. Not too many. Mostly integration. for a contrarian take about unit testing...
Testing is Not for Beginners: why learning to test is hard. This shouldn't demotivate you though!

Tools

DevDocs API Documentation: a repository for multiple API docs (see also Dash for macOS).
DevChecklist: a collaborative space for sharing checklists that help ensure software quality

Version control (Git)

Git Cheat Sheet
git-tips
Git from the inside out

Work ethics, productivity & work/life balance

Your non-linear problem of 90% utilization, Jason Cohen: why constantly running at 90% utilization is actually counter-productive.
Evidence-based advice on how to be successful in any jobs: most self-help advices are not research-based. The ones listed in this article are.
The Complete Guide to Deep Work

The ability to perform deep work is becoming increasingly rare at exactly the same time it is becoming increasingly valuable in our economy.
Choose Your Deep Work Strategy
Build a Deep Work Routine
Discipline #1: Focus on the Wildly Important
Discipline #2: Act on the Lead Measures
Discipline #4: Create a Cadence of Accountability
Our Ability for Deep Work is Finite
The Craftsman Approach to Tool Selection
Stop Using Social Media
Get Your Boss on Board With Deep Work


Every productivity thought I've ever had, as concisely as possible

Context intentionality as the key difference between home and every other place on planet earth
Rules are about exceptions



Web development

grab/front-end-guide: a study guide and introduction to the modern front end stack.
Maintainable CSS
Front-End Developer Handbook 2019, Cody Lindley
A Directory of design and front-end resources

Writing

Undervalued Software Engineering Skills: Writing Well

From the HN discussion: ""Writing a couple of pages of design docs or an Amazon-style 6 pager or whatever might take a few days of work, but can save weeks or more of wasted implementation time when you realise your system design was flawed or it doesn't address any real user needs.""



Writing for performance

Numbers Everyone Should Know
Latency numbers every programmer should know
Rob Pike's 5 Rules of Programming

Concepts
Glossary

DDD
TDD
BDD
CAP theorem
OOP
YAGNI
DRY
KISS
SOLID
GRASP
Make it run, make it right, make it fast

",GitHub - charlax/professional-programming: A collection of full-stack resources for programmers.
77,Shell,"Git Extras

Little git extras.
Screencasts
Just getting started? Check out these screencasts:

introduction -- covering git-ignore, git-setup, git-changelog, git-release, git-effort and more

Installation
See Installation page.
Commands
Go to Commands page for basic usage and examples.
GIT utilities -- repo summary, repl, changelog population, author commit percentages and more
Contributing
Interested in contributing? Awesome!
Please read Contributing before you make a PR, thanks!
","GitHub - tj/git-extras: GIT utilities -- repo summary, repl, changelog population, author commit percentages and more"
78,Shell,"Hack






Don't like the development noise from the repository but want to keep up with changes? Check out our gitter Hack channel.  Have a quick question that doesn't require an issue report?  Drop by our gitter Help channel and ask away.
A typeface designed for source code
Hack is designed to be a workhorse typeface for source code. It has deep roots in the free, open source typeface community and expands upon the contributions of the Bitstream Vera & DejaVu projects.  The large x-height + wide aperture + low contrast design make it legible at commonly used source code text sizes with a sweet spot that runs in the 8 - 14 range.  The full set of changes to the upstream source are available in the changelog.
The project is in active development, and we welcome your input and contributions.  You may view our design objectives and instructions on how to contribute in CONTRIBUTING.md.
Frequently asked questions are answered in our FAQ.
Contents

Features
Quick installation
Package manager installation
Web font usage
Additional tools for font customization
Resources
Contributing
Build tools
Acknowledgments
License

Specimen

Overview of features

Typeface Name: Hack
Category: Monospaced
Character set support: ASCII, Latin-1, Latin Extended A, Greek, Cyrillic
Powerline Support: Yes, included by default
Included Styles: Regular, Bold, Italic, Bold Italic

Quick installation
NOTE ON FONT UPDATES
If you are updating your version of Hack, be sure to remove the previously installed version and clear your font cache first to avoid conflicts that can lead to platform-specific rendering errors.  Many platforms/distros offer package managers that automate this process. We release a Windows installer to automate the install/update process on the Windows platform. See below for additional details.
Linux

Download the latest version of Hack.
Extract the files from the archive (.zip).
Copy the font files to either your system font folder (often /usr/share/fonts/) or user font folder (often ~/.local/share/fonts/ or /usr/local/share/fonts).
Copy the font configuration file in config/fontconfig/ to either the system font configuration folder (often /etc/fonts/conf.d/) or the font user folder (often ~/.config/fontconfig/conf.d)
Clear and regenerate your font cache and indexes with the following command:

$ fc-cache -f -v

You can confirm that the fonts are installed with the following command:
$ fc-list | grep ""Hack""

Some Linux users may find that font rendering is improved on their distro with these instructions.
macOS

Download the latest version of Hack.
Extract the files from the archive (.zip) and click to open them.
Follow the instructions from your operating system.
Enjoy!

Windows

The Hack Windows Installer simplifies installation on the Windows platform.  The tool guides you through the installation process and addresses some common rendering issues that occur with font installs/updates on the Windows platform.  This tool is the recommended installation approach for Windows users.
Chrome/ChromeOS
To use with Secure Shell,
edit the following fields in Options:

font-family: ""Hack""
user-css: https://cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css

Package managers
We highly recommend the convenience of a community developed package manager or other auto-updating utility if this is available on your platform. While the package manager releases may be a bit delayed relative to the repository releases, the packages distributed through these package managers were designed to tune and automate font installs and updates on your system.
We are aware of Hack support in the following package managers (with associated package names):

Arch Linux: ttf-hack
Chocolatey (Windows): hackfont
Debian: fonts-hack-ttf
Fedora / CentOS: dnf-plugins-core :: heliocastro/hack-fonts :: hack-fonts
Gentoo Linux: media-fonts/hack
Homebrew Cask (OS X): caskroom/fonts/font-hack
Open BSD: fonts/hack-fonts
OpenSUSE: hack-fonts
Ubuntu: fonts-hack-ttf
Visual Studio Package Manager: hack.font

For installation issues with Hack packages, please contact the package maintainer directly.
Web font usage
Hack is available in the woff and woff2 web font formats. Bold and italic styles are included by default and work out-of-the-box via the <strong> and <em> tags.
Hack is available through the jsDelivr and cdnjs CDN services:


The following snippets provide examples of Hack web font use through the jsDelivr CDN.  Adjust the URL paths to those provided by cdnjs (click the link above to find the appropriate URL) to switch to the cdnjs CDN.
1. Add Hack to HTML
Include one of the following in the <head> section of your HTML file:
Subset web fonts
<link rel=""stylesheet"" href=""//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack-subset.css"">
Full character set web fonts
<link rel=""stylesheet"" href=""//cdn.jsdelivr.net/npm/hack-font@3/build/web/hack.css"">
2. Add Hack to CSS
pre, code { font-family: Hack, monospace; }
See the WEBFONT_USAGE.md documentation for additional details, including instructions on how to download, host, and serve the web fonts on your web server.
Additional tools for Hack font customization
Customize your build with alternate glyph styles
The alt-hack library includes a (growing) collection of alternate glyph styles that can be used to customize your Hack fonts.  Don't like the default zero style?  Swap out the UFO design source with a slashed zero or dotted zero and build new fonts that work better for you.
We welcome contributions of new alternate glyph styles in the alt-hack repository.  Design something new that works well with rest of the typeface and submit a pull request to the alt-hack repository so that others can use it in their custom builds.
Detailed build instructions are available on the alt-hack repository README.md.  Font renaming instructions to support side-by-side installs with upstream Hack are available below.
Line spacing adjustments
font-line is a tool that modifies the default line spacing used in the Hack design (20% UPM).
The following gist installs font-line and modifies line spacing for all desktop font files contained in the same directory:

linespace.sh (download .zip)

Install modified and unmodified versions of Hack on the same system
If you modify the upstream Hack source or the released font binaries and would like to install your modified fonts on the same system with the Hack fonts as released here, you can use the fontname.py Python script to define a new font name in the binary files.  For example, you can install Hack on your platform along with a 15% UPM line spacing adjusted version as Hack Fifteen.  Modify default glyphs with those in our alt-hack repository or design your own and define your creation with any name that you'd like.  Following installation, your renamed fonts should show under the name that you define with the script so that you can switch between any of them as you need.
Usage details and examples are provided on the fontname.py repository README.
Resources

Full specimen
Changelog
Project website
Contributors

Contributing
We welcome contributions to Hack!  Please review our CONTRIBUTING.md document for details.
Built With
Hack is built with the following free, open source projects:

Font Bakery - post-compilation modifications
fontmake - UFO to ttf compilation
fontTools - OpenType table read/write
font-v - font version string editor
ink - stream editor for text file templating
OpenFV - open specification for semantic typeface versioning
sfnt2woff_zopfli - ttf to woff web font compilation
ttfautohint - ttf instruction sets
uni - Unicode code point search
ufodiff - UFO source diffs
ufoLib - UFO source file reads/writes/testing
ufolint - UFO source file linting for CI testing
woff2 - ttf to woff2 web font compilation

Acknowledgments
We would like to acknowledge and thank the jsDelivr and cdnjs teams for their support of the Hack project through their free web font CDN services.  We greatly appreciate the tremendous support of open source software development by the Semaphore CI team.  Their free CI testing service and rapid, excellent technical support have been tremendous assets for our project. Lastly, a huge thanks go out to all of those who do the unrecognized work to get Hack out there to users so that it is easy to access, install, upgrade, and use.  There are redistribution package managers, review committee members, testers, and others across platforms/distros/applications who perform this thankless work and often go unrecognized.  Your efforts are much appreciated.
License
Hack work is ¬© 2018 Source Foundry Authors. MIT License
Bitstream Vera Sans Mono ¬© 2003 Bitstream, Inc. (with Reserved Font Names Bitstream and Vera). Bitstream Vera License.
The font binaries are released under a license that permits unlimited print, desktop, web, and software embedding use for commercial and non-commercial applications.
See LICENSE.md for the full texts of the licenses.
",GitHub - source-foundry/Hack: A typeface designed for source code
79,Shell,".NET Core Home
The dotnet/core repository is a good starting point for .NET Core.
The latest major release is .NET Core 3.1. The latest patch updates are listed in .NET Core release notes.
Download the latest .NET Core SDK

.NET Core 3.1 SDK

.NET Core Releases and Daily Builds

.NET Core released builds
.NET Core daily builds

Learn about .NET Core

Learn about .NET Core
.NET Core Roadmap
Learn about the .NET platform
.NET Core release notes
.NET Core Announcements
.NET Core blog

Getting help

File a .NET Core issue
File an ASP.NET Core issue
File an issue for other components
Ask on Stack Overflow
Contact Microsoft Support
VS Developer Community Portal for ""full"" .NET Framework feedback (or via Report a Problem tool)

How to Engage, Contribute and Provide Feedback
The .NET Core team encourages contributions, both issues and PRs. The first step is finding the .NET Core repository that you want to contribute to.
Community
This project uses the .NET Foundation Code of Conduct to define expected conduct in our community.
Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting a project maintainer at conduct@dotnetfoundation.org.
.NET Foundation
The .NET Core platform is part of the .NET Foundation.
Licenses
.NET Core repos typically use either the MIT or
Apache 2 licenses for code.
Some projects license documentation and other forms of content under
Creative Commons Attribution 4.0.
See specific repos to understand the license used.
",GitHub - dotnet/core: Home repository for .NET Core
