,language,readme,title
0,Python,"English ‚àô Êó•Êú¨Ë™û ‚àô ÁÆÄ‰Ωì‰∏≠Êñá ‚àô ÁπÅÈ´î‰∏≠Êñá | ÿßŸÑÿπŸéÿ±Ÿéÿ®ŸêŸäŸéŸëÿ©‚Äé ‚àô ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‚àô Portugu√™s do Brasil ‚àô Deutsch ‚àô ŒµŒªŒªŒ∑ŒΩŒπŒ∫Œ¨ ‚àô ◊¢◊ë◊®◊ô◊™ ‚àô Italiano ‚àô ÈüìÂúãË™û ‚àô ŸÅÿßÿ±ÿ≥€å ‚àô Polski ‚àô —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ ‚àô Espa√±ol ‚àô ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‚àô T√ºrk√ße ‚àô ti·∫øng Vi·ªát ‚àô Fran√ßais | Add Translation
The System Design Primer




Motivation

Learn how to design large-scale systems.
Prep for the system design interview.

Learn how to design large-scale systems
Learning how to design scalable systems will help you become a better engineer.
System design is a broad topic.  There is a vast amount of resources scattered throughout the web on system design principles.
This repo is an organized collection of resources to help you learn how to build systems at scale.
Learn from the open source community
This is a continually updated, open source project.
Contributions are welcome!
Prep for the system design interview
In addition to coding interviews, system design is a required component of the technical interview process at many tech companies.
Practice common system design interview questions and compare your results with sample solutions: discussions, code, and diagrams.
Additional topics for interview prep:

Study guide
How to approach a system design interview question
System design interview questions, with solutions
Object-oriented design interview questions, with solutions
Additional system design interview questions

Anki flashcards




The provided Anki flashcard decks use spaced repetition to help you retain key system design concepts.

System design deck
System design exercises deck
Object oriented design exercises deck

Great for use while on-the-go.
Coding Resource: Interactive Coding Challenges
Looking for resources to help you prep for the Coding Interview?




Check out the sister repo Interactive Coding Challenges, which contains an additional Anki deck:

Coding deck

Contributing

Learn from the community.

Feel free to submit pull requests to help:

Fix errors
Improve sections
Add new sections
Translate

Content that needs some polishing is placed under development.
Review the Contributing Guidelines.
Index of system design topics

Summaries of various system design topics, including pros and cons.  Everything is a trade-off.
Each section contains links to more in-depth resources.






System design topics: start here

Step 1: Review the scalability video lecture
Step 2: Review the scalability article
Next steps


Performance vs scalability
Latency vs throughput
Availability vs consistency

CAP theorem

CP - consistency and partition tolerance
AP - availability and partition tolerance




Consistency patterns

Weak consistency
Eventual consistency
Strong consistency


Availability patterns

Fail-over
Replication
Availability in numbers


Domain name system
Content delivery network

Push CDNs
Pull CDNs


Load balancer

Active-passive
Active-active
Layer 4 load balancing
Layer 7 load balancing
Horizontal scaling


Reverse proxy (web server)

Load balancer vs reverse proxy


Application layer

Microservices
Service discovery


Database

Relational database management system (RDBMS)

Master-slave replication
Master-master replication
Federation
Sharding
Denormalization
SQL tuning


NoSQL

Key-value store
Document store
Wide column store
Graph Database


SQL or NoSQL


Cache

Client caching
CDN caching
Web server caching
Database caching
Application caching
Caching at the database query level
Caching at the object level
When to update the cache

Cache-aside
Write-through
Write-behind (write-back)
Refresh-ahead




Asynchronism

Message queues
Task queues
Back pressure


Communication

Transmission control protocol (TCP)
User datagram protocol (UDP)
Remote procedure call (RPC)
Representational state transfer (REST)


Security
Appendix

Powers of two table
Latency numbers every programmer should know
Additional system design interview questions
Real world architectures
Company architectures
Company engineering blogs


Under development
Credits
Contact info
License

Study guide

Suggested topics to review based on your interview timeline (short, medium, long).


Q: For interviews, do I need to know everything here?
A: No, you don't need to know everything here to prepare for the interview.
What you are asked in an interview depends on variables such as:

How much experience you have
What your technical background is
What positions you are interviewing for
Which companies you are interviewing with
Luck

More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.
Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.

Short timeline - Aim for breadth with system design topics.  Practice by solving some interview questions.
Medium timeline - Aim for breadth and some depth with system design topics.  Practice by solving many interview questions.
Long timeline - Aim for breadth and more depth with system design topics.  Practice by solving most interview questions.





Short
Medium
Long




Read through the System design topics to get a broad understanding of how systems work
üëç
üëç
üëç


Read through a few articles in the Company engineering blogs for the companies you are interviewing with
üëç
üëç
üëç


Read through a few Real world architectures
üëç
üëç
üëç


Review How to approach a system design interview question
üëç
üëç
üëç


Work through System design interview questions with solutions
Some
Many
Most


Work through Object-oriented design interview questions with solutions
Some
Many
Most


Review Additional system design interview questions
Some
Many
Most



How to approach a system design interview question

How to tackle a system design interview question.

The system design interview is an open-ended conversation.  You are expected to lead it.
You can use the following steps to guide the discussion.  To help solidify this process, work through the System design interview questions with solutions section using the following steps.
Step 1: Outline use cases, constraints, and assumptions
Gather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.

Who is going to use it?
How are they going to use it?
How many users are there?
What does the system do?
What are the inputs and outputs of the system?
How much data do we expect to handle?
How many requests per second do we expect?
What is the expected read to write ratio?

Step 2: Create a high level design
Outline a high level design with all important components.

Sketch the main components and connections
Justify your ideas

Step 3: Design core components
Dive into details for each core component.  For example, if you were asked to design a url shortening service, discuss:

Generating and storing a hash of the full url

MD5 and Base62
Hash collisions
SQL or NoSQL
Database schema


Translating a hashed url to the full url

Database lookup


API and object-oriented design

Step 4: Scale the design
Identify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?

Load balancer
Horizontal scaling
Caching
Database sharding

Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using principles of scalable system design.
Back-of-the-envelope calculations
You might be asked to do some estimates by hand.  Refer to the Appendix for the following resources:

Use back of the envelope calculations
Powers of two table
Latency numbers every programmer should know

Source(s) and further reading
Check out the following links to get a better idea of what to expect:

How to ace a systems design interview
The system design interview
Intro to Architecture and Systems Design Interviews

System design interview questions with solutions

Common system design interview questions with sample discussions, code, and diagrams.
Solutions linked to content in the solutions/ folder.




Question





Design Pastebin.com (or Bit.ly)
Solution


Design the Twitter timeline and search (or Facebook feed and search)
Solution


Design a web crawler
Solution


Design Mint.com
Solution


Design the data structures for a social network
Solution


Design a key-value store for a search engine
Solution


Design Amazon's sales ranking by category feature
Solution


Design a system that scales to millions of users on AWS
Solution


Add a system design question
Contribute



Design Pastebin.com (or Bit.ly)
View exercise and solution

Design the Twitter timeline and search (or Facebook feed and search)
View exercise and solution

Design a web crawler
View exercise and solution

Design Mint.com
View exercise and solution

Design the data structures for a social network
View exercise and solution

Design a key-value store for a search engine
View exercise and solution

Design Amazon's sales ranking by category feature
View exercise and solution

Design a system that scales to millions of users on AWS
View exercise and solution

Object-oriented design interview questions with solutions

Common object-oriented design interview questions with sample discussions, code, and diagrams.
Solutions linked to content in the solutions/ folder.


Note: This section is under development




Question





Design a hash map
Solution


Design a least recently used cache
Solution


Design a call center
Solution


Design a deck of cards
Solution


Design a parking lot
Solution


Design a chat server
Solution


Design a circular array
Contribute


Add an object-oriented design question
Contribute



System design topics: start here
New to system design?
First, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.
Step 1: Review the scalability video lecture
Scalability Lecture at Harvard

Topics covered:

Vertical scaling
Horizontal scaling
Caching
Load balancing
Database replication
Database partitioning



Step 2: Review the scalability article
Scalability

Topics covered:

Clones
Databases
Caches
Asynchronism



Next steps
Next, we'll look at high-level trade-offs:

Performance vs scalability
Latency vs throughput
Availability vs consistency

Keep in mind that everything is a trade-off.
Then we'll dive into more specific topics such as DNS, CDNs, and load balancers.
Performance vs scalability
A service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.1
Another way to look at performance vs scalability:

If you have a performance problem, your system is slow for a single user.
If you have a scalability problem, your system is fast for a single user but slow under heavy load.

Source(s) and further reading

A word on scalability
Scalability, availability, stability, patterns

Latency vs throughput
Latency is the time to perform some action or to produce some result.
Throughput is the number of such actions or results per unit of time.
Generally, you should aim for maximal throughput with acceptable latency.
Source(s) and further reading

Understanding latency vs throughput

Availability vs consistency
CAP theorem



Source: CAP theorem revisited

In a distributed computer system, you can only support two of the following guarantees:

Consistency - Every read receives the most recent write or an error
Availability - Every request receives a response, without guarantee that it contains the most recent version of the information
Partition Tolerance - The system continues to operate despite arbitrary partitioning due to network failures

Networks aren't reliable, so you'll need to support partition tolerance.  You'll need to make a software tradeoff between consistency and availability.
CP - consistency and partition tolerance
Waiting for a response from the partitioned node might result in a timeout error.  CP is a good choice if your business needs require atomic reads and writes.
AP - availability and partition tolerance
Responses return the most recent version of the data available on a node, which might not be the latest.  Writes might take some time to propagate when the partition is resolved.
AP is a good choice if the business needs allow for eventual consistency or when the system needs to continue working despite external errors.
Source(s) and further reading

CAP theorem revisited
A plain english introduction to CAP theorem
CAP FAQ

Consistency patterns
With multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data.  Recall the definition of consistency from the CAP theorem - Every read receives the most recent write or an error.
Weak consistency
After a write, reads may or may not see it.  A best effort approach is taken.
This approach is seen in systems such as memcached.  Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games.  For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.
Eventual consistency
After a write, reads will eventually see it (typically within milliseconds).  Data is replicated asynchronously.
This approach is seen in systems such as DNS and email.  Eventual consistency works well in highly available systems.
Strong consistency
After a write, reads will see it.  Data is replicated synchronously.
This approach is seen in file systems and RDBMSes.  Strong consistency works well in systems that need transactions.
Source(s) and further reading

Transactions across data centers

Availability patterns
There are two main patterns to support high availability: fail-over and replication.
Fail-over
Active-passive
With active-passive fail-over, heartbeats are sent between the active and the passive server on standby.  If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.
The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby.  Only the active server handles traffic.
Active-passive failover can also be referred to as master-slave failover.
Active-active
In active-active, both servers are managing traffic, spreading the load between them.
If the servers are public-facing, the DNS would need to know about the public IPs of both servers.  If the servers are internal-facing, application logic would need to know about both servers.
Active-active failover can also be referred to as master-master failover.
Disadvantage(s): failover

Fail-over adds more hardware and additional complexity.
There is a potential for loss of data if the active system fails before any newly written data can be replicated to the passive.

Replication
Master-slave and master-master
This topic is further discussed in the Database section:

Master-slave replication
Master-master replication

Availability in numbers
Availability is often quantified by uptime (or downtime) as a percentage of time the service is available.  Availability is generally measured in number of 9s--a service with 99.99% availability is described as having four 9s.
99.9% availability - three 9s



Duration
Acceptable downtime




Downtime per year
8h 45min 57s


Downtime per month
43m 49.7s


Downtime per week
10m 4.8s


Downtime per day
1m 26.4s



99.99% availability - four 9s



Duration
Acceptable downtime




Downtime per year
52min 35.7s


Downtime per month
4m 23s


Downtime per week
1m 5s


Downtime per day
8.6s



Availability in parallel vs in sequence
If a service consists of multiple components prone to failure, the service's overall availability depends on whether the components are in sequence or in parallel.
In sequence
Overall availability decreases when two components with availability < 100% are in sequence:
Availability (Total) = Availability (Foo) * Availability (Bar)

If both Foo and Bar each had 99.9% availability, their total availability in sequence would be 99.8%.
In parallel
Overall availability increases when two components with availability < 100% are in parallel:
Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))

If both Foo and Bar each had 99.9% availability, their total availability in parallel would be 99.9999%.
Domain name system



Source: DNS security presentation

A Domain Name System (DNS) translates a domain name such as www.example.com to an IP address.
DNS is hierarchical, with a few authoritative servers at the top level.  Your router or ISP provides information about which DNS server(s) to contact when doing a lookup.  Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays.  DNS results can also be cached by your browser or OS for a certain period of time, determined by the time to live (TTL).

NS record (name server) - Specifies the DNS servers for your domain/subdomain.
MX record (mail exchange) - Specifies the mail servers for accepting messages.
A record (address) - Points a name to an IP address.
CNAME (canonical) - Points a name to another name or CNAME (example.com to www.example.com) or to an A record.

Services such as CloudFlare and Route 53 provide managed DNS services.  Some DNS services can route traffic through various methods:

Weighted round robin

Prevent traffic from going to servers under maintenance
Balance between varying cluster sizes
A/B testing


Latency-based
Geolocation-based

Disadvantage(s): DNS

Accessing a DNS server introduces a slight delay, although mitigated by caching described above.
DNS server management could be complex and is generally managed by governments, ISPs, and large companies.
DNS services have recently come under DDoS attack, preventing users from accessing websites such as Twitter without knowing Twitter's IP address(es).

Source(s) and further reading

DNS architecture
Wikipedia
DNS articles

Content delivery network



Source: Why use a CDN

A content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user.  Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content.  The site's DNS resolution will tell clients which server to contact.
Serving content from CDNs can significantly improve performance in two ways:

Users receive content at data centers close to them
Your servers do not have to serve requests that the CDN fulfills

Push CDNs
Push CDNs receive new content whenever changes occur on your server.  You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN.  You can configure when content expires and when it is updated.  Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.
Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs.  Content is placed on the CDNs once, instead of being re-pulled at regular intervals.
Pull CDNs
Pull CDNs grab new content from your server when the first user requests the content.  You leave the content on your server and rewrite URLs to point to the CDN.  This results in a slower request until the content is cached on the CDN.
A time-to-live (TTL) determines how long content is cached.  Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.
Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.
Disadvantage(s): CDN

CDN costs could be significant depending on traffic, although this should be weighed with additional costs you would incur not using a CDN.
Content might be stale if it is updated before the TTL expires it.
CDNs require changing URLs for static content to point to the CDN.

Source(s) and further reading

Globally distributed content delivery
The differences between push and pull CDNs
Wikipedia

Load balancer



Source: Scalable system design patterns

Load balancers distribute incoming client requests to computing resources such as application servers and databases.  In each case, the load balancer returns the response from the computing resource to the appropriate client.  Load balancers are effective at:

Preventing requests from going to unhealthy servers
Preventing overloading resources
Helping eliminate single points of failure

Load balancers can be implemented with hardware (expensive) or with software such as HAProxy.
Additional benefits include:

SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations

Removes the need to install X.509 certificates on each server


Session persistence - Issue cookies and route a specific client's requests to same instance if the web apps do not keep track of sessions

To protect against failures, it's common to set up multiple load balancers, either in active-passive or active-active mode.
Load balancers can route traffic based on various metrics, including:

Random
Least loaded
Session/cookies
Round robin or weighted round robin
Layer 4
Layer 7

Layer 4 load balancing
Layer 4 load balancers look at info at the transport layer to decide how to distribute requests.  Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet.  Layer 4 load balancers forward network packets to and from the upstream server, performing Network Address Translation (NAT).
Layer 7 load balancing
Layer 7 load balancers look at the application layer to decide how to distribute requests.  This can involve contents of the header, message, and cookies.  Layer 7 load balancers terminates network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server.  For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.
At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.
Horizontal scaling
Load balancers can also help with horizontal scaling, improving performance and availability.  Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called Vertical Scaling.  It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.
Disadvantage(s): horizontal scaling

Scaling horizontally introduces complexity and involves cloning servers

Servers should be stateless: they should not contain any user-related data like sessions or profile pictures
Sessions can be stored in a centralized data store such as a database (SQL, NoSQL) or a persistent cache (Redis, Memcached)


Downstream servers such as caches and databases need to handle more simultaneous connections as upstream servers scale out

Disadvantage(s): load balancer

The load balancer can become a performance bottleneck if it does not have enough resources or if it is not configured properly.
Introducing a load balancer to help eliminate single points of failure results in increased complexity.
A single load balancer is a single point of failure, configuring multiple load balancers further increases complexity.

Source(s) and further reading

NGINX architecture
HAProxy architecture guide
Scalability
Wikipedia
Layer 4 load balancing
Layer 7 load balancing
ELB listener config

Reverse proxy (web server)



Source: Wikipedia


A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public.  Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.
Additional benefits include:

Increased security - Hide information about backend servers, blacklist IPs, limit number of connections per client
Increased scalability and flexibility - Clients only see the reverse proxy's IP, allowing you to scale servers or change their configuration
SSL termination - Decrypt incoming requests and encrypt server responses so backend servers do not have to perform these potentially expensive operations

Removes the need to install X.509 certificates on each server


Compression - Compress server responses
Caching - Return the response for cached requests
Static content - Serve static content directly

HTML/CSS/JS
Photos
Videos
Etc



Load balancer vs reverse proxy

Deploying a load balancer is useful when you have multiple servers.  Often, load balancers  route traffic to a set of servers serving the same function.
Reverse proxies can be useful even with just one web server or application server, opening up the benefits described in the previous section.
Solutions such as NGINX and HAProxy can support both layer 7 reverse proxying and load balancing.

Disadvantage(s): reverse proxy

Introducing a reverse proxy results in increased complexity.
A single reverse proxy is a single point of failure, configuring multiple reverse proxies (ie a failover) further increases complexity.

Source(s) and further reading

Reverse proxy vs load balancer
NGINX architecture
HAProxy architecture guide
Wikipedia

Application layer



Source: Intro to architecting systems for scale

Separating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently.  Adding a new API results in adding application servers without necessarily adding additional web servers.  The single responsibility principle advocates for small and autonomous services that work together.  Small teams with small services can plan more aggressively for rapid growth.
Workers in the application layer also help enable asynchronism.
Microservices
Related to this discussion are microservices, which can be described as a suite of independently deployable, small, modular services.  Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. 1
Pinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.
Service Discovery
Systems such as Consul, Etcd, and Zookeeper can help services find each other by keeping track of registered names, addresses, and ports.  Health checks help verify service integrity and are often done using an HTTP endpoint.  Both Consul and Etcd have a built in key-value store that can be useful for storing config values and other shared data.
Disadvantage(s): application layer

Adding an application layer with loosely coupled services requires a different approach from an architectural, operations, and process viewpoint (vs a monolithic system).
Microservices can add complexity in terms of deployments and operations.

Source(s) and further reading

Intro to architecting systems for scale
Crack the system design interview
Service oriented architecture
Introduction to Zookeeper
Here's what you need to know about building microservices

Database



Source: Scaling up to your first 10 million users

Relational database management system (RDBMS)
A relational database like SQL is a collection of data items organized in tables.
ACID is a set of properties of relational database transactions.

Atomicity - Each transaction is all or nothing
Consistency - Any transaction will bring the database from one valid state to another
Isolation - Executing transactions concurrently has the same results as if the transactions were executed serially
Durability - Once a transaction has been committed, it will remain so

There are many techniques to scale a relational database: master-slave replication, master-master replication, federation, sharding, denormalization, and SQL tuning.
Master-slave replication
The master serves reads and writes, replicating writes to one or more slaves, which serve only reads.  Slaves can also replicate to additional slaves in a tree-like fashion.  If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.



Source: Scalability, availability, stability, patterns

Disadvantage(s): master-slave replication

Additional logic is needed to promote a slave to a master.
See Disadvantage(s): replication for points related to both master-slave and master-master.

Master-master replication
Both masters serve reads and writes and coordinate with each other on writes.  If either master goes down, the system can continue to operate with both reads and writes.



Source: Scalability, availability, stability, patterns

Disadvantage(s): master-master replication

You'll need a load balancer or you'll need to make changes to your application logic to determine where to write.
Most master-master systems are either loosely consistent (violating ACID) or have increased write latency due to synchronization.
Conflict resolution comes more into play as more write nodes are added and as latency increases.
See Disadvantage(s): replication for points related to both master-slave and master-master.

Disadvantage(s): replication

There is a potential for loss of data if the master fails before any newly written data can be replicated to other nodes.
Writes are replayed to the read replicas.  If there are a lot of writes, the read replicas can get bogged down with replaying writes and can't do as many reads.
The more read slaves, the more you have to replicate, which leads to greater replication lag.
On some systems, writing to the master can spawn multiple threads to write in parallel, whereas read replicas only support writing sequentially with a single thread.
Replication adds more hardware and additional complexity.

Source(s) and further reading: replication

Scalability, availability, stability, patterns
Multi-master replication

Federation



Source: Scaling up to your first 10 million users

Federation (or functional partitioning) splits up databases by function.  For example, instead of a single, monolithic database, you could have three databases: forums, users, and products, resulting in less read and write traffic to each database and therefore less replication lag.  Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality.  With no single central master serializing writes you can write in parallel, increasing throughput.
Disadvantage(s): federation

Federation is not effective if your schema requires huge functions or tables.
You'll need to update your application logic to determine which database to read and write.
Joining data from two databases is more complex with a server link.
Federation adds more hardware and additional complexity.

Source(s) and further reading: federation

Scaling up to your first 10 million users

Sharding



Source: Scalability, availability, stability, patterns

Sharding distributes data across different databases such that each database can only manage a subset of the data.  Taking a users database as an example, as the number of users increases, more shards are added to the cluster.
Similar to the advantages of federation, sharding results in less read and write traffic, less replication, and more cache hits.  Index size is also reduced, which generally improves performance with faster queries.  If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss.  Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.
Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.
Disadvantage(s): sharding

You'll need to update your application logic to work with shards, which could result in complex SQL queries.
Data distribution can become lopsided in a shard.  For example, a set of power users on a shard could result in increased load to that shard compared to others.

Rebalancing adds additional complexity.  A sharding function based on consistent hashing can reduce the amount of transferred data.


Joining data from multiple shards is more complex.
Sharding adds more hardware and additional complexity.

Source(s) and further reading: sharding

The coming of the shard
Shard database architecture
Consistent hashing

Denormalization
Denormalization attempts to improve read performance at the expense of some write performance.  Redundant copies of the data are written in multiple tables to avoid expensive joins.  Some RDBMS such as PostgreSQL and Oracle support materialized views which handle the work of storing redundant information and keeping redundant copies consistent.
Once data becomes distributed with techniques such as federation and sharding, managing joins across data centers further increases complexity.  Denormalization might circumvent the need for such complex joins.
In most systems, reads can heavily outnumber writes 100:1 or even 1000:1.  A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.
Disadvantage(s): denormalization

Data is duplicated.
Constraints can help redundant copies of information stay in sync, which increases complexity of the database design.
A denormalized database under heavy write load might perform worse than its normalized counterpart.

Source(s) and further reading: denormalization

Denormalization

SQL tuning
SQL tuning is a broad topic and many books have been written as reference.
It's important to benchmark and profile to simulate and uncover bottlenecks.

Benchmark - Simulate high-load situations with tools such as ab.
Profile - Enable tools such as the slow query log to help track performance issues.

Benchmarking and profiling might point you to the following optimizations.
Tighten up the schema

MySQL dumps to disk in contiguous blocks for fast access.
Use CHAR instead of VARCHAR for fixed-length fields.

CHAR effectively allows for fast, random access, whereas with VARCHAR, you must find the end of a string before moving onto the next one.


Use TEXT for large blocks of text such as blog posts.  TEXT also allows for boolean searches.  Using a TEXT field results in storing a pointer on disk that is used to locate the text block.
Use INT for larger numbers up to 2^32 or 4 billion.
Use DECIMAL for currency to avoid floating point representation errors.
Avoid storing large BLOBS, store the location of where to get the object instead.
VARCHAR(255) is the largest number of characters that can be counted in an 8 bit number, often maximizing the use of a byte in some RDBMS.
Set the NOT NULL constraint where applicable to improve search performance.

Use good indices

Columns that you are querying (SELECT, GROUP BY, ORDER BY, JOIN) could be faster with indices.
Indices are usually represented as self-balancing B-tree that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time.
Placing an index can keep the data in memory, requiring more space.
Writes could also be slower since the index also needs to be updated.
When loading large amounts of data, it might be faster to disable indices, load the data, then rebuild the indices.

Avoid expensive joins

Denormalize where performance demands it.

Partition tables

Break up a table by putting hot spots in a separate table to help keep it in memory.

Tune the query cache

In some cases, the query cache could lead to performance issues.

Source(s) and further reading: SQL tuning

Tips for optimizing MySQL queries
Is there a good reason i see VARCHAR(255) used so often?
How do null values affect performance?
Slow query log

NoSQL
NoSQL is a collection of data items represented in a key-value store, document store, wide column store, or a graph database.  Data is denormalized, and joins are generally done in the application code.  Most NoSQL stores lack true ACID transactions and favor eventual consistency.
BASE is often used to describe the properties of NoSQL databases.  In comparison with the CAP Theorem, BASE chooses availability over consistency.

Basically available - the system guarantees availability.
Soft state - the state of the system may change over time, even without input.
Eventual consistency - the system will become consistent over a period of time, given that the system doesn't receive input during that period.

In addition to choosing between SQL or NoSQL, it is helpful to understand which type of NoSQL database best fits your use case(s).  We'll review key-value stores, document stores, wide column stores, and graph databases in the next section.
Key-value store

Abstraction: hash table

A key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD.  Data stores can maintain keys in lexicographic order, allowing efficient retrieval of key ranges.  Key-value stores can allow for storing of metadata with a value.
Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer.  Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.
A key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.
Source(s) and further reading: key-value store

Key-value database
Disadvantages of key-value stores
Redis architecture
Memcached architecture

Document store

Abstraction: key-value store with documents stored as values

A document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object.  Document stores provide APIs or a query language to query based on the internal structure of the document itself.  Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.
Based on the underlying implementation, documents are organized by collections, tags, metadata, or directories.  Although documents can be organized or grouped together, documents may have fields that are completely different from each other.
Some document stores like MongoDB and CouchDB also provide a SQL-like language to perform complex queries.  DynamoDB supports both key-values and documents.
Document stores provide high flexibility and are often used for working with occasionally changing data.
Source(s) and further reading: document store

Document-oriented database
MongoDB architecture
CouchDB architecture
Elasticsearch architecture

Wide column store



Source: SQL & NoSQL, a brief history


Abstraction: nested map ColumnFamily<RowKey, Columns<ColKey, Value, Timestamp>>

A wide column store's basic unit of data is a column (name/value pair).  A column can be grouped in column families (analogous to a SQL table).  Super column families further group column families.  You can access each column independently with a row key, and columns with the same row key form a row.  Each value contains a timestamp for versioning and for conflict resolution.
Google introduced Bigtable as the first wide column store, which influenced the open-source HBase often-used in the Hadoop ecosystem, and Cassandra from Facebook.  Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.
Wide column stores offer high availability and high scalability.  They are often used for very large data sets.
Source(s) and further reading: wide column store

SQL & NoSQL, a brief history
Bigtable architecture
HBase architecture
Cassandra architecture

Graph database



Source: Graph database


Abstraction: graph

In a graph database, each node is a record and each arc is a relationship between two nodes.  Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.
Graphs databases offer high performance for data models with complex relationships, such as a social network.  They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources.  Many graphs can only be accessed with REST APIs.
Source(s) and further reading: graph

Graph database
Neo4j
FlockDB

Source(s) and further reading: NoSQL

Explanation of base terminology
NoSQL databases a survey and decision guidance
Scalability
Introduction to NoSQL
NoSQL patterns

SQL or NoSQL



Source: Transitioning from RDBMS to NoSQL

Reasons for SQL:

Structured data
Strict schema
Relational data
Need for complex joins
Transactions
Clear patterns for scaling
More established: developers, community, code, tools, etc
Lookups by index are very fast

Reasons for NoSQL:

Semi-structured data
Dynamic or flexible schema
Non-relational data
No need for complex joins
Store many TB (or PB) of data
Very data intensive workload
Very high throughput for IOPS

Sample data well-suited for NoSQL:

Rapid ingest of clickstream and log data
Leaderboard or scoring data
Temporary data, such as a shopping cart
Frequently accessed ('hot') tables
Metadata/lookup tables

Source(s) and further reading: SQL or NoSQL

Scaling up to your first 10 million users
SQL vs NoSQL differences

Cache



Source: Scalable system design patterns

Caching improves page load times and can reduce the load on your servers and databases.  In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.
Databases often benefit from a uniform distribution of reads and writes across its partitions.  Popular items can skew the distribution, causing bottlenecks.  Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.
Client caching
Caches can be located on the client side (OS or browser), server side, or in a distinct cache layer.
CDN caching
CDNs are considered a type of cache.
Web server caching
Reverse proxies and caches such as Varnish can serve static and dynamic content directly.  Web servers can also cache requests, returning responses without having to contact application servers.
Database caching
Your database usually includes some level of caching in a default configuration, optimized for a generic use case.  Tweaking these settings for specific usage patterns can further boost performance.
Application caching
In-memory caches such as Memcached and Redis are key-value stores between your application and your data storage.  Since the data is held in RAM, it is much faster than typical databases where data is stored on disk.  RAM is more limited than disk, so cache invalidation algorithms such as least recently used (LRU) can help invalidate 'cold' entries and keep 'hot' data in RAM.
Redis has the following additional features:

Persistence option
Built-in data structures such as sorted sets and lists

There are multiple levels you can cache that fall into two general categories: database queries and objects:

Row level
Query-level
Fully-formed serializable objects
Fully-rendered HTML

Generally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.
Caching at the database query level
Whenever you query the database, hash the query as a key and store the result to the cache.  This approach suffers from expiration issues:

Hard to delete a cached result with complex queries
If one piece of data changes such as a table cell, you need to delete all cached queries that might include the changed cell

Caching at the object level
See your data as an object, similar to what you do with your application code.  Have your application assemble the dataset from the database into a class instance or a data structure(s):

Remove the object from cache if its underlying data has changed
Allows for asynchronous processing: workers assemble objects by consuming the latest cached object

Suggestions of what to cache:

User sessions
Fully rendered web pages
Activity streams
User graph data

When to update the cache
Since you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.
Cache-aside



Source: From cache to in-memory data grid

The application is responsible for reading and writing from storage.  The cache does not interact with storage directly.  The application does the following:

Look for entry in cache, resulting in a cache miss
Load entry from the database
Add entry to cache
Return entry

def get_user(self, user_id):
    user = cache.get(""user.{0}"", user_id)
    if user is None:
        user = db.query(""SELECT * FROM users WHERE user_id = {0}"", user_id)
        if user is not None:
            key = ""user.{0}"".format(user_id)
            cache.set(key, json.dumps(user))
    return user
Memcached is generally used in this manner.
Subsequent reads of data added to cache are fast.  Cache-aside is also referred to as lazy loading.  Only requested data is cached, which avoids filling up the cache with data that isn't requested.
Disadvantage(s): cache-aside

Each cache miss results in three trips, which can cause a noticeable delay.
Data can become stale if it is updated in the database.  This issue is mitigated by setting a time-to-live (TTL) which forces an update of the cache entry, or by using write-through.
When a node fails, it is replaced by a new, empty node, increasing latency.

Write-through



Source: Scalability, availability, stability, patterns

The application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:

Application adds/updates entry in cache
Cache synchronously writes entry to data store
Return

Application code:
set_user(12345, {""foo"":""bar""})
Cache code:
def set_user(user_id, values):
    user = db.query(""UPDATE Users WHERE id = {0}"", user_id, values)
    cache.set(user_id, user)
Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast.  Users are generally more tolerant of latency when updating data than reading data.  Data in the cache is not stale.
Disadvantage(s): write through

When a new node is created due to failure or scaling, the new node will not cache entries until the entry is updated in the database.  Cache-aside in conjunction with write through can mitigate this issue.
Most data written might never be read, which can be minimized with a TTL.

Write-behind (write-back)



Source: Scalability, availability, stability, patterns

In write-behind, the application does the following:

Add/update entry in cache
Asynchronously write entry to the data store, improving write performance

Disadvantage(s): write-behind

There could be data loss if the cache goes down prior to its contents hitting the data store.
It is more complex to implement write-behind than it is to implement cache-aside or write-through.

Refresh-ahead



Source: From cache to in-memory data grid

You can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.
Refresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.
Disadvantage(s): refresh-ahead

Not accurately predicting which items are likely to be needed in the future can result in reduced performance than without refresh-ahead.

Disadvantage(s): cache

Need to maintain consistency between caches and the source of truth such as the database through cache invalidation.
Cache invalidation is a difficult problem, there is additional complexity associated with when to update the cache.
Need to make application changes such as adding Redis or memcached.

Source(s) and further reading

From cache to in-memory data grid
Scalable system design patterns
Introduction to architecting systems for scale
Scalability, availability, stability, patterns
Scalability
AWS ElastiCache strategies
Wikipedia

Asynchronism



Source: Intro to architecting systems for scale

Asynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line.  They can also help by doing time-consuming work in advance, such as periodic aggregation of data.
Message queues
Message queues receive, hold, and deliver messages.  If an operation is too slow to perform inline, you can use a message queue with the following workflow:

An application publishes a job to the queue, then notifies the user of job status
A worker picks up the job from the queue, processes it, then signals the job is complete

The user is not blocked and the job is processed in the background.  During this time, the client might optionally do a small amount of processing to make it seem like the task has completed.  For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.
Redis is useful as a simple message broker but messages can be lost.
RabbitMQ is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.
Amazon SQS is hosted but can have high latency and has the possibility of messages being delivered twice.
Task queues
Tasks queues receive tasks and their related data, runs them, then delivers their results.  They can support scheduling and can be used to run computationally-intensive jobs in the background.
Celery has support for scheduling and primarily has python support.
Back pressure
If queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance.  Back pressure can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue.  Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later.  Clients can retry the request at a later time, perhaps with exponential backoff.
Disadvantage(s): asynchronism

Use cases such as inexpensive calculations and realtime workflows might be better suited for synchronous operations, as introducing queues can add delays and complexity.

Source(s) and further reading

It's all a numbers game
Applying back pressure when overloaded
Little's law
What is the difference between a message queue and a task queue?

Communication



Source: OSI 7 layer model

Hypertext transfer protocol (HTTP)
HTTP is a method for encoding and transporting data between a client and a server.  It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.  HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.
A basic HTTP request consists of a verb (method) and a resource (endpoint).  Below are common HTTP verbs:



Verb
Description
Idempotent*
Safe
Cacheable




GET
Reads a resource
Yes
Yes
Yes


POST
Creates a resource or trigger a process that handles data
No
No
Yes if response contains freshness info


PUT
Creates or replace a resource
Yes
No
No


PATCH
Partially updates a resource
No
No
Yes if response contains freshness info


DELETE
Deletes a resource
Yes
No
No



*Can be called many times without different outcomes.
HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP.
Source(s) and further reading: HTTP

What is HTTP?
Difference between HTTP and TCP
Difference between PUT and PATCH

Transmission control protocol (TCP)



Source: How to make a multiplayer game

TCP is a connection-oriented protocol over an IP network.  Connection is established and terminated using a handshake.  All packets sent are guaranteed to reach the destination in the original order and without corruption through:

Sequence numbers and checksum fields for each packet
Acknowledgement packets and automatic retransmission

If the sender does not receive a correct response, it will resend the packets.  If there are multiple timeouts, the connection is dropped.  TCP also implements flow control and congestion control.  These guarantees cause delays and generally result in less efficient transmission than UDP.
To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage.  It can be expensive to have a large number of open connections between web server threads and say, a memcached server.  Connection pooling can help in addition to switching to UDP where applicable.
TCP is useful for applications that require high reliability but are less time critical.  Some examples include web servers, database info, SMTP, FTP, and SSH.
Use TCP over UDP when:

You need all of the data to arrive intact
You want to automatically make a best estimate use of the network throughput

User datagram protocol (UDP)



Source: How to make a multiplayer game

UDP is connectionless.  Datagrams (analogous to packets) are guaranteed only at the datagram level.  Datagrams might reach their destination out of order or not at all.  UDP does not support congestion control.  Without the guarantees that TCP support, UDP is generally more efficient.
UDP can broadcast, sending datagrams to all devices on the subnet.  This is useful with DHCP because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.
UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.
Use UDP over TCP when:

You need the lowest latency
Late data is worse than loss of data
You want to implement your own error correction

Source(s) and further reading: TCP and UDP

Networking for game programming
Key differences between TCP and UDP protocols
Difference between TCP and UDP
Transmission control protocol
User datagram protocol
Scaling memcache at Facebook

Remote procedure call (RPC)



Source: Crack the system design interview

In an RPC, a client causes a procedure to execute on a different address space, usually a remote server.  The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program.  Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls.  Popular RPC frameworks include Protobuf, Thrift, and Avro.
RPC is a request-response protocol:

Client program - Calls the client stub procedure.  The parameters are pushed onto the stack like a local procedure call.
Client stub procedure - Marshals (packs) procedure id and arguments into a request message.
Client communication module - OS sends the message from the client to the server.
Server communication module - OS passes the incoming packets to the server stub procedure.
Server stub procedure -  Unmarshalls the results, calls the server procedure matching the procedure id and passes the given arguments.
The server response repeats the steps above in reverse order.

Sample RPC calls:
GET /someoperation?data=anId

POST /anotheroperation
{
  ""data"":""anId"";
  ""anotherdata"": ""another value""
}

RPC is focused on exposing behaviors.  RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.
Choose a native library (aka SDK) when:

You know your target platform.
You want to control how your ""logic"" is accessed.
You want to control how error control happens off your library.
Performance and end user experience is your primary concern.

HTTP APIs following REST tend to be used more often for public APIs.
Disadvantage(s): RPC

RPC clients become tightly coupled to the service implementation.
A new API must be defined for every new operation or use case.
It can be difficult to debug RPC.
You might not be able to leverage existing technologies out of the box.  For example, it might require additional effort to ensure RPC calls are properly cached on caching servers such as Squid.

Representational state transfer (REST)
REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server.  The server provides a representation of resources and actions that can either manipulate or get a new representation of resources.  All communication must be stateless and cacheable.
There are four qualities of a RESTful interface:

Identify resources (URI in HTTP) - use the same URI regardless of any operation.
Change with representations (Verbs in HTTP) - use verbs, headers, and body.
Self-descriptive error message (status response in HTTP) - Use status codes, don't reinvent the wheel.
HATEOAS (HTML interface for HTTP) - your web service should be fully accessible in a browser.

Sample REST calls:
GET /someresources/anId

PUT /someresources/anId
{""anotherdata"": ""another value""}

REST is focused on exposing data.  It minimizes the coupling between client/server and is often used for public HTTP APIs.  REST uses a more generic and uniform method of exposing resources through URIs, representation through headers, and actions through verbs such as GET, POST, PUT, DELETE, and PATCH.  Being stateless, REST is great for horizontal scaling and partitioning.
Disadvantage(s): REST

With REST being focused on exposing data, it might not be a good fit if resources are not naturally organized or accessed in a simple hierarchy.  For example, returning all updated records from the past hour matching a particular set of events is not easily expressed as a path.  With REST, it is likely to be implemented with a combination of URI path, query parameters, and possibly the request body.
REST typically relies on a few verbs (GET, POST, PUT, DELETE, and PATCH) which sometimes doesn't fit your use case.  For example, moving expired documents to the archive folder might not cleanly fit within these verbs.
Fetching complicated resources with nested hierarchies requires multiple round trips between the client and server to render single views, e.g. fetching content of a blog entry and the comments on that entry. For mobile applications operating in variable network conditions, these multiple roundtrips are highly undesirable.
Over time, more fields might be added to an API response and older clients will receive all new data fields, even those that they do not need, as a result, it bloats the payload size and leads to larger latencies.

RPC and REST calls comparison



Operation
RPC
REST




Signup
POST /signup
POST /persons


Resign
POST /resign{""personid"": ""1234""}
DELETE /persons/1234


Read a person
GET /readPerson?personid=1234
GET /persons/1234


Read a person‚Äôs items list
GET /readUsersItemsList?personid=1234
GET /persons/1234/items


Add an item to a person‚Äôs items
POST /addItemToUsersItemsList{""personid"": ""1234"";""itemid"": ""456""}
POST /persons/1234/items{""itemid"": ""456""}


Update an item
POST /modifyItem{""itemid"": ""456"";""key"": ""value""}
PUT /items/456{""key"": ""value""}


Delete an item
POST /removeItem{""itemid"": ""456""}
DELETE /items/456




Source: Do you really know why you prefer REST over RPC

Source(s) and further reading: REST and RPC

Do you really know why you prefer REST over RPC
When are RPC-ish approaches more appropriate than REST?
REST vs JSON-RPC
Debunking the myths of RPC and REST
What are the drawbacks of using REST
Crack the system design interview
Thrift
Why REST for internal use and not RPC

Security
This section could use some updates.  Consider contributing!
Security is a broad topic.  Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:

Encrypt in transit and at rest.
Sanitize all user inputs or any input parameters exposed to user to prevent XSS and SQL injection.
Use parameterized queries to prevent SQL injection.
Use the principle of least privilege.

Source(s) and further reading

API security checklist
Security guide for developers
OWASP top ten

Appendix
You'll sometimes be asked to do 'back-of-the-envelope' estimates.  For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take.  The Powers of two table and Latency numbers every programmer should know are handy references.
Powers of two table
Power           Exact Value         Approx Value        Bytes
---------------------------------------------------------------
7                             128
8                             256
10                           1024   1 thousand           1 KB
16                         65,536                       64 KB
20                      1,048,576   1 million            1 MB
30                  1,073,741,824   1 billion            1 GB
32                  4,294,967,296                        4 GB
40              1,099,511,627,776   1 trillion           1 TB

Source(s) and further reading

Powers of two

Latency numbers every programmer should know
Latency Comparison Numbers
--------------------------
L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                           25   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy            10,000   ns       10 us
Send 1 KB bytes over 1 Gbps network     10,000   ns       10 us
Read 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
Disk seek                           10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD
Read 1 MB sequentially from disk    30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD
Send packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms

Notes
-----
1 ns = 10^-9 seconds
1 us = 10^-6 seconds = 1,000 ns
1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns

Handy metrics based on numbers above:

Read sequentially from disk at 30 MB/s
Read sequentially from 1 Gbps Ethernet at 100 MB/s
Read sequentially from SSD at 1 GB/s
Read sequentially from main memory at 4 GB/s
6-7 world-wide round trips per second
2,000 round trips per second within a data center

Latency numbers visualized

Source(s) and further reading

Latency numbers every programmer should know - 1
Latency numbers every programmer should know - 2
Designs, lessons, and advice from building large distributed systems
Software Engineering Advice from Building Large-Scale Distributed Systems

Additional system design interview questions

Common system design interview questions, with links to resources on how to solve each.




Question
Reference(s)




Design a file sync service like Dropbox
youtube.com


Design a search engine like Google
queue.acm.orgstackexchange.comardendertat.comstanford.edu


Design a scalable web crawler like Google
quora.com


Design Google docs
code.google.comneil.fraser.name


Design a key-value store like Redis
slideshare.net


Design a cache system like Memcached
slideshare.net


Design a recommendation system like Amazon's
hulu.comijcai13.org


Design a tinyurl system like Bitly
n00tc0d3r.blogspot.com


Design a chat app like WhatsApp
highscalability.com


Design a picture sharing system like Instagram
highscalability.comhighscalability.com


Design the Facebook news feed function
quora.comquora.comslideshare.net


Design the Facebook timeline function
facebook.comhighscalability.com


Design the Facebook chat function
erlang-factory.comfacebook.com


Design a graph search function like Facebook's
facebook.comfacebook.comfacebook.com


Design a content delivery network like CloudFlare
figshare.com


Design a trending topic system like Twitter's
michael-noll.comsnikolov .wordpress.com


Design a random ID generation system
blog.twitter.comgithub.com


Return the top k requests during a time interval
cs.ucsb.eduwpi.edu


Design a system that serves data from multiple data centers
highscalability.com


Design an online multiplayer card game
indieflashblog.combuildnewgames.com


Design a garbage collection system
stuffwithstuff.comwashington.edu


Design an API rate limiter
https://stripe.com/blog/


Add a system design question
Contribute



Real world architectures

Articles on how real world systems are designed.




Source: Twitter timelines at scale

Don't focus on nitty gritty details for the following articles, instead:

Identify shared principles, common technologies, and patterns within these articles
Study what problems are solved by each component, where it works, where it doesn't
Review the lessons learned




Type
System
Reference(s)




Data processing
MapReduce - Distributed data processing from Google
research.google.com


Data processing
Spark - Distributed data processing from Databricks
slideshare.net


Data processing
Storm - Distributed data processing from Twitter
slideshare.net







Data store
Bigtable - Distributed column-oriented database from Google
harvard.edu


Data store
HBase - Open source implementation of Bigtable
slideshare.net


Data store
Cassandra - Distributed column-oriented database from Facebook
slideshare.net


Data store
DynamoDB - Document-oriented database from Amazon
harvard.edu


Data store
MongoDB - Document-oriented database
slideshare.net


Data store
Spanner - Globally-distributed database from Google
research.google.com


Data store
Memcached - Distributed memory caching system
slideshare.net


Data store
Redis - Distributed memory caching system with persistence and value types
slideshare.net







File system
Google File System (GFS) - Distributed file system
research.google.com


File system
Hadoop File System (HDFS) - Open source implementation of GFS
apache.org







Misc
Chubby - Lock service for loosely-coupled distributed systems from Google
research.google.com


Misc
Dapper - Distributed systems tracing infrastructure
research.google.com


Misc
Kafka - Pub/sub message queue from LinkedIn
slideshare.net


Misc
Zookeeper - Centralized infrastructure and services enabling synchronization
slideshare.net



Add an architecture
Contribute



Company architectures



Company
Reference(s)




Amazon
Amazon architecture


Cinchcast
Producing 1,500 hours of audio every day


DataSift
Realtime datamining At 120,000 tweets per second


DropBox
How we've scaled Dropbox


ESPN
Operating At 100,000 duh nuh nuhs per second


Google
Google architecture


Instagram
14 million users, terabytes of photosWhat powers Instagram


Justin.tv
Justin.Tv's live video broadcasting architecture


Facebook
Scaling memcached at FacebookTAO: Facebook‚Äôs distributed data store for the social graphFacebook‚Äôs photo storageHow Facebook Live Streams To 800,000 Simultaneous Viewers


Flickr
Flickr architecture


Mailbox
From 0 to one million users in 6 weeks


Netflix
A 360 Degree View Of The Entire Netflix StackNetflix: What Happens When You Press Play?


Pinterest
From 0 To 10s of billions of page views a month18 million visitors, 10x growth, 12 employees


Playfish
50 million monthly users and growing


PlentyOfFish
PlentyOfFish architecture


Salesforce
How they handle 1.3 billion transactions a day


Stack Overflow
Stack Overflow architecture


TripAdvisor
40M visitors, 200M dynamic page views, 30TB data


Tumblr
15 billion page views a month


Twitter
Making Twitter 10000 percent fasterStoring 250 million tweets a day using MySQL150M active users, 300K QPS, a 22 MB/S firehoseTimelines at scaleBig and small data at TwitterOperations at Twitter: scaling beyond 100 million usersHow Twitter Handles 3,000 Images Per Second


Uber
How Uber scales their real-time market platformLessons Learned From Scaling Uber To 2000 Engineers, 1000 Services, And 8000 Git Repositories


WhatsApp
The WhatsApp architecture Facebook bought for $19 billion


YouTube
YouTube scalabilityYouTube architecture



Company engineering blogs

Architectures for companies you are interviewing with.
Questions you encounter might be from the same domain.


Airbnb Engineering
Atlassian Developers
AWS Blog
Bitly Engineering Blog
Box Blogs
Cloudera Developer Blog
Dropbox Tech Blog
Engineering at Quora
Ebay Tech Blog
Evernote Tech Blog
Etsy Code as Craft
Facebook Engineering
Flickr Code
Foursquare Engineering Blog
GitHub Engineering Blog
Google Research Blog
Groupon Engineering Blog
Heroku Engineering Blog
Hubspot Engineering Blog
High Scalability
Instagram Engineering
Intel Software Blog
Jane Street Tech Blog
LinkedIn Engineering
Microsoft Engineering
Microsoft Python Engineering
Netflix Tech Blog
Paypal Developer Blog
Pinterest Engineering Blog
Quora Engineering
Reddit Blog
Salesforce Engineering Blog
Slack Engineering Blog
Spotify Labs
Twilio Engineering Blog
Twitter Engineering
Uber Engineering Blog
Yahoo Engineering Blog
Yelp Engineering Blog
Zynga Engineering Blog

Source(s) and further reading
Looking to add a blog?  To avoid duplicating work, consider adding your company blog to the following repo:

kilimchoi/engineering-blogs

Under development
Interested in adding a section or helping complete one in-progress?  Contribute!

Distributed computing with MapReduce
Consistent hashing
Scatter gather
Contribute

Credits
Credits and sources are provided throughout this repo.
Special thanks to:

Hired in tech
Cracking the coding interview
High scalability
checkcheckzz/system-design-interview
shashank88/system_design
mmcgrana/services-engineering
System design cheat sheet
A distributed systems reading list
Cracking the system design interview

Contact info
Feel free to contact me to discuss any issues, questions, or comments.
My contact info can be found on my GitHub page.
License
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2017 Donne Martin

Creative Commons Attribution 4.0 International License (CC BY 4.0)

http://creativecommons.org/licenses/by/4.0/

",GitHub - donnemartin/system-design-primer: Learn how to design large-scale systems. Prep for the system design interview.  Includes Anki flashcards.
1,Python,"Awesome Python 
A curated list of awesome Python frameworks, libraries, software and resources.
Inspired by awesome-php.

Awesome Python

Admin Panels
Algorithms and Design Patterns
Asynchronous Programming
Audio
Authentication
Build Tools
Built-in Classes Enhancement
Caching
ChatOps Tools
CMS
Code Analysis
Command-line Interface Development
Command-line Tools
Compatibility
Computer Vision
Concurrency and Parallelism
Configuration
Cryptography
Data Analysis
Data Validation
Data Visualization
Database Drivers
Database
Date and Time
Debugging Tools
Deep Learning
DevOps Tools
Distributed Computing
Distribution
Documentation
Downloader
E-commerce
Editor Plugins and IDEs
Email
Environment Management
Files
Foreign Function Interface
Forms
Functional Programming
Game Development
Geolocation
GUI Development
Hardware
HTML Manipulation
HTTP Clients
Image Processing
Implementations
Interactive Interpreter
Internationalization
Job Scheduler
Logging
Machine Learning
Miscellaneous
Natural Language Processing
Network Virtualization
News Feed
ORM
Package Management
Package Repositories
Permissions
Processes
Recommender Systems
RESTful API
Robotics
RPC Servers
Science
Search
Serialization
Serverless Frameworks
Specific Formats Processing
Static Site Generator
Tagging
Task Queues
Template Engine
Testing
Text Processing
Third-party APIs
URL Manipulation
Video
Web Asset Management
Web Content Extracting
Web Crawling
Web Frameworks
WebSocket
WSGI Servers


Resources

Podcasts
Twitter
Websites
Weekly


Contributing


Admin Panels
Libraries for administrative interfaces.

ajenti - The admin panel your servers deserve.
django-grappelli - A jazzy skin for the Django Admin-Interface.
django-jet - Modern responsive template for the Django admin interface with improved functionality.
django-suit - Alternative Django Admin-Interface (free only for Non-commercial use).
django-xadmin - Drop-in replacement of Django admin comes with lots of goodies.
jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django)
flask-admin - Simple and extensible administrative interface framework for Flask.
flower - Real-time monitor and web admin for Celery.
wooey - A Django app which creates automatic web UIs for Python scripts.

Algorithms and Design Patterns
Python implementation of algorithms and design patterns.

algorithms - Minimal examples of data structures and algorithms in Python.
PyPattyrn - A simple yet effective library for implementing common design patterns.
python-patterns - A collection of design patterns in Python.
sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types.

Asynchronous Programming

asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks.

awesome-asyncio


uvloop - Ultra fast asyncio event loop.
Twisted - An event-driven networking engine.

Audio
Libraries for manipulating audio and its metadata.

Audio

audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding.
dejavu - Audio fingerprinting and recognition.
mingus - An advanced music theory and notation package with MIDI file and playback support.
pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications.
pydub - Manipulate audio with a simple and easy high level interface.
TimeSide - Open web audio processing framework.


Metadata

beets - A music library manager and MusicBrainz tagger.
eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata.
mutagen - A Python module to handle audio metadata.
tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files.



Authentication
Libraries for implementing authentications schemes.

OAuth

authlib - JavaScript Object Signing and Encryption draft implementation.
django-allauth - Authentication app for Django that ""just works.""
django-oauth-toolkit - OAuth 2 goodies for Django.
oauthlib - A generic and thorough implementation of the OAuth request-signing logic.
python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers.
python-social-auth - An easy-to-setup social authentication mechanism.


JWT

pyjwt - JSON Web Token implementation in Python.
python-jose - A JOSE implementation in Python.
python-jwt - A module for generating and verifying JSON Web Tokens.



Build Tools
Compile software from source code.

BitBake - A make-like build tool for embedded Linux.
buildout - A build system for creating, assembling and deploying applications from multiple parts.
PlatformIO - A console tool to build code with different development platforms.
pybuilder - A continuous build tool written in pure Python.
SCons - A software construction tool.

Built-in Classes Enhancement
Libraries for enhancing Python built-in classes.

dataclasses - (Python standard library) Data classes.
attrs - Replacement for __init__, __eq__, __repr__, etc. boilerplate in class definitions.
bidict - Efficient, Pythonic bidirectional map data structures and related functionality..
Box - Python dictionaries with advanced dot notation access.
DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation.

CMS
Content Management Systems.

wagtail - A Django content management system.
django-cms - An Open source enterprise CMS based on the Django.
feincms - One of the most advanced Content Management Systems built on Django.
Kotti - A high-level, Pythonic web application framework built on Pyramid.
mezzanine - A powerful, consistent, and flexible content management platform.
plone - A CMS built on top of the open source application server Zope.
quokka - Flexible, extensible, small CMS powered by Flask and MongoDB.

Caching
Libraries for caching data.

beaker - A WSGI middleware for sessions and caching.
django-cache-machine - Automatic caching and invalidation for Django models.
django-cacheops - A slick ORM cache with automatic granular event-driven invalidation.
dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors.
HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention.
pylibmc - A Python wrapper around the libmemcached interface.
python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis.

ChatOps Tools
Libraries for chatbot development.

errbot - The easiest and most popular chatbot to implement ChatOps.

Code Analysis
Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis.

Code Analysis

coala - Language independent and easily extendable code analysis application.
code2flow - Turn your Python and JavaScript code into DOT flowcharts.
prospector - A tool to analyse Python code.
pycallgraph - A library that visualises the flow (call graph) of your Python application.


Code Linters

flake8 - A wrapper around pycodestyle, pyflakes and McCabe.

awesome-flake8-extensions


pylint - A fully customizable source code analyzer.
pylama - A code audit tool for Python and JavaScript.
wemake-python-styleguide - The strictest and most opinionated python linter ever.


Code Formatters

black - The uncompromising Python code formatter.
yapf - Yet another Python code formatter from Google.


Static Type Checkers, also see awesome-python-typing

mypy - Check variable types during compile time.
pyre-check - Performant type checking.


Static Type Annotations Generators

MonkeyType - A system for Python that generates static type annotations by collecting runtime types



Command-line Interface Development
Libraries for building command-line applications.

Command-line Application Development

cement - CLI Application Framework for Python.
click - A package for creating beautiful command line interfaces in a composable way.
cliff - A framework for creating command-line programs with multi-level commands.
clint - Python Command-line Application Tools.
docopt - Pythonic command line arguments parser.
python-fire - A library for creating command line interfaces from absolutely any Python object.
python-prompt-toolkit - A library for building powerful interactive command lines.


Terminal Rendering

asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations).
bashplotlib - Making basic plots in the terminal.
colorama - Cross-platform colored terminal text.
tqdm - Fast, extensible progress bar for loops and CLI.



Command-line Tools
Useful CLI-based tools for productivity.

Productivity Tools

cookiecutter - A command-line utility that creates projects from cookiecutters (project templates).
doitlive - A tool for live presentations in the terminal.
howdoi - Instant coding answers via the command line.
PathPicker - Select files out of bash output.
percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX.
thefuck - Correcting your previous console command.
tmuxp - A tmux session manager.
try - A dead simple CLI to try out python packages - it's never been easier.


CLI Enhancements

httpie - A command line HTTP client, a user-friendly cURL replacement.
kube-shell - An integrated shell for working with the Kubernetes CLI.
mycli - A Terminal Client for MySQL with AutoCompletion and Syntax Highlighting.
pgcli - Postgres CLI with autocompletion and syntax highlighting.
saws - A Supercharged aws-cli.



Compatibility
Libraries for migrating from Python 2 to 3.

python-future - The missing compatibility layer between Python 2 and Python 3.
python-modernize - Modernizes Python code for eventual Python 3 migration.
six - Python 2 and 3 compatibility utilities.

Computer Vision
Libraries for computer vision.

OpenCV - Open Source Computer Vision Library.
pytesseract - Another wrapper for Google Tesseract OCR.
SimpleCV - An open source framework for building computer vision applications.

Concurrency and Parallelism
Libraries for concurrent and parallel execution. Also see awesome-asyncio.

concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables.
multiprocessing - (Python standard library) Process-based parallelism.
eventlet - Asynchronous framework with WSGI support.
gevent - A coroutine-based Python networking library that uses greenlet.
uvloop - Ultra fast implementation of asyncio event loop on top of libuv.
scoop - Scalable Concurrent Operations in Python.

Configuration
Libraries for storing and parsing configuration options.

configobj - INI file parser with validation.
configparser - (Python standard library) INI file parser.
profig - Config from multiple formats with value conversion.
python-decouple - Strict separation of settings from code.

Cryptography

cryptography - A package designed to expose cryptographic primitives and recipes to Python developers.
paramiko - The leading native Python SSHv2 protocol library.
passlib - Secure password storage/hashing library, very high level.
pynacl - Python binding to the Networking and Cryptography (NaCl) library.

Data Analysis
Libraries for data analyzing.

Blaze - NumPy and Pandas interface to Big Data.
Open Mining - Business Intelligence (BI) in Pandas interface.
Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Optimus - Agile Data Science Workflows made easy with PySpark.

Data Validation
Libraries for validating data. Used for forms in many cases.

Cerberus - A lightweight and extensible data validation library.
colander - Validating and deserializing data obtained via XML, JSON, an HTML form post.
jsonschema - An implementation of JSON Schema for Python.
schema - A library for validating Python data structures.
Schematics - Data Structure Validation.
valideer - Lightweight extensible data validation and adaptation library.
voluptuous - A Python data validation library.

Data Visualization
Libraries for visualizing data. Also see awesome-javascript.

Altair - Declarative statistical visualization library for Python.
Bokeh - Interactive Web Plotting for Python.
bqplot - Interactive Plotting Library for the Jupyter Notebook
Dash - Built on top of Flask, React and Plotly aimed at analytical web applications.

awesome-dash


plotnine - A grammar of graphics for Python based on ggplot2.
Matplotlib - A Python 2D plotting library.
Pygal - A Python SVG Charts Creator.
PyGraphviz - Python interface to Graphviz.
PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets.
Seaborn - Statistical data visualization using Matplotlib.
VisPy - High-performance scientific visualization based on OpenGL.

Database
Databases implemented in Python.

pickleDB - A simple and lightweight key-value store for Python.
tinydb - A tiny, document-oriented database.
ZODB - A native object database for Python. A key-value and object graph database.

Database Drivers
Libraries for connecting and operating databases.

MySQL - awesome-mysql

mysqlclient - MySQL connector with Python 3 support (mysql-python fork).
PyMySQL - A pure Python MySQL driver compatible to mysql-python.


PostgreSQL - awesome-postgres

psycopg2 - The most popular PostgreSQL adapter for Python.
queries - A wrapper of the psycopg2 library for interacting with PostgreSQL.


Other Relational Databases

pymssql - A simple database interface to Microsoft SQL Server.
SuperSQLite - A supercharged SQLite library built on top of apsw.


NoSQL Databases

cassandra-driver - The Python Driver for Apache Cassandra.
happybase - A developer-friendly library for Apache HBase.
kafka-python - The Python client for Apache Kafka.
py2neo - A client library and toolkit for working with Neo4j.
pymongo - The official Python client for MongoDB.
redis-py - The Python client for Redis.


Asynchronous Clients

motor - The async Python driver for MongoDB.



Date and Time
Libraries for working with dates and times.

Chronyk - A Python 3 library for parsing human-written times and dates.
dateutil - Extensions to the standard Python datetime module.
delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes.
moment - A Python library for dealing with dates/times. Inspired by Moment.js.
Pendulum - Python datetimes made easy.
PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string.
pytz - World timezone definitions, modern and historical. Brings the tz database into Python.
when.py - Providing user-friendly functions to help perform common date and time actions.
maya - Datetimes for Humans.

Debugging Tools
Libraries for debugging code.

pdb-like Debugger

ipdb - IPython-enabled pdb.
pdb++ - Another drop-in replacement for pdb.
pudb - A full-screen, console-based Python debugger.
wdb - An improbable web debugger through WebSockets.


Tracing

lptrace - strace for Python programs.
manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt.
pyringe - Debugger capable of attaching to and injecting code into Python processes.
python-hunter - A flexible code tracing toolkit.


Profiler

line_profiler - Line-by-line profiling.
memory_profiler - Monitor Memory usage of Python code.
profiling - An interactive Python profiler.
py-spy - A sampling profiler for Python programs. Written in Rust.
pyflame - A ptracing profiler For Python.
vprof - Visual Python profiler.


Others

icecream - Inspect variables, expressions, and program execution with a single, simple function call.
django-debug-toolbar - Display various debug information for Django.
django-devserver - A drop-in replacement for Django's runserver.
flask-debugtoolbar - A port of the django-debug-toolbar to flask.
pyelftools - Parsing and analyzing ELF files and DWARF debugging information.



Deep Learning
Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning.

caffe - A fast open framework for deep learning..
keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano.
mxnet - A deep learning framework designed for both efficiency and flexibility.
pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration.
SerpentAI - Game agent framework. Use any video game as a deep learning sandbox.
tensorflow - The most popular Deep Learning framework created by Google.
Theano - A library for fast numerical computation.

DevOps Tools
Software and libraries for DevOps.

ansible - A radically simple IT automation platform.
cloudinit - A multi-distribution package that handles early initialization of a cloud instance.
cuisine - Chef-like functionality for Fabric.
docker-compose - Fast, isolated development environments using Docker.
fabric - A simple, Pythonic tool for remote execution and deployment.
fabtools - Tools for writing awesome Fabric files.
honcho - A Python clone of Foreman, for managing Procfile-based applications.
OpenStack - Open source software for building private and public clouds.
pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect.
psutil - A cross-platform process and system utilities module.
saltstack - Infrastructure automation and management system.
supervisor - Supervisor process control system for UNIX.

Distributed Computing
Frameworks and libraries for Distributed Computing.

Batch Processing

PySpark - Apache Spark Python API.
dask - A flexible parallel computing library for analytic computing.
luigi - A module that helps you build complex pipelines of batch jobs.
mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services.
Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem.


Stream Processing

faust - A stream processing library, porting the ideas from Kafka Streams to Python.
streamparse - Run Python code against real-time streams of data via Apache Storm.



Distribution
Libraries to create packaged executables for release distribution.

dh-virtualenv - Build and distribute a virtualenv as a Debian package.
Nuitka - Compile scripts, modules, packages to an executable or extension module.
py2app - Freezes Python scripts (Mac OS X).
py2exe - Freezes Python scripts (Windows).
PyInstaller - Converts Python programs into stand-alone executables (cross-platform).
pynsist - A tool to build Windows installers, installers bundle Python itself.

Documentation
Libraries for generating project documentation.

sphinx - Python Documentation generator.

awesome-sphinxdoc


pdoc - Epydoc replacement to auto generate API documentation for Python libraries.
pycco - The literate-programming-style documentation generator.

Downloader
Libraries for downloading.

s3cmd - A command line tool for managing Amazon S3 and CloudFront.
s4cmd - Super S3 command line tool, good for higher performance.
you-get - A YouTube/Youku/Niconico video downloader written in Python 3.
youtube-dl - A small command-line program to download videos from YouTube.

E-commerce
Frameworks and libraries for e-commerce and payments.

alipay - Unofficial Alipay API for Python.
Cartridge - A shopping cart app built using the Mezzanine.
django-oscar - An open-source e-commerce framework for Django.
django-shop - A Django based shop system.
merchant - A Django app to accept payments from various payment processors.
money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange.
python-currencies - Display money format and its filthy currencies.
forex-python - Foreign exchange rates, Bitcoin price index and currency conversion.
saleor - An e-commerce storefront for Django.
shoop - An open source E-Commerce platform based on Django.

Editor Plugins and IDEs

Emacs

elpy - Emacs Python Development Environment.


Sublime Text

anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE.
SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi.


Vim

jedi-vim - Vim bindings for the Jedi auto-completion library for Python.
python-mode - An all in one plugin for turning Vim into a Python IDE.
YouCompleteMe - Includes Jedi-based completion engine for Python.


Visual Studio

PTVS - Python Tools for Visual Studio.


Visual Studio Code

Python - The official VSCode extension with rich support for Python.


IDE

PyCharm - Commercial Python IDE by JetBrains. Has free community edition available.
spyder - Open Source Python IDE.



Email
Libraries for sending and parsing email.

envelopes - Mailing for human beings.
flanker - An email address and Mime parsing library.
imbox - Python IMAP for Humans.
inbox.py - Python SMTP Server for Humans.
lamson - Pythonic SMTP Application Server.
Marrow Mailer - High-performance extensible mail delivery framework.
modoboa - A mail hosting and management platform including a modern and simplified Web UI.
Nylas Sync Engine - Providing a RESTful API on top of a powerful email sync platform.
yagmail - Yet another Gmail/SMTP client.

Environment Management
Libraries for Python version and virtual environment management.

pyenv - Simple Python version management.
pipenv - Python Development Workflow for Humans.
poetry - Python dependency management and packaging made easy.
virtualenv - A tool to create isolated Python environments.

Files
Libraries for file manipulation and MIME type detection.

mimetypes - (Python standard library) Map filenames to MIME types.
path.py - A module wrapper for os.path.
pathlib - (Python standard library) An cross-platform, object-oriented path library.
PyFilesystem2 - Python's filesystem abstraction layer.
python-magic - A Python interface to the libmagic file type identification library.
Unipath - An object-oriented approach to file/directory operations.
watchdog - API and shell utilities to monitor file system events.

Foreign Function Interface
Libraries for providing foreign function interface.

cffi - Foreign Function Interface for Python calling C code.
ctypes - (Python standard library) Foreign Function Interface for Python calling C code.
PyCUDA - A Python wrapper for Nvidia's CUDA API.
SWIG - Simplified Wrapper and Interface Generator.

Forms
Libraries for working with forms.

Deform - Python HTML form generation library influenced by the formish form generation library.
django-bootstrap3 - Bootstrap 3 integration with Django.
django-bootstrap4 - Bootstrap 4 integration with Django.
django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way.
django-remote-forms - A platform independent Django form serializer.
WTForms - A flexible forms validation and rendering library.

Functional Programming
Functional Programming with Python.

Coconut - Coconut is a variant of Python built for simple, elegant, Pythonic functional programming.
CyToolz - Cython implementation of Toolz: High performance functional utilities.
fn.py - Functional programming in Python: implementation of missing features to enjoy FP.
funcy - A fancy and practical functional tools.
Toolz - A collection of functional utilities for iterators, functions, and dictionaries.

GUI Development
Libraries for working with graphical user interface applications.

curses - Built-in wrapper for ncurses used to create terminal GUI applications.
Eel - A library for making simple Electron-like offline HTML/JS GUI apps.
enaml - Creating beautiful user-interfaces with Declarative Syntax like QML.
Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering.
Gooey - Turn command line programs into a full GUI application with one line.
kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS.
pyglet - A cross-platform windowing and multimedia library for Python.
PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3).
PyQt - Python bindings for the Qt cross-platform application and UI framework.
PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi.
pywebview - A lightweight cross-platform native wrapper around a webview component.
Tkinter - Tkinter is Python's de-facto standard GUI package.
Toga - A Python native, OS native GUI toolkit.
urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc.
wxPython - A blending of the wxWidgets C++ class library with the Python.

Game Development
Awesome game development libraries.

Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications.
Harfang3D - Python framework for 3D, VR and game development.
Panda3D - 3D game engine developed by Disney.
Pygame - Pygame is a set of Python modules designed for writing games.
PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D.
PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs.
PySDL2 - A ctypes based wrapper for the SDL2 library.
RenPy - A Visual Novel engine.

Geolocation
Libraries for geocoding addresses and working with latitudes and longitudes.

django-countries - A Django app that provides a country field for models and forms.
GeoDjango - A world-class geographic web framework.
GeoIP - Python API for MaxMind GeoIP Legacy Database.
geojson - Python bindings and utilities for GeoJSON.
geopy - Python Geocoding Toolbox.
pygeoip - Pure Python GeoIP API.

HTML Manipulation
Libraries for working with HTML and XML.

BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML.
bleach - A whitelist-based HTML sanitization and text linkification library.
cssutils - A CSS library for Python.
html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments.
lxml - A very fast, easy-to-use and versatile library for handling HTML and XML.
MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python.
pyquery - A jQuery-like library for parsing HTML.
untangle - Converts XML documents to Python objects for easy access.
WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF.
xmldataset - Simple XML Parsing.
xmltodict - Working with XML feel like you are working with JSON.

HTTP Clients
Libraries for working with HTTP.

grequests - requests + gevent for asynchronous HTTP requests.
httplib2 - Comprehensive HTTP client library.
requests - HTTP Requests for Humans.
treq - Python requests like API built on top of Twisted's HTTP client.
urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.

Hardware
Libraries for programming with hardware.

ino - Command line toolkit for working with Arduino.
keyboard - Hook and simulate global keyboard events on Windows and Linux.
mouse - Hook and simulate global mouse events on Windows and Linux.
Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc.
PyUserInput - A module for cross-platform control of the mouse and keyboard.
scapy - A brilliant packet manipulation library.
wifi - A Python library and command line tool for working with WiFi on Linux.

Image Processing
Libraries for manipulating images.

hmap - Image histogram remapping.
imgSeek - A project for searching a collection of images using visual similarity.
nude.py - Nudity detection.
pagan - Retro identicon (Avatar) generation based on input string and hash.
pillow - Pillow is the friendly PIL fork.
pyBarcode - Create barcodes in Python without needing PIL.
pygram - Instagram-like image filters.
python-qrcode - A pure Python QR Code generator.
Quads - Computer art based on quadtrees.
scikit-image - A Python library for (scientific) image processing.
thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images.
wand - Python bindings for MagickWand, C API for ImageMagick.

Implementations
Implementations of Python.

CPython - Default, most widely used implementation of the Python programming language written in C.
Cython - Optimizing Static Compiler for Python.
CLPython - Implementation of the Python programming language written in Common Lisp.
Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha).
IronPython - Implementation of the Python programming language written in C#.
Jython - Implementation of Python programming language written in Java for the JVM.
MicroPython - A lean and efficient Python programming language implementation.
Numba - Python JIT compiler to LLVM aimed at scientific Python.
PeachPy - x86-64 assembler embedded in Python.
Pyjion - A JIT for Python based upon CoreCLR.
PyPy - A very fast and compliant implementation of the Python language.
Pyston - A Python implementation using JIT techniques.
Stackless Python - An enhanced version of the Python programming language.

Interactive Interpreter
Interactive Python interpreters (REPL).

bpython - A fancy interface to the Python interpreter.
Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively.

awesome-jupyter


ptpython - Advanced Python REPL built on top of the python-prompt-toolkit.

Internationalization
Libraries for working with i18n.

Babel - An internationalization library for Python.
PyICU - A wrapper of International Components for Unicode C++ library (ICU).

Job Scheduler
Libraries for scheduling jobs.

APScheduler - A light but powerful in-process task scheduler that lets you schedule functions.
django-schedule - A calendaring app for Django.
doit - A task runner and build tool.
gunnery - Multipurpose task execution tool for distributed systems with web-based interface.
Joblib - A set of tools to provide lightweight pipelining in Python.
Plan - Writing crontab file in Python like a charm.
schedule - Python job scheduling for humans.
Spiff - A powerful workflow engine implemented in pure Python.
TaskFlow - A Python library that helps to make task execution easy, consistent and reliable.
Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows.

Logging
Libraries for generating and working with logs.

Eliot - Logging for complex & distributed systems.
logbook - Logging replacement for Python.
logging - (Python standard library) Logging facility for Python.
raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.

Machine Learning
Libraries for Machine Learning. Also see awesome-machine-learning.

H2O - Open Source Fast Scalable Machine Learning Platform.
Metrics - Machine learning evaluation metrics.
NuPIC - Numenta Platform for Intelligent Computing.
scikit-learn - The most popular Python library for Machine Learning.
Spark ML - Apache Spark's scalable Machine Learning library.
vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit.
xgboost - A scalable, portable, and distributed gradient boosting library.

Microsoft Windows
Python programming on Microsoft Windows.

Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder.
pythonlibs - Unofficial Windows binaries for Python extension packages.
PythonNet - Python Integration with the .NET Common Language Runtime (CLR).
PyWin32 - Python Extensions for Windows.
WinPython - Portable development environment for Windows 7/8.

Miscellaneous
Useful libraries or tools that don't fit in the categories above.

blinker - A fast Python in-process signal/event dispatching system.
boltons - A set of pure-Python utilities.
itsdangerous - Various helpers to pass trusted data to untrusted environments.
pluginbase - A simple but flexible plugin system for Python.
tryton - A general purpose business framework.

Natural Language Processing
Libraries for working with human languages.

General

gensim - Topic Modeling for Humans.
langid.py - Stand-alone language identification system.
nltk - A leading platform for building Python programs to work with human language data.
pattern - A web mining module for the Python.
polyglot - Natural language pipeline supporting hundreds of languages.
pytext - A natural language modeling framework based on PyTorch.
PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research.
spacy - A library for industrial-strength natural language processing in Python and Cython.
stanfordnlp - The Stanford NLP Group's official Python library, supporting 50+ languages.


Chinese

jieba - The most popular Chinese text segmentation library.
pkuseg-python - A toolkit for Chinese word segmentation in various domains.
snownlp - A library for processing Chinese text.
funNLP - A collection of tools and datasets for Chinese NLP.



Network Virtualization
Tools and libraries for Virtual Networking and SDN (Software Defined Networking).

mininet - A popular network emulator and API written in Python.
napalm - Cross-vendor API to manipulate network devices.
pox - A Python-based SDN control applications, such as OpenFlow SDN controllers.

News Feed
Libraries for building user's activities.

django-activity-stream - Generating generic activity streams from the actions on your site.
Stream Framework - Building news feed and notification systems using Cassandra and Redis.

ORM
Libraries that implement Object-Relational Mapping or data mapping techniques.

Relational Databases

Django Models - The Django ORM.
SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper.

awesome-sqlalchemy


dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL.
orator -  The Orator ORM provides a simple yet beautiful ActiveRecord implementation.
orm - An async ORM.
peewee - A small, expressive ORM.
pony - ORM that provides a generator-oriented interface to SQL.
pydal - A pure Python Database Abstraction Layer.


NoSQL Databases

hot-redis - Rich Python data types for Redis.
mongoengine - A Python Object-Document-Mapper for working with MongoDB.
PynamoDB - A Pythonic interface for Amazon DynamoDB.
redisco - A Python Library for Simple Models and Containers Persisted in Redis.



Package Management
Libraries for package and dependency management.

pip - The Python package and dependency manager.

PyPI
pip-tools - A set of tools to keep your pinned Python dependencies fresh.


conda - Cross-platform, Python-agnostic binary package manager.

Package Repositories
Local PyPI repository server and proxies.

warehouse - Next generation Python Package Repository (PyPI).
bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA).
devpi - PyPI server and packaging/testing/release tool.
localshop - Local PyPI server (custom packages and auto-mirroring of pypi).

Permissions
Libraries that allow or deny users access to data or functionality.

django-guardian - Implementation of per object permissions for Django 1.2+
django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database.

Processes
Libraries for starting and communicating with OS processes.

delegator.py - Subprocesses for Humans 2.0.
sarge - Yet another wrapper for subprocess.
sh - A full-fledged subprocess replacement for Python.

Recommender Systems
Libraries for building recommender systems.

annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage.
fastFM - A library for Factorization Machines.
implicit - A fast Python implementation of collaborative filtering for implicit datasets.
libffm - A library for Field-aware Factorization Machine (FFM).
lightfm - A Python implementation of a number of popular recommendation algorithms.
spotlight - Deep recommender models using PyTorch.
Surprise - A scikit for building and analyzing recommender systems.
tensorrec - A Recommendation Engine Framework in TensorFlow.

RESTful API
Libraries for building RESTful APIs.

Django

django-rest-framework - A powerful and flexible toolkit to build web APIs.
django-tastypie - Creating delicious APIs for Django apps.


Flask

eve - REST API framework powered by Flask, MongoDB and good intentions.
flask-api - Browsable Web APIs for Flask.
flask-restful - Quickly building REST APIs for Flask.


Pyramid

cornice - A RESTful framework for Pyramid.


Framework agnostic

apistar - A smart Web API framework, designed for Python 3.
falcon - A high-performance framework for building cloud APIs and web app backends.
fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints.
hug - A Python 3 framework for cleanly exposing APIs.
sandman2 - Automated REST APIs for existing database-driven systems.
sanic - A Python 3.6+ web server and web framework that's written to go fast.
vibora - Fast, efficient and asynchronous Web framework inspired by Flask.



Robotics
Libraries for robotics.

PythonRobotics - This is a compilation of various robotics algorithms with visualizations.
rospy - This is a library for ROS (Robot Operating System).

RPC Servers
RPC-compatible servers.

zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack.

Science
Libraries for scientific computing. Also see Python-for-Scientists

astropy - A community Python library for Astronomy.
bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis.
bccb - Collection of useful code related to biological analysis.
Biopython - Biopython is a set of freely available tools for biological computation.
cclib - A library for parsing and interpreting the results of computational chemistry packages.
Colour - Implementing a comprehensive number of colour theory transformations and algorithms.
NetworkX - A high-productivity software for complex networks.
NIPY - A collection of neuroimaging toolkits.
NumPy - A fundamental package for scientific computing with Python.
Open Babel - A chemical toolbox designed to speak the many languages of chemical data.
ObsPy - A Python toolbox for seismology.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion.
PyMC - Markov Chain Monte Carlo sampling toolkit.
QuTiP - Quantum Toolbox in Python.
RDKit - Cheminformatics and Machine Learning Software.
SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
statsmodels - Statistical modeling and econometrics in Python.
SymPy - A Python library for symbolic mathematics.
Zipline - A Pythonic algorithmic trading library.
SimPy -  A process-based discrete-event simulation framework.

Search
Libraries and software for indexing and performing search queries on data.

elasticsearch-py - The official low-level Python client for Elasticsearch.
elasticsearch-dsl-py - The official high-level Python client for Elasticsearch.
django-haystack - Modular search for Django.
pysolr - A lightweight Python wrapper for Apache Solr.
whoosh - A fast, pure Python search engine library.

Serialization
Libraries for serializing complex data types

marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes.
pysimdjson - A Python bindings for simdjson.
python-rapidjson - A Python wrapper around RapidJSON.
ultrajson - A fast JSON decoder and encoder written in C with Python bindings.

Serverless Frameworks
Frameworks for developing serverless Python code.

python-lambda - A toolkit for developing and deploying Python code in AWS Lambda.
Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway.

Specific Formats Processing
Libraries for parsing and manipulating specific text formats.

General

tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML.


Office

openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files.
pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files.
python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files.
python-pptx - Python library for creating and updating PowerPoint (.pptx) files.
unoconv - Convert between any document format supported by LibreOffice/OpenOffice.
XlsxWriter - A Python module for creating Excel .xlsx files.
xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa.
xlwt / xlrd - Writing and reading data and formatting information from Excel files.


PDF

PDFMiner - A tool for extracting information from PDF documents.
PyPDF2 - A library capable of splitting, merging and transforming PDF pages.
ReportLab - Allowing Rapid creation of rich PDF documents.


Markdown

Mistune - Fastest and full featured pure Python parsers of Markdown.
Python-Markdown - A Python implementation of John Gruber‚Äôs Markdown.


YAML

PyYAML - YAML implementations for Python.


CSV

csvkit - Utilities for converting to and working with CSV.


Archive

unp - A command line tool that can unpack archives easily.



Static Site Generator
Static site generator is a software that takes some text + templates as input and produces HTML files on the output.

mkdocs - Markdown friendly documentation generator.
pelican - Static site generator that supports Markdown and reST syntax.
lektor - An easy to use static CMS and blog engine.
nikola - A static website and blog generator.

Tagging
Libraries for tagging items.

django-taggit - Simple tagging for Django.

Task Queues
Libraries for working with task queues.

celery - An asynchronous task queue/job queue based on distributed message passing.
huey - Little multi-threaded task queue.
mrq - A distributed worker task queue in Python using Redis & gevent.
rq - Simple job queues for Python.

Template Engine
Libraries and tools for templating and lexing.

Jinja2 - A modern and designer friendly templating language.
Genshi - Python templating toolkit for generation of web-aware output.
Mako - Hyperfast and lightweight templating for the Python platform.

Testing
Libraries for testing codebases and generating test data.

Testing Frameworks

pytest - A mature full-featured Python testing tool.
hypothesis - Hypothesis is an advanced Quickcheck style property based testing library.
nose2 - The successor to nose, based on `unittest2.
Robot Framework - A generic test automation framework.
unittest - (Python standard library) Unit testing framework.


Test Runners

green - A clean, colorful test runner.
mamba - The definitive testing tool for Python. Born under the banner of BDD.
tox - Auto builds and tests distributions in multiple Python versions


GUI / Web Testing

locust - Scalable user load testing tool written in Python.
PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings.
Selenium - Python bindings for Selenium WebDriver.
sixpack - A language-agnostic A/B Testing framework.
splinter - Open source tool for testing web applications.


Mock

mock - (Python standard library) A mocking and patching library.
doublex - Powerful test doubles framework for Python.
freezegun - Travel through time by mocking the datetime module.
httmock - A mocking library for requests for Python 2.6+ and 3.2+.
httpretty - HTTP request mock tool for Python.
mocket - A socket mock framework with gevent/asyncio/SSL support.
responses - A utility library for mocking out the requests Python library.
VCR.py - Record and replay HTTP interactions on your tests.


Object Factories

factory_boy - A test fixtures replacement for Python.
mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc.
model_mommy - Creating random fixtures for testing in Django.


Code Coverage

coverage - Code coverage measurement.


Fake Data

mimesis - is a Python library that help you generate fake data.
fake2db - Fake database generator.
faker - A Python package that generates fake data.
radar - Generate random datetime / time.



Text Processing
Libraries for parsing and manipulating plain texts.

General

chardet - Python 2/3 compatible character encoding detector.
difflib - (Python standard library) Helpers for computing deltas.
ftfy - Makes Unicode text less broken and more consistent automagically.
fuzzywuzzy - Fuzzy String Matching.
Levenshtein - Fast computation of Levenshtein distance and string similarity.
pangu.py - Paranoid text spacing.
pyfiglet - An implementation of figlet written in Python.
pypinyin - Convert Chinese hanzi (Êº¢Â≠ó) to pinyin (ÊãºÈü≥).
textdistance - Compute distance between sequences with 30+ algorithms.
unidecode - ASCII transliterations of Unicode text.


Slugify

awesome-slugify - A Python slugify library that can preserve unicode.
python-slugify - A Python slugify library that translates unicode to ASCII.
unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency.


Unique identifiers

hashids - Implementation of hashids in Python.
shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs.


Parser

ply - Implementation of lex and yacc parsing tools for Python.
pygments - A generic syntax highlighter.
pyparsing - A general purpose framework for generating parsers.
python-nameparser - Parsing human names into their individual components.
python-phonenumbers - Parsing, formatting, storing and validating international phone numbers.
python-user-agents - Browser user agent parser.
sqlparse - A non-validating SQL parser.



Third-party APIs
Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries.

apache-libcloud - One Python library for all clouds.
boto3 - Python interface to Amazon Web Services.
django-wordpress - WordPress models and views for Django.
facebook-sdk - Facebook Platform Python SDK.
google-api-python-client - Google APIs Client Library for Python.
gspread - Google Spreadsheets Python API.
twython - A Python wrapper for the Twitter API.

URL Manipulation
Libraries for parsing URLs.

furl - A small Python library that makes parsing and manipulating URLs easy.
purl - A simple, immutable URL class with a clean API for interrogation and manipulation.
pyshorteners - A pure Python URL shortening lib.
webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.

Video
Libraries for manipulating video and GIFs.

moviepy - A module for script-based movie editing with many formats, including animated GIFs.
scikit-video - Video processing routines for SciPy.

Web Asset Management
Tools for managing, compressing and minifying website assets.

django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file.
django-pipeline - An asset packaging library for Django.
django-storages - A collection of custom storage back ends for Django.
fanstatic - Packages, optimizes, and serves static file dependencies as Python packages.
fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP.
flask-assets - Helps you integrate webassets into your Flask app.
webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources.

Web Content Extracting
Libraries for extracting web contents.

html2text - Convert HTML to Markdown-formatted text.
lassie - Web Content Retrieval for Humans.
micawber - A small library for extracting rich content from URLs.
newspaper - News extraction, article extraction and content curation in Python.
python-readability - Fast Python port of arc90's readability tool.
requests-html - Pythonic HTML Parsing for Humans.
sumy - A module for automatic summarization of text documents and HTML pages.
textract - Extract text from any document, Word, PowerPoint, PDFs, etc.
toapi - Every web site provides APIs.

Web Crawling
Libraries to automate web scraping.

cola - A distributed crawling framework.
feedparser - Universal feed parser.
grab - Site scraping framework.
MechanicalSoup - A Python library for automating interaction with websites.
pyspider - A powerful spider system.
robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser.
scrapy - A fast high-level screen scraping and web crawling framework.
portia - Visual scraping for Scrapy.

Web Frameworks
Traditional full stack web frameworks. Also see RESTful API

Synchronous

Django - The most popular web framework in Python.

awesome-django


Flask - A microframework for Python.

awesome-flask


Pyramid - A small, fast, down-to-earth, open source Python web framework.

awesome-pyramid


Masonite - The modern and developer centric Python web framework.


Asynchronous

Tornado - A web framework and asynchronous networking library.



WebSocket
Libraries for working with WebSocket.

autobahn-python - WebSocket & WAMP for Python on Twisted and asyncio.
channels - Developer-friendly asynchrony for Django.
websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity.

WSGI Servers
WSGI-compatible web servers.

bjoern - Asynchronous, very fast and written in C.
gunicorn - Pre-forked, partly written in C.
uWSGI - A project aims at developing a full stack for building hosting services, written in C.
waitress - Multi-threaded, powers Pyramid.
werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.

Resources
Where to discover new Python libraries.
Podcasts

From Python Import Podcast
Podcast.init
Python Bytes
Python Testing
Radio Free Python
Talk Python To Me
Test and Code

Twitter

@codetengu
@getpy
@importpython
@planetpython
@pycoders
@pypi
@pythontrending
@PythonWeekly
@TalkPython
@realpython

Websites

/r/CoolGithubProjects
/r/Python
Awesome Python @LibHunt
Django Packages
Full Stack Python
Python Cheatsheet
Python ZEEF
Python ÂºÄÂèëÁ§æÂå∫
Real Python
Trending Python repositories on GitHub today
–°–æ–æ–±—â–µ—Å—Ç–≤–æ Python –ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤

Weekly

CodeTengu Weekly Á¢ºÂ§©ÁãóÈÄ±Âàä
Import Python Newsletter
Pycoder's Weekly
Python Weekly
Python Tricks

Contributing
Your contributions are always welcome! Please take a look at the contribution guidelines first.
I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding üëç to them. Pull requests will be merged when their votes reach 20.

If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.
","GitHub - vinta/awesome-python: A curated list of awesome Python frameworks, libraries, software and resources"
2,Python,"Public APIs 
A collective list of free APIs for use in software and web development.
Sponsor:

A public API for this project can be found here - thanks to DigitalOcean for helping us provide this service!
For information on contributing to this project, please see the contributing guide.
Please note a passing build status indicates all listed APIs are available since the last update. A failing build status indicates that 1 or more services may be unavailable at the moment.
Index

Animals
Anime
Anti-Malware
Art & Design
Books
Business
Calendar
Cloud Storage & File Sharing
Continuous Integration
Cryptocurrency
Currency Exchange
Data Validation
Development
Dictionaries
Documents & Productivity
Environment
Events
Finance
Food & Drink
Fraud Prevention
Games & Comics
Geocoding
Government
Health
Jobs
Machine Learning
Music
News
Open Data
Open Source Projects
Patent
Personality
Photography
Science & Math
Security
Shopping
Social
Sports & Fitness
Test Data
Text Analysis
Tracking
Transportation
URL Shorteners
Vehicle
Video
Weather

Animals



API
Description
Auth
HTTPS
CORS




Cat Facts
Daily cat facts
No
Yes
No


Cats
Pictures of cats from Tumblr
apiKey
Yes
Unknown


Dogs
Based on the Stanford Dogs Dataset
No
Yes
Yes


HTTPCat
Cat for every HTTP Status
No
Yes
Unknown


IUCN
IUCN Red List of Threatened Species
apiKey
No
Unknown


Movebank
Movement and Migration data of animals
No
Yes
Unknown


Petfinder
Adoption
OAuth
Yes
Yes


PlaceGOAT
Placeholder goat images
No
Yes
Unknown


RandomCat
Random pictures of cats
No
Yes
Yes


RandomDog
Random pictures of dogs
No
Yes
Yes


RandomFox
Random pictures of foxes
No
Yes
No


RescueGroups
Adoption
No
Yes
Unknown


Shibe.Online
Random pictures of Shibu Inu, cats or birds
No
No
No



‚¨Ü Back to Index
Anime



API
Description
Auth
HTTPS
CORS




AniList
Anime discovery & tracking
OAuth
Yes
Unknown


AnimeNewsNetwork
Anime industry news
No
Yes
Yes


Jikan
Unofficial MyAnimeList API
No
Yes
Yes


Kitsu
Anime discovery platform
OAuth
Yes
Unknown


Studio Ghibli
Resources from Studio Ghibli films
No
Yes
Unknown



‚¨Ü Back to Index
Anti-Malware



API
Description
Auth
HTTPS
CORS




AbuseIPDB
IP/domain/URL reputation
apiKey
Yes
Unknown


AlienVault Open Threat Exchange (OTX)
IP/domain/URL reputation
apiKey
Yes
Unknown


Google Safe Browsing
Google Link/Domain Flagging
apiKey
Yes
Unknown


Metacert
Metacert Link Flagging
apiKey
Yes
Unknown


VirusTotal
VirusTotal File/URL Analysis
apiKey
Yes
Unknown


Web Of Trust (WOT)
Website reputation
apiKey
Yes
Unknown



‚¨Ü Back to Index
Art & Design



API
Description
Auth
HTTPS
CORS




Behance
Design
apiKey
Yes
Unknown


Cooper Hewitt
Smithsonian Design Museum
apiKey
Yes
Unknown


Dribbble
Design
OAuth
No
Unknown


Harvard Art Museums
Art
apiKey
No
Unknown


Iconfinder
Icons
apiKey
Yes
Unknown


Icons8
Icons
OAuth
Yes
Unknown


Noun Project
Icons
OAuth
No
Unknown


Rijksmuseum
Art
apiKey
Yes
Unknown



‚¨Ü Back to Index
Books



API
Description
Auth
HTTPS
CORS




Bhagavad Gita
Bhagavad Gita text
OAuth
Yes
Yes


BookNomads
Books published in the Netherlands and Flanders (about 2.5 million), book covers and related data
No
Yes
Unknown


British National Bibliography
Books
No
No
Unknown


Goodreads
Books
apiKey
Yes
Unknown


Google Books
Books
OAuth
Yes
Unknown


LibGen
Library Genesis search engine
No
No
Unknown


Open Library
Books, book covers and related data
No
Yes
Unknown


Penguin Publishing
Books, book covers and related data
No
Yes
Unknown



‚¨Ü Back to Index
Business



API
Description
Auth
HTTPS
CORS




Charity Search
Non-profit charity data
apiKey
No
Unknown


Clearbit Logo
Search for company logos and embed them in your projects
apiKey
Yes
Unknown


Domainsdb.info
Registered Domain Names Search
No
Yes
Unknown


Freelancer
Hire freelancers to get work done
OAuth
Yes
Unknown


Gmail
Flexible, RESTful access to the user's inbox
OAuth
Yes
Unknown


Google Analytics
Collect, configure and analyze your data to reach the right audience
OAuth
Yes
Unknown


MailboxValidator
Validate email address to improve deliverability
apiKey
Yes
Unknown


mailgun
Email Service
apiKey
Yes
Unknown


markerapi
Trademark Search
No
No
Unknown


Ticksel
Friendly website analytics made for humans
No
Yes
Unknown


Trello
Boards, lists and cards to help you organize and prioritize your projects
OAuth
Yes
Unknown



‚¨Ü Back to Index
Calendar



API
Description
Auth
HTTPS
CORS




Calendar Index
Worldwide Holidays and Working Days
apiKey
Yes
Yes


Church Calendar
Catholic liturgical calendar
No
No
Unknown


Czech Namedays Calendar
Lookup for a name and returns nameday date
No
No
Unknown


Google Calendar
Display, create and modify Google calendar events
OAuth
Yes
Unknown


Hebrew Calendar
Convert between Gregorian and Hebrew, fetch Shabbat and Holiday times, etc
No
No
Unknown


Holidays
Historical data regarding holidays
apiKey
Yes
Unknown


LectServe
Protestant liturgical calendar
No
No
Unknown


Nager.Date
Public holidays for more than 90 countries
No
Yes
No


Namedays Calendar
Provides namedays for multiple countries
No
Yes
Yes


Non-Working Days
Database of ICS files for non working days
No
Yes
Unknown


Russian Calendar
Check if a date is a Russian holiday or not
No
Yes
No



‚¨Ü Back to Index
Cloud Storage & File Sharing



API
Description
Auth
HTTPS
CORS




Box
File Sharing and Storage
OAuth
Yes
Unknown


Dropbox
File Sharing and Storage
OAuth
Yes
Unknown


Google Drive
File Sharing and Storage
OAuth
Yes
Unknown


OneDrive
File Sharing and Storage
OAuth
Yes
Unknown


Pastebin
Plain Text Storage
apiKey
Yes
Unknown


Temporal
IPFS based file storage and sharing with optional IPNS naming
apiKey
Yes
No


WeTransfer
File Sharing
apiKey
Yes
Yes



‚¨Ü Back to Index
Continuous Integration



API
Description
Auth
HTTPS
CORS




CircleCI
Automate the software development process using continuous integration and continuous delivery
apiKey
Yes
Unknown


Codeship
Codeship is a Continuous Integration Platform in the cloud
apiKey
Yes
Unknown


Travis CI
Sync your GitHub projects with Travis CI to test your code in minutes
apiKey
Yes
Unknown



‚¨Ü Back to Index
Cryptocurrency



API
Description
Auth
HTTPS
CORS




Binance
Exchange for Trading Cryptocurrencies based in China
apiKey
Yes
Unknown


BitcoinAverage
Digital Asset Price Data for the blockchain industry
apiKey
Yes
Unknown


BitcoinCharts
Financial and Technical Data related to the Bitcoin Network
No
Yes
Unknown


Bitfinex
Cryptocurrency Trading Platform
apiKey
Yes
Unknown


Bitmex
Real-Time Cryptocurrency derivatives trading platform based in Hong Kong
apiKey
Yes
Unknown


Bittrex
Next Generation Crypto Trading Platform
apiKey
Yes
Unknown


Block
Bitcoin Payment, Wallet & Transaction Data
apiKey
Yes
Unknown


Blockchain
Bitcoin Payment, Wallet & Transaction Data
No
Yes
Unknown


CoinAPI
All Currency Exchanges integrate under a single api
apiKey
Yes
No


Coinbase
Bitcoin, Bitcoin Cash, Litecoin and Ethereum Prices
apiKey
Yes
Unknown


Coinbase Pro
Cryptocurrency Trading Platform
apiKey
Yes
Unknown


CoinDesk
Bitcoin Price Index
No
No
Unknown


CoinGecko
Cryptocurrency Price, Market, and Developer/Social Data
No
Yes
Yes


Coinigy
Interacting with Coinigy Accounts and Exchange Directly
apiKey
Yes
Unknown


CoinLayer
Real-time Crypto Currency Exchange Rates
apiKey
Yes
Unknown


Coinlib
Crypto Currency Prices
apiKey
Yes
Unknown


Coinlore
Cryptocurrencies prices, volume and more
No
Yes
Unknown


CoinMarketCap
Cryptocurrencies Prices
apiKey
Yes
Unknown


Coinpaprika
Cryptocurrencies prices, volume and more
No
Yes
Yes


CoinRanking
Live Cryptocurrency data
No
Yes
Unknown


CryptoCompare
Cryptocurrencies Comparison
No
Yes
Unknown


Cryptonator
Cryptocurrencies Exchange Rates
No
Yes
Unknown


Gemini
Cryptocurrencies Exchange
No
Yes
Unknown


ICObench
Various information on listing, ratings, stats, and more
apiKey
Yes
Unknown


Livecoin
Cryptocurrency Exchange
No
Yes
Unknown


MercadoBitcoin
Brazilian Cryptocurrency Information
No
Yes
Unknown


Nexchange
Automated cryptocurrency exchange service
No
No
Yes


NiceHash
Largest Crypto Mining Marketplace
apiKey
Yes
Unknown


Poloniex
US based digital asset exchange
apiKey
Yes
Unknown


WorldCoinIndex
Cryptocurrencies Prices
apiKey
Yes
Unknown



‚¨Ü Back to Index
Currency Exchange



API
Description
Auth
HTTPS
CORS




1Forge
Forex currency market data
apiKey
Yes
Unknown


Currencylayer
Exchange rates and currency conversion
apiKey
Yes
Unknown


Czech National Bank
A collection of exchange rates
No
Yes
Unknown


ExchangeRate-API
Free currency conversion
No
Yes
Yes


Exchangeratesapi.io
Exchange rates with currency conversion
No
Yes
Yes


Fixer.io
Exchange rates and currency conversion
apiKey
Yes
Unknown


Frankfurter
Exchange rates, currency conversion and time series
No
Yes
Yes


ratesapi
Free exchange rates and historical rates
No
Yes
Unknown



‚¨Ü Back to Index
Data Validation



API
Description
Auth
HTTPS
CORS




Cloudmersive Validate
Validate email addresses, phone numbers, VAT numbers and domain names
apiKey
Yes
Yes


languagelayer
Language detection
No
Yes
Unknown


Lob.com
US Address Verification
apiKey
Yes
Unknown


mailboxlayer
Email address validation
No
Yes
Unknown


NumValidate
Open Source phone number validation
No
Yes
Unknown


numverify
Phone number validation
No
Yes
Unknown


PurgoMalum
Content validator against profanity & obscenity
No
No
Unknown


US Autocomplete
Enter address data quickly with real-time address suggestions
apiKey
Yes
Yes


US Extract
Extract postal addresses from any text including emails
apiKey
Yes
Yes


US Street Address
Validate and append data for any US postal address
apiKey
Yes
Yes


vatlayer
VAT number validation
No
Yes
Unknown



‚¨Ü Back to Index
Development



API
Description
Auth
HTTPS
CORS




24 Pull Requests
Project to promote open source collaboration during December
No
Yes
Yes


Agify.io
Estimates the age from a first name
No
Yes
Yes


ApiFlash
Chrome based screenshot API for developers
apiKey
Yes
Unknown


Apility.io
IP, Domains and Emails anti-abuse API blocklist
No
Yes
Yes


APIs.guru
Wikipedia for Web APIs, OpenAPI/Swagger specs for public APIs
No
Yes
Unknown


BetterMeta
Return a site's meta tags in JSON format
X-Mashape-Key
Yes
Unknown


Bitbucket
Pull public information for a Bitbucket account
No
Yes
Unknown


Bored
Find random activities to fight boredom
No
Yes
Unknown


Browshot
Easily make screenshots of web pages in any screen size, as any device
apiKey
Yes
Unknown


CDNJS
Library info on CDNJS
No
Yes
Unknown


Changelogs.md
Structured changelog metadata from open source projects
No
Yes
Unknown


CountAPI
Free and simple counting service. You can use it to track page hits and specific events
No
Yes
Yes


DigitalOcean Status
Status of all DigitalOcean services
No
Yes
Unknown


DomainDb Info
Domain name search to find all domains containing particular words/phrases/etc
No
Yes
Unknown


Faceplusplus
A tool to detect face
OAuth
Yes
Unknown


Genderize.io
Estimates a gender from a first name
No
Yes
Yes


GitHub
Make use of GitHub repositories, code and user info programmatically
OAuth
Yes
Yes


Gitlab
Automate GitLab interaction programmatically
OAuth
Yes
Unknown


Gitter
Chat for GitHub
OAuth
Yes
Unknown


HTTP2.Pro
Test endpoints for client and server HTTP/2 protocol support
No
Yes
Unknown


IBM Text to Speech
Convert text to speech
apiKey
Yes
Yes


import.io
Retrieve structured data from a website or RSS feed
apiKey
Yes
Unknown


IPify
A simple IP Address API
No
Yes
Unknown


IPinfo
Another simple IP Address API
No
Yes
Unknown


JSON 2 JSONP
Convert JSON to JSONP (on-the-fly) for easy cross-domain data requests using client-side JavaScript
No
Yes
Unknown


JSONbin.io
Free JSON storage service. Ideal for small scale Web apps, Websites and Mobile apps
apiKey
Yes
Yes


Judge0
Compile and run source code
No
Yes
Unknown


Let's Validate
Uncovers the technologies used on websites and URL to thumbnail
No
Yes
Unknown


License-API
Unofficial REST API for choosealicense.com
No
Yes
No


LiveEdu
Live Coding Streaming
OAuth
Yes
Unknown


MAC address vendor lookup
Retrieve vendor details and other information regarding a given MAC address or an OUI
apiKey
Yes
Yes


Myjson
A simple JSON store for your web or mobile app
No
No
Unknown


Nationalize.io
Estimate the nationality of a first name
No
Yes
Yes


OOPSpam
Multiple spam filtering service
No
Yes
Yes


Plino
Spam filtering system
No
Yes
Unknown


Postman
Tool for testing APIs
apiKey
Yes
Unknown


ProxyCrawl
Scraping and crawling anticaptcha service
apiKey
Yes
Unknown


Public APIs
A collective list of free JSON APIs for use in web development
No
Yes
Unknown


Pusher Beams
Push notifications for Android & iOS
apiKey
Yes
Unknown


QR code
Create an easy to read QR code and URL shortener
No
Yes
Yes


QR code
Generate and decode / read QR code graphics
No
Yes
Unknown


QuickChart
Generate chart and graph images
No
Yes
Yes


ReqRes
A hosted REST-API ready to respond to your AJAX requests
No
Yes
Unknown


Scrape Website Email
Grabs email addresses from a URL
X-Mashape-Key
Yes
Unknown


ScraperApi
Easily build scalable web scrapers
apiKey
Yes
Unknown


ScreenshotAPI.net
Create pixel-perfect website screenshots
apiKey
Yes
Yes


SHOUTCLOUD
ALL-CAPS AS A SERVICE
No
No
Unknown


StackExchange
Q&A forum for developers
OAuth
Yes
Unknown


Verse
Check what's the latest version of your favorite open-source project
No
Yes
Unknown


XML to JSON
Integration developer utility APIs
No
Yes
Unknown



‚¨Ü Back to Index
Dictionaries



API
Description
Auth
HTTPS
CORS




Lingua Robot
Word definitions, pronunciations, synonyms, antonyms and others
apiKey
Yes
Yes


Merriam-Webster
Dictionary and Thesaurus Data
apiKey
Yes
Unknown


OwlBot
Definitions with example sentence and photo if available
apiKey
Yes
Yes


Oxford
Dictionary Data
apiKey
Yes
No


Wordnik
Dictionary Data
apiKey
No
Unknown


Words
Definitions and synonyms for more than 150,000 words
apiKey
Yes
Unknown



‚¨Ü Back to Index
Documents & Productivity



API
Description
Auth
HTTPS
CORS




Cloudmersive Document and Data Conversion
HTML/URL to PDF/PNG, Office documents to PDF, image conversion
apiKey
Yes
Yes


File.io
File Sharing
No
Yes
Unknown


Mercury
Web parser
apiKey
Yes
Unknown


pdflayer
HTML/URL to PDF
apiKey
Yes
Unknown


Pocket
Bookmarking service
OAuth
Yes
Unknown


PrexView
Data from XML or JSON to PDF, HTML or Image
apiKey
Yes
Unknown


Restpack
Provides screenshot, HTML to PDF and content extraction APIs
apiKey
Yes
Unknown


Todoist
Todo Lists
OAuth
Yes
Unknown


Vector Express
Free vector file converting API
No
No
Yes


WakaTime
Automated time tracking leaderboards for programmers
No
Yes
Unknown


Wunderlist
Todo Lists
OAuth
Yes
Unknown



‚¨Ü Back to Index
Environment



API
Description
Auth
HTTPS
CORS




AirVisual
Air quality and weather data
apiKey
Yes
Unknown


Gr√ºnstromIndex
Green Power Index for Germany (Gr√ºnstromindex/GSI)
No
No
Yes


OpenAQ
Open air quality data
apiKey
Yes
Unknown


PM25.in
Air quality of China
apiKey
No
Unknown


PVWatts
Energy production photovoltaic (PV) energy systems
apiKey
Yes
Unknown


UK Carbon Intensity
The Official Carbon Intensity API for Great Britain developed by National Grid
No
Yes
Unknown



‚¨Ü Back to Index
Events



API
Description
Auth
HTTPS
CORS




Eventbrite
Find events
OAuth
Yes
Unknown


Picatic
Sell tickets anywhere
apiKey
Yes
Unknown


Ticketmaster
Search events, attractions, or venues
apiKey
Yes
Unknown



‚¨Ü Back to Index
Finance



API
Description
Auth
HTTPS
CORS




Alpha Vantage
Realtime and historical stock data
apiKey
Yes
Unknown


Barchart OnDemand
Stock, Futures and Forex Market Data
apiKey
Yes
Unknown


Consumer Financial Protection Bureau
Financial services consumer complaint data
apiKey
Yes
Unknown


Financial Modeling Prep
Stock information and data
No
Yes
Unknown


IEX
Realtime stock data
No
Yes
Yes


IEX Cloud
Realtime & Historical Stock and Market Data
apiKey
Yes
Yes


IG
Spreadbetting and CFD Market Data
apiKey
Yes
Unknown


Plaid
Connect with users‚Äô bank accounts and access transaction data
apiKey
Yes
Unknown


Razorpay IFSC
Indian Financial Systems Code (Bank Branch Codes)
No
Yes
Unknown


RoutingNumbers.info
ACH/NACHA Bank Routing Numbers
No
Yes
Unknown


Tradier
US equity/option market data (delayed, intraday, historical)
OAuth
Yes
Yes


VAT Rates
A collection of all VAT rates for EU countries
No
Yes
Unknown


YNAB
Budgeting & Planning
OAuth
Yes
Yes



‚¨Ü Back to Index
Food & Drink



API
Description
Auth
HTTPS
CORS




Edamam
Recipe Search
apiKey
Yes
Unknown


LCBO
Alcohol
apiKey
Yes
Unknown


Open Brewery DB
Breweries, Cideries and Craft Beer Bottle Shops
No
Yes
Yes


Open Food Facts
Food Products Database
No
Yes
Unknown


PunkAPI
Brewdog Beer Recipes
No
Yes
Unknown


Recipe Puppy
Food
No
No
Unknown


TacoFancy
Community-driven taco database
No
No
Unknown


The Report of the Week
Food & Drink Reviews
No
Yes
Unknown


TheCocktailDB
Cocktail Recipes
apiKey
Yes
Yes


TheMealDB
Meal Recipes
apiKey
Yes
Yes


What's on the menu?
NYPL human-transcribed historical menu collection
apiKey
No
Unknown


Zomato
Discover restaurants
apiKey
Yes
Unknown



‚¨Ü Back to Index
Fraud Prevention



API
Description
Auth
HTTPS
CORS




FraudLabs Pro
Screen order information using AI to detect frauds
apiKey
Yes
Unknown


Whitepages Pro
Global identity verification with phone, address, email and IP
apiKey
Yes
Unknown


Whitepages Pro
Phone reputation to detect spammy phones
apiKey
Yes
Unknown


Whitepages Pro
Get an owner‚Äôs name, address, demographics based on the phone number
apiKey
Yes
Unknown


Whitepages Pro
Phone number validation, line_type, carrier append
apiKey
Yes
Unknown


Whitepages Pro
Get normalized physical address, residents, address type and validity
apiKey
Yes
Unknown



‚¨Ü Back to Index
Games & Comics



API
Description
Auth
HTTPS
CORS




Age of Empires II
Get information about Age of Empires II resources
No
Yes
Unknown


AmiiboAPI
Amiibo Information
No
No
Yes


Battle.net
Blizzard Entertainment
apiKey
Yes
Unknown


Chuck Norris Database
Jokes
No
No
Unknown


Clash of Clans
Clash of Clans Game Information
apiKey
Yes
Unknown


Clash Royale
Clash Royale Game Information
apiKey
Yes
Unknown


Comic Vine
Comics
No
Yes
Unknown


Deck of Cards
Deck of Cards
No
No
Unknown


Destiny The Game
Bungie Platform API
apiKey
Yes
Unknown


Dota 2
Provides information about Player stats , Match stats, Rankings for Dota 2
No
Yes
Unknown


Dungeons and Dragons
Reference for 5th edition spells, classes, monsters, and more
No
No
No


Eve Online
Third-Party Developer Documentation
OAuth
Yes
Unknown


Final Fantasy XIV
Final Fantasy XIV Game data API
No
Yes
Yes


Fortnite
Fortnite Stats & Cosmetics
apiKey
Yes
Yes


Fortnite
Fortnite Stats
apiKey
Yes
Unknown


Giant Bomb
Video Games
No
Yes
Unknown


Guild Wars 2
Guild Wars 2 Game Information
apiKey
Yes
Unknown


Halo
Halo 5 and Halo Wars 2 Information
apiKey
Yes
Unknown


Hearthstone
Hearthstone Cards Information
X-Mashape-Key
Yes
Unknown


Hypixel
Hypixel player stats
apiKey
Yes
Unknown


IGDB.com
Video Game Database
apiKey
Yes
Unknown


JokeAPI
Programming, Miscellaneous and Dark Jokes
No
Yes
Yes


Jokes
Programming and general jokes
No
Yes
Unknown


Jservice
Jeopardy Question Database
No
No
Unknown


Magic The Gathering
Magic The Gathering Game Information
No
No
Unknown


Marvel
Marvel Comics
apiKey
No
Unknown


mod.io
Cross Platform Mod API
apiKey
Yes
Unknown


Open Trivia
Trivia Questions
No
Yes
Unknown


PandaScore
E-sports games and results
apiKey
Yes
Unknown


PlayerUnknown's Battlegrounds
PUBG Stats
apiKey
Yes
Unknown


Pok√©api
Pok√©mon Information
No
Yes
Unknown


Pok√©mon TCG
Pok√©mon TCG Information
No
Yes
Unknown


Rick and Morty
All the Rick and Morty information, including images
No
Yes
Yes


Riot Games
League of Legends Game Information
apiKey
Yes
Unknown


Scryfall
Magic: The Gathering database
No
Yes
Yes


Steam
Steam Client Interaction
OAuth
Yes
Unknown


SuperHeroes
All SuperHeroes and Villains data from all universes under a single API
apiKey
Yes
Unknown


Tronald Dump
The dumbest things Donald Trump has ever said
No
Yes
Unknown


Vainglory
Vainglory Players, Matches and Telemetry
apiKey
Yes
Yes


Wargaming.net
Wargaming.net info and stats
apiKey
Yes
No


xkcd
Retrieve xkcd comics as JSON
No
Yes
No



‚¨Ü Back to Index
Geocoding



API
Description
Auth
HTTPS
CORS




adresse.data.gouv.fr
Address database of France, geocoding and reverse
No
Yes
Unknown


Battuta
A (country/region/city) in-cascade location API
apiKey
No
Unknown


Bing Maps
Create/customize digital maps based on Bing Maps data
apiKey
Yes
Unknown


bng2latlong
Convert British OSGB36 easting and northing (British National Grid) to WGS84 latitude and longitude
No
Yes
Yes


CitySDK
Open APIs for select European cities
No
Yes
Unknown


Daum Maps
Daum Maps provide multiple APIs for Korean maps
apiKey
No
Unknown


FreeGeoIP
Free geo ip information, no registration required. 15k/hour rate limit
No
Yes
Yes


GeoApi
French geographical data
No
Yes
Unknown


Geocod.io
Address geocoding / reverse geocoding in bulk
apiKey
Yes
Unknown


Geocode.xyz
Provides worldwide forward/reverse geocoding, batch geocoding and geoparsing
No
Yes
Unknown


GeoDataSource
Geocoding of city name by using latitude and longitude coordinates
apiKey
Yes
Unknown


GeoJS
IP geolocation with ChatOps integration
No
Yes
Yes


GeoNames
Place names and other geographical data
No
No
Unknown


geoPlugin
IP geolocation and currency conversion
No
Yes
Yes


Google Earth Engine
A cloud-based platform for planetary-scale environmental data analysis
apiKey
Yes
Unknown


Google Maps
Create/customize digital maps based on Google Maps data
apiKey
Yes
Unknown


HelloSalut
Get hello translation following user language
No
Yes
Unknown


HERE Maps
Create/customize digital maps based on HERE Maps data
apiKey
Yes
Unknown


Indian Cities
Get all Indian cities in a clean JSON Format
No
Yes
Yes


IP 2 Country
Map an IP to a country
No
Yes
Unknown


IP Address Details
Find geolocation with ip address
No
Yes
Unknown


IP Location
Find location with ip address
No
No
Unknown


IP Location
Find IP address location information
No
Yes
Unknown


IP Sidekick
Geolocation API that returns extra information about an IP address
apiKey
Yes
Unknown


IP Vigilante
Free IP Geolocation API
No
Yes
Unknown


IP2Location
IP geolocation web service to get more than 55 parameters
apiKey
Yes
Unknown


IP2Proxy
Detect proxy and VPN using IP address
apiKey
Yes
Unknown


IPGeolocationAPI.com
Locate your visitors by IP with country details
No
Yes
Yes


IPInfoDB
Free Geolocation tools and APIs for country, region, city and time zone lookup by IP address
apiKey
Yes
Unknown


ipstack
Locate and identify website visitors by IP address
apiKey
Yes
Unknown


Kwelo Network
Locate and get detailed information on IP address
No
Yes
Yes


LocationIQ
Provides forward/reverse geocoding and batch geocoding
apiKey
Yes
Yes


Mapbox
Create/customize beautiful digital maps
apiKey
Yes
Unknown


Mexico
Mexico RESTful zip codes API
No
Yes
Unknown


One Map, Singapore
Singapore Land Authority REST API services for Singapore addresses
apiKey
Yes
Unknown


OnWater
Determine if a lat/lon is on water or land
No
Yes
Unknown


OpenCage
Forward and reverse geocoding using open data
apiKey
Yes
Yes


OpenStreetMap
Navigation, geolocation and geographical data
OAuth
No
Unknown


PostcodeData.nl
Provide geolocation data based on postcode for Dutch addresses
No
No
Unknown


Postcodes.io
Postcode lookup & Geolocation for the UK
No
Yes
Yes


REST Countries
Get information about countries via a RESTful API
No
Yes
Unknown


SmartIP.io
IP Geolocation and Threat Intelligence API
apiKey
Yes
Yes


Uebermaps
Discover and share maps with friends
apiKey
Yes
Unknown


US ZipCode
Validate and append data for any US ZipCode
apiKey
Yes
Yes


Utah AGRC
Utah Web API for geocoding Utah addresses
apiKey
Yes
Unknown


ViaCep
Brazil RESTful zip codes API
No
Yes
Unknown


ZipCodeAPI
US zip code distance, radius and location API
apiKey
Yes
Unknown


Zippopotam
Get information about place such as country, city, state, etc
No
No
Unknown



‚¨Ü Back to Index
Government



API
Description
Auth
HTTPS
CORS




BCLaws
Access to the laws of British Columbia
No
No
Unknown


BusinessUSA
Authoritative information on U.S. programs, events, services and more
apiKey
Yes
Unknown


Census.gov
The US Census Bureau provides various APIs and data sets on demographics and businesses
No
Yes
Unknown


City, Lyon Opendata
Lyon(FR) City Open Data
apiKey
Yes
Unknown


City, Nantes Opendata
Nantes(FR) City Open Data
apiKey
Yes
Unknown


City, Prague Opendata
Prague(CZ) City Open Data
No
No
Unknown


Code.gov
The primary platform for Open Source and code sharing for the U.S. Federal Government
apiKey
Yes
Unknown


Colorado Data Engine
Formatted and geolocated Colorado public data
No
Yes
Unknown


Colorado Information Marketplace
Colorado State Government Open Data
No
Yes
Unknown


Data USA
US Public Data
No
Yes
Unknown


Data.gov
US Government Data
apiKey
Yes
Unknown


Data.parliament.uk
Contains live datasets including information about petitions, bills, MP votes, attendance and more
No
No
Unknown


District of Columbia Open Data
Contains D.C. government public datasets, including crime, GIS, financial data, and so on
No
Yes
Unknown


EPA
Web services and data sets from the US Environmental Protection Agency
No
Yes
Unknown


FEC
Information on campaign donations in federal elections
apiKey
Yes
Unknown


Federal Register
The Daily Journal of the United States Government
No
Yes
Unknown


Food Standards Agency
UK food hygiene rating data API
No
No
Unknown


Open Government, Australia
Australian Government Open Data
No
Yes
Unknown


Open Government, Belgium
Belgium Government Open Data
No
Yes
Unknown


Open Government, Canada
Canadian Government Open Data
No
No
Unknown


Open Government, France
French Government Open Data
apiKey
Yes
Unknown


Open Government, India
Indian Government Open Data
apiKey
Yes
Unknown


Open Government, Italy
Italy Government Open Data
No
Yes
Unknown


Open Government, New Zealand
New Zealand Government Open Data
No
Yes
Unknown


Open Government, Romania
Romania Government Open Data
No
No
Unknown


Open Government, Taiwan
Taiwan Government Open Data
No
Yes
Unknown


Open Government, USA
United States Government Open Data
No
Yes
Unknown


Regulations.gov
Federal regulatory materials to increase understanding of the Federal rule making process
apiKey
Yes
Unknown


Represent by Open North
Find Canadian Government Representatives
No
Yes
Unknown


USAspending.gov
US federal spending data
No
Yes
Unknown



‚¨Ü Back to Index
Health



API
Description
Auth
HTTPS
CORS




BetterDoctor
Detailed information about doctors in your area
apiKey
Yes
Unknown


Diabetes
Logging and retrieving diabetes information
No
No
Unknown


Flutrack
Influenza-like symptoms with geotracking
No
No
Unknown


Healthcare.gov
Educational content about the US Health Insurance Marketplace
No
Yes
Unknown


Lexigram
NLP that extracts mentions of clinical concepts from text, gives access to clinical ontology
apiKey
Yes
Unknown


Makeup
Makeup Information
No
No
Unknown


Medicare
Access to the data from the CMS - medicare.gov
No
Yes
Unknown


NPPES
National Plan & Provider Enumeration System, info on healthcare providers registered in US
No
Yes
Unknown


Nutritionix
Worlds largest verified nutrition database
apiKey
Yes
Unknown


openFDA
Public FDA data about drugs, devices and foods
No
Yes
Unknown


USDA Nutrients
National Nutrient Database for Standard Reference
No
Yes
Unknown



‚¨Ü Back to Index
Jobs



API
Description
Auth
HTTPS
CORS




Adzuna
Job board aggregator
apiKey
Yes
Unknown


Authentic Jobs
Job board for designers, hackers and creative pros
apiKey
Yes
Unknown


Careerjet
Job search engine
apiKey
No
Unknown


Github Jobs
Jobs for software developers
No
Yes
Yes


GraphQL Jobs
Jobs with GraphQL
No
Yes
Yes


Indeed
Job board aggregator
apiKey
Yes
Unknown


Jobs2Careers
Job aggregator
apiKey
Yes
Unknown


Jooble
Job search engine
apiKey
Yes
Unknown


Juju
Job search engine
apiKey
No
Unknown


Open Skills
Job titles, skills and related jobs data
No
No
Unknown


Reed
Job board aggregator
apiKey
Yes
Unknown


Search.gov Jobs
Tap into a list of current jobs openings with the United States government
No
Yes
Unknown


The Muse
Job board and company profiles
apiKey
Yes
Unknown


Upwork
Freelance job board and management system
OAuth
Yes
Unknown


USAJOBS
US government job board
apiKey
Yes
Unknown


ZipRecruiter
Job search app and website
apiKey
Yes
Unknown



‚¨Ü Back to Index
Machine Learning



API
Description
Auth
HTTPS
CORS




Clarifai
Computer Vision
OAuth
Yes
Unknown


Cloudmersive
Image captioning, face recognition, NSFW classification
apiKey
Yes
Yes


Deepcode
AI for code review
No
Yes
Unknown


Dialogflow
Natural Language Processing
apiKey
Yes
Unknown


Keen IO
Data Analytics
apiKey
Yes
Unknown


Time Door
A time series analysis API
apiKey
Yes
Yes


Unplugg
Forecasting API for timeseries data
apiKey
Yes
Unknown


Wit.ai
Natural Language Processing
OAuth
Yes
Unknown



‚¨Ü Back to Index
Music



API
Description
Auth
HTTPS
CORS




AI Mastering
Automated Music Mastering
apiKey
Yes
Yes


Bandsintown
Music Events
No
Yes
Unknown


Deezer
Music
OAuth
Yes
Unknown


Discogs
Music
OAuth
Yes
Unknown


Genius
Crowdsourced lyrics and music knowledge
OAuth
Yes
Unknown


Genrenator
Music genre generator
No
Yes
Unknown


iTunes Search
Software products
No
Yes
Unknown


Jamendo
Music
OAuth
Yes
Unknown


KKBOX
Get music libraries, playlists, charts, and perform out of KKBOX's platform
OAuth
Yes
Unknown


LastFm
Music
apiKey
Yes
Unknown


Lyrics.ovh
Simple API to retrieve the lyrics of a song
No
Yes
Unknown


Mixcloud
Music
OAuth
Yes
Yes


MusicBrainz
Music
No
Yes
Unknown


Musikki
Music
apiKey
Yes
Unknown


Musixmatch
Music
apiKey
Yes
Unknown


Openwhyd
Download curated playlists of streaming tracks (YouTube, SoundCloud, etc...)
No
Yes
No


Songkick
Music Events
OAuth
Yes
Unknown


Songsterr
Provides guitar, bass and drums tabs and chords
No
Yes
Unknown


SoundCloud
Allow users to upload and share sounds
OAuth
Yes
Unknown


Spotify
View Spotify music catalog, manage users' libraries, get recommendations and more
OAuth
Yes
Unknown


TasteDive
Similar artist API (also works for movies and TV shows)
apiKey
Yes
Unknown


TheAudioDB
Music
apiKey
Yes
Unknown


Vagalume
Crowdsourced lyrics and music knowledge
apiKey
Yes
Unknown



‚¨Ü Back to Index
News



API
Description
Auth
HTTPS
CORS




Associated Press
Search for news and metadata from Associated Press
apiKey
Yes
Unknown


Chronicling America
Provides access to millions of pages of historic US newspapers from the Library of Congress
No
No
Unknown


Currents
Latest news published in various news sources, blogs and forums
apiKey
Yes
Yes


Feedbin
RSS reader
OAuth
Yes
Unknown


Feedster
Searchable and categorized collections of RSS feeds
apiKey
Yes
Unknown


New York Times
Provides news
apiKey
Yes
Unknown


News
Headlines currently published on a range of news sources and blogs
apiKey
Yes
Unknown


NPR One
Personalized news listening experience from NPR
OAuth
Yes
Unknown


The Guardian
Access all the content the Guardian creates, categorised by tags and section
apiKey
Yes
Unknown


The Old Reader
RSS reader
apiKey
Yes
Unknown



‚¨Ü Back to Index
Open Data



API
Description
Auth
HTTPS
CORS




18F
Unofficial US Federal Government API Development
No
No
Unknown


Abbreviation
Get abbreviations and meanings
X-Mashape-Key
Yes
Unknown


Archive.org
The Internet Archive
No
Yes
Unknown


ARSAT
ARSAT public data
apiKey
Yes
Unknown


Callook.info
United States ham radio callsigns
No
Yes
Unknown


CARTO
Location Information Prediction
apiKey
Yes
Unknown


Celebinfo
Celebrity information
X-Mashape-Key
Yes
Unknown


CivicFeed
News articles and public datasets
apiKey
Yes
Unknown


Datakick
The open product database
apiKey
Yes
Unknown


Enigma Public
Broadest collection of public data
apiKey
Yes
Yes


fonoApi
Mobile Device Description
No
Yes
Unknown


French Address Search
Address search via the French Government
No
Yes
Unknown


LinkPreview
Get JSON formatted summary with title, description and preview image for any requested URL
apiKey
Yes
Yes


Marijuana Strains
Marijuana strains, races, flavors and effects
apiKey
No
Unknown


Microlink.io
Extract structured data from any website
No
Yes
Yes


OpenCorporates
Data on corporate entities and directors in many countries
apiKey
Yes
Unknown


Qmeta
Global Search Engine
apiKey
Yes
Unknown


Quandl
Stock Market Data
No
Yes
Unknown


Recreation Information Database
Recreational areas, federal lands, historic sites, museums, and other attractions/resources(US)
apiKey
Yes
Unknown


Scoop.it
Content Curation Service
apiKey
No
Unknown


Teleport
Quality of Life Data
No
Yes
Unknown


Universities List
University names, countries and domains
No
Yes
Unknown


University of Oslo
Courses, lecture videos, detailed information for courses etc. for the University of Oslo (Norway)
No
Yes
Unknown


UPC database
More than 1.5 million barcode numbers from all around the world
apiKey
Yes
Unknown


Wikidata
Collaboratively edited knowledge base operated by the Wikimedia Foundation
OAuth
Yes
Unknown


Wikipedia
Mediawiki Encyclopedia
No
Yes
Unknown


Yelp
Find Local Business
OAuth
Yes
Unknown



‚¨Ü Back to Index
Open Source Projects



API
Description
Auth
HTTPS
CORS




Countly
Countly web analytics
No
No
Unknown


Drupal.org
Drupal.org
No
Yes
Unknown


Evil Insult Generator
Evil Insults
No
Yes
Yes


Libraries.io
Open source software libraries
apiKey
Yes
Unknown



‚¨Ü Back to Index
Patent



API
Description
Auth
HTTPS
CORS




EPO
European patent search system api
OAuth
Yes
Unknown


TIPO
Taiwan patent search system api
apiKey
Yes
Unknown


USPTO
USA patent api services
No
Yes
Unknown



‚¨Ü Back to Index
Personality



API
Description
Auth
HTTPS
CORS




Advice Slip
Generate random advice slips
No
Yes
Unknown


chucknorris.io
JSON API for hand curated Chuck Norris jokes
No
Yes
Unknown


FavQs.com
FavQs allows you to collect, discover and share your favorite quotes
apiKey
Yes
Unknown


FOAAS
Fuck Off As A Service
No
No
Unknown


Forismatic
Inspirational Quotes
No
No
Unknown


icanhazdadjoke
The largest selection of dad jokes on the internet
No
Yes
Unknown


kanye.rest
REST API for random Kanye West quotes
No
Yes
Yes


Medium
Community of readers and writers offering unique perspectives on ideas
OAuth
Yes
Unknown


NaMoMemes
Memes on Narendra Modi
No
Yes
Unknown


Programming Quotes
Programming Quotes API for open source projects
No
Yes
Unknown


Quote Garden
REST API for more than 5000 famous quotes
No
Yes
Unknown


Quotes on Design
Inspirational Quotes
No
Yes
Unknown


Traitify
Assess, collect and analyze Personality
No
Yes
Unknown


tronalddump.io
Api & web archive for the things Donald Trump has said
No
Yes
Unknown



‚¨Ü Back to Index
Photography



API
Description
Auth
HTTPS
CORS




Flickr
Flickr Services
OAuth
Yes
Unknown


Getty Images
Build applications using the world's most powerful imagery
OAuth
Yes
Unknown


Gfycat
Jiffier GIFs
OAuth
Yes
Unknown


Giphy
Get all your gifs
apiKey
Yes
Unknown


Gyazo
Upload images
apiKey
Yes
Unknown


Imgur
Images
OAuth
Yes
Unknown


Lorem Picsum
Images from Unsplash
No
Yes
Unknown


Pexels
Free Stock Photos and Videos
apiKey
Yes
Yes


Pixabay
Photography
apiKey
Yes
Unknown


Pixhost
Upload images, photos, galleries
No
Yes
Unknown


PlaceKitten
Resizable kitten placeholder images
No
Yes
Unknown


ScreenShotLayer
URL 2 Image
No
Yes
Unknown


Unsplash
Photography
OAuth
Yes
Unknown


Wallhaven
Wallpapers
apiKey
Yes
Unknown



‚¨Ü Back to Index
Science & Math



API
Description
Auth
HTTPS
CORS




arcsecond.io
Multiple astronomy data sources
No
Yes
Unknown


CORE
Access the world's Open Access research papers
apiKey
Yes
Unknown


GBIF
Global Biodiversity Information Facility
No
Yes
Yes


iDigBio
Access millions of museum specimens from organizations around the world
No
Yes
Unknown


inspirehep.net
High Energy Physics info. system
No
Yes
Unknown


ITIS
Integrated Taxonomic Information System
No
Yes
Unknown


Launch Library
Upcoming Space Launches
No
Yes
Unknown


Minor Planet Center
Asterank.com Information
No
No
Unknown


NASA
NASA data, including imagery
No
Yes
Unknown


NASA APOD (unofficial API)
API for getting APOD (Astronomy Image of the Day) images along with metadata
No
Yes
Yes


Newton
Symbolic and Arithmetic Math Calculator
No
Yes
Unknown


Numbers
Facts about numbers
No
No
Unknown


Open Notify
ISS astronauts, current location, etc
No
No
Unknown


Open Science Framework
Repository and archive for study designs, research materials, data, manuscripts, etc
No
Yes
Unknown


SHARE
A free, open, dataset about research and scholarly activities
No
Yes
Unknown


SpaceX
Company, vehicle, launchpad and launch data
No
Yes
Unknown


Sunrise and Sunset
Sunset and sunrise times for a given latitude and longitude
No
Yes
Unknown


Trefle
Botanical data for plant species
apiKey
Yes
Unknown


USGS Earthquake Hazards Program
Earthquakes data real-time
No
Yes
Unknown


USGS Water Services
Water quality and level info for rivers and lakes
No
Yes
Unknown


World Bank
World Data
No
No
Unknown



‚¨Ü Back to Index
Security



API
Description
Auth
HTTPS
CORS




Censys.io
Search engine for Internet connected host and devices
apiKey
Yes
No


CRXcavator
Chrome extension risk scoring
apiKey
Yes
Unknown


FilterLists
Lists of filters for adblockers and firewalls
No
Yes
Unknown


HaveIBeenPwned
Passwords which have previously been exposed in data breaches
apiKey
Yes
Unknown


National Vulnerability Database
U.S. National Vulnerability Database
No
Yes
Unknown


SecurityTrails
Domain and IP related information such as current and historical WHOIS and DNS records
apiKey
Yes
Unknown


Shodan
Search engine for Internet connected devices
apiKey
Yes
Unknown


UK Police
UK Police data
No
Yes
Unknown



‚¨Ü Back to Index
Shopping



API
Description
Auth
HTTPS
CORS




Best Buy
Products, Buying Options, Categories, Recommendations, Stores and Commerce
apiKey
Yes
Unknown


Bratabase
Database of different types of Bra Sizes
OAuth
Yes
Unknown


eBay
Sell and Buy on eBay
OAuth
Yes
Unknown


Wal-Mart
Item price and availability
apiKey
Yes
Unknown


Wegmans
Wegmans Food Markets
apiKey
Yes
Unknown



‚¨Ü Back to Index
Social



API
Description
Auth
HTTPS
CORS




Buffer
Access to pending and sent updates in Buffer
OAuth
Yes
Unknown


Cisco Spark
Team Collaboration Software
OAuth
Yes
Unknown


Discord
Make bots for Discord, integrate Discord onto an external platform
OAuth
Yes
Unknown


Disqus
Communicate with Disqus data
OAuth
Yes
Unknown


Facebook
Facebook Login, Share on FB, Social Plugins, Analytics and more
OAuth
Yes
Unknown


Foursquare
Interact with Foursquare users and places (geolocation-based checkins, photos, tips, events, etc)
OAuth
Yes
Unknown


Fuck Off as a Service
Asks someone to fuck off
No
Yes
Unknown


Full Contact
Get Social Media profiles and contact Information
OAuth
Yes
Unknown


HackerNews
Social news for CS and entrepreneurship
No
Yes
Unknown


Instagram
Instagram Login, Share on Instagram, Social Plugins and more
OAuth
Yes
Unknown


LinkedIn
The foundation of all digital integrations with LinkedIn
OAuth
Yes
Unknown


Meetup.com
Data about Meetups from Meetup.com
apiKey
Yes
Unknown


Mixer
Game Streaming API
OAuth
Yes
Unknown


MySocialApp
Seamless Social Networking features, API, SDK to any app
apiKey
Yes
Unknown


Open Collective
Get Open Collective data
No
Yes
Unknown


Pinterest
The world's catalog of ideas
OAuth
Yes
Unknown


PWRTelegram bot
Boosted version of the Telegram bot API
OAuth
Yes
Unknown


Reddit
Homepage of the internet
OAuth
Yes
Unknown


SharedCount
Social media like and share data for any URL
apiKey
Yes
Unknown


Slack
Team Instant Messaging
OAuth
Yes
Unknown


Telegram Bot
Simplified HTTP version of the MTProto API for bots
OAuth
Yes
Unknown


Telegram MTProto
Read and write Telegram data
OAuth
Yes
Unknown


Trash Nothing
A freecycling community with thousands of free items posted every day
OAuth
Yes
Yes


Tumblr
Read and write Tumblr Data
OAuth
Yes
Unknown


Twitch
Game Streaming API
OAuth
Yes
Unknown


Twitter
Read and write Twitter data
OAuth
Yes
No


vk
Read and write vk data
OAuth
Yes
Unknown



‚¨Ü Back to Index
Sports & Fitness



API
Description
Auth
HTTPS
CORS




balldontlie
Ballldontlie provides access to stats data from the NBA
No
Yes
Yes


BikeWise
Bikewise is a place to learn about and report bike crashes, hazards and thefts
No
Yes
Unknown


Canadian Football League (CFL)
Official JSON API providing real-time league, team and player statistics about the CFL
apiKey
Yes
No


Cartola FC
The Cartola FC API serves to check the partial points of your team
No
Yes
Unknown


City Bikes
City Bikes around the world
No
No
Unknown


Cricket Live Scores
Live cricket scores
X-Mashape-Key
Yes
Unknown


Ergast F1
F1 data from the beginning of the world championships in 1950
No
Yes
Unknown


Fitbit
Fitbit Information
OAuth
Yes
Unknown


Football (Soccer) Videos
Embed codes for goals and highlights from Premier League, Bundesliga, Serie A and many more
No
Yes
Yes


Football Prediction
Predictions for upcoming football matches, odds, results and stats
X-Mashape-Key
Yes
Unknown


Football-Data.org
Football Data
No
No
Unknown


JCDecaux Bike
JCDecaux's self-service bicycles
apiKey
Yes
Unknown


NBA Stats
Current and historical NBA Statistics
No
Yes
Unknown


NFL Arrests
NFL Arrest Data
No
No
Unknown


NHL Records and Stats
NHL historical data and statistics
No
Yes
Unknown


Pro Motocross
The RESTful AMA Pro Motocross lap times for every racer on the start gate
No
No
Unknown


Strava
Connect with athletes, activities and more
OAuth
Yes
Unknown


SuredBits
Query sports data, including teams, players, games, scores and statistics
No
No
No


TheSportsDB
Crowd-Sourced Sports Data and Artwork
apiKey
Yes
Yes


Wger
Workout manager data as exercises, muscles or equipment
apiKey
Yes
Unknown



‚¨Ü Back to Index
Test Data



API
Description
Auth
HTTPS
CORS




Adorable Avatars
Generate random cartoon avatars
No
Yes
Unknown


Bacon Ipsum
A Meatier Lorem Ipsum Generator
No
Yes
Unknown


Dicebear Avatars
Generate random pixel-art avatars
No
Yes
No


FakeJSON
Service to generate test and fake data
apiKey
Yes
Yes


FHIR
Fast Healthcare Interoperability Resources test data
No
Yes
Unknown


Hipster Ipsum
Generates Hipster Ipsum text
No
No
Unknown


Identicon
Generates abstract avatar image
No
Yes
Yes


JSONPlaceholder
Fake data for testing and prototyping
No
No
Unknown


Lorem Text
Generates Lorem Ipsum text
X-Mashape-Key
Yes
Unknown


LoremPicsum
Generate placeholder pictures
No
No
Unknown


Loripsum
The ""lorem ipsum"" generator that doesn't suck
No
No
Unknown


RandomUser
Generates random user data
No
Yes
Unknown


RoboHash
Generate random robot/alien avatars
No
Yes
Unknown


This Person Does not Exist
Generates real-life faces of people who do not exist
No
Yes
Unknown


UI Names
Generate random fake names
No
Yes
Unknown


Yes No
Generate yes or no randomly
No
Yes
Unknown



‚¨Ü Back to Index
Text Analysis



API
Description
Auth
HTTPS
CORS




Aylien Text Analysis
A collection of information retrieval and natural language APIs
apiKey
Yes
Unknown


Cloudmersive Natural Language Processing
Natural language processing and text analysis
apiKey
Yes
Yes


Detect Language
Detects text language
apiKey
Yes
Unknown


Google Cloud Natural
Natural language understanding technology, including sentiment, entity and syntax analysis
apiKey
Yes
Unknown


Language Identification
Automatic language detection for any texts, supports over 175 languages
X-Mashape-Key
Yes
Unknown


Semantira
Text Analytics with sentiment analysis, categorization & named entity extraction
OAuth
Yes
Unknown


Watson Natural Language Understanding
Natural language processing for advanced text analysis
OAuth
Yes
Unknown



‚¨Ü Back to Index
Tracking



API
Description
Auth
HTTPS
CORS




Postmon
An API to query Brazilian ZIP codes and orders easily, quickly and free
No
No
Unknown


Sweden
Provides information about parcels in transport
apiKey
No
Unknown


UPS
Shipment and Address information
apiKey
Yes
Unknown


WhatPulse
Small application that measures your keyboard/mouse usage
No
Yes
Unknown



‚¨Ü Back to Index
Transportation



API
Description
Auth
HTTPS
CORS




ADS-B Exchange
Access real-time and historical data of any and all airborne aircraft
No
Yes
Unknown


AIS Hub
Real-time data of any marine and inland vessel equipped with AIS tracking system
apiKey
No
Unknown


AIS Web
Aeronautical information in digital media produced by the Department of Airspace Control (DECEA)
apiKey
No
Unknown


Amadeus Travel Innovation Sandbox
Travel Search - Limited usage
apiKey
Yes
Unknown


Bay Area Rapid Transit
Stations and predicted arrivals for BART
apiKey
No
Unknown


BlaBlaCar
Search car sharing trips
apiKey
Yes
Unknown


Community Transit
Transitland API
No
Yes
Unknown


Goibibo
API for travel search
apiKey
Yes
Unknown


GraphHopper
A-to-B routing with turn-by-turn instructions
apiKey
Yes
Unknown


Icelandic APIs
Open APIs that deliver services in or regarding Iceland
No
Yes
Unknown


Indian Railways
Indian Railways Information
apiKey
No
Unknown


Izi
Audio guide for travellers
apiKey
Yes
Unknown


Metro Lisboa
Delays in subway lines
No
No
No


Navitia
The open API for building cool stuff with transport data
apiKey
Yes
Unknown


REFUGE Restrooms
Provides safe restroom access for transgender, intersex and gender nonconforming individuals
No
Yes
Unknown


Schiphol Airport
Schiphol
apiKey
Yes
Unknown


TransitLand
Transit Aggregation
No
Yes
Unknown


Transport for Atlanta, US
Marta
No
No
Unknown


Transport for Auckland, New Zealand
Auckland Transport
No
Yes
Unknown


Transport for Belgium
Belgian transport API
No
Yes
Unknown


Transport for Berlin, Germany
Third-party VBB API
No
Yes
Unknown


Transport for Bordeaux, France
Bordeaux M√©tropole public transport and more (France)
apiKey
Yes
Unknown


Transport for Boston, US
MBTA API
No
No
Unknown


Transport for Budapest, Hungary
Budapest public transport API
No
Yes
Unknown


Transport for Chicago, US
CTA
No
No
Unknown


Transport for Czech Republic
Czech transport API
No
Yes
Unknown


Transport for Denver, US
RTD
No
No
Unknown


Transport for Finland
Finnish transport API
No
Yes
Unknown


Transport for Germany
Deutsche Bahn (DB) API
apiKey
No
Unknown


Transport for Grenoble, France
Grenoble public transport
No
No
No


Transport for Honolulu, US
Honolulu Transportation Information
apiKey
No
Unknown


Transport for India
India Public Transport API
apiKey
Yes
Unknown


Transport for Lisbon, Portugal
Data about buses routes, parking and traffic
apiKey
Yes
Unknown


Transport for London, England
TfL API
No
Yes
Unknown


Transport for Madrid, Spain
Madrid BUS transport API
apiKey
No
Unknown


Transport for Manchester, England
TfGM transport network data
apiKey
Yes
No


Transport for Minneapolis, US
NexTrip API
OAuth
No
Unknown


Transport for New York City, US
MTA
apiKey
No
Unknown


Transport for Norway
Norwegian transport API
No
No
Unknown


Transport for Ottawa, Canada
OC Transpo next bus arrival API
No
No
Unknown


Transport for Paris, France
Live schedules made simple
No
No
Unknown


Transport for Paris, France
RATP Open Data API
No
No
Unknown


Transport for Philadelphia, US
SEPTA APIs
No
No
Unknown


Transport for Sao Paulo, Brazil
SPTrans
OAuth
No
Unknown


Transport for Sweden
Public Transport consumer
OAuth
Yes
Unknown


Transport for Switzerland
Official Swiss Public Transport Open Data
apiKey
Yes
Unknown


Transport for Switzerland
Swiss public transport API
No
Yes
Unknown


Transport for The Netherlands
NS, only trains
apiKey
No
Unknown


Transport for The Netherlands
OVAPI, country-wide public transport
No
Yes
Unknown


Transport for Toronto, Canada
TTC
No
Yes
Unknown


Transport for United States
NextBus API
No
No
Unknown


Transport for Vancouver, Canada
TransLink
OAuth
Yes
Unknown


Transport for Victoria, AU
PTV API
apiKey
Yes
Unknown


Transport for Washington, US
Washington Metro transport API
OAuth
Yes
Unknown


Uber
Uber ride requests and price estimation
OAuth
Yes
Yes


WhereIsMyTransport
Platform for public transport data in emerging cities
OAuth
Yes
Unknown



‚¨Ü Back to Index
URL Shorteners



API
Description
Auth
HTTPS
CORS




Bitly
URL shortener and link management
OAuth
Yes
Unknown


CleanURI
URL shortener service
No
Yes
Yes


ClickMeter
Monitor, compare and optimize your marketing links
apiKey
Yes
Unknown


Rebrandly
Custom URL shortener for sharing branded links
apiKey
Yes
Unknown


Relink
Free and secure URL shortener
No
Yes
Yes



‚¨Ü Back to Index
Vehicle



API
Description
Auth
HTTPS
CORS




Brazilian Vehicles and Prices
Vehicles information from Funda√ß√£o Instituto de Pesquisas Econ√¥micas - Fipe
No
Yes
Unknown


Kelley Blue Book
Vehicle info, pricing, configuration, plus much more
apiKey
Yes
No


Mercedes-Benz
Telematics data, remotely access vehicle functions, car configurator, locate service dealers
apiKey
Yes
No


NHTSA
NHTSA Product Information Catalog and Vehicle Listing
No
Yes
Unknown


Smartcar
Lock and unlock vehicles and get data like odometer reading and location. Works on most new cars
OAuth
Yes
Yes



‚¨Ü Back to Index
Video



API
Description
Auth
HTTPS
CORS




An API of Ice And Fire
Game Of Thrones API
No
Yes
Unknown


Breaking Bad
Breaking Bad API
No
Yes
Unknown


Breaking Bad Quotes
Some Breaking Bad quotes
No
Yes
Unknown


Czech Television
TV programme of Czech TV
No
No
Unknown


Dailymotion
Dailymotion Developer API
OAuth
Yes
Unknown


Harry Potter
Harry Potter API
apiKey
Yes
Yes


Open Movie Database
Movie information
apiKey
Yes
Unknown


Ron Swanson Quotes
Television
No
Yes
Unknown


STAPI
Information on all things Star Trek
No
No
No


SWAPI
Star Wars Information
No
Yes
Unknown


The Lord of the Rings
The Lord of the Rings API
apiKey
Yes
Unknown


TMDb
Community-based movie data
apiKey
Yes
Unknown


Trakt
Movie and TV Data
apiKey
Yes
Yes


TVDB
Television data
apiKey
Yes
Unknown


TVMaze
TV Show Data
No
No
Unknown


Utelly
Check where a tv show or movie is available
X-Mashape-Key
Yes
Unknown


Vimeo
Vimeo Developer API
OAuth
Yes
Unknown


YouTube
Add YouTube functionality to your sites and apps
OAuth
Yes
Unknown



‚¨Ü Back to Index
Weather



API
Description
Auth
HTTPS
CORS




7Timer!
Weather, especially for Astroweather
No
No
Unknown


APIXU
Weather
apiKey
Yes
Unknown


Dark Sky
Weather
apiKey
Yes
No


MetaWeather
Weather
No
Yes
No


Meteorologisk Institutt
Weather and climate data
No
Yes
Unknown


NOAA Climate Data
Weather and climate data
apiKey
Yes
Unknown


ODWeather
Weather and weather webcams
No
No
Unknown


OpenUV
Real-time UV Index Forecast
apiKey
Yes
Unknown


OpenWeatherMap
Weather
apiKey
No
Unknown


Storm Glass
Global marine weather from multiple sources
apiKey
Yes
Yes


Weatherbit
Weather
apiKey
Yes
Unknown


Yahoo! Weather
Weather
apiKey
Yes
Unknown



‚¨Ü Back to Index
",GitHub - public-apis/public-apis: A collective list of free APIs for use in software and web development.
3,Python,"The Algorithms - Python
¬†
¬†
¬†
¬†
¬†
¬†
All algorithms implemented in Python (for education)
These implementations are for learning purposes. They may be less efficient than the implementations in the Python standard library.
Contribution Guidelines
Read our Contribution Guidelines before you contribute.
Community Channel
We're on Gitter! Please join us.
List of Algorithms
See our directory.

",GitHub - TheAlgorithms/Python: All Algorithms implemented in Python
4,Python,"The Fuck     
The Fuck is a magnificent app, inspired by a @liamosaur
tweet,
that corrects errors in previous console commands.
Is The Fuck too slow? Try the experimental instant mode!

More examples:
‚ûú apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

‚ûú fuck
sudo apt-get install vim [enter/‚Üë/‚Üì/ctrl+c]
[sudo] password for nvbn:
Reading package lists... Done
...
‚ûú git push
fatal: The current branch master has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin master


‚ûú fuck
git push --set-upstream origin master [enter/‚Üë/‚Üì/ctrl+c]
Counting objects: 9, done.
...
‚ûú puthon
No command 'puthon' found, did you mean:
 Command 'python' from package 'python-minimal' (main)
 Command 'python' from package 'python3' (main)
zsh: command not found: puthon

‚ûú fuck
python [enter/‚Üë/‚Üì/ctrl+c]
Python 3.4.2 (default, Oct  8 2014, 13:08:17)
...
‚ûú git brnch
git: 'brnch' is not a git command. See 'git --help'.

Did you mean this?
    branch

‚ûú fuck
git branch [enter/‚Üë/‚Üì/ctrl+c]
* master
‚ûú lein rpl
'rpl' is not a task. See 'lein help'.

Did you mean this?
         repl

‚ûú fuck
lein repl [enter/‚Üë/‚Üì/ctrl+c]
nREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848
REPL-y 0.3.1
...
If you're not afraid of blindly running corrected commands, the
require_confirmation settings option can be disabled:
‚ûú apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

‚ûú fuck
sudo apt-get install vim
[sudo] password for nvbn:
Reading package lists... Done
...
Requirements

python (3.4+)
pip
python-dev

Installation
On OS X, you can install The Fuck via Homebrew (or via Linuxbrew on Linux):
brew install thefuck
On Ubuntu / Mint, install The Fuck with the following commands:
sudo apt update
sudo apt install python3-dev python3-pip python3-setuptools
sudo pip3 install thefuck
On FreeBSD, install The Fuck with the following commands:
pkg install thefuck
On ChromeOS, install The Fuck using chromebrew with the following command:
crew install thefuck
On other systems, install The Fuck  by using pip:
pip install thefuck
Alternatively, you may use an OS package manager (OS X, Ubuntu, Arch).
#
It is recommended that you place this command in your .bash_profile,
.bashrc, .zshrc or other startup script:
eval $(thefuck --alias)
# You can use whatever you want as an alias, like for Mondays:
eval $(thefuck --alias FUCK)
Or in your shell config (Bash, Zsh, Fish, Powershell, tcsh).
Changes are only available in a new shell session. To make changes immediately
available, run source ~/.bashrc (or your shell config file like .zshrc).
To run fixed commands without confirmation, use the --yeah option (or just -y for short, or --hard if you're especially frustrated):
fuck --yeah
To fix commands recursively until succeeding, use the -r option:
fuck -r
Updating
pip3 install thefuck --upgrade
Note: Alias functionality was changed in v1.34 of The Fuck
How it works
The Fuck attempts to match the previous command with a rule. If a match is
found, a new command is created using the matched rule and executed. The
following rules are enabled by default:

adb_unknown_command ‚Äì fixes misspelled commands like adb logcta;
ag_literal ‚Äì adds -Q to ag when suggested;
aws_cli ‚Äì fixes misspelled commands like aws dynamdb scan;
az_cli ‚Äì fixes misspelled commands like az providers;
cargo ‚Äì runs cargo build instead of cargo;
cargo_no_command ‚Äì fixes wrongs commands like cargo buid;
cat_dir ‚Äì replaces cat with ls when you try to cat a directory;
cd_correction ‚Äì spellchecks and correct failed cd commands;
cd_mkdir ‚Äì creates directories before cd'ing into them;
cd_parent ‚Äì changes cd.. to cd ..;
chmod_x ‚Äì add execution bit;
choco_install ‚Äì append common suffixes for chocolatey packages;
composer_not_command ‚Äì fixes composer command name;
cp_omitting_directory ‚Äì adds -a when you cp directory;
cpp11 ‚Äì adds missing -std=c++11 to g++ or clang++;
dirty_untar ‚Äì fixes tar x command that untarred in the current directory;
dirty_unzip ‚Äì fixes unzip command that unzipped in the current directory;
django_south_ghost ‚Äì adds --delete-ghost-migrations to failed because ghosts django south migration;
django_south_merge ‚Äì adds --merge to inconsistent django south migration;
docker_login ‚Äì executes a docker login and repeats the previous command;
docker_not_command ‚Äì fixes wrong docker commands like docker tags;
docker_image_being_used_by_container ‚Äê removes the container that is using the image before removing the image;
dry ‚Äì fixes repetitions like git git push;
fab_command_not_found ‚Äì fix misspelled fabric commands;
fix_alt_space ‚Äì replaces Alt+Space with Space character;
fix_file ‚Äì opens a file with an error in your $EDITOR;
gem_unknown_command ‚Äì fixes wrong gem commands;
git_add ‚Äì fixes ""pathspec 'foo' did not match any file(s) known to git."";
git_add_force ‚Äì adds --force to git add <pathspec>... when paths are .gitignore'd;
git_bisect_usage ‚Äì fixes git bisect strt, git bisect goood, git bisect rset, etc. when bisecting;
git_branch_delete ‚Äì changes git branch -d to git branch -D;
git_branch_delete_checked_out ‚Äì changes git branch -d to git checkout master && git branch -D when trying to delete a checked out branch;
git_branch_exists ‚Äì offers git branch -d foo, git branch -D foo or git checkout foo when creating a branch that already exists;
git_branch_list ‚Äì catches git branch list in place of git branch and removes created branch;
git_checkout ‚Äì fixes branch name or creates new branch;
git_commit_amend ‚Äì offers git commit --amend after previous commit;
git_commit_reset ‚Äì offers git reset HEAD~ after previous commit;
git_diff_no_index ‚Äì adds --no-index to previous git diff on untracked files;
git_diff_staged ‚Äì adds --staged to previous git diff with unexpected output;
git_fix_stash ‚Äì fixes git stash commands (misspelled subcommand and missing save);
git_flag_after_filename ‚Äì fixes fatal: bad flag '...' after filename
git_help_aliased ‚Äì fixes git help <alias> commands replacing  with the aliased command;
git_merge ‚Äì adds remote to branch names;
git_merge_unrelated ‚Äì adds --allow-unrelated-histories when required
git_not_command ‚Äì fixes wrong git commands like git brnch;
git_pull ‚Äì sets upstream before executing previous git pull;
git_pull_clone ‚Äì clones instead of pulling when the repo does not exist;
git_pull_uncommitted_changes ‚Äì stashes changes before pulling and pops them afterwards;
git_push ‚Äì adds --set-upstream origin $branch to previous failed git push;
git_push_different_branch_names ‚Äì fixes pushes when local brach name does not match remote branch name;
git_push_pull ‚Äì runs git pull when push was rejected;
git_push_without_commits ‚Äì Creates an initial commit if you forget and only git add ., when setting up a new project;
git_rebase_no_changes ‚Äì runs git rebase --skip instead of git rebase --continue when there are no changes;
git_remote_delete ‚Äì replaces git remote delete remote_name with git remote remove remote_name;
git_rm_local_modifications ‚Äì  adds -f or --cached when you try to rm a locally modified file;
git_rm_recursive ‚Äì adds -r when you try to rm a directory;
git_rm_staged ‚Äì  adds -f or --cached when you try to rm a file with staged changes
git_rebase_merge_dir ‚Äì offers git rebase (--continue | --abort | --skip) or removing the .git/rebase-merge dir when a rebase is in progress;
git_remote_seturl_add ‚Äì runs git remote add when git remote set_url on nonexistent remote;
git_stash ‚Äì stashes your local modifications before rebasing or switching branch;
git_stash_pop ‚Äì adds your local modifications before popping stash, then resets;
git_tag_force ‚Äì adds --force to git tag <tagname> when the tag already exists;
git_two_dashes ‚Äì adds a missing dash to commands like git commit -amend or git rebase -continue;
go_run ‚Äì appends .go extension when compiling/running Go programs;
go_unknown_command ‚Äì fixes wrong go commands, for example go bulid;
gradle_no_task ‚Äì fixes not found or ambiguous gradle task;
gradle_wrapper ‚Äì replaces gradle with ./gradlew;
grep_arguments_order ‚Äì fixes grep arguments order for situations like grep -lir . test;
grep_recursive ‚Äì adds -r when you try to grep directory;
grunt_task_not_found ‚Äì fixes misspelled grunt commands;
gulp_not_task ‚Äì fixes misspelled gulp tasks;
has_exists_script ‚Äì prepends ./ when script/binary exists;
heroku_multiple_apps ‚Äì add --app <app> to heroku commands like heroku pg;
heroku_not_command ‚Äì fixes wrong heroku commands like heroku log;
history ‚Äì tries to replace command with most similar command from history;
hostscli ‚Äì tries to fix hostscli usage;
ifconfig_device_not_found ‚Äì fixes wrong device names like wlan0 to wlp2s0;
java ‚Äì removes .java extension when running Java programs;
javac ‚Äì appends missing .java when compiling Java files;
lein_not_task ‚Äì fixes wrong lein tasks like lein rpl;
long_form_help ‚Äì changes -h to --help when the short form version is not supported
ln_no_hard_link ‚Äì catches hard link creation on directories, suggest symbolic link;
ln_s_order ‚Äì fixes ln -s arguments order;
ls_all ‚Äì adds -A to ls when output is empty;
ls_lah ‚Äì adds -lah to ls;
man ‚Äì changes manual section;
man_no_space ‚Äì fixes man commands without spaces, for example mandiff;
mercurial ‚Äì fixes wrong hg commands;
missing_space_before_subcommand ‚Äì fixes command with missing space like npminstall;
mkdir_p ‚Äì adds -p when you try to create a directory without parent;
mvn_no_command ‚Äì adds clean package to mvn;
mvn_unknown_lifecycle_phase ‚Äì fixes misspelled life cycle phases with mvn;
npm_missing_script ‚Äì fixes npm custom script name in npm run-script <script>;
npm_run_script ‚Äì adds missing run-script for custom npm scripts;
npm_wrong_command ‚Äì fixes wrong npm commands like npm urgrade;
no_command ‚Äì fixes wrong console commands, for example vom/vim;
no_such_file ‚Äì creates missing directories with mv and cp commands;
open ‚Äì either prepends http:// to address passed to open or create a new file or directory and passes it to open;
pip_install ‚Äì fixes permission issues with pip install commands by adding --user or prepending sudo if necessary;
pip_unknown_command ‚Äì fixes wrong pip commands, for example pip instatl/pip install;
php_s ‚Äì replaces -s by -S when trying to run a local php server;
port_already_in_use ‚Äì kills process that bound port;
prove_recursively ‚Äì adds -r when called with directory;
pyenv_no_such_command ‚Äì fixes wrong pyenv commands like pyenv isntall or pyenv list;
python_command ‚Äì prepends python when you try to run non-executable/without ./ python script;
python_execute ‚Äì appends missing .py when executing Python files;
quotation_marks ‚Äì fixes uneven usage of ' and "" when containing args';
path_from_history ‚Äì replaces not found path with similar absolute path from history;
react_native_command_unrecognized ‚Äì fixes unrecognized react-native commands;
remove_shell_prompt_literal ‚Äì remove leading shell prompt symbol $, common when copying commands from documentations;
remove_trailing_cedilla ‚Äì remove trailing cedillas √ß, a common typo for european keyboard layouts;
rm_dir ‚Äì adds -rf when you try to remove a directory;
scm_correction ‚Äì corrects wrong scm like hg log to git log;
sed_unterminated_s ‚Äì adds missing '/' to sed's s commands;
sl_ls ‚Äì changes sl to ls;
ssh_known_hosts ‚Äì removes host from known_hosts on warning;
sudo ‚Äì prepends sudo to previous command if it failed because of permissions;
sudo_command_from_user_path ‚Äì runs commands from users $PATH with sudo;
switch_lang ‚Äì switches command from your local layout to en;
systemctl ‚Äì correctly orders parameters of confusing systemctl;
terraform_init.py ‚Äì run terraform init before plan or apply;
test.py ‚Äì runs py.test instead of test.py;
touch ‚Äì creates missing directories before ""touching"";
tsuru_login ‚Äì runs tsuru login if not authenticated or session expired;
tsuru_not_command ‚Äì fixes wrong tsuru commands like tsuru shell;
tmux ‚Äì fixes tmux commands;
unknown_command ‚Äì fixes hadoop hdfs-style ""unknown command"", for example adds missing '-' to the command on hdfs dfs ls;
unsudo ‚Äì removes sudo from previous command if a process refuses to run on super user privilege.
vagrant_up ‚Äì starts up the vagrant instance;
whois ‚Äì fixes whois command;
workon_doesnt_exists ‚Äì fixes virtualenvwrapper env name os suggests to create new.
yarn_alias ‚Äì fixes aliased yarn commands like yarn ls;
yarn_command_not_found ‚Äì fixes misspelled yarn commands;
yarn_command_replaced ‚Äì fixes replaced yarn commands;
yarn_help ‚Äì makes it easier to open yarn documentation;

The following rules are enabled by default on specific platforms only:

apt_get ‚Äì installs app from apt if it not installed (requires python-commandnotfound / python3-commandnotfound);
apt_get_search ‚Äì changes trying to search using apt-get with searching using apt-cache;
apt_invalid_operation ‚Äì fixes invalid apt and apt-get calls, like apt-get isntall vim;
apt_list_upgradable ‚Äì helps you run apt list --upgradable after apt update;
apt_upgrade ‚Äì helps you run apt upgrade after apt list --upgradable;
brew_cask_dependency ‚Äì installs cask dependencies;
brew_install ‚Äì fixes formula name for brew install;
brew_reinstall ‚Äì turns brew install <formula> into brew reinstall <formula>;
brew_link ‚Äì adds --overwrite --dry-run if linking fails;
brew_uninstall ‚Äì adds --force to brew uninstall if multiple versions were installed;
brew_unknown_command ‚Äì fixes wrong brew commands, for example brew docto/brew doctor;
brew_update_formula ‚Äì turns brew update <formula> into brew upgrade <formula>;
dnf_no_such_command ‚Äì fixes mistyped DNF commands;
nixos_cmd_not_found ‚Äì installs apps on NixOS;
pacman ‚Äì installs app with pacman if it is not installed (uses yay or yaourt if available);
pacman_not_found ‚Äì fixes package name with pacman, yay or yaourt.
yum_invalid_operation ‚Äì fixes invalid yum calls, like yum isntall vim;

The following commands are bundled with The Fuck, but are not enabled by
default:

git_push_force ‚Äì adds --force-with-lease to a git push (may conflict with git_push_pull);
rm_root ‚Äì adds --no-preserve-root to rm -rf / command.

Creating your own rules
To add your own rule, create a file named your-rule-name.py
in ~/.config/thefuck/rules. The rule file must contain two functions:
match(command: Command) -> bool
get_new_command(command: Command) -> str | list[str]
Additionally, rules can contain optional functions:
side_effect(old_command: Command, fixed_command: str) -> None
Rules can also contain the optional variables enabled_by_default, requires_output and priority.
Command has three attributes: script, output and script_parts.
Your rule should not change Command.
Rules api changed in 3.0: To access a rule's settings, import it with
from thefuck.conf import settings
settings is a special object assembled from ~/.config/thefuck/settings.py,
and values from env (see more below).
A simple example rule for running a script with sudo:
def match(command):
    return ('permission denied' in command.output.lower()
            or 'EACCES' in command.output)


def get_new_command(command):
    return 'sudo {}'.format(command.script)

# Optional:
enabled_by_default = True

def side_effect(command, fixed_command):
    subprocess.call('chmod 777 .', shell=True)

priority = 1000  # Lower first, default is 1000

requires_output = True
More examples of rules,
utility functions for rules,
app/os-specific helpers.
Settings
Several The Fuck parameters can be changed in the file $XDG_CONFIG_HOME/thefuck/settings.py
($XDG_CONFIG_HOME defaults to ~/.config):

rules ‚Äì list of enabled rules, by default thefuck.const.DEFAULT_RULES;
exclude_rules ‚Äì list of disabled rules, by default [];
require_confirmation ‚Äì requires confirmation before running new command, by default True;
wait_command ‚Äì max amount of time in seconds for getting previous command output;
no_colors ‚Äì disable colored output;
priority ‚Äì dict with rules priorities, rule with lower priority will be matched first;
debug ‚Äì enables debug output, by default False;
history_limit ‚Äì numeric value of how many history commands will be scanned, like 2000;
alter_history ‚Äì push fixed command to history, by default True;
wait_slow_command ‚Äì max amount of time in seconds for getting previous command output if it in slow_commands list;
slow_commands ‚Äì list of slow commands;
num_close_matches ‚Äì maximum number of close matches to suggest, by default 3.

An example of settings.py:
rules = ['sudo', 'no_command']
exclude_rules = ['git_push']
require_confirmation = True
wait_command = 10
no_colors = False
priority = {'sudo': 100, 'no_command': 9999}
debug = False
history_limit = 9999
wait_slow_command = 20
slow_commands = ['react-native', 'gradle']
num_close_matches = 5
Or via environment variables:

THEFUCK_RULES ‚Äì list of enabled rules, like DEFAULT_RULES:rm_root or sudo:no_command;
THEFUCK_EXCLUDE_RULES ‚Äì list of disabled rules, like git_pull:git_push;
THEFUCK_REQUIRE_CONFIRMATION ‚Äì require confirmation before running new command, true/false;
THEFUCK_WAIT_COMMAND ‚Äì max amount of time in seconds for getting previous command output;
THEFUCK_NO_COLORS ‚Äì disable colored output, true/false;
THEFUCK_PRIORITY ‚Äì priority of the rules, like no_command=9999:apt_get=100,
rule with lower priority will be matched first;
THEFUCK_DEBUG ‚Äì enables debug output, true/false;
THEFUCK_HISTORY_LIMIT ‚Äì how many history commands will be scanned, like 2000;
THEFUCK_ALTER_HISTORY ‚Äì push fixed command to history true/false;
THEFUCK_WAIT_SLOW_COMMAND ‚Äì max amount of time in seconds for getting previous command output if it in slow_commands list;
THEFUCK_SLOW_COMMANDS ‚Äì list of slow commands, like lein:gradle;
THEFUCK_NUM_CLOSE_MATCHES ‚Äì maximum number of close matches to suggest, like 5.

For example:
export THEFUCK_RULES='sudo:no_command'
export THEFUCK_EXCLUDE_RULES='git_pull:git_push'
export THEFUCK_REQUIRE_CONFIRMATION='true'
export THEFUCK_WAIT_COMMAND=10
export THEFUCK_NO_COLORS='false'
export THEFUCK_PRIORITY='no_command=9999:apt_get=100'
export THEFUCK_HISTORY_LIMIT='2000'
export THEFUCK_NUM_CLOSE_MATCHES='5'
Third-party packages with rules
If you'd like to make a specific set of non-public rules, but would still like
to share them with others, create a package named thefuck_contrib_* with
the following structure:
thefuck_contrib_foo
  thefuck_contrib_foo
    rules
      __init__.py
      *third-party rules*
    __init__.py
    *third-party-utils*
  setup.py

The Fuck will find rules located in the rules module.
Experimental instant mode
The default behavior of The Fuck requires time to re-run previous commands.
When in instant mode, The Fuck saves time by logging output with script,
then reading the log.

Currently, instant mode only supports Python 3 with bash or zsh. zsh's autocorrect function also needs to be disabled in order for thefuck to work properly.
To enable instant mode, add --enable-experimental-instant-mode
to the alias initialization in .bashrc, .bash_profile or .zshrc.
For example:
eval $(thefuck --alias --enable-experimental-instant-mode)
Developing
See CONTRIBUTING.md
License MIT
Project License can be found here.
",GitHub - nvbn/thefuck: Magnificent app which corrects your previous console command.
5,Python,"HTTPie: a CLI, cURL-like tool for humans
HTTPie (pronounced aitch-tee-tee-pie) is a command line HTTP client.
Its goal is to make CLI interaction with web services as human-friendly
as possible. It provides a simple http command that allows for sending
arbitrary HTTP requests using a simple and natural syntax, and displays
colorized output. HTTPie can be used for testing, debugging, and
generally interacting with HTTP servers.
 
   


Contents

1¬†¬†¬†Main features
2¬†¬†¬†Installation
2.1¬†¬†¬†macOS
2.2¬†¬†¬†Linux
2.3¬†¬†¬†Windows, etc.
2.4¬†¬†¬†Python version
2.5¬†¬†¬†Unstable version


3¬†¬†¬†Usage
3.1¬†¬†¬†Examples


4¬†¬†¬†HTTP method
5¬†¬†¬†Request URL
5.1¬†¬†¬†Querystring parameters
5.2¬†¬†¬†URL shortcuts for localhost
5.3¬†¬†¬†Other default schemes


6¬†¬†¬†Request items
6.1¬†¬†¬†Escaping rules


7¬†¬†¬†JSON
7.1¬†¬†¬†Default behaviour
7.2¬†¬†¬†Explicit JSON
7.3¬†¬†¬†Non-string JSON fields


8¬†¬†¬†Forms
8.1¬†¬†¬†Regular forms
8.2¬†¬†¬†File upload forms


9¬†¬†¬†HTTP headers
9.1¬†¬†¬†Default request headers
9.2¬†¬†¬†Empty headers and header un-setting
9.3¬†¬†¬†Limiting response headers


10¬†¬†¬†Cookies
11¬†¬†¬†Authentication
11.1¬†¬†¬†Basic auth
11.2¬†¬†¬†Digest auth
11.3¬†¬†¬†Password prompt
11.4¬†¬†¬†.netrc
11.5¬†¬†¬†Auth plugins


12¬†¬†¬†HTTP redirects
12.1¬†¬†¬†Follow Location
12.2¬†¬†¬†Showing intermediary redirect responses
12.3¬†¬†¬†Limiting maximum redirects followed


13¬†¬†¬†Proxies
13.1¬†¬†¬†Environment variables
13.2¬†¬†¬†SOCKS


14¬†¬†¬†HTTPS
14.1¬†¬†¬†Server SSL certificate verification
14.2¬†¬†¬†Custom CA bundle
14.3¬†¬†¬†Client side SSL certificate
14.4¬†¬†¬†SSL version


15¬†¬†¬†Output options
15.1¬†¬†¬†What parts of the HTTP exchange should be printed
15.2¬†¬†¬†Viewing intermediary requests/responses
15.3¬†¬†¬†Conditional body download


16¬†¬†¬†Redirected Input
16.1¬†¬†¬†Request data from a filename


17¬†¬†¬†Terminal output
17.1¬†¬†¬†Colors and formatting
17.2¬†¬†¬†Binary data


18¬†¬†¬†Redirected output
19¬†¬†¬†Download mode
19.1¬†¬†¬†Downloaded filename
19.2¬†¬†¬†Piping while downloading
19.3¬†¬†¬†Resuming downloads
19.4¬†¬†¬†Other notes


20¬†¬†¬†Streamed responses
20.1¬†¬†¬†Disabling buffering
20.2¬†¬†¬†Examples use cases


21¬†¬†¬†Sessions
21.1¬†¬†¬†Named sessions
21.2¬†¬†¬†Anonymous sessions
21.3¬†¬†¬†Readonly session


22¬†¬†¬†Config
22.1¬†¬†¬†Config file directory
22.2¬†¬†¬†Configurable options
22.2.1¬†¬†¬†default_options


22.3¬†¬†¬†Un-setting previously specified options


23¬†¬†¬†Scripting
23.1¬†¬†¬†Best practices


24¬†¬†¬†Meta
24.1¬†¬†¬†Interface design
24.2¬†¬†¬†User support
24.3¬†¬†¬†Related projects
24.3.1¬†¬†¬†Dependencies
24.3.2¬†¬†¬†HTTPie friends
24.3.3¬†¬†¬†Alternatives


24.4¬†¬†¬†Contributing
24.5¬†¬†¬†Change log
24.6¬†¬†¬†Artwork
24.7¬†¬†¬†Licence
24.8¬†¬†¬†Authors





1¬†¬†¬†Main features

Expressive and intuitive syntax
Formatted and colorized terminal output
Built-in JSON support
Forms and file uploads
HTTPS, proxies, and authentication
Arbitrary request data
Custom headers
Persistent sessions
Wget-like downloads
Linux, macOS and Windows support
Plugins
Documentation
Test coverage



2¬†¬†¬†Installation

2.1¬†¬†¬†macOS
On macOS, HTTPie can be installed via Homebrew
(recommended):
$ brew install httpie
A MacPorts port is also available:
$ port install httpie

2.2¬†¬†¬†Linux
Most Linux distributions provide a package that can be installed using the
system package manager, for example:
# Debian, Ubuntu, etc.
$ apt-get install httpie
# Fedora
$ dnf install httpie
# CentOS, RHEL, ...
$ yum install httpie
# Arch Linux
$ pacman -S httpie

2.3¬†¬†¬†Windows, etc.
A universal installation method (that works on Windows, Mac OS X, Linux, ‚Ä¶,
and always provides the latest version) is to use pip:
# Make sure we have an up-to-date version of pip and setuptools:
$ pip install --upgrade pip setuptools

$ pip install --upgrade httpie
(If pip installation fails for some reason, you can try
easy_install httpie as a fallback.)

2.4¬†¬†¬†Python version
Starting with version 2.0.0 (currently under development) Python 3.6+ is required.

2.5¬†¬†¬†Unstable version
You can also install the latest unreleased development version directly from
the master branch on GitHub.  It is a work-in-progress of a future stable
release so the experience might be not as smooth.


On macOS you can install it with Homebrew:
$ brew install httpie --HEAD
Otherwise with pip:
$ pip install --upgrade https://github.com/jakubroztocil/httpie/archive/master.tar.gz
Verify that now we have the
current development version identifier
with the -dev suffix, for example:
$ http --version
1.0.0-dev

3¬†¬†¬†Usage
Hello World:
$ http httpie.org
Synopsis:
$ http [flags] [METHOD] URL [ITEM [ITEM]]
See also http --help.

3.1¬†¬†¬†Examples
Custom HTTP method, HTTP headers and JSON data:
$ http PUT example.org X-API-Token:123 name=John
Submitting forms:
$ http -f POST example.org hello=World
See the request that is being sent using one of the output options:
$ http -v example.org
Use Github API to post a comment on an
issue
with authentication:
$ http -a USERNAME POST https://api.github.com/repos/jakubroztocil/httpie/issues/83/comments body='HTTPie is awesome! :heart:'
Upload a file using redirected input:
$ http example.org < file.json
Download a file and save it via redirected output:
$ http example.org/file > file
Download a file wget style:
$ http --download example.org/file
Use named sessions to make certain aspects or the communication persistent
between requests to the same host:
$ http --session=logged-in -a username:password httpbin.org/get API-Key:123

$ http --session=logged-in httpbin.org/headers
Set a custom Host header to work around missing DNS records:
$ http localhost:8000 Host:example.com

4¬†¬†¬†HTTP method
The name of the HTTP method comes right before the URL argument:
$ http DELETE example.org/todos/7
Which looks similar to the actual Request-Line that is sent:
DELETE /todos/7 HTTP/1.1
When the METHOD argument is omitted from the command, HTTPie defaults to
either GET (with no request data) or POST (with request data).

5¬†¬†¬†Request URL
The only information HTTPie needs to perform a request is a URL.
The default scheme is, somewhat unsurprisingly, http://,
and can be omitted from the argument ‚Äì http example.org works just fine.

5.1¬†¬†¬†Querystring parameters
If you find yourself manually constructing URLs with querystring parameters
on the terminal, you may appreciate the param==value syntax for appending
URL parameters.
With that, you don't have to worry about escaping the &
separators for your shell. Additionally, any special characters in the
parameter name or value get automatically URL-escaped
(as opposed to parameters specified in the full URL, which HTTPie doesn‚Äôt
modify).
$ http https://api.github.com/search/repositories q==httpie per_page==1
GET /search/repositories?q=httpie&per_page=1 HTTP/1.1

5.2¬†¬†¬†URL shortcuts for localhost
Additionally, curl-like shorthand for localhost is supported.
This means that, for example :3000 would expand to http://localhost:3000
If the port is omitted, then port 80 is assumed.
$ http :/foo
GET /foo HTTP/1.1
Host: localhost
$ http :3000/bar
GET /bar HTTP/1.1
Host: localhost:3000
$ http :
GET / HTTP/1.1
Host: localhost

5.3¬†¬†¬†Other default schemes
When HTTPie is invoked as https then the default scheme is https://
($ https example.org will make a request to https://example.org).
You can also use the --default-scheme <URL_SCHEME> option to create
shortcuts for other protocols than HTTP (possibly supported via plugins).
Example for the httpie-unixsocket plugin:
# Before
$ http http+unix://%2Fvar%2Frun%2Fdocker.sock/info
# Create an alias
$ alias http-unix='http --default-scheme=""http+unix""'
# Now the scheme can be omitted
$ http-unix %2Fvar%2Frun%2Fdocker.sock/info

6¬†¬†¬†Request items
There are a few different request item types that provide a
convenient mechanism for specifying HTTP headers, simple JSON and
form data, files, and URL parameters.
They are key/value pairs specified after the URL. All have in
common that they become part of the actual request that is sent and that
their type is distinguished only by the separator used:
:, =, :=, ==, @, =@, and :=@. The ones with an
@ expect a file path as value.


Item Type
Description



HTTP Headers
Name:Value
Arbitrary HTTP header, e.g. X-API-Token:123.

URL parameters
name==value
Appends the given name/value pair as a query
string parameter to the URL.
The == separator is used.

Data Fields
field=value,
field=@file.txt
Request data fields to be serialized as a JSON
object (default), or to be form-encoded
(--form, -f).

Raw JSON fields
field:=json,
field:=@file.json
Useful when sending JSON and one or
more fields need to be a Boolean, Number,
nested Object, or an Array,  e.g.,
meals:='[""ham"",""spam""]' or pies:=[1,2,3]
(note the quotes).

Form File Fields
field@/dir/file
Only available with --form, -f.
For example screenshot@~/Pictures/img.png.
The presence of a file field results
in a multipart/form-data request.



Note that data fields aren't the only way to specify request data:
Redirected input is a mechanism for passing arbitrary request data.

6.1¬†¬†¬†Escaping rules
You can use \ to escape characters that shouldn't be used as separators
(or parts thereof). For instance, foo\==bar will become a data key/value
pair (foo= and bar) instead of a URL parameter.
Often it is necessary to quote the values, e.g. foo='bar baz'.
If any of the field names or headers starts with a minus
(e.g., -fieldname), you need to place all such items after the special
token -- to prevent confusion with --arguments:
$ http httpbin.org/post  --  -name-starting-with-dash=foo -Unusual-Header:bar
POST /post HTTP/1.1
-Unusual-Header: bar
Content-Type: application/json

{
    ""-name-starting-with-dash"": ""foo""
}

7¬†¬†¬†JSON
JSON is the lingua franca of modern web services and it is also the
implicit content type HTTPie uses by default.
Simple example:
$ http PUT example.org name=John email=john@example.org
PUT / HTTP/1.1
Accept: application/json, */*
Accept-Encoding: gzip, deflate
Content-Type: application/json
Host: example.org

{
    ""name"": ""John"",
    ""email"": ""john@example.org""
}

7.1¬†¬†¬†Default behaviour
If your command includes some data request items, they are serialized as a JSON
object by default. HTTPie also automatically sets the following headers,
both of which can be overwritten:


Content-Type
application/json

Accept
application/json, */*




7.2¬†¬†¬†Explicit JSON
You can use --json, -j to explicitly set Accept
to application/json regardless of whether you are sending data
(it's a shortcut for setting the header via the usual header notation:
http url Accept:'application/json, */*'). Additionally,
HTTPie will try to detect JSON responses even when the
Content-Type is incorrectly text/plain or unknown.

7.3¬†¬†¬†Non-string JSON fields
Non-string fields use the := separator, which allows you to embed raw JSON
into the resulting object. Text and raw JSON files can also be embedded into
fields using =@ and :=@:
$ http PUT api.example.com/person/1 \
    name=John \
    age:=29 married:=false hobbies:='[""http"", ""pies""]' \  # Raw JSON
    description=@about-john.txt \   # Embed text file
    bookmarks:=@bookmarks.json      # Embed JSON file
PUT /person/1 HTTP/1.1
Accept: application/json, */*
Content-Type: application/json
Host: api.example.com

{
    ""age"": 29,
    ""hobbies"": [
        ""http"",
        ""pies""
    ],
    ""description"": ""John is a nice guy who likes pies."",
    ""married"": false,
    ""name"": ""John"",
    ""bookmarks"": {
        ""HTTPie"": ""https://httpie.org"",
    }
}
Please note that with this syntax the command gets unwieldy when sending
complex data. In that case it's always better to use redirected input:
$ http POST api.example.com/person/1 < person.json

8¬†¬†¬†Forms
Submitting forms is very similar to sending JSON requests. Often the only
difference is in adding the --form, -f option, which ensures that
data fields are serialized as, and Content-Type is set to,
application/x-www-form-urlencoded; charset=utf-8. It is possible to make
form data the implicit content type instead of JSON
via the config file.

8.1¬†¬†¬†Regular forms
$ http --form POST api.example.org/person/1 name='John Smith'
POST /person/1 HTTP/1.1
Content-Type: application/x-www-form-urlencoded; charset=utf-8

name=John+Smith

8.2¬†¬†¬†File upload forms
If one or more file fields is present, the serialization and content type is
multipart/form-data:
$ http -f POST example.com/jobs name='John Smith' cv@~/Documents/cv.pdf
The request above is the same as if the following HTML form were
submitted:
<form enctype=""multipart/form-data"" method=""post"" action=""http://example.com/jobs"">
    <input type=""text"" name=""name"" />
    <input type=""file"" name=""cv"" />
</form>
Note that @ is used to simulate a file upload form field, whereas
=@ just embeds the file content as a regular text field value.

9¬†¬†¬†HTTP headers
To set custom headers you can use the Header:Value notation:
$ http example.org  User-Agent:Bacon/1.0  'Cookie:valued-visitor=yes;foo=bar'  \
    X-Foo:Bar  Referer:https://httpie.org/
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Cookie: valued-visitor=yes;foo=bar
Host: example.org
Referer: https://httpie.org/
User-Agent: Bacon/1.0
X-Foo: Bar

9.1¬†¬†¬†Default request headers
There are a couple of default headers that HTTPie sets:
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
User-Agent: HTTPie/<version>
Host: <taken-from-URL>
Any of these except Host can be overwritten and some of them unset.

9.2¬†¬†¬†Empty headers and header un-setting
To unset a previously specified header
(such a one of the default headers), use Header::
$ http httpbin.org/headers Accept: User-Agent:
To send a header with an empty value, use Header;:
$ http httpbin.org/headers 'Header;'

9.3¬†¬†¬†Limiting response headers
The --max-headers=n options allows you to control the number of headers
HTTPie reads before giving up (the default 0, i.e., there‚Äôs no limit).
$ http --max-headers=100 httpbin.org/get

10¬†¬†¬†Cookies
HTTP clients send cookies to the server as regular HTTP headers. That means,
HTTPie does not offer any special syntax for specifying cookies ‚Äî the usual
Header:Value notation is used:
Send a single cookie:
$ http example.org Cookie:sessionid=foo
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Connection: keep-alive
Cookie: sessionid=foo
Host: example.org
User-Agent: HTTPie/0.9.9
Send multiple cookies
(note the header is quoted to prevent the shell from interpreting the ;):
$ http example.org 'Cookie:sessionid=foo;another-cookie=bar'
GET / HTTP/1.1
Accept: */*
Accept-Encoding: gzip, deflate
Connection: keep-alive
Cookie: sessionid=foo;another-cookie=bar
Host: example.org
User-Agent: HTTPie/0.9.9
If you often deal with cookies in your requests, then chances are you'd appreciate
the sessions feature.

11¬†¬†¬†Authentication
The currently supported authentication schemes are Basic and Digest
(see auth plugins for more). There are two flags that control authentication:


--auth, -a
Pass a username:password pair as
the argument. Or, if you only specify a username
(-a username), you'll be prompted for
the password before the request is sent.
To send an empty password, pass username:.
The username:password@hostname URL syntax is
supported as well (but credentials passed via -a
have higher priority).

--auth-type, -A
Specify the auth mechanism. Possible values are
basic and digest. The default value is
basic so it can often be omitted.




11.1¬†¬†¬†Basic auth
$ http -a username:password example.org

11.2¬†¬†¬†Digest auth
$ http -A digest -a username:password example.org

11.3¬†¬†¬†Password prompt
$ http -a username example.org

11.4¬†¬†¬†.netrc
Authentication information from your ~/.netrc
file is by default honored as well.
For example:
$ cat ~/.netrc
machine httpbin.org
login httpie
password test
$ http httpbin.org/basic-auth/httpie/test
HTTP/1.1 200 OK
[...]
This can be disabled with the --ignore-netrc option:
$ http --ignore-netrc httpbin.org/basic-auth/httpie/test
HTTP/1.1 401 UNAUTHORIZED
[...]

11.5¬†¬†¬†Auth plugins
Additional authentication mechanism can be installed as plugins.
They can be found on the Python Package Index.
Here's a few picks:

httpie-api-auth: ApiAuth
httpie-aws-auth: AWS / Amazon S3
httpie-edgegrid: EdgeGrid
httpie-hmac-auth: HMAC
httpie-jwt-auth: JWTAuth (JSON Web Tokens)
httpie-negotiate: SPNEGO (GSS Negotiate)
httpie-ntlm: NTLM (NT LAN Manager)
httpie-oauth: OAuth
requests-hawk: Hawk


12¬†¬†¬†HTTP redirects
By default, HTTP redirects are not followed and only the first
response is shown:
$ http httpbin.org/redirect/3

12.1¬†¬†¬†Follow Location
To instruct HTTPie to follow the Location header of 30x responses
and show the final response instead, use the --follow, -F option:
$ http --follow httpbin.org/redirect/3

12.2¬†¬†¬†Showing intermediary redirect responses
If you additionally wish to see the intermediary requests/responses,
then use the --all option as well:
$ http --follow --all httpbin.org/redirect/3

12.3¬†¬†¬†Limiting maximum redirects followed
To change the default limit of maximum 30 redirects, use the
--max-redirects=<limit> option:
$ http --follow --all --max-redirects=5 httpbin.org/redirect/3

13¬†¬†¬†Proxies
You can specify proxies to be used through the --proxy argument for each
protocol (which is included in the value in case of redirects across protocols):
$ http --proxy=http:http://10.10.1.10:3128 --proxy=https:https://10.10.1.10:1080 example.org
With Basic authentication:
$ http --proxy=http:http://user:pass@10.10.1.10:3128 example.org

13.1¬†¬†¬†Environment variables
You can also configure proxies by environment variables ALL_PROXY,
HTTP_PROXY and HTTPS_PROXY, and the underlying Requests library will
pick them up as well. If you want to disable proxies configured through
the environment variables for certain hosts, you can specify them in NO_PROXY.
In your ~/.bash_profile:
export HTTP_PROXY=http://10.10.1.10:3128
export HTTPS_PROXY=https://10.10.1.10:1080
export NO_PROXY=localhost,example.com

13.2¬†¬†¬†SOCKS
Homebrew-installed HTTPie comes with SOCKS proxy support out of the box.
To enable SOCKS proxy support for non-Homebrew  installations, you'll
might need to install requests[socks] manually using pip:
$ pip install -U requests[socks]
Usage is the same as for other types of proxies:
$ http --proxy=http:socks5://user:pass@host:port --proxy=https:socks5://user:pass@host:port example.org

14¬†¬†¬†HTTPS

14.1¬†¬†¬†Server SSL certificate verification
To skip the host's SSL certificate verification, you can pass --verify=no
(default is yes):
$ http --verify=no https://example.org

14.2¬†¬†¬†Custom CA bundle
You can also use --verify=<CA_BUNDLE_PATH> to set a custom CA bundle path:
$ http --verify=/ssl/custom_ca_bundle https://example.org

14.3¬†¬†¬†Client side SSL certificate
To use a client side certificate for the SSL communication, you can pass
the path of the cert file with --cert:
$ http --cert=client.pem https://example.org
If the private key is not contained in the cert file you may pass the
path of the key file with --cert-key:
$ http --cert=client.crt --cert-key=client.key https://example.org

14.4¬†¬†¬†SSL version
Use the --ssl=<PROTOCOL> to specify the desired protocol version to use.
This will default to SSL v2.3 which will negotiate the highest protocol that both
the server and your installation of OpenSSL support. The available protocols
are ssl2.3, ssl3, tls1, tls1.1, tls1.2, tls1.3. (The actually
available set of protocols may vary depending on your OpenSSL installation.)
# Specify the vulnerable SSL v3 protocol to talk to an outdated server:
$ http --ssl=ssl3 https://vulnerable.example.org

15¬†¬†¬†Output options
By default, HTTPie only outputs the final response and the whole response
message is printed (headers as well as the body). You can control what should
be printed via several options:


--headers, -h
Only the response headers are printed.

--body, -b
Only the response body is printed.

--verbose, -v
Print the whole HTTP exchange (request and response).
This option also enables --all (see below).

--print, -p
Selects parts of the HTTP exchange.



--verbose can often be useful for debugging the request and generating
documentation examples:
$ http --verbose PUT httpbin.org/put hello=world
PUT /put HTTP/1.1
Accept: application/json, */*
Accept-Encoding: gzip, deflate
Content-Type: application/json
Host: httpbin.org
User-Agent: HTTPie/0.2.7dev

{
    ""hello"": ""world""
}


HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 477
Content-Type: application/json
Date: Sun, 05 Aug 2012 00:25:23 GMT
Server: gunicorn/0.13.4

{
    [‚Ä¶]
}

15.1¬†¬†¬†What parts of the HTTP exchange should be printed
All the other output options are under the hood just shortcuts for
the more powerful --print, -p. It accepts a string of characters each
of which represents a specific part of the HTTP exchange:


Character
Stands for



H
request headers

B
request body

h
response headers

b
response body



Print request and response headers:
$ http --print=Hh PUT httpbin.org/put hello=world

15.2¬†¬†¬†Viewing intermediary requests/responses
To see all the HTTP communication, i.e. the final request/response as
well as any possible  intermediary requests/responses, use the --all
option. The intermediary HTTP communication include followed redirects
(with --follow), the first unauthorized request when HTTP digest
authentication is used (--auth=digest), etc.
# Include all responses that lead to the final one:
$ http --all --follow httpbin.org/redirect/3
The intermediary requests/response are by default formatted according to
--print, -p (and its shortcuts described above). If you'd like to change
that, use the --history-print, -P option. It takes the same
arguments as --print, -p but applies to the intermediary requests only.
# Print the intermediary requests/responses differently than the final one:
$ http -A digest -a foo:bar --all -p Hh -P H httpbin.org/digest-auth/auth/foo/bar

15.3¬†¬†¬†Conditional body download
As an optimization, the response body is downloaded from the server
only if it's part of the output. This is similar to performing a HEAD
request, except that it applies to any HTTP method you use.
Let's say that there is an API that returns the whole resource when it is
updated, but you are only interested in the response headers to see the
status code after an update:
$ http --headers PATCH example.org/Really-Huge-Resource name='New Name'
Since we are only printing the HTTP headers here, the connection to the server
is closed as soon as all the response headers have been received.
Therefore, bandwidth and time isn't wasted downloading the body
which you don't care about. The response headers are downloaded always,
even if they are not part of the output

16¬†¬†¬†Redirected Input
The universal method for passing request data is through redirected stdin
(standard input)‚Äîpiping. Such data is buffered and then with no further
processing used as the request body. There are multiple useful ways to use
piping:
Redirect from a file:
$ http PUT example.com/person/1 X-API-Token:123 < person.json
Or the output of another program:
$ grep '401 Unauthorized' /var/log/httpd/error_log | http POST example.org/intruders
You can use echo for simple data:
$ echo '{""name"": ""John""}' | http PATCH example.com/person/1 X-API-Token:123
You can also use a Bash here string:
$ http example.com/ <<<'{""name"": ""John""}'
You can even pipe web services together using HTTPie:
$ http GET https://api.github.com/repos/jakubroztocil/httpie | http POST httpbin.org/post
You can use cat to enter multiline data on the terminal:
$ cat | http POST example.com
<paste>
^D
$ cat | http POST example.com/todos Content-Type:text/plain
- buy milk
- call parents
^D
On OS X, you can send the contents of the clipboard with pbpaste:
$ pbpaste | http PUT example.com
Passing data through stdin cannot be combined with data fields specified
on the command line:
$ echo 'data' | http POST example.org more=data   # This is invalid
To prevent HTTPie from reading stdin data you can use the
--ignore-stdin option.

16.1¬†¬†¬†Request data from a filename
An alternative to redirected stdin is specifying a filename (as
@/path/to/file) whose content is used as if it came from stdin.
It has the advantage that the Content-Type
header is automatically set to the appropriate value based on the
filename extension. For example, the following request sends the
verbatim contents of that XML file with Content-Type: application/xml:
$ http PUT httpbin.org/put @/data/file.xml

17¬†¬†¬†Terminal output
HTTPie does several things by default in order to make its terminal output
easy to read.

17.1¬†¬†¬†Colors and formatting
Syntax highlighting is applied to HTTP headers and bodies (where it makes
sense). You can choose your preferred color scheme via the --style option
if you don't like the default one (see $ http --help for the possible
values).
Also, the following formatting is applied:

HTTP headers are sorted by name.
JSON data is indented, sorted by keys, and unicode escapes are converted
to the characters they represent.

One of these options can be used to control output processing:


--pretty=all
Apply both colors and formatting.
Default for terminal output.

--pretty=colors
Apply colors.

--pretty=format
Apply formatting.

--pretty=none
Disables output processing.
Default for redirected output.




17.2¬†¬†¬†Binary data
Binary data is suppressed for terminal output, which makes it safe to perform
requests to URLs that send back binary data. Binary data is suppressed also in
redirected, but prettified output. The connection is closed as soon as we know
that the response body is binary,
$ http example.org/Movie.mov
You will nearly instantly see something like this:
HTTP/1.1 200 OK
Accept-Ranges: bytes
Content-Encoding: gzip
Content-Type: video/quicktime
Transfer-Encoding: chunked

+-----------------------------------------+
| NOTE: binary data not shown in terminal |
+-----------------------------------------+

18¬†¬†¬†Redirected output
HTTPie uses a different set of defaults for redirected output than for
terminal output. The differences being:

Formatting and colors aren't applied (unless --pretty is specified).
Only the response body is printed (unless one of the output options is set).
Also, binary data isn't suppressed.

The reason is to make piping HTTPie's output to another programs and
downloading files work with no extra flags. Most of the time, only the raw
response body is of an interest when the output is redirected.
Download a file:
$ http example.org/Movie.mov > Movie.mov
Download an image of Octocat, resize it using ImageMagick, upload it elsewhere:
$ http octodex.github.com/images/original.jpg | convert - -resize 25% -  | http example.org/Octocats
Force colorizing and formatting, and show both the request and the response in
less pager:
$ http --pretty=all --verbose example.org | less -R
The -R flag tells less to interpret color escape sequences included
HTTPie`s output.
You can create a shortcut for invoking HTTPie with colorized and paged output
by adding the following to your ~/.bash_profile:
function httpless {
    # `httpless example.org'
    http --pretty=all --print=hb ""$@"" | less -R;
}

19¬†¬†¬†Download mode
HTTPie features a download mode in which it acts similarly to wget.
When enabled using the --download, -d flag, response headers are printed to
the terminal (stderr), and a progress bar is shown while the response body
is being saved to a file.
$ http --download https://github.com/jakubroztocil/httpie/archive/master.tar.gz
HTTP/1.1 200 OK
Content-Disposition: attachment; filename=httpie-master.tar.gz
Content-Length: 257336
Content-Type: application/x-gzip

Downloading 251.30 kB to ""httpie-master.tar.gz""
Done. 251.30 kB in 2.73862s (91.76 kB/s)

19.1¬†¬†¬†Downloaded filename
There are three mutually exclusive ways through which HTTPie determines
the output filename (with decreasing priority):

You can explicitly provide it via --output, -o.
The file gets overwritten if it already exists
(or appended to with --continue, -c).
The server may specify the filename in the optional Content-Disposition
response header. Any leading dots are stripped from a server-provided filename.
The last resort HTTPie uses is to generate the filename from a combination
of the request URL and the response Content-Type.
The initial URL is always used as the basis for
the generated filename ‚Äî even if there has been one or more redirects.

To prevent data loss by overwriting, HTTPie adds a unique numerical suffix to the
filename when necessary (unless specified with --output, -o).

19.2¬†¬†¬†Piping while downloading
You can also redirect the response body to another program while the response
headers and progress are still shown in the terminal:
$ http -d https://github.com/jakubroztocil/httpie/archive/master.tar.gz |  tar zxf -

19.3¬†¬†¬†Resuming downloads
If --output, -o is specified, you can resume a partial download using the
--continue, -c option. This only works with servers that support
Range requests and 206 Partial Content responses. If the server doesn't
support that, the whole file will simply be downloaded:
$ http -dco file.zip example.org/file

19.4¬†¬†¬†Other notes

The --download option only changes how the response body is treated.
You can still set custom headers, use sessions, --verbose, -v, etc.
--download always implies --follow (redirects are followed).
HTTPie exits with status code 1 (error) if the body hasn't been fully
downloaded.
Accept-Encoding cannot be set with --download.


20¬†¬†¬†Streamed responses
Responses are downloaded and printed in chunks which allows for streaming
and large file downloads without using too much memory. However, when
colors and formatting is applied, the whole response is buffered and only
then processed at once.

20.1¬†¬†¬†Disabling buffering
You can use the --stream, -S flag to make two things happen:

The output is flushed in much smaller chunks without any buffering,
which makes HTTPie behave kind of like tail -f for URLs.
Streaming becomes enabled even when the output is prettified: It will be
applied to each line of the response and flushed immediately. This makes
it possible to have a nice output for long-lived requests, such as one
to the Twitter streaming API.


20.2¬†¬†¬†Examples use cases
Prettified streamed response:
$ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track='Justin Bieber'
Streamed output by small chunks al√° tail -f:
# Send each new tweet (JSON object) mentioning ""Apple"" to another
# server as soon as it arrives from the Twitter streaming API:
$ http --stream -f -a YOUR-TWITTER-NAME https://stream.twitter.com/1/statuses/filter.json track=Apple \
| while read tweet; do echo ""$tweet"" | http POST example.org/tweets ; done

21¬†¬†¬†Sessions
By default, every request HTTPie makes is completely independent of any
previous ones to the same host.
However, HTTPie also supports persistent
sessions via the --session=SESSION_NAME_OR_PATH option. In a session,
custom HTTP headers (except for the ones starting with Content- or If-),
authentication, and cookies
(manually specified or sent by the server) persist between requests
to the same host.
# Create a new session
$ http --session=/tmp/session.json example.org API-Token:123

# Re-use an existing session ‚Äî API-Token will be set:
$ http --session=/tmp/session.json example.org
All session data, including credentials, cookie data,
and custom headers are stored in plain text.
That means session files can also be created and edited manually in a text
editor‚Äîthey are regular JSON. It also means that they can be read by anyone
who has access to the session file.

21.1¬†¬†¬†Named sessions
You can create one or more named session per host. For example, this is how
you can create a new session named user1 for example.org:
$ http --session=user1 -a user1:password example.org X-Foo:Bar
From now on, you can refer to the session by its name. When you choose to
use the session again, any previously specified authentication or HTTP headers
will automatically be set:
$ http --session=user1 example.org
To create or reuse a different session, simple specify a different name:
$ http --session=user2 -a user2:password example.org X-Bar:Foo
Named sessions‚Äôs data is stored in JSON files in the the sessions
subdirectory of the config directory:
~/.httpie/sessions/<host>/<name>.json
(%APPDATA%\httpie\sessions\<host>\<name>.json on Windows).

21.2¬†¬†¬†Anonymous sessions
Instead of a name, you can also directly specify a path to a session file. This
allows for sessions to be re-used across multiple hosts:
$ http --session=/tmp/session.json example.org
$ http --session=/tmp/session.json admin.example.org
$ http --session=~/.httpie/sessions/another.example.org/test.json example.org
$ http --session-read-only=/tmp/session.json example.org

21.3¬†¬†¬†Readonly session
To use an existing session file without updating it from the request/response
exchange once it is created, specify the session name via
--session-read-only=SESSION_NAME_OR_PATH instead.

22¬†¬†¬†Config
HTTPie uses a simple config.json file. The file doesn‚Äôt exist by default
but you can create it manually.

22.1¬†¬†¬†Config file directory
The default location of the configuration file is ~/.httpie/config.json
(or %APPDATA%\httpie\config.json on Windows).
The config directory can be changed by setting the $HTTPIE_CONFIG_DIR
environment variable:
$ export HTTPIE_CONFIG_DIR=/tmp/httpie
$ http example.org
To view the exact location run http --debug.

22.2¬†¬†¬†Configurable options
Currently HTTPie offers a single configurable option:

22.2.1¬†¬†¬†default_options
An Array (by default empty) of default options that should be applied to
every invocation of HTTPie.
For instance, you can use this config option to change your default color theme:
$ cat ~/.httpie/config.json
{
    ""default_options"": [
      ""--style=fruity""
    ]
}
Even though it is technically possible to include there any of HTTPie‚Äôs
options, it is not recommended to modify the default behaviour in a way
that would break your compatibility with the wider world as that can
generate a lot of confusion.

22.3¬†¬†¬†Un-setting previously specified options
Default options from the config file, or specified any other way,
can be unset for a particular invocation via --no-OPTION arguments passed
on the command line (e.g., --no-style or --no-session).

23¬†¬†¬†Scripting
When using HTTPie from shell scripts, it can be handy to set the
--check-status flag. It instructs HTTPie to exit with an error if the
HTTP status is one of 3xx, 4xx, or 5xx. The exit status will
be 3 (unless --follow is set), 4, or 5,
respectively.
#!/bin/bash

if http --check-status --ignore-stdin --timeout=2.5 HEAD example.org/health &> /dev/null; then
    echo 'OK!'
else
    case $? in
        2) echo 'Request timed out!' ;;
        3) echo 'Unexpected HTTP 3xx Redirection!' ;;
        4) echo 'HTTP 4xx Client Error!' ;;
        5) echo 'HTTP 5xx Server Error!' ;;
        6) echo 'Exceeded --max-redirects=<n> redirects!' ;;
        *) echo 'Other Error!' ;;
    esac
fi

23.1¬†¬†¬†Best practices
The default behaviour of automatically reading stdin is typically not
desirable during non-interactive invocations. You most likely want to
use the --ignore-stdin option to disable it.
It is a common gotcha that without this option HTTPie seemingly hangs.
What happens is that when HTTPie is invoked for example from a cron job,
stdin is not connected to a terminal.
Therefore, rules for redirected input apply, i.e., HTTPie starts to read it
expecting that the request body will be passed through.
And since there's no data nor EOF, it will be stuck. So unless you're
piping some data to HTTPie, this flag should be used in scripts.
Also, it might be good to set a connection --timeout limit to prevent
your program from hanging if the server never responds.

24¬†¬†¬†Meta

24.1¬†¬†¬†Interface design
The syntax of the command arguments closely corresponds to the actual HTTP
requests sent over the wire. It has the advantage  that it's easy to remember
and read. It is often possible to translate an HTTP request to an HTTPie
argument list just by inlining the request elements. For example, compare this
HTTP request:
POST /collection HTTP/1.1
X-API-Key: 123
User-Agent: Bacon/1.0
Content-Type: application/x-www-form-urlencoded

name=value&name2=value2
with the HTTPie command that sends it:
$ http -f POST example.org/collection \
  X-API-Key:123 \
  User-Agent:Bacon/1.0 \
  name=value \
  name2=value2
Notice that both the order of elements and the syntax is very similar,
and that only a small portion of the command is used to control HTTPie and
doesn't directly correspond to any part of the request (here it's only -f
asking HTTPie to send a form request).
The two modes, --pretty=all (default for terminal) and --pretty=none
(default for redirected output), allow for both user-friendly interactive use
and usage from scripts, where HTTPie serves as a generic HTTP client.
As HTTPie is still under heavy development, the existing command line
syntax and some of the --OPTIONS may change slightly before
HTTPie reaches its final version 1.0. All changes are recorded in the
change log.

24.2¬†¬†¬†User support
Please use the following support channels:

GitHub issues
for bug reports and feature requests.
Our Gitter chat room
to ask questions, discuss features, and for general discussion.
StackOverflow
to ask questions (please make sure to use the
httpie tag).
Tweet directly to @clihttp.
You can also tweet directly to @jakubroztocil.


24.3¬†¬†¬†Related projects

24.3.1¬†¬†¬†Dependencies
Under the hood, HTTPie uses these two amazing libraries:

Requests
‚Äî Python HTTP library for humans
Pygments
‚Äî Python syntax highlighter


24.3.2¬†¬†¬†HTTPie friends
HTTPie plays exceptionally well with the following tools:

jq
‚Äî CLI JSON processor that
works great in conjunction with HTTPie
http-prompt
‚Äî  interactive shell for HTTPie featuring autocomplete
and command syntax highlighting


24.3.3¬†¬†¬†Alternatives

httpcat ‚Äî a lower-level sister utility
of HTTPie for constructing raw HTTP requests on the command line.
curl ‚Äî a ""Swiss knife"" command line tool and
an exceptional library for transferring data with URLs.


24.4¬†¬†¬†Contributing
See CONTRIBUTING.rst.

24.5¬†¬†¬†Change log
See CHANGELOG.

24.6¬†¬†¬†Artwork

Logo by Cl√°udia Delgado.
Animation by Allen Smith of GitHub.


24.7¬†¬†¬†Licence
BSD-3-Clause: LICENSE.

24.8¬†¬†¬†Authors
Jakub Roztocil  (@jakubroztocil) created HTTPie and these fine people
have contributed.
","GitHub - jakubroztocil/httpie: As easy as HTTPie /aitch-tee-tee-pie/  ü•ß Modern command line HTTP client ‚Äì user-friendly curl alternative with intuitive UI, JSON support, syntax highlighting, wget-like downloads, extensions, etc.  https://twitter.com/clihttp"
6,Python,"Awesome Machine Learning 
A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by awesome-php.
If you want to contribute to this list (please do), send me a pull request or contact me @josephmisiti.
Also, a listed repository should be deprecated if:

Repository's owner explicitly say that ""this library is not maintained"".
Not committed for long time (2~3 years).

Further resources:


For a list of free machine learning books available for download, go here.


For a list of (mostly) free machine learning courses available online, go here.


For a list of blogs and newsletters on data science and machine learning, go here.


For a list of free-to-attend meetups and local events, go here.


Table of Contents
Frameworks and Libraries

Awesome Machine Learning 

Table of Contents

Frameworks and Libraries
Tools


APL

General-Purpose Machine Learning


C

General-Purpose Machine Learning
Computer Vision


C++

Computer Vision
General-Purpose Machine Learning
Natural Language Processing
Speech Recognition
Sequence Analysis
Gesture Detection


Common Lisp

General-Purpose Machine Learning


Clojure

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Crystal

General-Purpose Machine Learning


Elixir

General-Purpose Machine Learning
Natural Language Processing


Erlang

General-Purpose Machine Learning


Go

Natural Language Processing
General-Purpose Machine Learning
Spatial analysis and geometry
Data Analysis / Data Visualization
Computer vision


Haskell

General-Purpose Machine Learning


Java

Natural Language Processing
General-Purpose Machine Learning
Speech Recognition
Data Analysis / Data Visualization
Deep Learning


Javascript

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning
Misc
Demos and Scripts


Julia

General-Purpose Machine Learning
Natural Language Processing
Data Analysis / Data Visualization
Misc Stuff / Presentations


Lua

General-Purpose Machine Learning
Demos and Scripts


Matlab

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


.NET

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization


Objective C

General-Purpose Machine Learning


OCaml

General-Purpose Machine Learning


Perl

Data Analysis / Data Visualization
General-Purpose Machine Learning


Perl 6

Data Analysis / Data Visualization
General-Purpose Machine Learning


PHP

Natural Language Processing
General-Purpose Machine Learning


Python

Computer Vision
Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc Scripts / iPython Notebooks / Codebases
Neural Networks
Kaggle Competition Source Code
Reinforcement Learning


Ruby

Natural Language Processing
General-Purpose Machine Learning
Data Analysis / Data Visualization
Misc


Rust

General-Purpose Machine Learning


R

General-Purpose Machine Learning
Data Analysis / Data Visualization


SAS

General-Purpose Machine Learning
Data Analysis / Data Visualization
Natural Language Processing
Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning


Tools

Neural Networks
Misc


Credits
ÂÜô‰∏™ËÑöÊú¨ÊääÂÆÉ‰ª¨Áà¨‰∏ãÊù• - Demos and Scripts


Scala

Natural Language Processing
Data Analysis / Data Visualization
General-Purpose Machine Learning


Scheme

Neural Networks


Swift

General-Purpose Machine Learning


TensorFlow

General-Purpose Machine Learning



Tools

Neural Networks
Misc

Credits

APL

General-Purpose Machine Learning

naive-apl - Naive Bayesian Classifier implementation in APL. [Deprecated]


C

General-Purpose Machine Learning

Darknet - Darknet is an open source neural network framework written in C and CUDA. It is fast, easy to install, and supports CPU and GPU computation.
Recommender - A C library for product recommendations/suggestions using collaborative filtering (CF).
Hybrid Recommender System - A hybrid recommender system based upon scikit-learn algorithms. [Deprecated]
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.


Computer Vision

CCV - C-based/Cached/Core Computer Vision Library, A Modern Computer Vision Library.
VLFeat - VLFeat is an open and portable library of computer vision algorithms, which has Matlab toolbox.


C++

Computer Vision

DLib - DLib has C++ and Python interfaces for face detection and training general object detectors.
EBLearn - Eblearn is an object-oriented C++ library that implements various machine learning models [Deprecated]
OpenCV - OpenCV has C++, C, Python, Java and MATLAB interfaces and supports Windows, Linux, Android and Mac OS.
VIGRA - VIGRA is a generic cross-platform C++ computer vision and machine learning library for volumes of arbitrary dimensionality with Python bindings.


General-Purpose Machine Learning

BanditLib - A simple Multi-armed Bandit library. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind. [DEEP LEARNING]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, contains fast inference implementation and supports CPU and GPU (even multi-GPU) computation.
CNTK - The Computational Network Toolkit (CNTK) by Microsoft Research, is a unified deep-learning toolkit that describes neural networks as a series of computational steps via a directed graph.
CUDA - This is a fast C++/CUDA implementation of convolutional [DEEP LEARNING]
DeepDetect - A machine learning API and server written in C++11. It makes state of the art machine learning easy to work with and integrate into existing applications.
Distributed Machine learning Tool Kit (DMTK) - A distributed machine learning (parameter server) framework by Microsoft. Enables training models on large data sets across multiple machines. Current tools bundled with it include: LightLDA and Distributed (Multisense) Word Embedding.
DLib - A suite of ML tools designed to be easy to imbed in other applications.
DSSTNE - A software library created by Amazon for training and deploying deep neural networks using GPUs which emphasizes speed and scale over experimental flexibility.
DyNet - A dynamic neural network library working well with networks that have dynamic structures that change for every training instance. Written in C++ with bindings in Python.
Fido - A highly-modular C++ machine learning library for embedded electronics and robotics.
igraph - General purpose graph library.
Intel(R) DAAL - A high performance software library developed by Intel and optimized for Intel's architectures. Library provides algorithmic building blocks for all stages of data analytics and allows to process data in batch, online and distributed modes.
LightGBM - Microsoft's fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks.
libfm - A generic approach that allows to mimic most factorization models by feature engineering.
MLDB - The Machine Learning Database is a database designed for machine learning. Send it commands over a RESTful API to store data, explore it using SQL, then train machine learning models and expose them as APIs.
mlpack - A scalable C++ machine learning library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
proNet-core - A general-purpose network embedding framework: pair-wise representations optimization Network Edit.
PyCUDA - Python interface to CUDA
ROOT - A modular scientific software framework. It provides all the functionalities needed to deal with big data processing, statistical analysis, visualization and storage.
shark - A fast, modular, feature-rich open-source C++ machine learning library.
Shogun - The Shogun Machine Learning Toolbox.
sofia-ml - Suite of fast incremental algorithms.
Stan - A probabilistic programming language implementing full Bayesian statistical inference with Hamiltonian Monte Carlo sampling.
Timbl - A software package/C++ library implementing several memory-based learning algorithms, among which IB1-IG, an implementation of k-nearest neighbor classification, and IGTree, a decision-tree approximation of IB1-IG. Commonly used for NLP.
Vowpal Wabbit (VW) - A fast out-of-core learning system.
Warp-CTC - A fast parallel implementation of Connectionist Temporal Classification (CTC), on both CPU and GPU.
XGBoost - A parallelized optimized general purpose gradient boosting library.
ThunderGBM - A fast library for GBDTs and Random Forests on GPUs.
ThunderSVM - A fast SVM library on GPUs and CPUs.
LKYDeepNN - A header-only C++11 Neural Network library. Low dependency, native traditional chinese document.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
Featuretools - A library for automated feature engineering. It excels at transforming transactional and relational datasets into feature matrices for machine learning using reusable feature engineering ""primitives"".
skynet - A library for learning neural network, has C-interface, net set in JSON. Written in C++ with bindings in Python, C++ and C#.
Feast - A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving.
Polyaxon - A platform for reproducible and scalable machine learning and deep learning.


Natural Language Processing

BLLIP Parser - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser).
colibri-core - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
CRF++ - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks. [Deprecated]
CRFsuite - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data. [Deprecated]
frog - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.
libfolia - C++ library for the FoLiA format
MeTA - MeTA : ModErn Text Analysis is a C++ Data Sciences Toolkit that facilitates mining big text data.
MIT Information Extraction Toolkit - C, C++, and Python tools for named entity recognition and relation extraction
ucto - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.


Speech Recognition

Kaldi - Kaldi is a toolkit for speech recognition written in C++ and licensed under the Apache License v2.0. Kaldi is intended for use by speech recognition researchers.


Sequence Analysis

ToPS - This is an objected-oriented framework that facilitates the integration of probabilistic models for sequences over a user defined alphabet. [Deprecated]


Gesture Detection

grt - The Gesture Recognition Toolkit (GRT) is a cross-platform, open-source, C++ machine learning library designed for real-time gesture recognition.


Common Lisp

General-Purpose Machine Learning

mgl - Neural networks (boltzmann machines, feed-forward and recurrent nets), Gaussian Processes.
mgl-gpr - Evolutionary algorithms. [Deprecated]
cl-libsvm - Wrapper for the libsvm support vector machine library. [Deprecated]
cl-online-learning - Online learning algorithms (Perceptron, AROW, SCW, Logistic Regression).
cl-random-forest - Implementation of Random Forest in Common Lisp.


Clojure

Natural Language Processing

Clojure-openNLP - Natural Language Processing in Clojure (opennlp).
Infections-clj - Rails-like inflection library for Clojure and ClojureScript.


General-Purpose Machine Learning

Touchstone - Clojure A/B testing library. [Deprecated]
Clojush - The Push programming language and the PushGP genetic programming system implemented in Clojure.
Infer - Inference and machine learning in Clojure. [Deprecated]
Clj-ML - A machine learning library for Clojure built on top of Weka and friends. [Deprecated]
DL4CLJ - Clojure wrapper for Deeplearning4j.
Encog - Clojure wrapper for Encog (v3) (Machine-Learning framework that specializes in neural-nets). [Deprecated]
Fungp - A genetic programming library for Clojure. [Deprecated]
Statistiker - Basic Machine Learning algorithms in Clojure. [Deprecated]
clortex - General Machine Learning library using Numenta‚Äôs Cortical Learning Algorithm. [Deprecated]
comportex - Functionally composable Machine Learning library using Numenta‚Äôs Cortical Learning Algorithm. [Deprecated]
cortex - Neural networks, regression and feature learning in Clojure.
lambda-ml - Simple, concise implementations of machine learning techniques and utilities in Clojure.


Data Analysis / Data Visualization

Incanter - Incanter is a Clojure-based, R-like platform for statistical computing and graphics.
PigPen - Map-Reduce for Clojure.
Envision - Clojure Data Visualisation library, based on Statistiker and D3.


Crystal

General-Purpose Machine Learning

machine - Simple machine learning algorithm.
crystal-fann - FANN (Fast Artificial Neural Network) binding.


Elixir

General-Purpose Machine Learning

Simple Bayes - A Simple Bayes / Naive Bayes implementation in Elixir.
emel - A simple and functional machine learning library written in Elixir.
Tensorflex - Tensorflow bindings for the Elixir programming language.


Natural Language Processing

Stemmer - An English (Porter2) stemming implementation in Elixir.


Erlang

General-Purpose Machine Learning

Disco - Map Reduce in Erlang. [Deprecated]
Yanni - ANN neural networks using Erlangs leightweight processes.


Go

Natural Language Processing

snowball - Snowball Stemmer for Go.
word-embedding - Word Embeddings: the full implementation of word2vec, GloVe in Go.
sentences - Golang implementation of Punkt sentence tokenizer.
go-ngram - In-memory n-gram index with compression. [Deprecated]
paicehusk - Golang implementation of the Paice/Husk Stemming Algorithm. [Deprecated]
go-porterstemmer - A native Go clean room implementation of the Porter Stemming algorithm. [Deprecated]


General-Purpose Machine Learning

birdland - A recommendation library in Go.
eaopt - An evolutionary optimization library.
leaves - A pure Go implementation of the prediction part of GBRTs, including XGBoost and LightGBM.
gobrain - Neural Networks written in Go.
go-mxnet-predictor - Go binding for MXNet c_predict_api to do inference with pre-trained model.
go-ml-transpiler - An open source Go transpiler for machine learning models.
golearn - Machine learning for Go.
goml - Machine learning library written in pure Go.
gorgonia - Deep learning in Go.
gorse - An offline recommender system backend based on collaborative filtering written in Go.
therfoo - An embedded deep learning library for Go.
neat - Plug-and-play, parallel Go framework for NeuroEvolution of Augmenting Topologies (NEAT). [Deprecated]
go-pr - Pattern recognition package in Go lang. [Deprecated]
go-ml - Linear / Logistic regression, Neural Networks, Collaborative Filtering and Gaussian Multivariate Distribution. [Deprecated]
GoNN - GoNN is an implementation of Neural Network in Go Language, which includes BPNN, RBF, PCN. [Deprecated]
bayesian - Naive Bayesian Classification for Golang. [Deprecated]
go-galib - Genetic Algorithms library written in Go / Golang. [Deprecated]
Cloudforest - Ensembles of decision trees in Go/Golang. [Deprecated]
go-dnn - Deep Neural Networks for Golang (powered by MXNet)


Spatial analysis and geometry

go-geom - Go library to handle geometries.
gogeo - Spherical geometry in Go.


Data Analysis / Data Visualization

gota - Dataframes.
gonum/mat - A linear algebra package for Go.
gonum/optimize - Implementations of optimization algorithms.
gonum/plot - A plotting library.
gonum/stat - A statistics library.
SVGo - The Go Language library for SVG generation.
glot - Glot is a plotting library for Golang built on top of gnuplot.
globe - Globe wireframe visualization.
gonum/graph - General-purpose graph library.
go-graph - Graph library for Go/Golang language. [Deprecated]
RF - Random forests implementation in Go. [Deprecated]


Computer vision

GoCV - Package for computer vision using OpenCV 4 and beyond.


Haskell

General-Purpose Machine Learning

haskell-ml - Haskell implementations of various ML algorithms. [Deprecated]
HLearn - a suite of libraries for interpreting machine learning models according to their algebraic structure. [Deprecated]
hnn - Haskell Neural Network library.
hopfield-networks - Hopfield Networks for unsupervised learning in Haskell. [Deprecated]
DNNGraph - A DSL for deep neural networks. [Deprecated]
LambdaNet - Configurable Neural Networks in Haskell. [Deprecated]


Java

Natural Language Processing

Cortical.io - Retina: an API performing complex NLP operations (disambiguation, classification, streaming text filtering, etc...) as quickly and intuitively as the brain.
IRIS - Cortical.io's FREE NLP, Retina API Analysis Tool (written in JavaFX!) - See the Tutorial Video.
CoreNLP - Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words.
Stanford Parser - A natural language parser is a program that works out the grammatical structure of sentences.
Stanford POS Tagger - A Part-Of-Speech Tagger (POS Tagger).
Stanford Name Entity Recognizer - Stanford NER is a Java implementation of a Named Entity Recognizer.
Stanford Word Segmenter - Tokenization of raw text is a standard pre-processing step for many NLP tasks.
Tregex, Tsurgeon and Semgrex - Tregex is a utility for matching patterns in trees, based on tree relationships and regular expression matches on nodes (the name is short for ""tree regular expressions"").
Stanford Phrasal: A Phrase-Based Translation System
Stanford English Tokenizer - Stanford Phrasal is a state-of-the-art statistical phrase-based machine translation system, written in Java.
Stanford Tokens Regex - A tokenizer divides text into a sequence of tokens, which roughly correspond to ""words"".
Stanford Temporal Tagger - SUTime is a library for recognizing and normalizing time expressions.
Stanford SPIED - Learning entities from unlabeled text starting with seed sets using patterns in an iterative fashion.
Stanford Topic Modeling Toolbox - Topic modeling tools to social scientists and others who wish to perform analysis on datasets.
Twitter Text Java - A Java implementation of Twitter's text processing library.
MALLET - A Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.
OpenNLP - a machine learning based toolkit for the processing of natural language text.
LingPipe - A tool kit for processing text using computational linguistics.
ClearTK - ClearTK provides a framework for developing statistical natural language processing (NLP) components in Java and is built on top of Apache UIMA. [Deprecated]
Apache cTAKES - Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) is an open-source natural language processing system for information extraction from electronic medical record clinical free-text.
NLP4J - The NLP4J project provides software and resources for natural language processing. The project started at the Center for Computational Language and EducAtion Research, and is currently developed by the Center for Language and Information Research at Emory University. [Deprecated]
CogcompNLP - This project collects a number of core libraries for Natural Language Processing (NLP) developed in the University of Illinois' Cognitive Computation Group, for example illinois-core-utilities which provides a set of NLP-friendly data structures and a number of NLP-related utilities that support writing NLP applications, running experiments, etc, illinois-edison a library for feature extraction from illinois-core-utilities data structures and many other packages.


General-Purpose Machine Learning

aerosolve - A machine learning library by Airbnb designed from the ground up to be human friendly.
AMIDST Toolbox - A Java Toolbox for Scalable Probabilistic Machine Learning.
Datumbox - Machine Learning framework for rapid development of Machine Learning and Statistical applications.
ELKI - Java toolkit for data mining. (unsupervised: clustering, outlier detection etc.)
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
H2O - ML engine that supports distributed learning on Hadoop, Spark or your laptop via APIs in R, Python, Scala, REST/JSON.
htm.java - General Machine Learning library using Numenta‚Äôs Cortical Learning Algorithm.
liblinear-java - Java version of liblinear.
Mahout - Distributed machine learning.
Meka - An open source implementation of methods for multi-label classification and evaluation (extension to Weka).
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Neuroph - Neuroph is lightweight Java neural network framework
ORYX - Lambda Architecture Framework using Apache Spark and Apache Kafka with a specialization for real-time large-scale machine learning.
Samoa SAMOA is a framework that includes distributed machine learning for data streams with an interface to plug-in different stream processing platforms.
RankLib - RankLib is a library of learning to rank algorithms. [Deprecated]
rapaio - statistics, data mining and machine learning toolbox in Java.
RapidMiner - RapidMiner integration into Java code.
Stanford Classifier - A classifier is a machine learning tool that will take data items and place them into one of k classes.
SmileMiner - Statistical Machine Intelligence & Learning Engine.
SystemML - flexible, scalable machine learning (ML) language.
Weka - Weka is a collection of machine learning algorithms for data mining tasks.
LBJava - Learning Based Java is a modeling language for the rapid development of software systems, offers a convenient, declarative syntax for classifier and constraint definition directly in terms of the objects in the programmer's application.


Speech Recognition

CMU Sphinx - Open Source Toolkit For Speech Recognition purely based on Java speech recognition library.


Data Analysis / Data Visualization

Flink - Open source platform for distributed stream and batch data processing.
Hadoop - Hadoop/HDFS.
Onyx - Distributed, masterless, high performance, fault tolerant data processing. Written entirely in Clojure.
Spark - Spark is a fast and general engine for large-scale data processing.
Storm - Storm is a distributed realtime computation system.
Impala - Real-time Query for Hadoop.
DataMelt - Mathematics software for numeric computation, statistics, symbolic calculations, data analysis and data visualization.
Dr. Michael Thomas Flanagan's Java Scientific Library [Deprecated]


Deep Learning

Deeplearning4j - Scalable deep learning for industry with parallel GPUs.
Keras Beginner Tutorial - Friendly guide on using Keras to implement a simple Neural Network in Python


Javascript

Natural Language Processing

Twitter-text - A JavaScript implementation of Twitter's text processing library.
natural - General natural language facilities for node.
Knwl.js - A Natural Language Processor in JS.
Retext - Extensible system for analyzing and manipulating natural language.
NLP Compromise - Natural Language processing in the browser.
nlp.js - An NLP library built in node over Natural, with entity extraction, sentiment analysis, automatic language identify, and so more


Data Analysis / Data Visualization

D3.js
High Charts
NVD3.js
dc.js
chartjs
dimple
amCharts
D3xter - Straight forward plotting built on D3. [Deprecated]
statkit - Statistics kit for JavaScript. [Deprecated]
datakit - A lightweight framework for data analysis in JavaScript
science.js - Scientific and statistical computing in JavaScript. [Deprecated]
Z3d - Easily make interactive 3d plots built on Three.js [Deprecated]
Sigma.js - JavaScript library dedicated to graph drawing.
C3.js - customizable library based on D3.js for easy chart drawing.
Datamaps - Customizable SVG map/geo visualizations using D3.js. [Deprecated]
ZingChart - library written on Vanilla JS for big data visualization.
cheminfo - Platform for data visualization and analysis, using the visualizer project.
Learn JS Data
AnyChart
FusionCharts
Nivo - built on top of the awesome d3 and Reactjs libraries


General-Purpose Machine Learning

Auto ML - Automated machine learning, data formatting, ensembling, and hyperparameter optimization for competitions and exploration- just give it a .csv file!
Convnet.js - ConvNetJS is a Javascript library for training Deep Learning models[DEEP LEARNING] [Deprecated]
Clusterfck - Agglomerative hierarchical clustering implemented in Javascript for Node.js and the browser. [Deprecated]
Clustering.js - Clustering algorithms implemented in Javascript for Node.js and the browser. [Deprecated]
Decision Trees - NodeJS Implementation of Decision Tree using ID3 Algorithm. [Deprecated]
DN2A - Digital Neural Networks Architecture. [Deprecated]
figue - K-means, fuzzy c-means and agglomerative clustering.
Gaussian Mixture Model - Unsupervised machine learning with multivariate Gaussian mixture model.
Node-fann - FANN (Fast Artificial Neural Network Library) bindings for Node.js [Deprecated]
Keras.js - Run Keras models in the browser, with GPU support provided by WebGL 2.
Kmeans.js - Simple Javascript implementation of the k-means algorithm, for node.js and the browser. [Deprecated]
LDA.js - LDA topic modeling for Node.js
Learning.js - Javascript implementation of logistic regression/c4.5 decision tree [Deprecated]
machinelearn.js - Machine Learning library for the web, Node.js and developers
mil-tokyo - List of several machine learning libraries.
Node-SVM - Support Vector Machine for Node.js
Brain - Neural networks in JavaScript [Deprecated]
Brain.js - Neural networks in JavaScript - continued community fork of Brain.
Bayesian-Bandit - Bayesian bandit implementation for Node and the browser. [Deprecated]
Synaptic - Architecture-free neural network library for Node.js and the browser.
kNear - JavaScript implementation of the k nearest neighbors algorithm for supervised learning.
NeuralN - C++ Neural Network library for Node.js. It has advantage on large dataset and multi-threaded training. [Deprecated]
kalman - Kalman filter for Javascript. [Deprecated]
shaman - Node.js library with support for both simple and multiple linear regression. [Deprecated]
ml.js - Machine learning and numerical analysis tools for Node.js and the Browser!
ml5 - Friendly machine learning for the web!
Pavlov.js - Reinforcement learning using Markov Decision Processes.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TensorFlow.js - A WebGL accelerated, browser based JavaScript library for training and deploying ML models.
JSMLT - Machine learning toolkit with classification and clustering for Node.js; supports visualization (see visualml.io).
xgboost-node - Run XGBoost model and make predictions in Node.js.
Netron - Visualizer for machine learning models.
WebDNN - Fast Deep Neural Network Javascript Framework. WebDNN uses next generation JavaScript API, WebGPU for GPU execution, and WebAssembly for CPU execution.


Misc

stdlib - A standard library for JavaScript and Node.js, with an emphasis on numeric computing. The library provides a collection of robust, high performance libraries for mathematics, statistics, streams, utilities, and more.
sylvester - Vector and Matrix math for JavaScript. [Deprecated]
simple-statistics - A JavaScript implementation of descriptive, regression, and inference statistics. Implemented in literate JavaScript with no dependencies, designed to work in all modern browsers (including IE) as well as in Node.js.
regression-js - A javascript library containing a collection of least squares fitting methods for finding a trend in a set of data.
Lyric - Linear Regression library. [Deprecated]
GreatCircle - Library for calculating great circle distance.
MLPleaseHelp - MLPleaseHelp is a simple ML resource search engine. You can use this search engine right now at https://jgreenemi.github.io/MLPleaseHelp/, provided via Github Pages.


Demos and Scripts

The Bot - Example of how the neural network learns to predict the angle between two points created with Synaptic.
Half Beer - Beer glass classifier created with Synaptic.
NSFWJS - Indecent content checker with TensorFlow.js
Rock Paper Scissors - Rock Paper Scissors trained in the browser with TensorFlow.js


Julia

General-Purpose Machine Learning

MachineLearning - Julia Machine Learning library. [Deprecated]
MLBase - A set of functions to support the development of machine learning algorithms.
PGM - A Julia framework for probabilistic graphical models.
DA - Julia package for Regularized Discriminant Analysis.
Regression - Algorithms for regression analysis (e.g. linear regression and logistic regression). [Deprecated]
Local Regression - Local regression, so smooooth!
Naive Bayes - Simple Naive Bayes implementation in Julia. [Deprecated]
Mixed Models - A Julia package for fitting (statistical) mixed-effects models.
Simple MCMC - basic mcmc sampler implemented in Julia. [Deprecated]
Distances - Julia module for Distance evaluation.
Decision Tree - Decision Tree Classifier and Regressor.
Neural - A neural network in Julia.
MCMC - MCMC tools for Julia. [Deprecated]
Mamba - Markov chain Monte Carlo (MCMC) for Bayesian analysis in Julia.
GLM - Generalized linear models in Julia.
Gaussian Processes - Julia package for Gaussian processes.
Online Learning [Deprecated]
GLMNet - Julia wrapper for fitting Lasso/ElasticNet GLM models using glmnet.
Clustering - Basic functions for clustering data: k-means, dp-means, etc.
SVM - SVM's for Julia. [Deprecated]
Kernel Density - Kernel density estimators for julia.
MultivariateStats - Methods for dimensionality reduction.
NMF - A Julia package for non-negative matrix factorization.
ANN - Julia artificial neural networks. [Deprecated]
Mocha - Deep Learning framework for Julia inspired by Caffe. [Deprecated]
XGBoost - eXtreme Gradient Boosting Package in Julia.
ManifoldLearning - A Julia package for manifold learning and nonlinear dimensionality reduction.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Merlin - Flexible Deep Learning Framework in Julia.
ROCAnalysis - Receiver Operating Characteristics and functions for evaluation probabilistic binary classifiers.
GaussianMixtures - Large scale Gaussian Mixture Models.
ScikitLearn - Julia implementation of the scikit-learn API.
Knet - Ko√ß University Deep Learning Framework.
Flux - Relax! Flux is the ML library that doesn't make you tensor


Natural Language Processing

Topic Models - TopicModels for Julia. [Deprecated]
Text Analysis - Julia package for text analysis.
Word Tokenizers - Tokenizers for Natural Language Processing in Julia
Corpus Loaders - A julia package providing a variety of loaders for various NLP corpora.
Embeddings - Functions and data dependencies for loading various word embeddings
Languages - Julia package for working with various human languages
WordNet - A Julia package for Princeton's WordNet


Data Analysis / Data Visualization

Graph Layout - Graph layout algorithms in pure Julia.
LightGraphs - Graph modeling and analysis.
Data Frames Meta - Metaprogramming tools for DataFrames.
Julia Data - library for working with tabular data in Julia. [Deprecated]
Data Read - Read files from Stata, SAS, and SPSS.
Hypothesis Tests - Hypothesis tests for Julia.
Gadfly - Crafty statistical graphics for Julia.
Stats - Statistical tests for Julia.
RDataSets - Julia package for loading many of the data sets available in R.
DataFrames - library for working with tabular data in Julia.
Distributions - A Julia package for probability distributions and associated functions.
Data Arrays - Data structures that allow missing values. [Deprecated]
Time Series - Time series toolkit for Julia.
Sampling - Basic sampling algorithms for Julia.


Misc Stuff / Presentations

DSP - Digital Signal Processing (filtering, periodograms, spectrograms, window functions).
JuliaCon Presentations - Presentations for JuliaCon.
SignalProcessing - Signal Processing tools for Julia.
Images - An image library for Julia.
DataDeps - Reproducible data setup for reproducible science.


Lua

General-Purpose Machine Learning

Torch7

cephes - Cephes mathematical functions library, wrapped for Torch. Provides and wraps the 180+ special mathematical functions from the Cephes mathematical library, developed by Stephen L. Moshier. It is used, among many other places, at the heart of SciPy. [Deprecated]
autograd - Autograd automatically differentiates native Torch code. Inspired by the original Python version.
graph - Graph package for Torch. [Deprecated]
randomkit - Numpy's randomkit, wrapped for Torch. [Deprecated]
signal - A signal processing toolbox for Torch-7. FFT, DCT, Hilbert, cepstrums, stft.
nn - Neural Network package for Torch.
torchnet - framework for torch which provides a set of abstractions aiming at encouraging code re-use as well as encouraging modular programming.
nngraph - This package provides graphical computation for nn library in Torch7.
nnx - A completely unstable and experimental package that extends Torch's builtin nn library.
rnn - A Recurrent Neural Network library that extends Torch's nn. RNNs, LSTMs, GRUs, BRNNs, BLSTMs, etc.
dpnn - Many useful features that aren't part of the main nn package.
dp - A deep learning library designed for streamlining research and development using the Torch7 distribution. It emphasizes flexibility through the elegant use of object-oriented design patterns. [Deprecated]
optim - An optimization library for Torch. SGD, Adagrad, Conjugate-Gradient, LBFGS, RProp and more.
unsup - A package for unsupervised learning in Torch. Provides modules that are compatible with nn (LinearPsd, ConvPsd, AutoEncoder, ...), and self-contained algorithms (k-means, PCA). [Deprecated]
manifold - A package to manipulate manifolds.
svm - Torch-SVM library. [Deprecated]
lbfgs - FFI Wrapper for liblbfgs. [Deprecated]
vowpalwabbit - An old vowpalwabbit interface to torch. [Deprecated]
OpenGM - OpenGM is a C++ library for graphical modeling, and inference. The Lua bindings provide a simple way of describing graphs, from Lua, and then optimizing them with OpenGM. [Deprecated]
spaghetti - Spaghetti (sparse linear) module for torch7 by @MichaelMathieu [Deprecated]
LuaSHKit - A lua wrapper around the Locality sensitive hashing library SHKit [Deprecated]
kernel smoothing - KNN, kernel-weighted average, local linear regression smoothers. [Deprecated]
cutorch - Torch CUDA Implementation.
cunn - Torch CUDA Neural Network Implementation.
imgraph - An image/graph library for Torch. This package provides routines to construct graphs on images, segment them, build trees out of them, and convert them back to images. [Deprecated]
videograph - A video/graph library for Torch. This package provides routines to construct graphs on videos, segment them, build trees out of them, and convert them back to videos. [Deprecated]
saliency - code and tools around integral images. A library for finding interest points based on fast integral histograms. [Deprecated]
stitch - allows us to use hugin to stitch images and apply same stitching to a video sequence. [Deprecated]
sfm - A bundle adjustment/structure from motion package. [Deprecated]
fex - A package for feature extraction in Torch. Provides SIFT and dSIFT modules. [Deprecated]
OverFeat - A state-of-the-art generic dense feature extractor. [Deprecated]
wav2letter - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research.


Numeric Lua
Lunatic Python
SciLua
Lua - Numerical Algorithms [Deprecated]
Lunum [Deprecated]


Demos and Scripts

Core torch7 demos repository.

linear-regression, logistic-regression
face detector (training and detection as separate demos)
mst-based-segmenter
train-a-digit-classifier
train-autoencoder
optical flow demo
train-on-housenumbers
train-on-cifar
tracking with deep nets
kinect demo
filter-bank visualization
saliency-networks


Training a Convnet for the Galaxy-Zoo Kaggle challenge(CUDA demo)
Music Tagging - Music Tagging scripts for torch7.
torch-datasets - Scripts to load several popular datasets including:

BSR 500
CIFAR-10
COIL
Street View House Numbers
MNIST
NORB


Atari2600 - Scripts to generate a dataset with static frames from the Arcade Learning Environment.


Matlab

Computer Vision

Contourlets - MATLAB source code that implements the contourlet transform and its utility functions.
Shearlets - MATLAB code for shearlet transform.
Curvelets - The Curvelet transform is a higher dimensional generalization of the Wavelet transform designed to represent images at different scales and different angles.
Bandlets - MATLAB code for bandlet transform.
mexopencv - Collection and a development kit of MATLAB mex functions for OpenCV library.


Natural Language Processing

NLP - An NLP library for Matlab.


General-Purpose Machine Learning

Training a deep autoencoder or a classifier
on MNIST digits - Training a deep autoencoder or a classifier
on MNIST digits[DEEP LEARNING].
Convolutional-Recursive Deep Learning for 3D Object Classification - Convolutional-Recursive Deep Learning for 3D Object Classification[DEEP LEARNING].
Spider - The spider is intended to be a complete object orientated environment for machine learning in Matlab.
LibSVM - A Library for Support Vector Machines.
ThunderSVM - An Open-Source SVM Library on GPUs and CPUs
LibLinear - A Library for Large Linear Classification.
Machine Learning Module - Class on machine w/ PDF, lectures, code
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
Pattern Recognition Toolbox - A complete object-oriented environment for machine learning in Matlab.
Pattern Recognition and Machine Learning - This package contains the matlab implementation of the algorithms described in the book Pattern Recognition and Machine Learning by C. Bishop.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly with MATLAB.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
Machine Learning in MatLab/Octave - examples of popular machine learning algorithms (neural networks, linear/logistic regressions, K-Means, etc.) with code examples and mathematics behind them being explained.


Data Analysis / Data Visualization

matlab_bgl - MatlabBGL is a Matlab package for working with graphs.
gaimc - Efficient pure-Matlab implementations of graph algorithms to complement MatlabBGL's mex functions.


.NET

Computer Vision

OpenCVDotNet - A wrapper for the OpenCV project to be used with .NET applications.
Emgu CV - Cross platform wrapper of OpenCV which can be compiled in Mono to be run on Windows, Linus, Mac OS X, iOS, and Android.
AForge.NET - Open source C# framework for developers and researchers in the fields of Computer Vision and Artificial Intelligence. Development has now shifted to GitHub.
Accord.NET - Together with AForge.NET, this library can provide image processing and computer vision algorithms to Windows, Windows RT and Windows Phone. Some components are also available for Java and Android.


Natural Language Processing

Stanford.NLP for .NET - A full port of Stanford NLP packages to .NET and also available precompiled as a NuGet package.


General-Purpose Machine Learning

Accord-Framework -The Accord.NET Framework is a complete framework for building machine learning, computer vision, computer audition, signal processing and statistical applications.
Accord.MachineLearning - Support Vector Machines, Decision Trees, Naive Bayesian models, K-means, Gaussian Mixture models and general algorithms such as Ransac, Cross-validation and Grid-Search for machine-learning applications. This package is part of the Accord.NET Framework.
DiffSharp - An automatic differentiation (AD) library providing exact and efficient derivatives (gradients, Hessians, Jacobians, directional derivatives, and matrix-free Hessian- and Jacobian-vector products) for machine learning and optimization applications. Operations can be nested to any level, meaning that you can compute exact higher-order derivatives and differentiate functions that are internally making use of differentiation, for applications such as hyperparameter optimization.
Encog - An advanced neural network and machine learning framework. Encog contains classes to create a wide variety of networks, as well as support classes to normalize and process data for these neural networks. Encog trains using multithreaded resilient propagation. Encog can also make use of a GPU to further speed processing time. A GUI based workbench is also provided to help model and train neural networks.
GeneticSharp - Multi-platform genetic algorithm library for .NET Core and .NET Framework. The library has several implementations of GA operators, like: selection, crossover, mutation, reinsertion and termination.
Infer.NET - Infer.NET is a framework for running Bayesian inference in graphical models. One can use Infer.NET to solve many different kinds of machine learning problems, from standard problems like classification, recommendation or clustering through to customised solutions to domain-specific problems. Infer.NET has been used in a wide variety of domains including information retrieval, bioinformatics, epidemiology, vision, and many others.
ML.NET - ML.NET is a cross-platform open-source machine learning framework which makes machine learning accessible to .NET developers. ML.NET was originally developed in Microsoft Research and evolved into a significant framework over the last decade and is used across many product groups in Microsoft like Windows, Bing, PowerPoint, Excel and more.
Neural Network Designer - DBMS management system and designer for neural networks. The designer application is developed using WPF, and is a user interface which allows you to design your neural network, query the network, create and configure chat bots that are capable of asking questions and learning from your feed back. The chat bots can even scrape the internet for information to return in their output as well as to use for learning.
Synapses - Neural network library in F#.
Vulpes - Deep belief and deep learning implementation written in F# and leverages CUDA GPU execution with Alea.cuBase.


Data Analysis / Data Visualization

numl - numl is a machine learning library intended to ease the use of using standard modeling techniques for both prediction and clustering.
Math.NET Numerics - Numerical foundation of the Math.NET project, aiming to provide methods and algorithms for numerical computations in science, engineering and every day use. Supports .Net 4.0, .Net 3.5 and Mono on Windows, Linux and Mac; Silverlight 5, WindowsPhone/SL 8, WindowsPhone 8.1 and Windows 8 with PCL Portable Profiles 47 and 344; Android/iOS with Xamarin.
Sho - Sho is an interactive environment for data analysis and scientific computing that lets you seamlessly connect scripts (in IronPython) with compiled code (in .NET) to enable fast and flexible prototyping. The environment includes powerful and efficient libraries for linear algebra as well as data visualization that can be used from any .NET language, as well as a feature-rich interactive shell for rapid development.


Objective C

General-Purpose Machine Learning

YCML - A Machine Learning framework for Objective-C and Swift (OS X / iOS).
MLPNeuralNet - Fast multilayer perceptron neural network library for iOS and Mac OS X. MLPNeuralNet predicts new examples by trained neural network. It is built on top of the Apple's Accelerate Framework, using vectorized operations and hardware acceleration if available. [Deprecated]
MAChineLearning - An Objective-C multilayer perceptron library, with full support for training through backpropagation. Implemented using vDSP and vecLib, it's 20 times faster than its Java equivalent. Includes sample code for use from Swift.
BPN-NeuralNetwork - It implemented 3 layers neural network ( Input Layer, Hidden Layer and Output Layer ) and it named Back Propagation Neural Network (BPN). This network can be used in products recommendation, user behavior analysis, data mining and data analysis. [Deprecated]
Multi-Perceptron-NeuralNetwork - it implemented multi-perceptrons neural network („Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ) based on Back Propagation Neural Network (BPN) and designed unlimited-hidden-layers.
KRHebbian-Algorithm - It is a non-supervisor and self-learning algorithm (adjust the weights) in neural network of Machine Learning. [Deprecated]
KRKmeans-Algorithm - It implemented K-Means the clustering and classification algorithm. It could be used in data mining and image compression. [Deprecated]
KRFuzzyCMeans-Algorithm - It implemented Fuzzy C-Means (FCM) the fuzzy clustering / classification algorithm on Machine Learning. It could be used in data mining and image compression. [Deprecated]


OCaml

General-Purpose Machine Learning

Oml - A general statistics and machine learning library.
GPR - Efficient Gaussian Process Regression in OCaml.
Libra-Tk - Algorithms for learning and inference with discrete probabilistic models.
TensorFlow - OCaml bindings for TensorFlow.


Perl

Data Analysis / Data Visualization

Perl Data Language, a pluggable architecture for data and image processing, which can
be used for machine learning.


General-Purpose Machine Learning

MXnet for Deep Learning, in Perl,
also released in CPAN.
Perl Data Language,
using AWS machine learning platform from Perl.
Algorithm::SVMLight,
implementation of Support Vector Machines with SVMLight under it. [Deprecated]
Several machine learning and artificial intelligence models are
included in the AI
namespace. For instance, you can
find Na√Øve Bayes.


Perl 6

Support Vector Machines
Na√Øve Bayes

Data Analysis / Data Visualization

Perl Data Language,
a pluggable architecture for data and image processing, which can
be
used for machine learning.

General-Purpose Machine Learning

PHP

Natural Language Processing

jieba-php - Chinese Words Segmentation Utilities.


General-Purpose Machine Learning

PHP-ML - Machine Learning library for PHP. Algorithms, Cross Validation, Neural Network, Preprocessing, Feature Extraction and much more in one library.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Rubix ML - A high-level machine learning (ML) library that lets you build programs that learn from data using the PHP language.
19 Questions - A machine learning / bayesian inference assigning attributes to objects.


Python

Computer Vision

Scikit-Image - A collection of algorithms for image processing in Python.
SimpleCV - An open source computer vision framework that gives access to several high-powered computer vision libraries, such as OpenCV. Written on Python and runs on Mac, Windows, and Ubuntu Linux.
Vigranumpy - Python bindings for the VIGRA C++ computer vision library.
OpenFace - Free and open source face recognition with deep neural networks.
PCV - Open source Python module for computer vision. [Deprecated]
face_recognition - Face recognition library that recognize and manipulate faces from Python or from the command line.
dockerface - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.
Detectron - FAIR's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN. It is written in Python and powered by the Caffe2 deep learning framework.
albumentations - –ê fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops.
pytessarct - Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and ""read"" the text embedded in images.Python-tesseract is a wrapper for Google's Tesseract-OCR Engine>.
imutils - A library containg Convenience functions to make basic image processing operations such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and Python.
PyTorchCV - A PyTorch-Based Framework for Deep Learning in Computer Vision.
neural-style-pt - A PyTorch implementation of Justin Johnson's neural-style (neural style transfer).


Natural Language Processing

pkuseg-python - A better version of Jieba, developed by Peking University.
NLTK - A leading platform for building Python programs to work with human language data.
Pattern - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.
Quepy - A python framework to transform natural language questions to queries in a database query language.
TextBlob - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.
YAlign - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora. [Deprecated]
jieba - Chinese Words Segmentation Utilities.
SnowNLP - A library for processing Chinese text.
spammy - A library for email Spam filtering built on top of nltk
loso - Another Chinese segmentation library. [Deprecated]
genius - A Chinese segment base on Conditional Random Field.
KoNLPy - A Python package for Korean natural language processing.
nut - Natural language Understanding Toolkit. [Deprecated]
Rosetta - Text processing tools and wrappers (e.g. Vowpal Wabbit)
BLLIP Parser - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser). [Deprecated]
PyNLPl - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for FoLiA, but also ARPA language models, Moses phrasetables, GIZA++ alignments.
python-ucto - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages).
python-frog - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)
python-zpar - Python bindings for ZPar, a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.
colibri-core - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.
spaCy - Industrial strength NLP with Python and Cython.
PyStanfordDependencies - Python interface for converting Penn Treebank trees to Stanford Dependencies.
Distance - Levenshtein and Hamming distance computation. [Deprecated]
Fuzzy Wuzzy - Fuzzy String Matching in Python.
jellyfish - a python library for doing approximate and phonetic matching of strings.
editdistance - fast implementation of edit distance.
textacy - higher-level NLP built on Spacy.
stanford-corenlp-python - Python wrapper for Stanford CoreNLP [Deprecated]
CLTK - The Classical Language Toolkit.
rasa_nlu - turn natural language into structured data.
yase - Transcode sentence (or other sequence) to list of word vector .
Polyglot - Multilingual text (NLP) processing toolkit.
DrQA - Reading Wikipedia to answer open-domain questions.
Dedupe - A python library for accurate and scalable fuzzy matching, record deduplication and entity-resolution.
Snips NLU - Natural Language Understanding library for intent classification and entity extraction
NeuroNER - Named-entity recognition using neural networks providing state-of-the-art-results
DeepPavlov - conversational AI library with many pretrained Russian NLP models.
BigARTM - topic modelling platform.


General-Purpose Machine Learning

PyOD -> Python Outlier Detection, comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. Featured for Advanced models, including Neural Networks/Deep Learning and Outlier Ensembles.
steppy -> Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces very simple interface that enables clean machine learning pipeline design.
steppy-toolkit -> Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective.
CNTK - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. Documentation can be found here.
auto_ml - Automated machine learning for production and analytics. Lets you focus on the fun parts of ML, while outputting production-ready code, and detailed analytics of your dataset and results. Includes support for NLP, XGBoost, CatBoost, LightGBM, and soon, deep learning.
machine learning - automated build consisting of a web-interface, and set of programmatic-interface API, for support vector machines. Corresponding dataset(s) are stored into a SQL database, then generated model(s) used for prediction(s), are stored into a NoSQL datastore.
XGBoost - Python bindings for eXtreme Gradient Boosting (Tree) Library.
Apache SINGA - An Apache Incubating project for developing an open source machine learning library.
Bayesian Methods for Hackers - Book/iPython notebooks on Probabilistic Programming in Python.
Featureforge A set of tools for creating and testing machine learning features, with a scikit-learn compatible API.
MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
scikit-learn - A Python module for machine learning built on top of SciPy.
metric-learn - A Python module for metric learning.
SimpleAI Python implementation of many of the artificial intelligence algorithms described on the book ""Artificial Intelligence, a Modern Approach"". It focuses on providing an easy to use, well documented and tested library.
astroML - Machine Learning and Data Mining for Astronomy.
graphlab-create - A library with various machine learning models (regression, clustering, recommender systems, graph analytics, etc.) implemented on top of a disk-backed DataFrame.
BigML - A library that contacts external servers.
pattern - Web mining module for Python.
NuPIC - Numenta Platform for Intelligent Computing.
Pylearn2 - A Machine Learning library based on Theano. [Deprecated]
keras - High-level neural networks frontend for TensorFlow, CNTK and Theano.
Lasagne - Lightweight library to build and train neural networks in Theano.
hebel - GPU-Accelerated Deep Learning Library in Python. [Deprecated]
Chainer - Flexible neural network framework.
prophet - Fast and automated time series forecasting framework by Facebook.
gensim - Topic Modelling for Humans.
topik - Topic modelling toolkit. [Deprecated]
PyBrain - Another Python Machine Learning Library.
Brainstorm - Fast, flexible and fun neural networks. This is the successor of PyBrain.
Surprise - A scikit for building and analyzing recommender systems.
Crab - A flexible, fast recommender engine. [Deprecated]
python-recsys - A Python library for implementing a Recommender System.
thinking bayes - Book on Bayesian Analysis.
Image-to-Image Translation with Conditional Adversarial Networks - Implementation of image to image (pix2pix) translation from the paper by isola et al.[DEEP LEARNING]
Restricted Boltzmann Machines -Restricted Boltzmann Machines in Python. [DEEP LEARNING]
Bolt - Bolt Online Learning Toolbox. [Deprecated]
CoverTree - Python implementation of cover trees, near-drop-in replacement for scipy.spatial.kdtree [Deprecated]
nilearn - Machine learning for NeuroImaging in Python.
neuropredict - Aimed at novice machine learners and non-expert programmers, this package offers easy (no coding needed) and comprehensive machine learning (evaluation and full report of predictive performance WITHOUT requiring you to code) in Python for NeuroImaging and any other type of features. This is aimed at absorbing the much of the ML workflow, unlike other packages like nilearn and pymvpa, which require you to learn their API and code to produce anything useful.
imbalanced-learn - Python module to perform under sampling and over sampling with various techniques.
Shogun - The Shogun Machine Learning Toolbox.
Pyevolve - Genetic algorithm framework. [Deprecated]
Caffe - A deep learning framework developed with cleanliness, readability, and speed in mind.
breze - Theano based library for deep and recurrent neural networks.
Cortex - Open source platform for deploying machine learning models in production.
pyhsmm - library for approximate unsupervised inference in Bayesian Hidden Markov Models (HMMs) and explicit-duration Hidden semi-Markov Models (HSMMs), focusing on the Bayesian Nonparametric extensions, the HDP-HMM and HDP-HSMM, mostly with weak-limit approximations.
mrjob - A library to let Python program run on Hadoop.
SKLL - A wrapper around scikit-learn that makes it simpler to conduct experiments.
neurolab
Spearmint - Spearmint is a package to perform Bayesian optimization according to the algorithms outlined in the paper: Practical Bayesian Optimization of Machine Learning Algorithms. Jasper Snoek, Hugo Larochelle and Ryan P. Adams. Advances in Neural Information Processing Systems, 2012. [Deprecated]
Pebl - Python Environment for Bayesian Learning. [Deprecated]
Theano - Optimizing GPU-meta-programming code generating array oriented optimizing math compiler in Python.
TensorFlow - Open source software library for numerical computation using data flow graphs.
pomegranate - Hidden Markov Models for Python, implemented in Cython for speed and efficiency.
python-timbl - A Python extension module wrapping the full TiMBL C++ programming interface. Timbl is an elaborate k-Nearest Neighbours machine learning toolkit.
deap - Evolutionary algorithm framework.
pydeep - Deep Learning In Python. [Deprecated]
mlxtend - A library consisting of useful tools for data science and machine learning tasks.
neon - Nervana's high-performance Python-based Deep Learning framework [DEEP LEARNING].
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search.
Neural Networks and Deep Learning - Code samples for my book ""Neural Networks and Deep Learning"" [DEEP LEARNING].
Annoy - Approximate nearest neighbours implementation.
TPOT - Tool that automatically creates and optimizes machine learning pipelines using genetic programming. Consider it your personal data science assistant, automating a tedious part of machine learning.
pgmpy A python library for working with Probabilistic Graphical Models.
DIGITS - The Deep Learning GPU Training System (DIGITS) is a web application for training deep learning models.
Orange - Open source data visualization and data analysis for novices and experts.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
milk - Machine learning toolkit focused on supervised classification. [Deprecated]
TFLearn - Deep learning library featuring a higher-level API for TensorFlow.
REP - an IPython-based environment for conducting data-driven research in a consistent and reproducible way. REP is not trying to substitute scikit-learn, but extends it and provides better user experience. [Deprecated]
rgf_python - Python bindings for Regularized Greedy Forest (Tree) Library.
skbayes - Python package for Bayesian Machine Learning with scikit-learn API.
fuku-ml - Simple machine learning library, including Perceptron, Regression, Support Vector Machine, Decision Tree and more, it's easy to use and easy to learn for beginners.
Xcessiv - A web-based application for quick, scalable, and automated hyperparameter tuning and stacked ensembling.
PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration
ML-From-Scratch - Implementations of Machine Learning models from scratch in Python with a focus on transparency. Aims to showcase the nuts and bolts of ML in an accessible way.
Edward - A library for probabilistic modeling, inference, and criticism. Built on top of TensorFlow.
xRBM - A library for Restricted Boltzmann Machine (RBM) and its conditional variants in Tensorflow.
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box. It is easy to install, well documented and supports CPU and GPU (even multi-GPU) computation.
stacked_generalization - Implementation of machine learning stacking technic as handy library in Python.
modAL - A modular active learning framework for Python, built on top of scikit-learn.
Cogitare: A Modern, Fast, and Modular Deep Learning and Machine Learning framework for Python.
Parris - Parris, the automated infrastructure setup tool for machine learning algorithms.
neonrvm - neonrvm is an open source machine learning library based on RVM technique. It's written in C programming language and comes with Python programming language bindings.
Turi Create - Machine learning from Apple. Turi Create simplifies the development of custom machine learning models. You don't have to be a machine learning expert to add recommendations, object detection, image classification, image similarity or activity classification to your app.
xLearn - A high performance, easy-to-use, and scalable machine learning package, which can be used to solve large-scale machine learning problems. xLearn is especially useful for solving machine learning problems on large-scale sparse data, which is very common in Internet services such as online advertisement and recommender systems.
mlens - A high performance, memory efficient, maximally parallelized ensemble learning, integrated with scikit-learn.
Netron - Visualizer for machine learning models.
Thampi - Machine Learning Prediction System on AWS Lambda
MindsDB - Open Source framework to streamline use of neural networks.
Microsoft Recommenders: Examples and best practices for building recommendation systems, provided as Jupyter notebooks. The repo contains some of the latest state of the art algorithms from Microsoft Research as well as from other companies and institutions.
StellarGraph: Machine Learning on Graphs, a Python library for machine learning on graph-structured (network-structured) data.
BentoML: Toolkit for package and deploy machine learning models for serving in production
MiraiML: An asynchronous engine for continuous & autonomous machine learning, built for real-time usage.
numpy-ML: Reference implementations of ML models written in numpy
creme: A framework for online machine learning.
Neuraxle: A framework providing the right abstractions to ease research, development, and deployment of your ML pipelines.
Cornac - A comparative framework for multimodal recommender systems with a focus on models leveraging auxiliary data.
JAX - JAX is Autograd and XLA, brought together for high-performance machine learning research.
fast.ai - A library simplifies training fast and accurate neural nets using modern best practices and already supports  vision, text, tabular, and collab (collaborative filtering) models ""out of the box""
Catalyst - High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.
Fastai - High-level wrapper built on the top of Pytorch which supports vision, text, tabular data and collaborative filtering.


Data Analysis / Data Visualization

SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering.
NumPy - A fundamental package for scientific computing with Python.
Numba - Python JIT (just in time) compiler to LLVM aimed at scientific Python by the developers of Cython and NumPy.
Mars - A tensor-based framework for large-scale data computation which often regarded as a parallel and distributed version of NumPy.
NetworkX - A high-productivity software for complex networks.
igraph - binding to igraph library - General purpose graph library.
Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools.
Open Mining - Business Intelligence (BI) in Python (Pandas web interface) [Deprecated]
PyMC - Markov Chain Monte Carlo sampling toolkit.
zipline - A Pythonic algorithmic trading library.
PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion based around NumPy, SciPy, IPython, and matplotlib.
SymPy - A Python library for symbolic mathematics.
statsmodels - Statistical modeling and econometrics in Python.
astropy - A community Python library for Astronomy.
matplotlib - A Python 2D plotting library.
bokeh - Interactive Web Plotting for Python.
plotly - Collaborative web plotting for Python and matplotlib.
altair - A Python to Vega translator.
d3py - A plotting library for Python, based on D3.js.
PyDexter - Simple plotting for Python. Wrapper for D3xterjs; easily render charts in-browser.
ggplot - Same API as ggplot2 for R. [Deprecated]
ggfortify - Unified interface to ggplot2 popular R packages.
Kartograph.py - Rendering beautiful SVG maps in Python.
pygal - A Python SVG Charts Creator.
PyQtGraph - A pure-python graphics and GUI library built on PyQt4 / PySide and NumPy.
pycascading [Deprecated]
Petrel - Tools for writing, submitting, debugging, and monitoring Storm topologies in pure Python.
Blaze - NumPy and Pandas interface to Big Data.
emcee - The Python ensemble sampling toolkit for affine-invariant MCMC.
windML - A Python Framework for Wind Energy Analysis and Prediction.
vispy - GPU-based high-performance interactive OpenGL 2D/3D data visualization library.
cerebro2 A web-based visualization and debugging platform for NuPIC. [Deprecated]
NuPIC Studio An all-in-one NuPIC Hierarchical Temporal Memory visualization and debugging super-tool! [Deprecated]
SparklingPandas Pandas on PySpark (POPS).
Seaborn - A python visualization library based on matplotlib.
bqplot - An API for plotting in Jupyter (IPython).
pastalog - Simple, realtime visualization of neural network training performance.
Superset - A data exploration platform designed to be visual, intuitive, and interactive.
Dora - Tools for exploratory data analysis in Python.
Ruffus - Computation Pipeline library for python.
SOMPY - Self Organizing Map written in Python (Uses neural networks for data analysis).
somoclu Massively parallel self-organizing maps: accelerate training on multicore CPUs, GPUs, and clusters, has python API.
HDBScan - implementation of the hdbscan algorithm in Python - used for clustering
visualize_ML - A python package for data exploration and data analysis. [Deprecated]
scikit-plot - A visualization library for quick and easy generation of common plots in data analysis and machine learning.
Bowtie - A dashboard library for interactive visualizations using flask socketio and react.
lime - Lime is about explaining what machine learning classifiers (or models) are doing. It is able to explain any black box classifier, with two or more classes.
PyCM - PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters
Dash - A framework for creating analytical web applications built on top of Plotly.js, React, and Flask
Lambdo - A workflow engine for solving machine learning problems by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation via user-defined (Python) functions.
TensorWatch - Debugging and visualization tool for machine learning and data science. It extensively leverages Jupyter Notebook to show real-time visualizations of data in running processes such as machine learning training.
dowel - A little logger for machine learning research. Output any object to the terminal, CSV, TensorBoard, text logs on disk, and more with just one call to logger.log().


Misc Scripts / iPython Notebooks / Codebases

Map/Reduce implementations of common ML algorithms: Jupyter notebooks that cover how to implement from scratch different ML algorithms (ordinary least squares, gradient descent, k-means, alternating least squares), using Python NumPy, and how to then make these implementations scalable using Map/Reduce and Spark.
BioPy - Biologically-Inspired and Machine Learning Algorithms in Python. [Deprecated]
SVM Explorer - Interactive SVM Explorer, using Dash and scikit-learn
pattern_classification
thinking stats 2
hyperopt
numpic
2012-paper-diginorm
A gallery of interesting IPython notebooks
ipython-notebooks
data-science-ipython-notebooks - Continually updated Data Science Python Notebooks: Spark, Hadoop MapReduce, HDFS, AWS, Kaggle, scikit-learn, matplotlib, pandas, NumPy, SciPy, and various command lines.
decision-weights
Sarah Palin LDA - Topic Modeling the Sarah Palin emails.
Diffusion Segmentation - A collection of image segmentation algorithms based on diffusion methods.
Scipy Tutorials - SciPy tutorials. This is outdated, check out scipy-lecture-notes.
Crab - A recommendation engine library for Python.
BayesPy - Bayesian Inference Tools in Python.
scikit-learn tutorials - Series of notebooks for learning scikit-learn.
sentiment-analyzer - Tweets Sentiment Analyzer
sentiment_classifier - Sentiment classifier using word sense disambiguation.
group-lasso - Some experiments with the coordinate descent algorithm used in the (Sparse) Group Lasso model.
jProcessing - Kanji / Hiragana / Katakana to Romaji Converter. Edict Dictionary & parallel sentences Search. Sentence Similarity between two JP Sentences. Sentiment Analysis of Japanese Text. Run Cabocha(ISO--8859-1 configured) in Python.
mne-python-notebooks - IPython notebooks for EEG/MEG data processing using mne-python.
Neon Course - IPython notebooks for a complete course around understanding Nervana's Neon.
pandas cookbook - Recipes for using Python's pandas library.
climin - Optimization library focused on machine learning, pythonic implementations of gradient descent, LBFGS, rmsprop, adadelta and others.
Allen Downey‚Äôs Data Science Course - Code for Data Science at Olin College, Spring 2014.
Allen Downey‚Äôs Think Bayes Code - Code repository for Think Bayes.
Allen Downey‚Äôs Think Complexity Code - Code for Allen Downey's book Think Complexity.
Allen Downey‚Äôs Think OS Code - Text and supporting code for Think OS: A Brief Introduction to Operating Systems.
Python Programming for the Humanities - Course for Python programming for the Humanities, assuming no prior knowledge. Heavy focus on text processing / NLP.
GreatCircle - Library for calculating great circle distance.
Optunity examples - Examples demonstrating how to use Optunity in synergy with machine learning libraries.
Dive into Machine Learning  with Python Jupyter notebook and scikit-learn - ""I learned Python by hacking first, and getting serious later. I wanted to do this with Machine Learning. If this is your style, join me in getting a bit ahead of yourself.""
TDB - TensorDebugger (TDB) is a visual debugger for deep learning. It features interactive, node-by-node debugging and visualization for TensorFlow.
Suiron - Machine Learning for RC Cars.
Introduction to machine learning with scikit-learn - IPython notebooks from Data School's video tutorials on scikit-learn.
Practical XGBoost in Python - comprehensive online course about using XGBoost in Python.
Introduction to Machine Learning with Python - Notebooks and code for the book ""Introduction to Machine Learning with Python""
Pydata book - Materials and IPython notebooks for ""Python for Data Analysis"" by Wes McKinney, published by O'Reilly Media
Homemade Machine Learning - Python examples of popular machine learning algorithms with interactive Jupyter demos and math being explained
Prodmodel - Build tool for data science pipelines.
the-elements-of-statistical-learning - This repository contains Jupyter notebooks implementing the algorithms found in the book and summary of the textbook.


Neural Networks

nn_builder - nn_builder is a python package that lets you build neural networks in 1 line
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences.
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg‚ÄìMarquardt algorithm.
=======
NeuralTalk - NeuralTalk is a Python+numpy project for learning Multimodal Recurrent Neural Networks that describe images with sentences. [Deprecated]
Neuron - Neuron is simple class for time series predictions. It's utilize LNU (Linear Neural Unit), QNU (Quadratic Neural Unit), RBF (Radial Basis Function), MLP (Multi Layer Perceptron), MLP-ELM (Multi Layer Perceptron - Extreme Learning Machine) neural networks learned with Gradient descent or LeLevenberg‚ÄìMarquardt algorithm. [Deprecated]
Data Driven Code - Very simple implementation of neural networks for dummies in python without using any libraries, with detailed comments.
Machine Learning, Data Science and Deep Learning with Python - LiveVideo course that covers machine learning, Tensorflow, artificial intelligence, and neural networks.


Kaggle Competition Source Code

open-solution-home-credit -> source code and experiments results for Home Credit Default Risk.
open-solution-googleai-object-detection -> source code and experiments results for Google AI Open Images - Object Detection Track.
open-solution-salt-identification -> source code and experiments results for TGS Salt Identification Challenge.
open-solution-ship-detection -> source code and experiments results for Airbus Ship Detection Challenge.
open-solution-data-science-bowl-2018 -> source code and experiments results for 2018 Data Science Bowl.
open-solution-value-prediction -> source code and experiments results for Santander Value Prediction Challenge.
open-solution-toxic-comments -> source code for Toxic Comment Classification Challenge.
wiki challenge - An implementation of Dell Zhang's solution to Wikipedia's Participation Challenge on Kaggle.
kaggle insults - Kaggle Submission for ""Detecting Insults in Social Commentary"".
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
kaggle-cifar - Code for the CIFAR-10 competition at Kaggle, uses cuda-convnet.
kaggle-blackbox - Deep learning made easy.
kaggle-accelerometer - Code for Accelerometer Biometric Competition at Kaggle.
kaggle-advertised-salaries - Predicting job salaries from ads - a Kaggle competition.
kaggle amazon - Amazon access control challenge.
kaggle-bestbuy_big - Code for the Best Buy competition at Kaggle.
kaggle-bestbuy_small
Kaggle Dogs vs. Cats - Code for Kaggle Dogs vs. Cats competition.
Kaggle Galaxy Challenge - Winning solution for the Galaxy Challenge on Kaggle.
Kaggle Gender - A Kaggle competition: discriminate gender based on handwriting.
Kaggle Merck - Merck challenge at Kaggle.
Kaggle Stackoverflow - Predicting closed questions on Stack Overflow.
kaggle_acquire-valued-shoppers-challenge - Code for the Kaggle acquire valued shoppers challenge.
wine-quality - Predicting wine quality.


Reinforcement Learning

DeepMind Lab - DeepMind Lab is a 3D learning environment based on id Software's Quake III Arena via ioquake3 and other open source software. Its primary purpose is to act as a testbed for research in artificial intelligence, especially deep reinforcement learning.
Gym - OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms.
Serpent.AI - Serpent.AI is a game agent framework that allows you to turn any video game you own into a sandbox to develop AI and machine learning experiments. For both researchers and hobbyists.
ViZDoom - ViZDoom allows developing AI bots that play Doom using only the visual information (the screen buffer). It is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.
Roboschool - Open-source software for robot simulation, integrated with OpenAI Gym.
Retro - Retro Games in Gym
SLM Lab - Modular Deep Reinforcement Learning framework in PyTorch.
Coach - Reinforcement Learning Coach by Intel¬Æ AI Lab enables easy experimentation with state of the art Reinforcement Learning algorithms
garage - A toolkit for reproducible reinforcement learning research
metaworld - An open source robotics benchmark for meta- and multi-task reinforcement learning


Ruby

Natural Language Processing

Awesome NLP with Ruby - Curated link list for practical natural language processing in Ruby.
Treat - Text REtrieval and Annotation Toolkit, definitely the most comprehensive toolkit I‚Äôve encountered so far for Ruby.
Stemmer - Expose libstemmer_c to Ruby. [Deprecated]
Raspel - raspell is an interface binding for ruby. [Deprecated]
UEA Stemmer - Ruby port of UEALite Stemmer - a conservative stemmer for search and indexing.
Twitter-text-rb - A library that does auto linking and extraction of usernames, lists and hashtags in tweets.


General-Purpose Machine Learning

Awesome Machine Learning with Ruby - Curated list of ML related resources for Ruby.
Ruby Machine Learning - Some Machine Learning algorithms, implemented in Ruby. [Deprecated]
Machine Learning Ruby [Deprecated]
jRuby Mahout - JRuby Mahout is a gem that unleashes the power of Apache Mahout in the world of JRuby. [Deprecated]
CardMagic-Classifier - A general classifier module to allow Bayesian and other types of classifications.
rb-libsvm - Ruby language bindings for LIBSVM which is a Library for Support Vector Machines.
Scoruby - Creates Random Forest classifiers from PMML files.


Data Analysis / Data Visualization

rsruby - Ruby - R bridge.
data-visualization-ruby - Source code and supporting content for my Ruby Manor presentation on Data Visualisation with Ruby. [Deprecated]
ruby-plot - gnuplot wrapper for Ruby, especially for plotting ROC curves into SVG files. [Deprecated]
plot-rb - A plotting library in Ruby built on top of Vega and D3. [Deprecated]
scruffy - A beautiful graphing toolkit for Ruby.
SciRuby
Glean - A data management tool for humans. [Deprecated]
Bioruby
Arel [Deprecated]


Misc

Big Data For Chimps
Listof - Community based data collection, packed in gem. Get list of pretty much anything (stop words, countries, non words) in txt, json or hash. Demo/Search for a list


Rust

General-Purpose Machine Learning

deeplearn-rs - deeplearn-rs provides simple networks that use matrix multiplication, addition, and ReLU under the MIT license.
rustlearn - a machine learning framework featuring logistic regression, support vector machines, decision trees and random forests.
rusty-machine - a pure-rust machine learning library.
leaf - open source framework for machine intelligence, sharing concepts from TensorFlow and Caffe. Available under the MIT license. [Deprecated]
RustNN - RustNN is a feedforward neural network library. [Deprecated]
RusticSOM - A Rust library for Self Organising Maps (SOM).


R

General-Purpose Machine Learning

ahaz - ahaz: Regularization for semiparametric additive hazards regression. [Deprecated]
arules - arules: Mining Association Rules and Frequent Itemsets
biglasso - biglasso: Extending Lasso Model Fitting to Big Data in R.
bmrm - bmrm: Bundle Methods for Regularized Risk Minimization Package.
Boruta - Boruta: A wrapper algorithm for all-relevant feature selection.
bst - bst: Gradient Boosting.
C50 - C50: C5.0 Decision Trees and Rule-Based Models.
caret - Classification and Regression Training: Unified interface to ~150 ML algorithms in R.
caretEnsemble - caretEnsemble: Framework for fitting multiple caret models as well as creating ensembles of such models. [Deprecated]
CatBoost - General purpose gradient boosting on decision trees library with categorical features support out of the box for R.
Clever Algorithms For Machine Learning
CORElearn - CORElearn: Classification, regression, feature evaluation and ordinal evaluation.
CoxBoost - CoxBoost: Cox models by likelihood based boosting for a single survival endpoint or competing risks [Deprecated]
Cubist - Cubist: Rule- and Instance-Based Regression Modeling.
e1071 - e1071: Misc Functions of the Department of Statistics (e1071), TU Wien
earth - earth: Multivariate Adaptive Regression Spline Models
elasticnet - elasticnet: Elastic-Net for Sparse Estimation and Sparse PCA.
ElemStatLearn - ElemStatLearn: Data sets, functions and examples from the book: ""The Elements of Statistical Learning, Data Mining, Inference, and Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman Prediction"" by Trevor Hastie, Robert Tibshirani and Jerome Friedman.
evtree - evtree: Evolutionary Learning of Globally Optimal Trees.
forecast - forecast: Timeseries forecasting using ARIMA, ETS, STLM, TBATS, and neural network models.
forecastHybrid - forecastHybrid: Automatic ensemble and cross validation of ARIMA, ETS, STLM, TBATS, and neural network models from the ""forecast"" package.
fpc - fpc: Flexible procedures for clustering.
frbs - frbs: Fuzzy Rule-based Systems for Classification and Regression Tasks. [Deprecated]
GAMBoost - GAMBoost: Generalized linear and additive models by likelihood based boosting. [Deprecated]
gamboostLSS - gamboostLSS: Boosting Methods for GAMLSS.
gbm - gbm: Generalized Boosted Regression Models.
glmnet - glmnet: Lasso and elastic-net regularized generalized linear models.
glmpath - glmpath: L1 Regularization Path for Generalized Linear Models and Cox Proportional Hazards Model.
GMMBoost - GMMBoost: Likelihood-based Boosting for Generalized mixed models. [Deprecated]
grplasso - grplasso: Fitting user specified models with Group Lasso penalty.
grpreg - grpreg: Regularization paths for regression models with grouped covariates.
h2o - A framework for fast, parallel, and distributed machine learning algorithms at scale -- Deeplearning, Random forests, GBM, KMeans, PCA, GLM.
hda - hda: Heteroscedastic Discriminant Analysis. [Deprecated]
Introduction to Statistical Learning
ipred - ipred: Improved Predictors.
kernlab - kernlab: Kernel-based Machine Learning Lab.
klaR - klaR: Classification and visualization.
L0Learn - L0Learn: Fast algorithms for best subset selection.
lars - lars: Least Angle Regression, Lasso and Forward Stagewise. [Deprecated]
lasso2 - lasso2: L1 constrained estimation aka ‚Äòlasso‚Äô.
LiblineaR - LiblineaR: Linear Predictive Models Based On The Liblinear C/C++ Library.
LogicReg - LogicReg: Logic Regression.
Machine Learning For Hackers
maptree - maptree: Mapping, pruning, and graphing tree models. [Deprecated]
mboost - mboost: Model-Based Boosting.
medley - medley: Blending regression models, using a greedy stepwise approach.
mlr - mlr: Machine Learning in R.
ncvreg - ncvreg: Regularization paths for SCAD- and MCP-penalized regression models.
nnet - nnet: Feed-forward Neural Networks and Multinomial Log-Linear Models. [Deprecated]
pamr - pamr: Pam: prediction analysis for microarrays. [Deprecated]
party - party: A Laboratory for Recursive Partytioning.
partykit - partykit: A Toolkit for Recursive Partytioning.
penalized - penalized: L1 (lasso and fused lasso) and L2 (ridge) penalized estimation in GLMs and in the Cox model.
penalizedLDA - penalizedLDA: Penalized classification using Fisher's linear discriminant. [Deprecated]
penalizedSVM - penalizedSVM: Feature Selection SVM using penalty functions.
quantregForest - quantregForest: Quantile Regression Forests.
randomForest - randomForest: Breiman and Cutler's random forests for classification and regression.
randomForestSRC - randomForestSRC: Random Forests for Survival, Regression and Classification (RF-SRC).
rattle - rattle: Graphical user interface for data mining in R.
rda - rda: Shrunken Centroids Regularized Discriminant Analysis.
rdetools - rdetools: Relevant Dimension Estimation (RDE) in Feature Spaces. [Deprecated]
REEMtree - REEMtree: Regression Trees with Random Effects for Longitudinal (Panel) Data. [Deprecated]
relaxo - relaxo: Relaxed Lasso. [Deprecated]
rgenoud - rgenoud: R version of GENetic Optimization Using Derivatives
Rmalschains - Rmalschains: Continuous Optimization using Memetic Algorithms with Local Search Chains (MA-LS-Chains) in R.
rminer - rminer: Simpler use of data mining methods (e.g. NN and SVM) in classification and regression. [Deprecated]
ROCR - ROCR: Visualizing the performance of scoring classifiers. [Deprecated]
RoughSets - RoughSets: Data Analysis Using Rough Set and Fuzzy Rough Set Theories. [Deprecated]
rpart - rpart: Recursive Partitioning and Regression Trees.
RPMM - RPMM: Recursively Partitioned Mixture Model.
RSNNS - RSNNS: Neural Networks in R using the Stuttgart Neural Network Simulator (SNNS).
RWeka - RWeka: R/Weka interface.
RXshrink - RXshrink: Maximum Likelihood Shrinkage via Generalized Ridge or Least Angle Regression.
sda - sda: Shrinkage Discriminant Analysis and CAT Score Variable Selection. [Deprecated]
spectralGraphTopology - spectralGraphTopology: Learning Graphs from Data via Spectral Constraints.
SuperLearner - Multi-algorithm ensemble learning packages.
svmpath - svmpath: svmpath: the SVM Path algorithm. [Deprecated]
tgp - tgp: Bayesian treed Gaussian process models. [Deprecated]
tree - tree: Classification and regression trees.
varSelRF - varSelRF: Variable selection using random forests.
XGBoost.R - R binding for eXtreme Gradient Boosting (Tree) Library.
Optunity - A library dedicated to automated hyperparameter optimization with a simple, lightweight API to facilitate drop-in replacement of grid search. Optunity is written in Python but interfaces seamlessly to R.
igraph - binding to igraph library - General purpose graph library.
MXNet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, Javascript and more.
TDSP-Utilities - Two data science utilities in R from Microsoft: 1) Interactive Data Exploration, Analysis, and Reporting (IDEAR) ; 2) Automated Modeling and Reporting (AMR).


Data Analysis / Data Visualization

ggplot2 - A data visualization package based on the grammar of graphics.
tmap for visualizing geospatial data with static maps and leaflet for interactive maps
tm and quanteda are the main packages for managing,  analyzing, and visualizing textual data.
shiny is the basis for truly interactive displays and dashboards in R. However, some measure of interactivity can be achieved with htmlwidgets bringing javascript libraries to R. These include, plotly, dygraphs, highcharter, and several others.


SAS

General-Purpose Machine Learning

Visual Data Mining and Machine Learning - Interactive, automated, and programmatic modeling with the latest machine learning algorithms in and end-to-end analytics environment, from data prep to deployment. Free trial available.
Enterprise Miner - Data mining and machine learning that creates deployable models using a GUI or code.
Factory Miner - Automatically creates deployable machine learning models across numerous market or customer segments using a GUI.


Data Analysis / Data Visualization

SAS/STAT - For conducting advanced statistical analysis.
University Edition - FREE! Includes all SAS packages necessary for data analysis and visualization, and includes online SAS courses.


Natural Language Processing

Contextual Analysis - Add structure to unstructured text using a GUI.
Sentiment Analysis - Extract sentiment from text using a GUI.
Text Miner - Text mining using a GUI or code.


Demos and Scripts

ML_Tables - Concise cheat sheets containing machine learning best practices.
enlighten-apply - Example code and materials that illustrate applications of SAS machine learning techniques.
enlighten-integration - Example code and materials that illustrate techniques for integrating SAS with other analytics technologies in Java, PMML, Python and R.
enlighten-deep - Example code and materials that illustrate using neural networks with several hidden layers in SAS.
dm-flow - Library of SAS Enterprise Miner process flow diagrams to help you learn by example about specific data mining topics.


Scala

Natural Language Processing

ScalaNLP - ScalaNLP is a suite of machine learning and numerical computing libraries.
Breeze - Breeze is a numerical processing library for Scala.
Chalk - Chalk is a natural language processing library. [Deprecated]
FACTORIE - FACTORIE is a toolkit for deployable probabilistic modeling, implemented as a software library in Scala. It provides its users with a succinct language for creating relational factor graphs, estimating parameters and performing inference.
Montague - Montague is a semantic parsing library for Scala with an easy-to-use DSL.
Spark NLP - Natural language processing library built on top of Apache Spark ML to provide simple, performant, and accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.


Data Analysis / Data Visualization

MLlib in Apache Spark - Distributed machine learning library in Spark
Hydrosphere Mist - a service for deployment Apache Spark MLLib machine learning models as realtime, batch or reactive web services.
Scalding - A Scala API for Cascading.
Summing Bird - Streaming MapReduce with Scalding and Storm.
Algebird - Abstract Algebra for Scala.
xerial - Data management utilities for Scala. [Deprecated]
PredictionIO - PredictionIO, a machine learning server for software developers and data engineers.
BIDMat - CPU and GPU-accelerated matrix library intended to support large-scale exploratory data analysis.
Flink - Open source platform for distributed stream and batch data processing.
Spark Notebook - Interactive and Reactive Data Science using Scala and Spark.


General-Purpose Machine Learning

DeepLearning.scala - Creating statically typed dynamic neural networks from object-oriented & functional programming constructs.
Conjecture - Scalable Machine Learning in Scalding.
brushfire - Distributed decision tree ensemble learning in Scala.
ganitha - Scalding powered machine learning. [Deprecated]
adam - A genomics processing engine and specialized file format built using Apache Avro, Apache Spark and Parquet. Apache 2 licensed.
bioscala - Bioinformatics for the Scala programming language
BIDMach - CPU and GPU-accelerated Machine Learning Library.
Figaro - a Scala library for constructing probabilistic models.
H2O Sparkling Water - H2O and Spark interoperability.
FlinkML in Apache Flink - Distributed machine learning library in Flink.
DynaML - Scala Library/REPL for Machine Learning Research.
Saul - Flexible Declarative Learning-Based Programming.
SwiftLearner - Simply written algorithms to help study ML or write your own implementations.
Smile - Statistical Machine Intelligence and Learning Engine.
doddle-model - An in-memory machine learning library built on top of Breeze. It provides immutable objects and exposes its functionality through a scikit-learn-like API.
TensorFlow Scala -   Strongly-typed Scala API for TensorFlow.


Scheme

Neural Networks

layer - Neural network inference from the command line, implemented in CHICKEN Scheme.


Swift

General-Purpose Machine Learning

Bender - Fast Neural Networks framework built on top of Metal. Supports TensorFlow models.
Swift AI - Highly optimized artificial intelligence and machine learning library written in Swift.
Swift for Tensorflow - a next-generation platform for machine learning, incorporating the latest research across machine learning, compilers, differentiable programming, systems design, and beyond.
BrainCore - The iOS and OS X neural network framework.
swix - A bare bones library that includes a general matrix language and wraps some OpenCV for iOS development. [Deprecated]
AIToolbox - A toolbox framework of AI modules written in Swift: Graphs/Trees, Linear Regression, Support Vector Machines, Neural Networks, PCA, KMeans, Genetic Algorithms, MDP, Mixture of Gaussians.
MLKit - A simple Machine Learning Framework written in Swift. Currently features Simple Linear Regression, Polynomial Regression, and Ridge Regression.
Swift Brain - The first neural network / machine learning library written in Swift. This is a project for AI algorithms in Swift for iOS and OS X development. This project includes algorithms focused on Bayes theorem, neural networks, SVMs, Matrices, etc...
Perfect TensorFlow - Swift Language Bindings of TensorFlow. Using native TensorFlow models on both macOS / Linux.
PredictionBuilder - A library for machine learning that builds predictions using a linear regression.
Awesome CoreML - A curated list of pretrained CoreML models.
Awesome Core ML Models - A curated list of machine learning models in CoreML format.


TensorFlow

General-Purpose Machine Learning

Awesome TensorFlow - A list of all things related to TensorFlow.
Golden TensorFlow - A page of content on TensorFlow, including academic papers and links to related topics.


Tools

Neural Networks

layer - Neural network inference from the command line


Misc

ML Workspace - All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code).
Notebooks - A starter kit for Jupyter notebooks and machine learning. Companion docker images consist of all combinations of python versions, machine learning frameworks (Keras, PyTorch and Tensorflow) and CPU/CUDA versions.
DVC - Data Science Version Control is an open-source version control system for machine learning projects with pipelines support. It makes ML projects reproducible and shareable.
Kedro - Kedro is a data and development workflow framework that implements best practices for data pipelines with an eye towards productionizing machine learning models.
guild.ai - Tool to log, analyze, compare and ""optimize"" experiments. It's cross-platform and framework independent, and provided integrated visualizers such as tensorboard.
Sacred - Python tool to help  you configure, organize, log and reproduce experiments. Like a notebook lab in the context of Chemestry/Biology. The community has built multiple add-ons leveraging the proposed standard.
MLFlow - platform to manage the ML lifecycle, including experimentation, reproducibility and deployment. Framework anf language agnostic, take a look at all the built-in integrations.
More tools to improve the ML lifecycle: Catalyst, PachydermIO. The following are Github-alike and targetting teams Weights & Biases, Neptune.Ml, Comet.ml, Valohai.ai.


Credits

Some of the python libraries were cut-and-pasted from vinta
References for Go were mostly cut-and-pasted from gopherdata

","GitHub - josephmisiti/awesome-machine-learning: A curated list of awesome Machine Learning frameworks, libraries and software."
7,Python,"Scrapy













Overview
Scrapy is a fast high-level web crawling and web scraping framework, used to
crawl websites and extract structured data from their pages. It can be used for
a wide range of purposes, from data mining to monitoring and automated testing.
Check the Scrapy homepage at https://scrapy.org for more information,
including a list of features.

Requirements

Python 3.5+
Works on Linux, Windows, Mac OSX, BSD


Install
The quick way:
pip install scrapy

See the install section in the documentation at
https://docs.scrapy.org/en/latest/intro/install.html for more details.

Documentation
Documentation is available online at https://docs.scrapy.org/ and in the docs
directory.

Releases
You can check https://docs.scrapy.org/en/latest/news.html for the release notes.

Community (blog, twitter, mail list, IRC)
See https://scrapy.org/community/ for details.

Contributing
See https://docs.scrapy.org/en/master/contributing.html for details.

Code of Conduct
Please note that this project is released with a Contributor Code of Conduct
(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).
By participating in this project you agree to abide by its terms.
Please report unacceptable behavior to opensource@scrapinghub.com.

Companies using Scrapy
See https://scrapy.org/companies/ for a list.

Commercial Support
See https://scrapy.org/support/ for details.
","GitHub - scrapy/scrapy: Scrapy, a fast high-level web crawling & scraping framework for Python."
8,Python,"Removed according to regulations.
",GitHub - shadowsocks/shadowsocks
9,Python,"Face Recognition
You can also read a translated version of this file in Chinese ÁÆÄ‰Ωì‰∏≠ÊñáÁâà or in Korean ÌïúÍµ≠Ïñ¥.
Recognize and manipulate faces from Python or from the command line with
the world's simplest face recognition library.
Built using dlib's state-of-the-art face recognition
built with deep learning. The model has an accuracy of 99.38% on the
Labeled Faces in the Wild benchmark.
This also provides a simple face_recognition command line tool that lets
you do face recognition on a folder of images from the command line!



Features
Find faces in pictures
Find all the faces that appear in a picture:

import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_locations = face_recognition.face_locations(image)
Find and manipulate facial features in pictures
Get the locations and outlines of each person's eyes, nose, mouth and chin.

import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)
Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff
like applying digital make-up (think 'Meitu'):

Identify faces in pictures
Recognize who appears in each photo.

import face_recognition
known_image = face_recognition.load_image_file(""biden.jpg"")
unknown_image = face_recognition.load_image_file(""unknown.jpg"")

biden_encoding = face_recognition.face_encodings(known_image)[0]
unknown_encoding = face_recognition.face_encodings(unknown_image)[0]

results = face_recognition.compare_faces([biden_encoding], unknown_encoding)
You can even use this library with other Python libraries to do real-time face recognition:

See this example for the code.
Online Demos
User-contributed shared Jupyter notebook demo (not officially supported): 
Installation
Requirements

Python 3.3+ or Python 2.7
macOS or Linux (Windows not officially supported, but might work)

Installation Options:
Installing on Mac or Linux
First, make sure you have dlib already installed with Python bindings:

How to install dlib from source on macOS or Ubuntu

Then, install this module from pypi using pip3 (or pip2 for Python 2):
pip3 install face_recognition
Alternatively, you can try this library with Docker, see this section.
If you are having trouble with installation, you can also try out a
pre-configured VM.
Installing on an Nvidia Jetson Nano board

Jetson Nano installation instructions

Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.



Installing on Raspberry Pi 2+

Raspberry Pi 2+ installation instructions

Installing on Windows
While Windows isn't officially supported, helpful users have posted instructions on how to install this library:

@masoudr's Windows 10 installation guide (dlib + face_recognition)

Installing a pre-configured Virtual Machine image

Download the pre-configured VM image (for VMware Player or VirtualBox).

Usage
Command-Line Interface
When you install face_recognition, you get two simple command-line
programs:

face_recognition - Recognize faces in a photograph or folder full for
photographs.
face_detection - Find faces in a photograph or folder full for photographs.

face_recognition command line tool
The face_recognition command lets you recognize faces in a photograph or
folder full  for photographs.
First, you need to provide a folder with one picture of each person you
already know. There should be one image file for each person with the
files named according to who is in the picture:

Next, you need a second folder with the files you want to identify:

Then in you simply run the command face_recognition, passing in
the folder of known people and the folder (or single image) with unknown
people and it tells you who is in each image:
$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
There's one line in the output for each face. The data is comma-separated
with the filename and the name of the person found.
An unknown_person is a face in the image that didn't match anyone in
your folder of known people.
face_detection command line tool
The face_detection command lets you find the location (pixel coordinatates)
of any faces in an image.
Just run the command face_detection, passing in a folder of images
to check (or a single image):
$ face_detection  ./folder_with_pictures/

examples/image1.jpg,65,215,169,112
examples/image2.jpg,62,394,211,244
examples/image2.jpg,95,941,244,792
It prints one line for each face that was detected. The coordinates
reported are the top, right, bottom and left coordinates of the face (in pixels).
Adjusting Tolerance / Sensitivity
If you are getting multiple matches for the same person, it might be that
the people in your photos look very similar and a lower tolerance value
is needed to make face comparisons more strict.
You can do that with the --tolerance parameter. The default tolerance
value is 0.6 and lower numbers make face comparisons more strict:
$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
If you want to see the face distance calculated for each match in order
to adjust the tolerance setting, you can use --show-distance true:
$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None
More Examples
If you simply want to know the names of the people in each photograph but don't
care about file names, you could do this:
$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2

Barack Obama
unknown_person
Speeding up Face Recognition
Face recognition can be done in parallel if you have a computer with
multiple CPU cores. For example, if your system has 4 CPU cores, you can
process about 4 times as many images in the same amount of time by using
all your CPU cores in parallel.
If you are using Python 3.4 or newer, pass in a --cpus <number_of_cpu_cores_to_use> parameter:
$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/
You can also pass in --cpus -1 to use all CPU cores in your system.
Python Module
You can import the face_recognition module and then easily manipulate
faces with just a couple of lines of code. It's super easy!
API Docs: https://face-recognition.readthedocs.io.
Automatically find all the faces in an image
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image)

# face_locations is now an array listing the co-ordinates of each face!
See this example
to try it out.
You can also opt-in to a somewhat more accurate deep-learning-based face detection model.
Note: GPU acceleration (via NVidia's CUDA library) is required for good
performance with this model. You'll also want to enable CUDA support
when compliling dlib.
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image, model=""cnn"")

# face_locations is now an array listing the co-ordinates of each face!
See this example
to try it out.
If you have a lot of images and a GPU, you can also
find faces in batches.
Automatically locate the facial features of a person in an image
import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)

# face_landmarks_list is now an array with the locations of each facial feature in each face.
# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.
See this example
to try it out.
Recognize faces in images and identify who they are
import face_recognition

picture_of_me = face_recognition.load_image_file(""me.jpg"")
my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]

# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!

unknown_picture = face_recognition.load_image_file(""unknown.jpg"")
unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]

# Now we can see the two face encodings are of the same person with `compare_faces`!

results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)

if results[0] == True:
    print(""It's a picture of me!"")
else:
    print(""It's not a picture of me!"")
See this example
to try it out.
Python Code Examples
All the examples are available here.
Face Detection

Find faces in a photograph
Find faces in a photograph (using deep learning)
Find faces in batches of images w/ GPU (using deep learning)
Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)

Facial Features

Identify specific facial features in a photograph
Apply (horribly ugly) digital make-up

Facial Recognition

Find and recognize unknown faces in a photograph based on photographs of known people
Identify and draw boxes around each person in a photo
Compare faces by numeric face distance instead of only True/False matches
Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)
Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)
Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)
Recognize faces on a Raspberry Pi w/ camera
Run a web service to recognize faces via HTTP (Requires Flask to be installed)
Recognize faces with a K-nearest neighbors classifier
Train multiple images per person then recognize faces using a SVM

Creating a Standalone Executable
If you want to create a standalone executable that can run without the need to install python or face_recognition, you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.
Articles and Guides that cover face_recognition

My article on how Face Recognition works: Modern Face Recognition with Deep Learning

Covers the algorithms and how they generally work


Face recognition with OpenCV, Python, and deep learning by Adrian Rosebrock

Covers how to use face recognition in practice


Raspberry Pi Face Recognition by Adrian Rosebrock

Covers how to use this on a Raspberry Pi


Face clustering with Python by Adrian Rosebrock

Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning



How Face Recognition Works
If you want to learn how face location and recognition work instead of
depending on a black box library, read my article.
Caveats

The face recognition model is trained on adults and does not work very well on children. It tends to mix
up children quite easy using the default comparison threshold of 0.6.
Accuracy may vary between ethnic groups. Please see this wiki page for more details.

Deployment to Cloud Hosts (Heroku, AWS, etc)
Since face_recognition depends on dlib which is written in C++, it can be tricky to deploy an app
using it to a cloud hosting provider like Heroku or AWS.
To make things easier, there's an example Dockerfile in this repo that shows how to run an app built with
face_recognition in a Docker container. With that, you should be able to deploy
to any service that supports Docker images.
You can try the Docker image locally by running: docker-compose up --build
Linux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the dockerfile: Dockerfile.gpu and runtime: nvidia lines.
Having problems?
If you run into problems, please read the Common Errors section of the wiki before filing a github issue.
Thanks

Many, many thanks to Davis King (@nulhom)
for creating dlib and for providing the trained facial feature detection and face encoding models
used in this library. For more information on the ResNet that powers the face encodings, check out
his blog post.
Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,
pillow, etc, etc that makes this kind of stuff so easy and fun in Python.
Thanks to Cookiecutter and the
audreyr/cookiecutter-pypackage project template
for making Python project packaging way more tolerable.

",GitHub - ageitgey/face_recognition: The world's simplest facial recognition api for Python and the command line
10,Python,"   





LocalStack - A fully functional local AWS cloud stack

LocalStack provides an easy-to-use test/mocking framework for developing Cloud applications.
Currently, the focus is primarily on supporting the AWS cloud stack.
Announcements

2019-10-09: LocalStack Pro is out! We're incredibly excited to announce the launch of LocalStack Pro - the enterprise version of LocalStack with additional APIs and advanced features. Check out the free trial at https://localstack.cloud
2018-01-10: Help wanted! Please fill out this survey to support a research study on the usage of Serverless and Function-as-a-Service (FaaS) services, conducted at Chalmers University of Technology. The survey only takes 5-10 minutes of your time. Many thanks for your participation!!

The result from this study can be found here


2017-08-27: We need your support! LocalStack is growing fast, we now have thousands of developers using the platform on a regular basis. Last month we have recorded a staggering 100k test runs, with 25k+ DynamoDB tables, 20k+ SQS queues, 15k+ Kinesis streams, 13k+ S3 buckets, and 10k+ Lambda functions created locally - for 0$ costs (more details to be published soon). Bug and feature requests are pouring in, and we now need some support from you to keep the open source version actively maintained. Please check out Open Collective and become a backer or supporter of the project today! Thanks everybody for contributing. ‚ô•
2017-07-20: Please note: Starting with version 0.7.0, the Docker image will be pushed
and kept up to date under the new name localstack/localstack. (This means that you may
have to update your CI configurations.) Please refer to the updated
End-User License Agreement (EULA) for the new versions.
The old Docker image (atlassianlabs/localstack) is still available but will not be maintained
any longer.

Overview
LocalStack spins up the following core Cloud APIs on your local machine:

API Gateway at http://localhost:4567
Kinesis at http://localhost:4568
DynamoDB at http://localhost:4569
DynamoDB Streams at http://localhost:4570
Elasticsearch at http://localhost:4571
S3 at http://localhost:4572
Firehose at http://localhost:4573
Lambda at http://localhost:4574
SNS at http://localhost:4575
SQS at http://localhost:4576
Redshift at http://localhost:4577
ES (Elasticsearch Service) at http://localhost:4578
SES at http://localhost:4579
Route53 at http://localhost:4580
CloudFormation at http://localhost:4581
CloudWatch at http://localhost:4582
SSM at http://localhost:4583
SecretsManager at http://localhost:4584
StepFunctions at http://localhost:4585
CloudWatch Logs at http://localhost:4586
EventBridge (CloudWatch Events) at http://localhost:4587
STS at http://localhost:4592
IAM at http://localhost:4593
EC2 at http://localhost:4597
KMS at http://localhost:4599

In addition to the above, the Pro version of LocalStack supports additional APIs and advanced features, including:

AppSync
Athena
Cognito
ElastiCache
ECS/EKS
IoT
Lambda Layers
RDS
XRay
Interactive UIs to manage resources
Test report dashboards
...and much, much more to come!

Why LocalStack?
LocalStack builds on existing best-of-breed mocking/testing tools, most notably
kinesalite/dynalite
and moto. While these tools are awesome (!), they lack functionality
for certain use cases. LocalStack combines the tools, makes them interoperable, and adds important
missing functionality on top of them:

Error injection: LocalStack allows to inject errors frequently occurring in real Cloud environments,
for instance ProvisionedThroughputExceededException which is thrown by Kinesis or DynamoDB if the amount of
read/write throughput is exceeded.
Isolated processes: All services in LocalStack run in separate processes. The overhead of additional
processes is negligible, and the entire stack can easily be executed on any developer machine and CI server.
In moto, components are often hard-wired in RAM (e.g., when forwarding a message on an SNS topic to an SQS queue,
the queue endpoint is looked up in a local hash map). In contrast, LocalStack services live in isolation
(separate processes available via HTTP), which fosters true decoupling and more closely resembles the real
cloud environment.
Pluggable services: All services in LocalStack are easily pluggable (and replaceable), due to the fact that
we are using isolated processes for each service. This allows us to keep the framework up-to-date and select
best-of-breed mocks for each individual service.

Requirements

python (both Python 2.x and 3.x supported)
pip (python package manager)
Docker

Installing
The easiest way to install LocalStack is via pip:
pip install localstack

Note: Please do not use sudo or the root user - LocalStack
should be installed and started entirely under a local non-root user. If you have problems
with permissions in MacOS X Sierra, install with pip install --user localstack
Running in Docker
By default, LocalStack gets started inside a Docker container using this command:
localstack start

(Note that on MacOS you may have to run TMPDIR=/private$TMPDIR localstack start --docker if
$TMPDIR contains a symbolic link that cannot be mounted by Docker.)
Using docker-compose
You can also use the docker-compose.yml file from the repository and use this command (currently requires docker-compose version 2.1+):
docker-compose up

(Note that on MacOS you may have to run TMPDIR=/private$TMPDIR docker-compose up if
$TMPDIR contains a symbolic link that cannot be mounted by Docker.)
Use on existing docker-compose project. Add in existing services. The project can be found in docker hub, no need to download or clone source:
version: '2.1'
services:
...
  localstack:
    image: localstack/localstack
    ports:
      - ""4567-4584:4567-4584""
      - ""${PORT_WEB_UI-8080}:${PORT_WEB_UI-8080}""
    environment:
      - SERVICES=${SERVICES- }
      - DEBUG=${DEBUG- }
      - DATA_DIR=${DATA_DIR- }
      - PORT_WEB_UI=${PORT_WEB_UI- }
      - LAMBDA_EXECUTOR=${LAMBDA_EXECUTOR- }
      - KINESIS_ERROR_PROBABILITY=${KINESIS_ERROR_PROBABILITY- }
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - ""${TMPDIR:-/tmp/localstack}:/tmp/localstack""

To facilitate interoperability, configuration variables can be prefixed with LOCALSTACK_ in docker. For instance, setting LOCALSTACK_SERVICES=s3 is equivalent to SERVICES=s3.
Starting locally (non-Docker mode)
Alternatively, the infrastructure can be spun up on the local host machine (without using Docker) using the following command:
localstack start --host

(Note that this will require additional dependencies, and currently is not supported on some operating systems, including Windows.)
LocalStack will attempt to automatically fetch the missing dependencies when you first start it up in ""host"" mode; alternatively, you can use the full profile to install all dependencies at pip installation time:
pip install ""localstack[full]""

Configurations
You can pass the following environment variables to LocalStack:


SERVICES: Comma-separated list of service names and (optional) ports they should run on.
If no port is specified, a default port is used. Service names basically correspond to the
service names of the AWS CLI
(kinesis, lambda, sqs, etc), although LocalStack only supports a subset of them.
Example value: kinesis,lambda:4569,sqs:4570 to start Kinesis on the default port,
Lambda on port 4569, and SQS on port 4570. In addition, the following shorthand values can be
specified to run a predefined ensemble of services:

serverless: run services often used for Serverless apps (iam, lambda, dynamodb, apigateway, s3, sns)



DEFAULT_REGION: AWS region to use when talking to the API (defaults to us-east-1).


HOSTNAME: Name of the host to expose the services internally (defaults to localhost).
Use this to customize the framework-internal communication, e.g., if services are
started in different containers using docker-compose.


HOSTNAME_EXTERNAL: Name of the host to expose the services externally (defaults to localhost).
This host is used, e.g., when returning queue URLs from the SQS service to the client.


<SERVICE>_PORT: Port number to bind a specific service to (defaults to service ports above).


<SERVICE>_PORT_EXTERNAL: Port number to expose a specific service externally (defaults to service ports above). SQS_PORT_EXTERNAL, for example, is used when returning queue URLs from the SQS service to the client.


USE_SSL: Whether to use https://... URLs with SSL encryption (defaults to false).


KINESIS_ERROR_PROBABILITY: Decimal value between 0.0 (default) and 1.0 to randomly
inject ProvisionedThroughputExceededException errors into Kinesis API responses.


KINESIS_SHARD_LIMIT: Integer value (defaults to 100) or Infinity (to disable), in which to kinesalite will start throwing exceptions to mimick the default shard limit.


KINESIS_LATENCY: Integer value (defaults to 500) or 0 (to disable), in which to kinesalite will delay returning a response in order to mimick latency from a live AWS call.


DYNAMODB_ERROR_PROBABILITY: Decimal value between 0.0 (default) and 1.0 to randomly
inject ProvisionedThroughputExceededException errors into DynamoDB API responses.


LAMBDA_EXECUTOR: Method to use for executing Lambda functions. Possible values are:

local: run Lambda functions in a temporary directory on the local machine
docker: run each function invocation in a separate Docker container
docker-reuse: create one Docker container per function and reuse it across invocations

For docker and docker-reuse, if LocalStack itself is started inside Docker, then
the docker command needs to be available inside the container (usually requires to run the
container in privileged mode). Default is docker, fallback to local if Docker is not available.


LAMBDA_REMOTE_DOCKER determines whether Lambda code is copied or mounted into containers.
Possible values are:

true (default): your Lambda function definitions will be passed to the container by
copying the zip file (potentially slower). It allows for remote execution, where the host
and the client are not on the same machine.
false: your Lambda function definitions will be passed to the container by mounting a
volume (potentially faster). This requires to have the Docker client and the Docker
host on the same machine.



LAMBDA_DOCKER_NETWORK Specifies the docker network for the container running your lambda function.


LAMBDA_CONTAINER_REGISTRY Use an alternative docker registry to pull lambda execution containers. Default is lambci/lambda.


DATA_DIR: Local directory for saving persistent data (currently only supported for these services:
Kinesis, DynamoDB, Elasticsearch, S3). Set it to /tmp/localstack/data to enable persistence
(/tmp/localstack is mounted into the Docker container), leave blank to disable
persistence (default).


PORT_WEB_UI: Port for the Web user interface (dashboard). Default is 8080.


<SERVICE>_BACKEND: Custom endpoint URL to use for a specific service, where <SERVICE> is the uppercase
service name (currently works for: APIGATEWAY, CLOUDFORMATION, DYNAMODB, ELASTICSEARCH,
KINESIS, S3, SNS, SQS). This allows to easily integrate third-party services into LocalStack.


FORCE_NONINTERACTIVE: when running with Docker, disables the --interactive and --tty flags. Useful when running headless.


DOCKER_FLAGS: Allows to pass custom flags (e.g., volume mounts) to ""docker run"" when running LocalStack in Docker.


DOCKER_CMD: Shell command used to run Docker containers, e.g., set to ""sudo docker"" to run as sudo (default: docker).


START_WEB: Flag to control whether the Web API should be started in Docker (values: 0/1; default: 1).


LAMBDA_FALLBACK_URL: Fallback URL to use when a non-existing Lambda is invoked. Either records invocations in DynamoDB (value dynamodb://<table_name>) or forwards invocations as a POST request (value http(s)://...).


EXTRA_CORS_ALLOWED_HEADERS: Comma-separated list of header names to be be added to Access-Control-Allow-Headers CORS header


EXTRA_CORS_EXPOSE_HEADERS: Comma-separated list of header names to be be added to Access-Control-Expose-Headers CORS header


LAMBDA_JAVA_OPTS: Allow passing custom JVM options (e.g., -Xmx512M) to Java Lambdas executed in Docker. Use _debug_port_ placeholder to configure the debug port (e.g., -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=_debug_port_).


Additionally, the following read-only environment variables are available:

LOCALSTACK_HOSTNAME: Name of the host where LocalStack services are available.
This is needed in order to access the services from within your Lambda functions
(e.g., to store an item to DynamoDB or S3 from Lambda).
The variable LOCALSTACK_HOSTNAME is available for both, local Lambda execution
(LAMBDA_EXECUTOR=local) and execution inside separate Docker containers (LAMBDA_EXECUTOR=docker).

Dynamically updating configuration at runtime
Each of the service APIs listed above defines
a backdoor API under the path /?_config_ which allows to dynamically update configuration variables
defined in config.py.
For example, to dynamically set KINESIS_ERROR_PROBABILITY=1 at runtime, use the following command:
curl -v -d '{""variable"":""KINESIS_ERROR_PROBABILITY"",""value"":1}' 'http://localhost:4568/?_config_'

Initializing a fresh instance
When a container is started for the first time, it will execute files with extensions .sh that are found in /docker-entrypoint-initaws.d. Files will be executed in alphabetical order. You can easily create aws resources on localstack using awslocal (or aws) cli tool in the initialization scripts.
A note about using custom SSL certificates (for USE_SSL=1)
If you need to use your own SSL Certificate and keep it persistent and not use the random automatic generated Certificate, you can place into the localstack temporary directory :
/tmp/localstack/

the three named files below :
server.test.pem
server.test.pem.crt
server.test.pem.key

the file server.test.pem must contains your key file content, your certificate and chain certificate files contents (do a cat in this order)
the file server.test.pem.crt must contains your certificate and chains files contents (do a 'cat' in this order)
the file server.test.pem.key must contains your key file content


Using USE_SSL and own persistent certificate with docker-compose
Typically with docker-compose you can add into docker-compose.yml this volume to the localstack services :
volumes:
      - ""${PWD}/ls_tmp:/tmp/localstack""
      - ""/var/run/docker.sock:/var/run/docker.sock""

local directory ls_tmp must contains the three files (server.test.pem, server.test.pem.crt, server.test.pem.key)

Accessing the infrastructure via CLI or code
You can point your aws CLI to use the local infrastructure, for example:
aws --endpoint-url=http://localhost:4568 kinesis list-streams
{
    ""StreamNames"": []
}

NEW: Check out awslocal, a thin CLI wrapper
that runs commands directly against LocalStack (no need to specify --endpoint-url anymore).
Install it via pip install awscli-local, and then use it as follows:
awslocal kinesis list-streams
{
    ""StreamNames"": []
}

UPDATE: Use the environment variable $LOCALSTACK_HOSTNAME to determine the target host
inside your Lambda function. See Configurations section for more details.
Client Libraries

Python: https://github.com/localstack/localstack-python-client

alternatively, you can also use boto3 and use the endpoint_url parameter when creating a connection


(more coming soon...)

Integration with nosetests
If you want to use LocalStack in your integration tests (e.g., nosetests), simply fire up the
infrastructure in your test setup method and then clean up everything in your teardown method:
from localstack.services import infra

def setup():
    infra.start_infra(asynchronous=True)

def teardown():
    infra.stop_infra()

def my_app_test():
    # here goes your test logic

See the example test file tests/integration/test_integration.py for more details.
Integration with Serverless
You can use the serverless-localstack plugin to easily run Serverless applications on LocalStack.
For more information, please check out the plugin repository here:
https://github.com/localstack/serverless-localstack
Using local code with Lambda
In order to mount a local folder, ensure that LAMBDA_REMOTE_DOCKER is set to false then set the S3 bucket name to __local__ and the S3 key to your local path:
    awslocal lambda create-function --function-name myLambda \
      --code S3Bucket=""__local__"",S3Key=""/my/local/lambda/folder"" \
      --handler index.myHandler \
      --runtime nodejs8.10 \
      --role whatever

Integration with Java/JUnit
In order to use LocalStack with Java, the project ships with a simple JUnit runner and a JUnit 5 extension. Take a look
at the example JUnit test in ext/java. When you run the test, all dependencies are automatically
downloaded and installed to a temporary directory in your system.
...
import cloud.localstack.LocalstackTestRunner;
import cloud.localstack.TestUtils;

@RunWith(LocalstackTestRunner.class)
public class MyCloudAppTest {

  @Test
  public void testLocalS3API() {
    AmazonS3 s3 = TestUtils.getClientS3()
    List<Bucket> buckets = s3.listBuckets();
    ...
  }

}

Or with JUnit 5 :
@ExtendWith(LocalstackExtension.class)
public class MyCloudAppTest {
   ...
}

Additionally, there is a version of the LocalStack Test Runner which runs in a docker container
instead of installing LocalStack on the current machine. The only dependency is to have docker
installed locally. The test runner will automatically pull the image and start the container for the
duration of the test.  The container can be configured by using the @LocalstackDockerProperties annotation.
@RunWith(LocalstackDockerTestRunner.class)
@LocalstackDockerProperties(services = { ""sqs"", ""kinesis:77077"" })
public class MyDockerCloudAppTest {

  @Test
  public void testKinesis() {
    AmazonKinesis kinesis = DockerTestUtils.getClientKinesis();

    ListStreamsResult streams = kinesis.listStreams();
    ...

Or with JUnit 5 :
@ExtendWith(LocalstackDockerExtension.class)
@LocalstackDockerProperties(services = { ""sqs"", ""kinesis:77077"" })
public class MyDockerCloudAppTest {
   ...
}

The LocalStack JUnit test runner is published as an artifact in Maven Central.
Simply add the following dependency to your pom.xml file:
<dependency>
    <groupId>cloud.localstack</groupId>
    <artifactId>localstack-utils</artifactId>
    <version>0.2.0</version>
</dependency>

You can configure the Docker behaviour using the @LocalstackDockerProperties annotation with the following parameters:



property
usage
type
default value




pullNewImage
Determines if a new image is pulled from the docker repo before the tests are run.
boolean
false


randomizePorts
Determines if the container should expose the default local stack ports (4567-4583) or if it should expose randomized ports.
boolean
false


services
Determines which services should be run when the localstack starts.
String[]
All


imageTag
Use a specific image tag for docker container
String
latest


hostNameResolver
Used for determining the host name of the machine running the docker containers so that the containers can be addressed.
IHostNameResolver
localhost


environmentVariableProvider
Used for injecting environment variables into the container.
IEnvironmentVariableProvider
Empty Map



NB : When specifying the port in the services property, you cannot use randomizePorts = true
Troubleshooting


If you're using AWS Java libraries with Kinesis, please, refer to CBOR protocol issues with the Java SDK guide how to disable CBOR protocol which is not supported by kinesalite.


Accessing local S3 from Java: To avoid domain name resolution issues, you need to enable path style access on your client:


s3.setS3ClientOptions(S3ClientOptions.builder().setPathStyleAccess(true).build());
// There is also an option to do this if you're using any of the client builder classes:
AmazonS3ClientBuilder builder = AmazonS3ClientBuilder.standard();
builder.withPathStyleAccessEnabled(true);
...



Mounting the temp. directory: Note that on MacOS you may have to run TMPDIR=/private$TMPDIR docker-compose up if
$TMPDIR contains a symbolic link that cannot be mounted by Docker.
(See details here: https://bitbucket.org/atlassian/localstack/issues/40/getting-mounts-failed-on-docker-compose-up)


If you run into file permission issues on pip install under Mac OS (e.g., Permission denied: '/Library/Python/2.7/site-packages/six.py'), then you may have to re-install pip via Homebrew (see this discussion thread). Alternatively, try installing
with the --user flag: pip install --user localstack


If you are deploying within OpenShift, please be aware: the pod must run as root, and the user must have capabilities added to the running pod, in order to allow Elasticsearch to be run as the non-root localstack user.


The environment variable no_proxy is rewritten by LocalStack.
(Internal requests will go straight via localhost, bypassing any proxy configuration).


For troubleshooting LocalStack start issues, you can check debug logs by running DEBUG=1 localstack start


In case you get errors related to node/nodejs, you may find (this issue comment: https://github.com/localstack/localstack/issues/227#issuecomment-319938530) helpful.


If you are using AWS Java libraries and need to disable SSL certificate checking, add -Dcom.amazonaws.sdk.disableCertChecking to the java invocation.


Developing
Requirements for developing or starting locally
To develop new features, or to start the stack locally (outside of Docker), the following additional tools are required:

make
npm (node.js package manager)
java/javac (Java 8 runtime environment and compiler)
mvn (Maven, the build system for Java)

Development Environment
If you pull the repo in order to extend/modify LocalStack, run this command to install
all the dependencies:
make install

This will install the required pip dependencies in a local Python virtualenv directory
.venv (your global python packages will remain untouched), as well as some node modules
in ./localstack/node_modules/. Depending on your system, some pip/npm modules may require
additional native libs installed.
The Makefile contains a target to conveniently run the local infrastructure for development:
make infra

Check out the
developer guide which
contains a few instructions on how to get started with developing (and debugging) features for
LocalStack.
Testing
The project contains a set of unit and integration tests that can be kicked off via a make
target:
make test

Web Dashboard
The projects also comes with a simple Web dashboard that allows to view the deployed AWS
components and the relationship between them.
localstack web

Other UI Clients

Commandeer desktop app
DynamoDB Admin Web UI

Change Log

v0.10.5: Various CloudFormation fixes: deployment of API GW method integrations, properly skip resource updates, Lambda SQS event source mapping, avoid duplicate resource creation, support for ApiGateway::GatewayResponse and Events::Rule, log groups for Lambdas; support adding Lambda policies; customize Docker registry for Lambda images; support multiple configurations in S3 notifications; fix encoding of non-ASCII results from API Gateway; allow docker-reuse to use mounted volumes; support presigned S3 URL upload notifications; fix lookup of Python Lambda handler in sub directories; upgrade kinesalite; fix duplicate CORS headers; fix mapping of Lambda versions and ARNs; fix SNS x-amz-sns-message-type header; send SNS confirmation message for HTTP(S) subscriptions; fix DynamoDB local libs for Docker Alpine; add CF support for SNS subscriptions; fix RecordId for firehose put-record-batch; fix SQS messages with multi-byte characters; avoid creating multiple SNS subscriptions; add .bat script and support running under Windows; fix S3 location constraint for CF
v0.10.4: Add checks for open UDP ports; fix S3 chunked encoding uploads; fix LatestStreamLabel; fix CORS headers for SQS/SNS; set Java lambda debug port only when needed; expose default region in a util function; fix MacOS tmp folder; clear tmp supervisord logs at container startup; fix signed header requests for S3; expose Web UI via HTTPS; add Timestamp to SNS messages; fix attributes for SQS queues addressed via URL
v0.10.3: Allow specifying data types for CF attributes; add API for service status and starting services at runtime; support NextShardIterator in DDB streams; add mock responses for S3 encryption and replication; fix rendering of resources in web UI; custom SQS queue attributes; fix Lambda docker command and imports; fix SQS queue physical ID in CF; allow proxy listener to define custom backend per request; support Lambda event body over stdin; exclude ingest-geoip ES module to optimize image size; skip checking MD5 on S3 copy; fix DynamoDB table ARN for CF; fix CF deployment of StepFunction activities; fix uploading of Java Lambda as JAR in ZIP; fix installing libs for plugins; added LAMBDA_JAVA_OPTS for Java Lambda debugging; bump Maven dependency versions; refactor Lambda API; fix boolean strings in CF templates; allow overriding AWS account id with TEST_AWS_ACCOUNT_ID; fix incorrect region for API GW resources created via CF; fix permissions for cache files in /tmp
v0.10.2: Fix logging issue with async Lambdas; fix kinesis records processing; add basic support for Ref in CloudFormation; fix ddb streams uuid generation; upgrade travis CI setup; fix DynamoDB error messages; cache server processes
v0.10.0: Lazy loading of libraries; fix handling of regions; add API multiserver; improve CPU profiling; fix ES xpack installation; add basic EventBridge support; refactor Lambda API and executor; add MessageAttributes on SNS payloads; tagging for SNS; ability to customize docker command
v0.9.6: Add API Gateway SQS proxy; fix command to push Docker image; fix Docker bridge IP configuration; fix SSL issue in dashboard infra; updates to README
v0.9.5: Reduce Docker image size by squashing; fix response body for presigned URL S3 PUT requests; fix CreateDate returned by IAM; fix account IDs for CF and SNS; fix topic checks for SMS using SNS; improve documentation around @LocalstackDockerProperties; add basic EC2 support; upgrade to ElasticSearch 6.7; set Last-Modified header in S3; preserve logic with uppercase event keys in Java; add support for nodejs 10.x Lambdas
v0.9.4: Fix ARNs in CloudFormation deployments; write stderr to file in supervisord; fix Lambda invocation times; fix canonicalization of service names when running in Docker; add support for @Nested in Junit5; add support for batch/transaction in DynamoDB; fix output buffering for subprocesses; assign unique ports under docker-reuse; check if topic ARN exists before publish
v0.9.3: Fix output buffering of child processes; new release of Java libs; add imageTag attribute for Java annotation
v0.9.2: Update to Python 3 in Dockerfile; preserve attributes when SNS Subscribe; fix event source mapping in Lambda; fix CORS ExposeHeaders; set Lambda timeout in secs; add tags support for Lambda/Firehose; add message attributes for SQS/Lambda; fix shard count support for Kinesis; fix port mappings for CloudFormation
v0.9.1: Define dependent and composite services in config; forward Lambda logs to CloudWatch Logs; add SQS event deserializing for Lambda; fix AWS_PROXY for JSON list payload; add START_WEB config parameter; return correct location for S3 multipart uploads; add support for Lambda custom runtime; fix account ID for IAM responses; fix using correct SSL cert; limit memory usage for Java processes; fix unicode encoding for SNS messages; allow using LOCALSTACK_ prefix in Docker environment variables; enable request forwarding for non-existing Lambdas; fix large downloads for S3; add API endpoint for dynamically updating config variables; fix CloudFormation stack update
v0.9.0: Enhance integration with Serverless; refactor CloudFormation implementation; add support for Step Functions, IAM, STS; fix CloudFormation integration; support mounting Lambda code locally; add docker-entrypoint-initaws.d dir for initializing resources; add S3Event Parser for Lambda; fix S3 chunk encoding; fix S3 multipart upload notification; add dotnetcore2.1 and ruby2.5 Lambda runtimes; fix issues with JDK 9; install ES plugins available in AWS
v0.8.10: Add kclpy to pip package; fix badges in README
v0.8.9: Replace moto-ext with upstream moto; fix SNS message attributes; fix swagger; make external SQS port configurable; support for SNS DeleteTopic; S3 notifications for multipart uploads; support requestContext in AWS_PROXY integration; update docs for SSL usage
v0.8.8: Support Docker network config for Lambda containers; support queryStringParameters for Lambda AWS_PROXY apigateway; add AWS SecretsManager service; add SQS/Lambda integration; add support for Firehose Kinesis source; add GetAlias to Lambda API; add function properties to LambdaContext for invocations; fix extraction of Java Lambda archives; check region headers for SNS; fix Lambda output buffering; fix S3 download of gzip; bump ElasticMQ to 0.14.5; fix Lambda response codes; fix syntax issues for Python 3.7
v0.8.7: Support .Net Core 2.0 and nodejs8.10 Lambdas; refactor Java libs and integrate with JUnit 5; support tags for ES domains; add CloudFormation support for SNS topics; fix kinesis error injection; fix override of ES_JAVA_OPTS; fix SQS CORS preflight response; fix S3 content md5 checks and Host header; fix ES startup issue; Bump elasticmq to 0.13.10; bump kinesalite version
v0.8.6: Fixes for Windows installation; bump ES to 6.2.0; support filter policy for SNS; upgrade kinesalite; refactor JUnit runner; support Lambda PutFunctionConcurrency and GetEventSourceMapping; fixes for Terraform; add golang support to Lambda; fix file permission issue in Java Lambda tests; fix S3 bucket notification config
v0.8.5: Fix DDB streams event type; implement CF Fn::GetAZs; async lambda for DDB events; fix S3 content-type; fix CF deployer for SQS; fix S3 ExposePorts; fix message subject in SNS; support for Firehose -> ES; pass external env vars to containers from Java; add mock for list-queue-tags; enhance docker test runner; fix Windows installation issues; new version of Java libs
v0.8.4: Fix pipenv dependency issue; Docker JUnit test runner; POJO type for Java Lambda RequestHandler; Java Lambda DynamoDB event; reuse Docker containers for Lambda invocations; API Gateway wildcard path segments; fix SNS RawMessageDelivery
v0.8.3: Fix DDB stream events for UPDATE operations; fix DDB streams sequence numbers; fix transfer-encoding for DDB; fix requests with missing content-length header; support non-ascii content in DynamoDB items; map external port for SQS queue URLs; default to LAMBDA_REMOTE_DOCKER=true if running in Docker; S3 lifecycle support; reduce Docker image size
v0.8.2: Fix S3 bucket notification configuration; CORS headers for API Gateway; fix >128k S3 multipart uploads; return valid ShardIDs in DynamoDB Streams; fix hardcoded ""ddblocal"" DynamoDB TableARN; import default service ports from localstack-client; fix S3 bucket policy response; Execute lambdas asynchronously if the source is a topic
v0.8.1: Improvements in Lambda API: publish-version, list-version, function aliases; use single map with Lambda function details; workaround for SQS .fifo queues; add test for S3 upload; initial support for SSM; fix regex to replace SQS queue URL hostnames; update linter (single quotes); use docker.for.mac.localhost to connect to LocalStack from Docker on Mac; fix b64 encoding for Java Lambdas; fix path of moto_server command
v0.8.0: Fix request data in GenericProxyHandler; add $PORT_WEB_UI and $HOSTNAME_EXTERNAL configs; API Gateway path parameters; enable flake8 linting; add config for service backend URLs; use ElasticMQ instead of moto for SQS; expose $LOCALSTACK_HOSTNAME; custom environment variable support for Lambda; improve error logging and installation for Java/JUnit; add support for S3 REST Object POST
v0.7.5: Fix issue with incomplete parallel downloads; bypass http_proxy for internal requests; use native Python code to unzip archives; download KCL client libs only for testing and not on pip install
v0.7.4: Refactor CLI and enable plugins; support unicode names for S3; fix SQS names containing a dot character; execute Java Lambda functions in Docker containers; fix DynamoDB error handling; update docs
v0.7.3: Extract proxy listeners into (sub-)classes; put java libs into a single ""fat"" jar; fix issue with non-daemonized threads; refactor code to start flask services
v0.7.2: Fix DATA_DIR config when running in Docker; fix Maven dependencies; return 'ConsumedCapacity' from DynamoDB get-item; use Queue ARN instead of URL for S3 bucket notifications
v0.7.1: Fix S3 API to GET bucket notifications; release Java artifacts to Maven Central; fix S3 file access from Spark; create DDB stream on UpdateTable; remove AUI dependency, optimize size of Docker image
v0.7.0: Support for Kinesis in CloudFormation; extend and integrate Java tests in CI; publish Docker image under new name; update READMEs and license agreements
v0.6.2: Major refactoring of installation process, lazy loading of dependencies
v0.6.1: Add CORS headers; platform compatibility fixes (remove shell commands and sh module); add CloudFormation validate-template; fix Lambda execution in Docker; basic domain handling in ES API; API Gateway authorizers
v0.6.0: Load services as plugins; fix service default ports; fix SQS->SNS and MD5 of message attributes; fix Host header for S3
v0.5.5: Enable SSL encryption for all service endpoints (USE_SSL config); create Docker base image; fix issue with DATA_DIR
v0.5.4: Remove hardcoded /tmp/ for Windows-compat.; update CLI and docs; fix S3/SNS notifications; disable Elasticsearch compression
v0.5.3: Add CloudFormation support for serverless / API Gateway deployments; fix installation via pypi; minor fix for Java (passing of environment variables)
v0.5.0: Extend DynamoDB Streams API; fix keep-alive connection for S3; fix deadlock in nested Lambda executions; add integration SNS->Lambda; CloudFormation serverless example; replace dynalite with DynamoDBLocal; support Lambda execution in remote Docker container; fix CloudWatch metrics for Lambda invocation errors
v0.4.3: Initial support for CloudWatch metrics (for Lambda functions); HTTP forwards for API Gateway; fix S3 message body signatures; download Lambda archive from S3 bucket; fix/extend ES tests
v0.4.2: Initial support for Java Lambda functions; CloudFormation deployments; API Gateway tests
v0.4.1: Python 3 compatibility; data persistence; add seq. numbers in Kinesis events; limit Elasticsearch memory
v0.4.0: Execute Lambda functions in Docker containers; CORS headers for S3
v0.3.11: Add Route53, SES, CloudFormation; DynamoDB fault injection; UI tweaks; refactor config
v0.3.10: Add initial support for S3 bucket notifications; fix subprocess32 installation
v0.3.9: Make services/ports configurable via $SERVICES; add tests for Firehose+S3
v0.3.8: Fix Elasticsearch via local bind and proxy; refactoring; improve error logging
v0.3.5: Fix lambda handler name; fix host name for S3 API; install web libs on pip install
v0.3.4: Fix file permissions in build; fix and add UI to Docker image; add stub of ES API
v0.3.3: Add version tags to Docker images
v0.3.2: Add support for Redshift API; code refactoring
v0.3.1: Add Dockerfile and push image to Docker Hub
v0.3.0: Add simple integration for JUnit; improve process signal handling
v0.2.11: Refactored the AWS assume role function
v0.2.10: Added AWS assume role functionality.
v0.2.9: Kinesis error response formatting
v0.2.7: Throw Kinesis errors randomly
v0.2.6: Decouple SNS/SQS: intercept SNS calls and forward to subscribed SQS queues
v0.2.5: Return error response from Kinesis if flag is set
v0.2.4: Allow Lambdas to use file (import from file instead of exec'ing)
v0.2.3: Improve Kinesis/KCL auto-checkpointing (leases in DDB)
v0.2.0: Speed up installation time by lazy loading libraries
v0.1.19: Pass shard_id in records sent from KCL process
v0.1.16: Minor restructuring and refactoring (create separate kinesis_util.py)
v0.1.14: Fix AWS tokens when creating Elasticsearch client
v0.1.11: Add startup/initialization notification for KCL process
v0.1.10: Bump version of amazon_kclpy to 1.4.1
v0.1.9: Add initial support for SQS/SNS
v0.1.8: Fix installation of JARs in amazon_kclpy if localstack is installed transitively
v0.1.7: Bump version of amazon_kclpy to 1.4.0
v0.1.6: Add travis-ci and coveralls configuration
v0.1.5: Refactor Elasticsearch utils; fix bug in method to delete all ES indexes
v0.1.4: Enhance logging; extend java KCL credentials provider (support STS assumed roles)
v0.1.2: Add configurable KCL log output
v0.1.0: Initial release

Contributing
We welcome feedback, bug reports, and pull requests!
For pull requests, please stick to the following guidelines:

Add tests for any new features and bug fixes. Ideally, each PR should increase the test coverage.
Follow the existing code style (e.g., indents). A PEP8 code linting target is included in the Makefile.
Put a reasonable amount of comments into the code.
Separate unrelated changes into multiple pull requests.
1 commit per PR: Please squash/rebase multiple commits into one single commit (to keep the history clean).

Please note that by contributing any code or documentation to this repository (by
raising pull requests, or otherwise) you explicitly agree to
the Contributor License Agreement.
Contributors
This project exists thanks to all the people who contribute.

Backers
Thank you to all our backers! üôè [Become a backer]

Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a link to your website. [Become a sponsor]










Stargazers over time

License
Copyright (c) 2017-2019 LocalStack maintainers and contributors.
Copyright (c) 2016 Atlassian and others.
This version of LocalStack is released under the Apache License, Version 2.0 (see LICENSE.txt).
By downloading and using this software you agree to the
End-User License Agreement (EULA).
We build on a number of third-party software tools, including the following:



Third-Party software
License




Python/pip modules:



airspeed
BSD License


amazon_kclpy
Amazon Software License


boto3
Apache License 2.0


coverage
Apache License 2.0


docopt
MIT License


elasticsearch
Apache License 2.0


flask
BSD License


flask_swagger
MIT License


jsonpath-rw
Apache License 2.0


moto
Apache License 2.0


requests
Apache License 2.0


subprocess32
PSF License


Node.js/npm modules:



kinesalite
MIT License


Other tools:



Elasticsearch
Apache License 2.0


local-kms
MIT License



",GitHub - localstack/localstack: üíª  A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline!
11,Python,"jieba
‚ÄúÁªìÂ∑¥‚Äù‰∏≠ÊñáÂàÜËØçÔºöÂÅöÊúÄÂ•ΩÁöÑ Python ‰∏≠ÊñáÂàÜËØçÁªÑ‰ª∂
""Jieba"" (Chinese for ""to stutter"") Chinese text segmentation: built to be the best Python Chinese word segmentation module.

Scroll down for English documentation.

ÁâπÁÇπ


ÊîØÊåÅ‰∏âÁßçÂàÜËØçÊ®°ÂºèÔºö

Á≤æÁ°ÆÊ®°ÂºèÔºåËØïÂõæÂ∞ÜÂè•Â≠êÊúÄÁ≤æÁ°ÆÂú∞ÂàáÂºÄÔºåÈÄÇÂêàÊñáÊú¨ÂàÜÊûêÔºõ
ÂÖ®Ê®°ÂºèÔºåÊääÂè•Â≠ê‰∏≠ÊâÄÊúâÁöÑÂèØ‰ª•ÊàêËØçÁöÑËØçËØ≠ÈÉΩÊâ´ÊèèÂá∫Êù•, ÈÄüÂ∫¶ÈùûÂ∏∏Âø´Ôºå‰ΩÜÊòØ‰∏çËÉΩËß£ÂÜ≥Ê≠ß‰πâÔºõ
ÊêúÁ¥¢ÂºïÊìéÊ®°ÂºèÔºåÂú®Á≤æÁ°ÆÊ®°ÂºèÁöÑÂü∫Á°Ä‰∏äÔºåÂØπÈïøËØçÂÜçÊ¨°ÂàáÂàÜÔºåÊèêÈ´òÂè¨ÂõûÁéáÔºåÈÄÇÂêàÁî®‰∫éÊêúÁ¥¢ÂºïÊìéÂàÜËØç„ÄÇ



ÊîØÊåÅÁπÅ‰ΩìÂàÜËØç


ÊîØÊåÅËá™ÂÆö‰πâËØçÂÖ∏


MIT ÊéàÊùÉÂçèËÆÆ


ÂèãÊÉÖÈìæÊé•

https://github.com/baidu/lac   ÁôæÂ∫¶‰∏≠ÊñáËØçÊ≥ïÂàÜÊûêÔºàÂàÜËØç+ËØçÊÄß+‰∏ìÂêçÔºâÁ≥ªÁªü
https://github.com/baidu/AnyQ  ÁôæÂ∫¶FAQËá™Âä®ÈóÆÁ≠îÁ≥ªÁªü
https://github.com/baidu/Senta ÁôæÂ∫¶ÊÉÖÊÑüËØÜÂà´Á≥ªÁªü

ÂÆâË£ÖËØ¥Êòé
‰ª£Á†ÅÂØπ Python 2/3 ÂùáÂÖºÂÆπ

ÂÖ®Ëá™Âä®ÂÆâË£ÖÔºöeasy_install jieba ÊàñËÄÖ pip install jieba / pip3 install jieba
ÂçäËá™Âä®ÂÆâË£ÖÔºöÂÖà‰∏ãËΩΩ http://pypi.python.org/pypi/jieba/ ÔºåËß£ÂéãÂêéËøêË°å python setup.py install
ÊâãÂä®ÂÆâË£ÖÔºöÂ∞Ü jieba ÁõÆÂΩïÊîæÁΩÆ‰∫éÂΩìÂâçÁõÆÂΩïÊàñËÄÖ site-packages ÁõÆÂΩï
ÈÄöËøá import jieba Êù•ÂºïÁî®

ÁÆóÊ≥ï

Âü∫‰∫éÂâçÁºÄËØçÂÖ∏ÂÆûÁé∞È´òÊïàÁöÑËØçÂõæÊâ´ÊèèÔºåÁîüÊàêÂè•Â≠ê‰∏≠Ê±âÂ≠óÊâÄÊúâÂèØËÉΩÊàêËØçÊÉÖÂÜµÊâÄÊûÑÊàêÁöÑÊúâÂêëÊó†ÁéØÂõæ (DAG)
ÈááÁî®‰∫ÜÂä®ÊÄÅËßÑÂàíÊü•ÊâæÊúÄÂ§ßÊ¶ÇÁéáË∑ØÂæÑ, ÊâæÂá∫Âü∫‰∫éËØçÈ¢ëÁöÑÊúÄÂ§ßÂàáÂàÜÁªÑÂêà
ÂØπ‰∫éÊú™ÁôªÂΩïËØçÔºåÈááÁî®‰∫ÜÂü∫‰∫éÊ±âÂ≠óÊàêËØçËÉΩÂäõÁöÑ HMM Ê®°ÂûãÔºå‰ΩøÁî®‰∫Ü Viterbi ÁÆóÊ≥ï

‰∏ªË¶ÅÂäüËÉΩ

ÂàÜËØç



jieba.cut ÊñπÊ≥ïÊé•Âèó‰∏â‰∏™ËæìÂÖ•ÂèÇÊï∞: ÈúÄË¶ÅÂàÜËØçÁöÑÂ≠óÁ¨¶‰∏≤Ôºõcut_all ÂèÇÊï∞Áî®Êù•ÊéßÂà∂ÊòØÂê¶ÈááÁî®ÂÖ®Ê®°ÂºèÔºõHMM ÂèÇÊï∞Áî®Êù•ÊéßÂà∂ÊòØÂê¶‰ΩøÁî® HMM Ê®°Âûã
jieba.cut_for_search ÊñπÊ≥ïÊé•Âèó‰∏§‰∏™ÂèÇÊï∞ÔºöÈúÄË¶ÅÂàÜËØçÁöÑÂ≠óÁ¨¶‰∏≤ÔºõÊòØÂê¶‰ΩøÁî® HMM Ê®°Âûã„ÄÇËØ•ÊñπÊ≥ïÈÄÇÂêàÁî®‰∫éÊêúÁ¥¢ÂºïÊìéÊûÑÂª∫ÂÄíÊéíÁ¥¢ÂºïÁöÑÂàÜËØçÔºåÁ≤íÂ∫¶ÊØîËæÉÁªÜ
ÂæÖÂàÜËØçÁöÑÂ≠óÁ¨¶‰∏≤ÂèØ‰ª•ÊòØ unicode Êàñ UTF-8 Â≠óÁ¨¶‰∏≤„ÄÅGBK Â≠óÁ¨¶‰∏≤„ÄÇÊ≥®ÊÑèÔºö‰∏çÂª∫ËÆÆÁõ¥Êé•ËæìÂÖ• GBK Â≠óÁ¨¶‰∏≤ÔºåÂèØËÉΩÊó†Ê≥ïÈ¢ÑÊñôÂú∞ÈîôËØØËß£Á†ÅÊàê UTF-8
jieba.cut ‰ª•Âèä jieba.cut_for_search ËøîÂõûÁöÑÁªìÊûÑÈÉΩÊòØ‰∏Ä‰∏™ÂèØËø≠‰ª£ÁöÑ generatorÔºåÂèØ‰ª•‰ΩøÁî® for Âæ™ÁéØÊù•Ëé∑ÂæóÂàÜËØçÂêéÂæóÂà∞ÁöÑÊØè‰∏Ä‰∏™ËØçËØ≠(unicode)ÔºåÊàñËÄÖÁî®
jieba.lcut ‰ª•Âèä jieba.lcut_for_search Áõ¥Êé•ËøîÂõû list
jieba.Tokenizer(dictionary=DEFAULT_DICT) Êñ∞Âª∫Ëá™ÂÆö‰πâÂàÜËØçÂô®ÔºåÂèØÁî®‰∫éÂêåÊó∂‰ΩøÁî®‰∏çÂêåËØçÂÖ∏„ÄÇjieba.dt ‰∏∫ÈªòËÆ§ÂàÜËØçÂô®ÔºåÊâÄÊúâÂÖ®Â±ÄÂàÜËØçÁõ∏ÂÖ≥ÂáΩÊï∞ÈÉΩÊòØËØ•ÂàÜËØçÂô®ÁöÑÊò†Â∞Ñ„ÄÇ

‰ª£Á†ÅÁ§∫‰æã
# encoding=utf-8
import jieba

seg_list = jieba.cut(""ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶"", cut_all=True)
print(""Full Mode: "" + ""/ "".join(seg_list))  # ÂÖ®Ê®°Âºè

seg_list = jieba.cut(""ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶"", cut_all=False)
print(""Default Mode: "" + ""/ "".join(seg_list))  # Á≤æÁ°ÆÊ®°Âºè

seg_list = jieba.cut(""‰ªñÊù•Âà∞‰∫ÜÁΩëÊòìÊù≠Á†îÂ§ßÂé¶"")  # ÈªòËÆ§ÊòØÁ≤æÁ°ÆÊ®°Âºè
print("", "".join(seg_list))

seg_list = jieba.cut_for_search(""Â∞èÊòéÁ°ïÂ£´ÊØï‰∏ö‰∫é‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊâÄÔºåÂêéÂú®Êó•Êú¨‰∫¨ÈÉΩÂ§ßÂ≠¶Ê∑±ÈÄ†"")  # ÊêúÁ¥¢ÂºïÊìéÊ®°Âºè
print("", "".join(seg_list))
ËæìÂá∫:
„ÄêÂÖ®Ê®°Âºè„Äë: Êàë/ Êù•Âà∞/ Âåó‰∫¨/ Ê∏ÖÂçé/ Ê∏ÖÂçéÂ§ßÂ≠¶/ ÂçéÂ§ß/ Â§ßÂ≠¶

„ÄêÁ≤æÁ°ÆÊ®°Âºè„Äë: Êàë/ Êù•Âà∞/ Âåó‰∫¨/ Ê∏ÖÂçéÂ§ßÂ≠¶

„ÄêÊñ∞ËØçËØÜÂà´„ÄëÔºö‰ªñ, Êù•Âà∞, ‰∫Ü, ÁΩëÊòì, Êù≠Á†î, Â§ßÂé¶    (Ê≠§Â§ÑÔºå‚ÄúÊù≠Á†î‚ÄùÂπ∂Ê≤°ÊúâÂú®ËØçÂÖ∏‰∏≠Ôºå‰ΩÜÊòØ‰πüË¢´ViterbiÁÆóÊ≥ïËØÜÂà´Âá∫Êù•‰∫Ü)

„ÄêÊêúÁ¥¢ÂºïÊìéÊ®°Âºè„ÄëÔºö Â∞èÊòé, Á°ïÂ£´, ÊØï‰∏ö, ‰∫é, ‰∏≠ÂõΩ, ÁßëÂ≠¶, Â≠¶Èô¢, ÁßëÂ≠¶Èô¢, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢, ËÆ°ÁÆó, ËÆ°ÁÆóÊâÄ, Âêé, Âú®, Êó•Êú¨, ‰∫¨ÈÉΩ, Â§ßÂ≠¶, Êó•Êú¨‰∫¨ÈÉΩÂ§ßÂ≠¶, Ê∑±ÈÄ†


Ê∑ªÂä†Ëá™ÂÆö‰πâËØçÂÖ∏


ËΩΩÂÖ•ËØçÂÖ∏

ÂºÄÂèëËÄÖÂèØ‰ª•ÊåáÂÆöËá™Â∑±Ëá™ÂÆö‰πâÁöÑËØçÂÖ∏Ôºå‰ª•‰æøÂåÖÂê´ jieba ËØçÂ∫ìÈáåÊ≤°ÊúâÁöÑËØç„ÄÇËôΩÁÑ∂ jieba ÊúâÊñ∞ËØçËØÜÂà´ËÉΩÂäõÔºå‰ΩÜÊòØËá™Ë°åÊ∑ªÂä†Êñ∞ËØçÂèØ‰ª•‰øùËØÅÊõ¥È´òÁöÑÊ≠£Á°ÆÁéá
Áî®Ê≥ïÔºö jieba.load_userdict(file_name) # file_name ‰∏∫Êñá‰ª∂Á±ªÂØπË±°ÊàñËá™ÂÆö‰πâËØçÂÖ∏ÁöÑË∑ØÂæÑ
ËØçÂÖ∏Ê†ºÂºèÂíå dict.txt ‰∏ÄÊ†∑Ôºå‰∏Ä‰∏™ËØçÂç†‰∏ÄË°åÔºõÊØè‰∏ÄË°åÂàÜ‰∏âÈÉ®ÂàÜÔºöËØçËØ≠„ÄÅËØçÈ¢ëÔºàÂèØÁúÅÁï•Ôºâ„ÄÅËØçÊÄßÔºàÂèØÁúÅÁï•ÔºâÔºåÁî®Á©∫Ê†ºÈöîÂºÄÔºåÈ°∫Â∫è‰∏çÂèØÈ¢†ÂÄí„ÄÇfile_name Ëã•‰∏∫Ë∑ØÂæÑÊàñ‰∫åËøõÂà∂ÊñπÂºèÊâìÂºÄÁöÑÊñá‰ª∂ÔºåÂàôÊñá‰ª∂ÂøÖÈ°ª‰∏∫ UTF-8 ÁºñÁ†Å„ÄÇ
ËØçÈ¢ëÁúÅÁï•Êó∂‰ΩøÁî®Ëá™Âä®ËÆ°ÁÆóÁöÑËÉΩ‰øùËØÅÂàÜÂá∫ËØ•ËØçÁöÑËØçÈ¢ë„ÄÇ

‰æãÂ¶ÇÔºö
ÂàõÊñ∞Âäû 3 i
‰∫ëËÆ°ÁÆó 5
Âá±ÁâπÁê≥ nz
Âè∞‰∏≠



Êõ¥ÊîπÂàÜËØçÂô®ÔºàÈªòËÆ§‰∏∫ jieba.dtÔºâÁöÑ tmp_dir Âíå cache_file Â±ûÊÄßÔºåÂèØÂàÜÂà´ÊåáÂÆöÁºìÂ≠òÊñá‰ª∂ÊâÄÂú®ÁöÑÊñá‰ª∂Â§πÂèäÂÖ∂Êñá‰ª∂ÂêçÔºåÁî®‰∫éÂèóÈôêÁöÑÊñá‰ª∂Á≥ªÁªü„ÄÇ


ËåÉ‰æãÔºö


Ëá™ÂÆö‰πâËØçÂÖ∏Ôºöhttps://github.com/fxsjy/jieba/blob/master/test/userdict.txt


Áî®Ê≥ïÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/test/test_userdict.py


‰πãÂâçÔºö ÊùéÂ∞èÁ¶è / ÊòØ / ÂàõÊñ∞ / Âäû / ‰∏ª‰ªª / ‰πü / ÊòØ / ‰∫ë / ËÆ°ÁÆó / ÊñπÈù¢ / ÁöÑ / ‰∏ìÂÆ∂ /


Âä†ËΩΩËá™ÂÆö‰πâËØçÂ∫ìÂêéÔºö„ÄÄÊùéÂ∞èÁ¶è / ÊòØ / ÂàõÊñ∞Âäû / ‰∏ª‰ªª / ‰πü / ÊòØ / ‰∫ëËÆ°ÁÆó / ÊñπÈù¢ / ÁöÑ / ‰∏ìÂÆ∂ /






Ë∞ÉÊï¥ËØçÂÖ∏


‰ΩøÁî® add_word(word, freq=None, tag=None) Âíå del_word(word) ÂèØÂú®Á®ãÂ∫è‰∏≠Âä®ÊÄÅ‰øÆÊîπËØçÂÖ∏„ÄÇ


‰ΩøÁî® suggest_freq(segment, tune=True) ÂèØË∞ÉËäÇÂçï‰∏™ËØçËØ≠ÁöÑËØçÈ¢ëÔºå‰ΩøÂÖ∂ËÉΩÔºàÊàñ‰∏çËÉΩÔºâË¢´ÂàÜÂá∫Êù•„ÄÇ


Ê≥®ÊÑèÔºöËá™Âä®ËÆ°ÁÆóÁöÑËØçÈ¢ëÂú®‰ΩøÁî® HMM Êñ∞ËØçÂèëÁé∞ÂäüËÉΩÊó∂ÂèØËÉΩÊó†Êïà„ÄÇ


‰ª£Á†ÅÁ§∫‰æãÔºö
>>> print('/'.join(jieba.cut('Â¶ÇÊûúÊîæÂà∞post‰∏≠Â∞ÜÂá∫Èîô„ÄÇ', HMM=False)))
Â¶ÇÊûú/ÊîæÂà∞/post/‰∏≠Â∞Ü/Âá∫Èîô/„ÄÇ
>>> jieba.suggest_freq(('‰∏≠', 'Â∞Ü'), True)
494
>>> print('/'.join(jieba.cut('Â¶ÇÊûúÊîæÂà∞post‰∏≠Â∞ÜÂá∫Èîô„ÄÇ', HMM=False)))
Â¶ÇÊûú/ÊîæÂà∞/post/‰∏≠/Â∞Ü/Âá∫Èîô/„ÄÇ
>>> print('/'.join(jieba.cut('„ÄåÂè∞‰∏≠„ÄçÊ≠£Á°ÆÂ∫îËØ•‰∏ç‰ºöË¢´ÂàáÂºÄ', HMM=False)))
„Äå/Âè∞/‰∏≠/„Äç/Ê≠£Á°Æ/Â∫îËØ•/‰∏ç‰ºö/Ë¢´/ÂàáÂºÄ
>>> jieba.suggest_freq('Âè∞‰∏≠', True)
69
>>> print('/'.join(jieba.cut('„ÄåÂè∞‰∏≠„ÄçÊ≠£Á°ÆÂ∫îËØ•‰∏ç‰ºöË¢´ÂàáÂºÄ', HMM=False)))
„Äå/Âè∞‰∏≠/„Äç/Ê≠£Á°Æ/Â∫îËØ•/‰∏ç‰ºö/Ë¢´/ÂàáÂºÄ

""ÈÄöËøáÁî®Êà∑Ëá™ÂÆö‰πâËØçÂÖ∏Êù•Â¢ûÂº∫Ê≠ß‰πâÁ∫†ÈîôËÉΩÂäõ"" --- https://github.com/fxsjy/jieba/issues/14


ÂÖ≥ÈîÆËØçÊèêÂèñ


Âü∫‰∫é TF-IDF ÁÆóÊ≥ïÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñ
import jieba.analyse

jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())

sentence ‰∏∫ÂæÖÊèêÂèñÁöÑÊñáÊú¨
topK ‰∏∫ËøîÂõûÂá†‰∏™ TF/IDF ÊùÉÈáçÊúÄÂ§ßÁöÑÂÖ≥ÈîÆËØçÔºåÈªòËÆ§ÂÄº‰∏∫ 20
withWeight ‰∏∫ÊòØÂê¶‰∏ÄÂπ∂ËøîÂõûÂÖ≥ÈîÆËØçÊùÉÈáçÂÄºÔºåÈªòËÆ§ÂÄº‰∏∫ False
allowPOS ‰ªÖÂåÖÊã¨ÊåáÂÆöËØçÊÄßÁöÑËØçÔºåÈªòËÆ§ÂÄº‰∏∫Á©∫ÔºåÂç≥‰∏çÁ≠õÈÄâ


jieba.analyse.TFIDF(idf_path=None) Êñ∞Âª∫ TFIDF ÂÆû‰æãÔºåidf_path ‰∏∫ IDF È¢ëÁéáÊñá‰ª∂

‰ª£Á†ÅÁ§∫‰æã ÔºàÂÖ≥ÈîÆËØçÊèêÂèñÔºâ
https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py
ÂÖ≥ÈîÆËØçÊèêÂèñÊâÄ‰ΩøÁî®ÈÄÜÂêëÊñá‰ª∂È¢ëÁéáÔºàIDFÔºâÊñáÊú¨ËØ≠ÊñôÂ∫ìÂèØ‰ª•ÂàáÊç¢ÊàêËá™ÂÆö‰πâËØ≠ÊñôÂ∫ìÁöÑË∑ØÂæÑ

Áî®Ê≥ïÔºö jieba.analyse.set_idf_path(file_name) # file_name‰∏∫Ëá™ÂÆö‰πâËØ≠ÊñôÂ∫ìÁöÑË∑ØÂæÑ
Ëá™ÂÆö‰πâËØ≠ÊñôÂ∫ìÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big
Áî®Ê≥ïÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py

ÂÖ≥ÈîÆËØçÊèêÂèñÊâÄ‰ΩøÁî®ÂÅúÊ≠¢ËØçÔºàStop WordsÔºâÊñáÊú¨ËØ≠ÊñôÂ∫ìÂèØ‰ª•ÂàáÊç¢ÊàêËá™ÂÆö‰πâËØ≠ÊñôÂ∫ìÁöÑË∑ØÂæÑ

Áî®Ê≥ïÔºö jieba.analyse.set_stop_words(file_name) # file_name‰∏∫Ëá™ÂÆö‰πâËØ≠ÊñôÂ∫ìÁöÑË∑ØÂæÑ
Ëá™ÂÆö‰πâËØ≠ÊñôÂ∫ìÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt
Áî®Ê≥ïÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py

ÂÖ≥ÈîÆËØç‰∏ÄÂπ∂ËøîÂõûÂÖ≥ÈîÆËØçÊùÉÈáçÂÄºÁ§∫‰æã

Áî®Ê≥ïÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_with_weight.py

Âü∫‰∫é TextRank ÁÆóÊ≥ïÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñ

jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) Áõ¥Êé•‰ΩøÁî®ÔºåÊé•Âè£Áõ∏ÂêåÔºåÊ≥®ÊÑèÈªòËÆ§ËøáÊª§ËØçÊÄß„ÄÇ
jieba.analyse.TextRank() Êñ∞Âª∫Ëá™ÂÆö‰πâ TextRank ÂÆû‰æã

ÁÆóÊ≥ïËÆ∫ÊñáÔºö TextRank: Bringing Order into Texts
Âü∫Êú¨ÊÄùÊÉ≥:

Â∞ÜÂæÖÊäΩÂèñÂÖ≥ÈîÆËØçÁöÑÊñáÊú¨ËøõË°åÂàÜËØç
‰ª•Âõ∫ÂÆöÁ™óÂè£Â§ßÂ∞è(ÈªòËÆ§‰∏∫5ÔºåÈÄöËøáspanÂ±ûÊÄßË∞ÉÊï¥)ÔºåËØç‰πãÈó¥ÁöÑÂÖ±Áé∞ÂÖ≥Á≥ªÔºåÊûÑÂª∫Âõæ
ËÆ°ÁÆóÂõæ‰∏≠ËäÇÁÇπÁöÑPageRankÔºåÊ≥®ÊÑèÊòØÊó†ÂêëÂ∏¶ÊùÉÂõæ

‰ΩøÁî®Á§∫‰æã:
ËßÅ test/demo.py

ËØçÊÄßÊ†áÊ≥®



jieba.posseg.POSTokenizer(tokenizer=None) Êñ∞Âª∫Ëá™ÂÆö‰πâÂàÜËØçÂô®Ôºåtokenizer ÂèÇÊï∞ÂèØÊåáÂÆöÂÜÖÈÉ®‰ΩøÁî®ÁöÑ jieba.Tokenizer ÂàÜËØçÂô®„ÄÇjieba.posseg.dt ‰∏∫ÈªòËÆ§ËØçÊÄßÊ†áÊ≥®ÂàÜËØçÂô®„ÄÇ
Ê†áÊ≥®Âè•Â≠êÂàÜËØçÂêéÊØè‰∏™ËØçÁöÑËØçÊÄßÔºåÈááÁî®Âíå ictclas ÂÖºÂÆπÁöÑÊ†áËÆ∞Ê≥ï„ÄÇ
Áî®Ê≥ïÁ§∫‰æã

>>> import jieba.posseg as pseg
>>> words = pseg.cut(""ÊàëÁà±Âåó‰∫¨Â§©ÂÆâÈó®"")
>>> for word, flag in words:
...    print('%s %s' % (word, flag))
...
Êàë r
Áà± v
Âåó‰∫¨ ns
Â§©ÂÆâÈó® ns

Âπ∂Ë°åÂàÜËØç




ÂéüÁêÜÔºöÂ∞ÜÁõÆÊ†áÊñáÊú¨ÊåâË°åÂàÜÈöîÂêéÔºåÊääÂêÑË°åÊñáÊú¨ÂàÜÈÖçÂà∞Â§ö‰∏™ Python ËøõÁ®ãÂπ∂Ë°åÂàÜËØçÔºåÁÑ∂ÂêéÂΩíÂπ∂ÁªìÊûúÔºå‰ªéËÄåËé∑ÂæóÂàÜËØçÈÄüÂ∫¶ÁöÑÂèØËßÇÊèêÂçá


Âü∫‰∫é python Ëá™Â∏¶ÁöÑ multiprocessing Ê®°ÂùóÔºåÁõÆÂâçÊöÇ‰∏çÊîØÊåÅ Windows


Áî®Ê≥ïÔºö

jieba.enable_parallel(4) # ÂºÄÂêØÂπ∂Ë°åÂàÜËØçÊ®°ÂºèÔºåÂèÇÊï∞‰∏∫Âπ∂Ë°åËøõÁ®ãÊï∞
jieba.disable_parallel() # ÂÖ≥Èó≠Âπ∂Ë°åÂàÜËØçÊ®°Âºè



‰æãÂ≠êÔºöhttps://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py


ÂÆûÈ™åÁªìÊûúÔºöÂú® 4 Ê†∏ 3.4GHz Linux Êú∫Âô®‰∏äÔºåÂØπÈáëÂ∫∏ÂÖ®ÈõÜËøõË°åÁ≤æÁ°ÆÂàÜËØçÔºåËé∑Âæó‰∫Ü 1MB/s ÁöÑÈÄüÂ∫¶ÔºåÊòØÂçïËøõÁ®ãÁâàÁöÑ 3.3 ÂÄç„ÄÇ


Ê≥®ÊÑèÔºöÂπ∂Ë°åÂàÜËØç‰ªÖÊîØÊåÅÈªòËÆ§ÂàÜËØçÂô® jieba.dt Âíå jieba.posseg.dt„ÄÇ



TokenizeÔºöËøîÂõûËØçËØ≠Âú®ÂéüÊñáÁöÑËµ∑Ê≠¢‰ΩçÁΩÆ



Ê≥®ÊÑèÔºåËæìÂÖ•ÂèÇÊï∞Âè™Êé•Âèó unicode
ÈªòËÆ§Ê®°Âºè

result = jieba.tokenize(u'Ê∞∏ÂíåÊúçË£ÖÈ•∞ÂìÅÊúâÈôêÂÖ¨Âè∏')
for tk in result:
    print(""word %s\t\t start: %d \t\t end:%d"" % (tk[0],tk[1],tk[2]))
word Ê∞∏Âíå                start: 0                end:2
word ÊúçË£Ö                start: 2                end:4
word È•∞ÂìÅ                start: 4                end:6
word ÊúâÈôêÂÖ¨Âè∏            start: 6                end:10



ÊêúÁ¥¢Ê®°Âºè

result = jieba.tokenize(u'Ê∞∏ÂíåÊúçË£ÖÈ•∞ÂìÅÊúâÈôêÂÖ¨Âè∏', mode='search')
for tk in result:
    print(""word %s\t\t start: %d \t\t end:%d"" % (tk[0],tk[1],tk[2]))
word Ê∞∏Âíå                start: 0                end:2
word ÊúçË£Ö                start: 2                end:4
word È•∞ÂìÅ                start: 4                end:6
word ÊúâÈôê                start: 6                end:8
word ÂÖ¨Âè∏                start: 8                end:10
word ÊúâÈôêÂÖ¨Âè∏            start: 6                end:10


ChineseAnalyzer for Whoosh ÊêúÁ¥¢ÂºïÊìé



ÂºïÁî®Ôºö from jieba.analyse import ChineseAnalyzer
Áî®Ê≥ïÁ§∫‰æãÔºöhttps://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py


ÂëΩ‰ª§Ë°åÂàÜËØç


‰ΩøÁî®Á§∫‰æãÔºöpython -m jieba news.txt > cut_result.txt
ÂëΩ‰ª§Ë°åÈÄâÈ°πÔºàÁøªËØëÔºâÔºö
‰ΩøÁî®: python -m jieba [options] filename

ÁªìÂ∑¥ÂëΩ‰ª§Ë°åÁïåÈù¢„ÄÇ

Âõ∫ÂÆöÂèÇÊï∞:
  filename              ËæìÂÖ•Êñá‰ª∂

ÂèØÈÄâÂèÇÊï∞:
  -h, --help            ÊòæÁ§∫Ê≠§Â∏ÆÂä©‰ø°ÊÅØÂπ∂ÈÄÄÂá∫
  -d [DELIM], --delimiter [DELIM]
                        ‰ΩøÁî® DELIM ÂàÜÈöîËØçËØ≠ÔºåËÄå‰∏çÊòØÁî®ÈªòËÆ§ÁöÑ' / '„ÄÇ
                        Ëã•‰∏çÊåáÂÆö DELIMÔºåÂàô‰ΩøÁî®‰∏Ä‰∏™Á©∫Ê†ºÂàÜÈöî„ÄÇ
  -p [DELIM], --pos [DELIM]
                        ÂêØÁî®ËØçÊÄßÊ†áÊ≥®ÔºõÂ¶ÇÊûúÊåáÂÆö DELIMÔºåËØçËØ≠ÂíåËØçÊÄß‰πãÈó¥
                        Áî®ÂÆÉÂàÜÈöîÔºåÂê¶ÂàôÁî® _ ÂàÜÈöî
  -D DICT, --dict DICT  ‰ΩøÁî® DICT ‰ª£ÊõøÈªòËÆ§ËØçÂÖ∏
  -u USER_DICT, --user-dict USER_DICT
                        ‰ΩøÁî® USER_DICT ‰Ωú‰∏∫ÈôÑÂä†ËØçÂÖ∏Ôºå‰∏éÈªòËÆ§ËØçÂÖ∏ÊàñËá™ÂÆö‰πâËØçÂÖ∏ÈÖçÂêà‰ΩøÁî®
  -a, --cut-all         ÂÖ®Ê®°ÂºèÂàÜËØçÔºà‰∏çÊîØÊåÅËØçÊÄßÊ†áÊ≥®Ôºâ
  -n, --no-hmm          ‰∏ç‰ΩøÁî®ÈöêÂê´È©¨Â∞îÂèØÂ§´Ê®°Âûã
  -q, --quiet           ‰∏çËæìÂá∫ËΩΩÂÖ•‰ø°ÊÅØÂà∞ STDERR
  -V, --version         ÊòæÁ§∫ÁâàÊú¨‰ø°ÊÅØÂπ∂ÈÄÄÂá∫

Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöÊñá‰ª∂ÂêçÔºåÂàô‰ΩøÁî®Ê†áÂáÜËæìÂÖ•„ÄÇ

--help ÈÄâÈ°πËæìÂá∫Ôºö
$> python -m jieba --help
Jieba command line interface.

positional arguments:
  filename              input file

optional arguments:
  -h, --help            show this help message and exit
  -d [DELIM], --delimiter [DELIM]
                        use DELIM instead of ' / ' for word delimiter; or a
                        space if it is used without DELIM
  -p [DELIM], --pos [DELIM]
                        enable POS tagging; if DELIM is specified, use DELIM
                        instead of '_' for POS delimiter
  -D DICT, --dict DICT  use DICT as dictionary
  -u USER_DICT, --user-dict USER_DICT
                        use USER_DICT together with the default dictionary or
                        DICT (if specified)
  -a, --cut-all         full pattern cutting (ignored with POS tagging)
  -n, --no-hmm          don't use the Hidden Markov Model
  -q, --quiet           don't print loading messages to stderr
  -V, --version         show program's version number and exit

If no filename specified, use STDIN instead.

Âª∂ËøüÂä†ËΩΩÊú∫Âà∂
jieba ÈááÁî®Âª∂ËøüÂä†ËΩΩÔºåimport jieba Âíå jieba.Tokenizer() ‰∏ç‰ºöÁ´ãÂç≥Ëß¶ÂèëËØçÂÖ∏ÁöÑÂä†ËΩΩÔºå‰∏ÄÊó¶ÊúâÂøÖË¶ÅÊâçÂºÄÂßãÂä†ËΩΩËØçÂÖ∏ÊûÑÂª∫ÂâçÁºÄÂ≠óÂÖ∏„ÄÇÂ¶ÇÊûú‰Ω†ÊÉ≥ÊâãÂ∑•ÂàùÂßã jiebaÔºå‰πüÂèØ‰ª•ÊâãÂä®ÂàùÂßãÂåñ„ÄÇ
import jieba
jieba.initialize()  # ÊâãÂä®ÂàùÂßãÂåñÔºàÂèØÈÄâÔºâ

Âú® 0.28 ‰πãÂâçÁöÑÁâàÊú¨ÊòØ‰∏çËÉΩÊåáÂÆö‰∏ªËØçÂÖ∏ÁöÑË∑ØÂæÑÁöÑÔºåÊúâ‰∫ÜÂª∂ËøüÂä†ËΩΩÊú∫Âà∂ÂêéÔºå‰Ω†ÂèØ‰ª•ÊîπÂèò‰∏ªËØçÂÖ∏ÁöÑË∑ØÂæÑ:
jieba.set_dictionary('data/dict.txt.big')

‰æãÂ≠êÔºö https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py
ÂÖ∂‰ªñËØçÂÖ∏


Âç†Áî®ÂÜÖÂ≠òËæÉÂ∞èÁöÑËØçÂÖ∏Êñá‰ª∂
https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small


ÊîØÊåÅÁπÅ‰ΩìÂàÜËØçÊõ¥Â•ΩÁöÑËØçÂÖ∏Êñá‰ª∂
https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big


‰∏ãËΩΩ‰Ω†ÊâÄÈúÄË¶ÅÁöÑËØçÂÖ∏ÔºåÁÑ∂ÂêéË¶ÜÁõñ jieba/dict.txt Âç≥ÂèØÔºõÊàñËÄÖÁî® jieba.set_dictionary('data/dict.txt.big')
ÂÖ∂‰ªñËØ≠Ë®ÄÂÆûÁé∞
ÁªìÂ∑¥ÂàÜËØç Java ÁâàÊú¨
‰ΩúËÄÖÔºöpiaolingxue
Âú∞ÂùÄÔºöhttps://github.com/huaban/jieba-analysis
ÁªìÂ∑¥ÂàÜËØç C++ ÁâàÊú¨
‰ΩúËÄÖÔºöyanyiwu
Âú∞ÂùÄÔºöhttps://github.com/yanyiwu/cppjieba
ÁªìÂ∑¥ÂàÜËØç Rust ÁâàÊú¨
‰ΩúËÄÖÔºömessense, MnO2
Âú∞ÂùÄÔºöhttps://github.com/messense/jieba-rs
ÁªìÂ∑¥ÂàÜËØç Node.js ÁâàÊú¨
‰ΩúËÄÖÔºöyanyiwu
Âú∞ÂùÄÔºöhttps://github.com/yanyiwu/nodejieba
ÁªìÂ∑¥ÂàÜËØç Erlang ÁâàÊú¨
‰ΩúËÄÖÔºöfalood
Âú∞ÂùÄÔºöhttps://github.com/falood/exjieba
ÁªìÂ∑¥ÂàÜËØç R ÁâàÊú¨
‰ΩúËÄÖÔºöqinwf
Âú∞ÂùÄÔºöhttps://github.com/qinwf/jiebaR
ÁªìÂ∑¥ÂàÜËØç iOS ÁâàÊú¨
‰ΩúËÄÖÔºöyanyiwu
Âú∞ÂùÄÔºöhttps://github.com/yanyiwu/iosjieba
ÁªìÂ∑¥ÂàÜËØç PHP ÁâàÊú¨
‰ΩúËÄÖÔºöfukuball
Âú∞ÂùÄÔºöhttps://github.com/fukuball/jieba-php
ÁªìÂ∑¥ÂàÜËØç .NET(C#) ÁâàÊú¨
‰ΩúËÄÖÔºöanderscui
Âú∞ÂùÄÔºöhttps://github.com/anderscui/jieba.NET/
ÁªìÂ∑¥ÂàÜËØç Go ÁâàÊú¨

‰ΩúËÄÖ: wangbin Âú∞ÂùÄ: https://github.com/wangbin/jiebago
‰ΩúËÄÖ: yanyiwu Âú∞ÂùÄ: https://github.com/yanyiwu/gojieba

ÁªìÂ∑¥ÂàÜËØçAndroidÁâàÊú¨

‰ΩúËÄÖ   Dongliang.W  Âú∞ÂùÄÔºöhttps://github.com/452896915/jieba-android

Á≥ªÁªüÈõÜÊàê

Solr: https://github.com/sing1ee/jieba-solr

ÂàÜËØçÈÄüÂ∫¶

1.5 MB / Second in Full Mode
400 KB / Second in Default Mode
ÊµãËØïÁéØÂ¢É: Intel(R) Core(TM) i7-2600 CPU @ 3.4GHzÔºõ„ÄäÂõ¥Âüé„Äã.txt

Â∏∏ËßÅÈóÆÈ¢ò
1. Ê®°ÂûãÁöÑÊï∞ÊçÆÊòØÂ¶Ç‰ΩïÁîüÊàêÁöÑÔºü
ËØ¶ËßÅÔºö https://github.com/fxsjy/jieba/issues/7
2. ‚ÄúÂè∞‰∏≠‚ÄùÊÄªÊòØË¢´ÂàáÊàê‚ÄúÂè∞ ‰∏≠‚ÄùÔºüÔºà‰ª•ÂèäÁ±ª‰ººÊÉÖÂÜµÔºâ
P(Âè∞‰∏≠) Ôºú P(Âè∞)√óP(‰∏≠)Ôºå‚ÄúÂè∞‰∏≠‚ÄùËØçÈ¢ë‰∏çÂ§üÂØºËá¥ÂÖ∂ÊàêËØçÊ¶ÇÁéáËæÉ‰Ωé
Ëß£ÂÜ≥ÊñπÊ≥ïÔºöÂº∫Âà∂Ë∞ÉÈ´òËØçÈ¢ë
jieba.add_word('Âè∞‰∏≠') ÊàñËÄÖ jieba.suggest_freq('Âè∞‰∏≠', True)
3. ‚Äú‰ªäÂ§©Â§©Ê∞î ‰∏çÈîô‚ÄùÂ∫îËØ•Ë¢´ÂàáÊàê‚Äú‰ªäÂ§© Â§©Ê∞î ‰∏çÈîô‚ÄùÔºüÔºà‰ª•ÂèäÁ±ª‰ººÊÉÖÂÜµÔºâ
Ëß£ÂÜ≥ÊñπÊ≥ïÔºöÂº∫Âà∂Ë∞É‰ΩéËØçÈ¢ë
jieba.suggest_freq(('‰ªäÂ§©', 'Â§©Ê∞î'), True)
ÊàñËÄÖÁõ¥Êé•Âà†Èô§ËØ•ËØç jieba.del_word('‰ªäÂ§©Â§©Ê∞î')
4. ÂàáÂá∫‰∫ÜËØçÂÖ∏‰∏≠Ê≤°ÊúâÁöÑËØçËØ≠ÔºåÊïàÊûú‰∏çÁêÜÊÉ≥Ôºü
Ëß£ÂÜ≥ÊñπÊ≥ïÔºöÂÖ≥Èó≠Êñ∞ËØçÂèëÁé∞
jieba.cut('‰∏∞Áî∞Â§™ÁúÅ‰∫Ü', HMM=False)
jieba.cut('Êàë‰ª¨‰∏≠Âá∫‰∫Ü‰∏Ä‰∏™ÂèõÂæí', HMM=False)
Êõ¥Â§öÈóÆÈ¢òËØ∑ÁÇπÂáªÔºöhttps://github.com/fxsjy/jieba/issues?sort=updated&state=closed
‰øÆËÆ¢ÂéÜÂè≤
https://github.com/fxsjy/jieba/blob/master/Changelog

jieba
""Jieba"" (Chinese for ""to stutter"") Chinese text segmentation: built to be the best Python Chinese word segmentation module.
Features

Support three types of segmentation mode:


Accurate Mode attempts to cut the sentence into the most accurate segmentations, which is suitable for text analysis.
Full Mode gets all the possible words from the sentence. Fast but not accurate.
Search Engine Mode, based on the Accurate Mode, attempts to cut long words into several short words, which can raise the recall rate. Suitable for search engines.


Supports Traditional Chinese
Supports customized dictionaries
MIT License

Online demo
http://jiebademo.ap01.aws.af.cm/
(Powered by Appfog)
Usage

Fully automatic installation: easy_install jieba or pip install jieba
Semi-automatic installation: Download http://pypi.python.org/pypi/jieba/ , run python setup.py install after extracting.
Manual installation: place the jieba directory in the current directory or python site-packages directory.
import jieba.

Algorithm

Based on a prefix dictionary structure to achieve efficient word graph scanning. Build a directed acyclic graph (DAG) for all possible word combinations.
Use dynamic programming to find the most probable combination based on the word frequency.
For unknown words, a HMM-based model is used with the Viterbi algorithm.

Main Functions

Cut



The jieba.cut function accepts three input parameters: the first parameter is the string to be cut; the second parameter is cut_all, controlling the cut mode; the third parameter is to control whether to use the Hidden Markov Model.
jieba.cut_for_search accepts two parameter: the string to be cut; whether to use the Hidden Markov Model. This will cut the sentence into short words suitable for search engines.
The input string can be an unicode/str object, or a str/bytes object which is encoded in UTF-8 or GBK. Note that using GBK encoding is not recommended because it may be unexpectly decoded as UTF-8.
jieba.cut and jieba.cut_for_search returns an generator, from which you can use a for loop to get the segmentation result (in unicode).
jieba.lcut and jieba.lcut_for_search returns a list.
jieba.Tokenizer(dictionary=DEFAULT_DICT) creates a new customized Tokenizer, which enables you to use different dictionaries at the same time. jieba.dt is the default Tokenizer, to which almost all global functions are mapped.

Code example: segmentation
#encoding=utf-8
import jieba

seg_list = jieba.cut(""ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶"", cut_all=True)
print(""Full Mode: "" + ""/ "".join(seg_list))  # ÂÖ®Ê®°Âºè

seg_list = jieba.cut(""ÊàëÊù•Âà∞Âåó‰∫¨Ê∏ÖÂçéÂ§ßÂ≠¶"", cut_all=False)
print(""Default Mode: "" + ""/ "".join(seg_list))  # ÈªòËÆ§Ê®°Âºè

seg_list = jieba.cut(""‰ªñÊù•Âà∞‰∫ÜÁΩëÊòìÊù≠Á†îÂ§ßÂé¶"")
print("", "".join(seg_list))

seg_list = jieba.cut_for_search(""Â∞èÊòéÁ°ïÂ£´ÊØï‰∏ö‰∫é‰∏≠ÂõΩÁßëÂ≠¶Èô¢ËÆ°ÁÆóÊâÄÔºåÂêéÂú®Êó•Êú¨‰∫¨ÈÉΩÂ§ßÂ≠¶Ê∑±ÈÄ†"")  # ÊêúÁ¥¢ÂºïÊìéÊ®°Âºè
print("", "".join(seg_list))
Output:
[Full Mode]: Êàë/ Êù•Âà∞/ Âåó‰∫¨/ Ê∏ÖÂçé/ Ê∏ÖÂçéÂ§ßÂ≠¶/ ÂçéÂ§ß/ Â§ßÂ≠¶

[Accurate Mode]: Êàë/ Êù•Âà∞/ Âåó‰∫¨/ Ê∏ÖÂçéÂ§ßÂ≠¶

[Unknown Words Recognize] ‰ªñ, Êù•Âà∞, ‰∫Ü, ÁΩëÊòì, Êù≠Á†î, Â§ßÂé¶    (In this case, ""Êù≠Á†î"" is not in the dictionary, but is identified by the Viterbi algorithm)

[Search Engine Mode]Ôºö Â∞èÊòé, Á°ïÂ£´, ÊØï‰∏ö, ‰∫é, ‰∏≠ÂõΩ, ÁßëÂ≠¶, Â≠¶Èô¢, ÁßëÂ≠¶Èô¢, ‰∏≠ÂõΩÁßëÂ≠¶Èô¢, ËÆ°ÁÆó, ËÆ°ÁÆóÊâÄ, Âêé, Âú®, Êó•Êú¨, ‰∫¨ÈÉΩ, Â§ßÂ≠¶, Êó•Êú¨‰∫¨ÈÉΩÂ§ßÂ≠¶, Ê∑±ÈÄ†


Add a custom dictionary


Load dictionary

Developers can specify their own custom dictionary to be included in the jieba default dictionary. Jieba is able to identify new words, but you can add your own new words can ensure a higher accuracy.
UsageÔºö jieba.load_userdict(file_name) # file_name is a file-like object or the path of the custom dictionary
The dictionary format is the same as that of dict.txt: one word per line; each line is divided into three parts separated by a space: word, word frequency, POS tag. If file_name is a path or a file opened in binary mode, the dictionary must be UTF-8 encoded.
The word frequency and POS tag can be omitted respectively. The word frequency will be filled with a suitable value if omitted.

For example:
ÂàõÊñ∞Âäû 3 i
‰∫ëËÆ°ÁÆó 5
Âá±ÁâπÁê≥ nz
Âè∞‰∏≠



Change a Tokenizer's tmp_dir and cache_file to specify the path of the cache file, for using on a restricted file system.


Example:
  ‰∫ëËÆ°ÁÆó 5
  ÊùéÂ∞èÁ¶è 2
  ÂàõÊñ∞Âäû 3

  [Before]Ôºö ÊùéÂ∞èÁ¶è / ÊòØ / ÂàõÊñ∞ / Âäû / ‰∏ª‰ªª / ‰πü / ÊòØ / ‰∫ë / ËÆ°ÁÆó / ÊñπÈù¢ / ÁöÑ / ‰∏ìÂÆ∂ /

  [After]Ôºö„ÄÄÊùéÂ∞èÁ¶è / ÊòØ / ÂàõÊñ∞Âäû / ‰∏ª‰ªª / ‰πü / ÊòØ / ‰∫ëËÆ°ÁÆó / ÊñπÈù¢ / ÁöÑ / ‰∏ìÂÆ∂ /



Modify dictionary


Use add_word(word, freq=None, tag=None) and del_word(word) to modify the dictionary dynamically in programs.


Use suggest_freq(segment, tune=True) to adjust the frequency of a single word so that it can (or cannot) be segmented.


Note that HMM may affect the final result.


Example:
>>> print('/'.join(jieba.cut('Â¶ÇÊûúÊîæÂà∞post‰∏≠Â∞ÜÂá∫Èîô„ÄÇ', HMM=False)))
Â¶ÇÊûú/ÊîæÂà∞/post/‰∏≠Â∞Ü/Âá∫Èîô/„ÄÇ
>>> jieba.suggest_freq(('‰∏≠', 'Â∞Ü'), True)
494
>>> print('/'.join(jieba.cut('Â¶ÇÊûúÊîæÂà∞post‰∏≠Â∞ÜÂá∫Èîô„ÄÇ', HMM=False)))
Â¶ÇÊûú/ÊîæÂà∞/post/‰∏≠/Â∞Ü/Âá∫Èîô/„ÄÇ
>>> print('/'.join(jieba.cut('„ÄåÂè∞‰∏≠„ÄçÊ≠£Á°ÆÂ∫îËØ•‰∏ç‰ºöË¢´ÂàáÂºÄ', HMM=False)))
„Äå/Âè∞/‰∏≠/„Äç/Ê≠£Á°Æ/Â∫îËØ•/‰∏ç‰ºö/Ë¢´/ÂàáÂºÄ
>>> jieba.suggest_freq('Âè∞‰∏≠', True)
69
>>> print('/'.join(jieba.cut('„ÄåÂè∞‰∏≠„ÄçÊ≠£Á°ÆÂ∫îËØ•‰∏ç‰ºöË¢´ÂàáÂºÄ', HMM=False)))
„Äå/Âè∞‰∏≠/„Äç/Ê≠£Á°Æ/Â∫îËØ•/‰∏ç‰ºö/Ë¢´/ÂàáÂºÄ

Keyword Extraction


import jieba.analyse

jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())

sentence: the text to be extracted
topK: return how many keywords with the highest TF/IDF weights. The default value is 20
withWeight: whether return TF/IDF weights with the keywords. The default value is False
allowPOS: filter words with which POSs are included. Empty for no filtering.


jieba.analyse.TFIDF(idf_path=None) creates a new TFIDF instance, idf_path specifies IDF file path.

Example (keyword extraction)
https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py
Developers can specify their own custom IDF corpus in jieba keyword extraction

UsageÔºö jieba.analyse.set_idf_path(file_name) # file_name is the path for the custom corpus
Custom Corpus SampleÔºöhttps://github.com/fxsjy/jieba/blob/master/extra_dict/idf.txt.big
Sample CodeÔºöhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_idfpath.py

Developers can specify their own custom stop words corpus in jieba keyword extraction

UsageÔºö jieba.analyse.set_stop_words(file_name) # file_name is the path for the custom corpus
Custom Corpus SampleÔºöhttps://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt
Sample CodeÔºöhttps://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py

There's also a TextRank implementation available.
Use: jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v'))
Note that it filters POS by default.
jieba.analyse.TextRank() creates a new TextRank instance.

Part of Speech Tagging



jieba.posseg.POSTokenizer(tokenizer=None) creates a new customized Tokenizer. tokenizer specifies the jieba.Tokenizer to internally use. jieba.posseg.dt is the default POSTokenizer.
Tags the POS of each word after segmentation, using labels compatible with ictclas.
Example:

>>> import jieba.posseg as pseg
>>> words = pseg.cut(""ÊàëÁà±Âåó‰∫¨Â§©ÂÆâÈó®"")
>>> for w in words:
...    print('%s %s' % (w.word, w.flag))
...
Êàë r
Áà± v
Âåó‰∫¨ ns
Â§©ÂÆâÈó® ns

Parallel Processing




Principle: Split target text by line, assign the lines into multiple Python processes, and then merge the results, which is considerably faster.


Based on the multiprocessing module of Python.


Usage:

jieba.enable_parallel(4) # Enable parallel processing. The parameter is the number of processes.
jieba.disable_parallel() # Disable parallel processing.



Example:
https://github.com/fxsjy/jieba/blob/master/test/parallel/test_file.py


Result: On a four-core 3.4GHz Linux machine, do accurate word segmentation on Complete Works of Jin Yong, and the speed reaches 1MB/s, which is 3.3 times faster than the single-process version.


Note that parallel processing supports only default tokenizers, jieba.dt and jieba.posseg.dt.



Tokenize: return words with position



The input must be unicode
Default mode

result = jieba.tokenize(u'Ê∞∏ÂíåÊúçË£ÖÈ•∞ÂìÅÊúâÈôêÂÖ¨Âè∏')
for tk in result:
    print(""word %s\t\t start: %d \t\t end:%d"" % (tk[0],tk[1],tk[2]))
word Ê∞∏Âíå                start: 0                end:2
word ÊúçË£Ö                start: 2                end:4
word È•∞ÂìÅ                start: 4                end:6
word ÊúâÈôêÂÖ¨Âè∏            start: 6                end:10



Search mode

result = jieba.tokenize(u'Ê∞∏ÂíåÊúçË£ÖÈ•∞ÂìÅÊúâÈôêÂÖ¨Âè∏',mode='search')
for tk in result:
    print(""word %s\t\t start: %d \t\t end:%d"" % (tk[0],tk[1],tk[2]))
word Ê∞∏Âíå                start: 0                end:2
word ÊúçË£Ö                start: 2                end:4
word È•∞ÂìÅ                start: 4                end:6
word ÊúâÈôê                start: 6                end:8
word ÂÖ¨Âè∏                start: 8                end:10
word ÊúâÈôêÂÖ¨Âè∏            start: 6                end:10


ChineseAnalyzer for Whoosh



from jieba.analyse import ChineseAnalyzer
Example: https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py


Command Line Interface


$> python -m jieba --help
Jieba command line interface.

positional arguments:
  filename              input file

optional arguments:
  -h, --help            show this help message and exit
  -d [DELIM], --delimiter [DELIM]
                        use DELIM instead of ' / ' for word delimiter; or a
                        space if it is used without DELIM
  -p [DELIM], --pos [DELIM]
                        enable POS tagging; if DELIM is specified, use DELIM
                        instead of '_' for POS delimiter
  -D DICT, --dict DICT  use DICT as dictionary
  -u USER_DICT, --user-dict USER_DICT
                        use USER_DICT together with the default dictionary or
                        DICT (if specified)
  -a, --cut-all         full pattern cutting (ignored with POS tagging)
  -n, --no-hmm          don't use the Hidden Markov Model
  -q, --quiet           don't print loading messages to stderr
  -V, --version         show program's version number and exit

If no filename specified, use STDIN instead.

Initialization
By default, Jieba don't build the prefix dictionary unless it's necessary. This takes 1-3 seconds, after which it is not initialized again. If you want to initialize Jieba manually, you can call:
import jieba
jieba.initialize()  # (optional)

You can also specify the dictionary (not supported before version 0.28) :
jieba.set_dictionary('data/dict.txt.big')

Using Other Dictionaries
It is possible to use your own dictionary with Jieba, and there are also two dictionaries ready for download:


A smaller dictionary for a smaller memory footprint:
https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.small


There is also a bigger dictionary that has better support for traditional Chinese (ÁπÅÈ´î):
https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big


By default, an in-between dictionary is used, called dict.txt and included in the distribution.
In either case, download the file you want, and then call jieba.set_dictionary('data/dict.txt.big') or just replace the existing dict.txt.
Segmentation speed

1.5 MB / Second in Full Mode
400 KB / Second in Default Mode
Test Env: Intel(R) Core(TM) i7-2600 CPU @ 3.4GHzÔºõ„ÄäÂõ¥Âüé„Äã.txt

",GitHub - fxsjy/jieba: ÁªìÂ∑¥‰∏≠ÊñáÂàÜËØç
12,Python,"YouCompleteMe: a code-completion engine for Vim



Early Warning: Dropping support for Python 2 in 2020
In early 2020, YCM will drop support for Python 2. But we will maintain
criticial fixes on a branch (name TBA) of YCM for a period of 1 year.
Why?
Over the past decade, YouCompleteMe has had an at times fractious,
but ultimately very successful relationship with Python 2. However, more
recently it has been carrying on a simultaneous relationship with Python 3.
Indeed all of YCM and ycmd code is Python 3 code, with a lot of gubbins
to make it work also on Python 2. This makes the code more complex,
requires double testing of everything, and restricts the developers from using
certain new langauge features, ultimately restricting the features we can
deliver to users.
On 1st January 2020, Python 2 will be officially end of life. And therefore, so
will its relationship with YouCompleteMe and ycmd.
Help, Advice, Support
Looking for help, advice or support? Having problems getting YCM to work?
First carefully read the installation instructions for your OS.
We recommend you use the supplied install.py.
Next check the User Guide section on the semantic completer that
you are using. For C/C++/Objective-C/Objective-C++/CUDA, you  must read this
section.
Finally, check the FAQ.
If, after reading the installation and user guides, and checking the FAQ, you're
still having trouble, check the contacts section below for how to
get in touch.
Please do NOT go to #vim on freenode for support. Please contact the
YouCompleteMe maintainers directly using the contact details below.
Contents

Intro
Installation

macOS
Linux 64-bit
Windows
FreeBSD/OpenBSD
Full Installation Guide


Quick Feature Summary
User Guide

General Usage
Client-Server Architecture
Completion String Ranking
General Semantic Completion
C-family Semantic Completion
Java Semantic Completion
Python Semantic Completion
Rust Semantic Completion
Go Semantic Completion
JavaScript and TypeScript Semantic Completion
Semantic Completion for Other Languages
LSP Configuration
Writing New Semantic Completers
Diagnostic Display

Diagnostic Highlighting Groups




Commands

YcmCompleter subcommands

GoTo Commands
Semantic Information Commands
Refactoring Commands
Miscellaneous Commands




Functions
Autocommands
Options
FAQ
Contributor Code of Conduct
Contact
License

Intro
YouCompleteMe is a fast, as-you-type, fuzzy-search code completion engine for
Vim. It has several completion engines:

an identifier-based engine that works with every programming language,
a Clang-based engine that provides native semantic code
completion for C/C++/Objective-C/Objective-C++/CUDA (from now on referred to
as ""the C-family languages""),
a powerful clangd-based completion engine for the C-family languages.
a Jedi-based completion engine for Python 2 and 3,
an OmniSharp-Roslyn-based completion engine for C#,
a Gopls-based completion engine for Go,
a TSServer-based completion engine for JavaScript and TypeScript,
a rls-based completion engine for Rust,
a jdt.ls-based experimental completion engine for Java.
a generic Language Server Protocol implementation for any language
and an omnifunc-based completer that uses data from Vim's omnicomplete system
to provide semantic completions for many other languages (Ruby, PHP etc.).


Here's an explanation of what happens in the short GIF demo above.
First, realize that no keyboard shortcuts had to be pressed to get the list
of completion candidates at any point in the demo. The user just types and the
suggestions pop up by themselves. If the user doesn't find the completion
suggestions relevant and/or just wants to type, they can do so; the completion
engine will not interfere.
When the user sees a useful completion string being offered, they press the TAB
key to accept it. This inserts the completion string. Repeated presses of the
TAB key cycle through the offered completions.
If the offered completions are not relevant enough, the user can continue typing
to further filter out unwanted completions.
A critical thing to notice is that the completion filtering is NOT based on
the input being a string prefix of the completion (but that works too). The
input needs to be a subsequence match of a completion. This is a fancy way
of saying that any input characters need to be present in a completion string in
the order in which they appear in the input. So abc is a subsequence of
xaybgc, but not of xbyxaxxc. After the filter, a complicated sorting system
ranks the completion strings so that the most relevant ones rise to the top of
the menu (so you usually need to press TAB just once).
All of the above works with any programming language because of the
identifier-based completion engine. It collects all of the identifiers in the
current file and other files you visit (and your tags files) and searches them
when you type (identifiers are put into per-filetype groups).
The demo also shows the semantic engine in use. When the user presses ., ->
or :: while typing in insert mode (for C++; different triggers are used for
other languages), the semantic engine is triggered (it can also be triggered
with a keyboard shortcut; see the rest of the docs).
The last thing that you can see in the demo is YCM's diagnostic display features
(the little red X that shows up in the left gutter; inspired by Syntastic)
if you are editing a C-family file. As the completer engine compiles your file
and detects warnings or errors, they will be presented in various ways. You
don't need to save your file or press any keyboard shortcut to trigger this, it
""just happens"" in the background.
In essence, YCM obsoletes the following Vim plugins because it has all of their
features plus extra:

clang_complete
AutoComplPop
Supertab
neocomplcache

And that's not all...
YCM also provides semantic IDE-like features in a
number of languages, including:

displaying signature help (argument hints) when entering the arguments to a
function call
finding declarations, definitions, usages, etc. of identifiers,
displaying type information for classes, variables, functions etc.,
displaying documentation for methods, members, etc. in the preview window,
fixing common coding errors, like missing semi-colons, typos, etc.,
semantic renaming of variables across files,
formatting code,
removing unused imports, sorting imports, etc.

For example, here's a demo of signature help:

Features vary by file type, so make sure to check out the file type feature
summary and the
full list of completer subcommands to
find out what's available for your favourite languages.
You'll also find that YCM has filepath completers (try typing ./ in a file)
and a completer that integrates with UltiSnips.
Installation
macOS
These instructions (using install.py) are the quickest way to install
YouCompleteMe, however they may not work for everyone. If the following
instructions don't work for you, check out the full installation
guide.
MacVim is required. YCM won't work with the pre-installed Vim from Apple as
its Python support is broken. If you don't already use MacVim, install it
with Homebrew. Install CMake as well:
brew install cmake macvim

Install YouCompleteMe with Vundle.
Remember: YCM is a plugin with a compiled component. If you update YCM
using Vundle and the ycm_core library APIs have changed (happens
rarely), YCM will notify you to recompile it. You should then rerun the install
process.
NOTE: If you want C-family completion, you MUST have the latest Xcode
installed along with the latest Command Line Tools (they are installed
automatically when you run clang for the first time, or manually by running
xcode-select --install)
Compiling YCM with semantic support for C-family languages through
libclang:
cd ~/.vim/bundle/YouCompleteMe
./install.py --clang-completer

Compiling YCM with semantic support for C-family languages through
clangd:
cd ~/.vim/bundle/YouCompleteMe
./install.py --clangd-completer

Note that you can install YCM with both libclang and clangd enabled. In
that case clangd will be preferred unless you have the following in your
vimrc:
let g:ycm_use_clangd = 0
Compiling YCM without semantic support for C-family languages:
cd ~/.vim/bundle/YouCompleteMe
./install.py

The following additional language support options are available:

C# support: install Mono with Homebrew or by downloading the Mono
macOS package and add --cs-completer when calling
install.py.
Go support: install Go and add --go-completer when calling
install.py.
JavaScript and TypeScript support: install Node.js and npm and
add --ts-completer when calling install.py.
Rust support: add --rust-completer when calling install.py.

If your Python interpreter is older than 2.7.9, you will also need
rustup in your PATH.


Java support: install JDK8 (version 8 required) and add
--java-completer when calling install.py.

To simply compile with everything enabled, there's a --all flag. Note that
this flag does not install clangd. You need to specify it manually by
adding --clangd-completer. So, to install with all language features, ensure
xbuild, go, tsserver, node and npm tools are
installed and in your PATH, then simply run:
cd ~/.vim/bundle/YouCompleteMe
./install.py --all

That's it. You're done. Refer to the User Guide section on how to use YCM.
Don't forget that if you want the C-family semantic completion engine to work,
you will need to provide the compilation flags for your project to YCM. It's all
in the User Guide.
YCM comes with sane defaults for its options, but you still may want to take a
look at what's available for configuration. There are a few interesting options
that are conservatively turned off by default that you may want to turn on.
Linux 64-bit
These instructions (using install.py) are the quickest way to install
YouCompleteMe, however they may not work for everyone. If the following
instructions don't work for you, check out the full installation
guide.
Make sure you have Vim 7.4.1578 with Python 2 or Python 3 support. The Vim
package on Fedora 27 and later and the pre-installed Vim on Ubuntu 16.04 and
later are recent enough. You can see the version of Vim installed by running
vim --version. If the version is too old, you may need to compile Vim from
source (don't worry, it's easy).
NOTE: For all features, such as signature help, use Vim 8.1.1875 or later.
Install YouCompleteMe with Vundle.
Remember: YCM is a plugin with a compiled component. If you update YCM
using Vundle and the ycm_core library APIs have changed (happens rarely), YCM
will notify you to recompile it. You should then rerun the install process.
Install development tools, CMake, and Python headers:


Fedora 27 and later:
sudo dnf install cmake gcc-c++ make python3-devel



Ubuntu 14.04:
sudo apt install build-essential cmake3 python3-dev



Ubuntu 16.04 and later:
sudo apt install build-essential cmake python3-dev



Compiling YCM with semantic support for C-family languages through
libclang:
cd ~/.vim/bundle/YouCompleteMe
python3 install.py --clang-completer

Compiling YCM with semantic support for C-family languages through
clangd:
cd ~/.vim/bundle/YouCompleteMe
python3 install.py --clangd-completer

Note that you can install YCM with both libclang and clangd enabled. In
that case clangd will be preferred unless you have the following in your
vimrc:
let g:ycm_use_clangd = 0
Compiling YCM without semantic support for C-family languages:
cd ~/.vim/bundle/YouCompleteMe
python3 install.py

The following additional language support options are available:

C# support: install Mono and add --cs-completer
when calling install.py.
Go support: install Go and add --go-completer when calling
install.py.
JavaScript and TypeScript support: install Node.js and npm and
add --ts-completer when calling install.py.
Rust support: add --rust-completer when calling install.py.

If your Python interpreter is older than 2.7.9, you will also need
rustup in your PATH.


Java support: install JDK8 (version 8 required) and add
--java-completer when calling install.py.

To simply compile with everything enabled, there's a --all flag. Note that
this flag does not install clangd. You need to specify it manually by
adding --clangd-completer. So, to install with all language features, ensure
xbuild, go, tsserver, node, npm and tools are
installed and in your PATH, then simply run:
cd ~/.vim/bundle/YouCompleteMe
python3 install.py --all

That's it. You're done. Refer to the User Guide section on how to use YCM.
Don't forget that if you want the C-family semantic completion engine to work,
you will need to provide the compilation flags for your project to YCM. It's all
in the User Guide.
YCM comes with sane defaults for its options, but you still may want to take a
look at what's available for configuration. There are a few interesting options
that are conservatively turned off by default that you may want to turn on.
Windows
These instructions (using install.py) are the quickest way to install
YouCompleteMe, however they may not work for everyone. If the following
instructions don't work for you, check out the full installation
guide.
Important: we assume that you are using the cmd.exe command prompt and
that you know how to add an executable to the PATH environment variable.
Make sure you have at least Vim 7.4.1578 with Python 2 or Python 3 support. You
can check the version and which Python is supported by typing :version inside
Vim. Look at the features included: +python/dyn for Python 2 and
+python3/dyn for Python 3. Take note of the Vim architecture, i.e. 32 or
64-bit. It will be important when choosing the Python installer. We recommend
using a 64-bit client. Daily updated installers of 32-bit and 64-bit Vim with
Python 2 and Python 3 support are available.
NOTE: For all features, such as signature help, use Vim 8.1.1875 or later.
Add the line:
set encoding=utf-8

to your vimrc if not already present. This option is required by YCM. Note
that it does not prevent you from editing a file in another encoding than UTF-8.
You can do that by specifying the ++enc argument to the :e command.
Install YouCompleteMe with Vundle.
Remember: YCM is a plugin with a compiled component. If you update YCM
using Vundle and the ycm_core library APIs have changed (happens
rarely), YCM will notify you to recompile it. You should then rerun the install
process.
Download and install the following software:

Python 2 or Python 3. Be sure to pick the version
corresponding to your Vim architecture. It is Windows x86 for a 32-bit Vim
and Windows x86-64 for a 64-bit Vim. We recommend installing Python 3.
Additionally, the version of Python you install must match up exactly with
the version of Python that Vim is looking for. Type :version and look at the
bottom of the page at the list of compiler flags. Look for flags that look
similar to -DDYNAMIC_PYTHON_DLL=\""python27.dll\"" and
-DDYNAMIC_PYTHON3_DLL=\""python35.dll\"". The former indicates that Vim is
looking for Python 2.7 and the latter indicates that Vim is looking for
Python 3.5. You'll need one or the other installed, matching the version
number exactly.
CMake. Add CMake executable to the PATH environment
variable.
Visual Studio Build Tools 2017. During setup,
select Visual C++ build tools in Workloads.

Compiling YCM with semantic support for C-family languages through
libclang:
cd %USERPROFILE%/vimfiles/bundle/YouCompleteMe
python install.py --clang-completer

Compiling YCM with semantic support for C-family languages through
clangd:
cd %USERPROFILE%/vimfiles/bundle/YouCompleteMe
python install.py --clangd-completer

Note that you can install YCM with both libclang and clangd enabled. In
that case clangd will be preferred unless you have the following in your
vimrc:
let g:ycm_use_clangd = 0
Compiling YCM without semantic support for C-family languages:
cd %USERPROFILE%/vimfiles/bundle/YouCompleteMe
python install.py

The following additional language support options are available:

C# support: add --cs-completer when calling install.py.
Be sure that the build utility msbuild is in your PATH.
Go support: install Go and add --go-completer when calling
install.py.
JavaScript and TypeScript support: install Node.js and npm and
add --ts-completer when calling install.py.
Rust support: add --rust-completer when calling install.py.

If your Python interpreter is older than 2.7.9, you will also need
rustup in your PATH.


Java support: install JDK8 (version 8 required) and add
--java-completer when calling install.py.

To simply compile with everything enabled, there's a --all flag. Note that
this flag does not install clangd. You need to specify it manually by
adding --clangd-completer. So, to install with all language features, ensure
msbuild, go, tsserver, node and npm tools are installed and
in your PATH, then simply run:
cd %USERPROFILE%/vimfiles/bundle/YouCompleteMe
python install.py --all

You can specify the Microsoft Visual C++ (MSVC) version using the --msvc
option. YCM officially supports MSVC 14 (Visual Studio 2015) and 15 (2017).
That's it. You're done. Refer to the User Guide section on how to use YCM.
Don't forget that if you want the C-family semantic completion engine to work,
you will need to provide the compilation flags for your project to YCM. It's all
in the User Guide.
YCM comes with sane defaults for its options, but you still may want to take a
look at what's available for configuration. There are a few interesting options
that are conservatively turned off by default that you may want to turn on.
FreeBSD/OpenBSD
These instructions (using install.py) are the quickest way to install
YouCompleteMe, however they may not work for everyone. If the following
instructions don't work for you, check out the full installation
guide.
NOTE: OpenBSD / FreeBSD are not officially supported platforms by YCM.
Make sure you have Vim 7.4.1578 with Python 2 or Python 3 support.
NOTE: For all features, such as signature help, use Vim 8.1.1875 or later.
OpenBSD 5.5 and later have a Vim that's recent enough. You can see the version of
Vim installed by running vim --version.
For FreeBSD 11.x, the requirement is cmake:
pkg install cmake

Install YouCompleteMe with Vundle.
Remember: YCM is a plugin with a compiled component. If you update YCM
using Vundle and the ycm_core library APIs have changed (happens
rarely), YCM will notify you to recompile it. You should then rerun the install
process.
Compiling YCM with semantic support for C-family languages through
libclang:
cd ~/.vim/bundle/YouCompleteMe
./install.py --clang-completer

Compiling YCM with semantic support for C-family languages through
clangd:
cd ~/.vim/bundle/YouCompleteMe
./install.py --clangd-completer

Note that you can install YCM with both libclang and clangd enabled. In
that case clangd will be preferred unless you have the following in your
vimrc:
let g:ycm_use_clangd = 0
Compiling YCM without semantic support for C-family languages:
cd ~/.vim/bundle/YouCompleteMe
./install.py

If the python executable is not present, or the default python is not the
one that should be compiled against, specify the python interpreter explicitly:
python3 install.py --clang-completer

The following additional language support options are available:

C# support: install Mono and add --cs-completer when calling
./install.py.
Go support: install Go and add --go-completer when calling
./install.py.
JavaScript and TypeScript support: install Node.js and npm and
add --ts-completer when calling install.py.
Rust support: add --rust-completer when calling ./install.py.

If your Python interpreter is older than 2.7.9, you will also need
rustup in your PATH.


Java support: install JDK8 (version 8 required) and add
--java-completer when calling ./install.py.

To simply compile with everything enabled, there's a --all flag. Note that
this flag does not install clangd. You need to specify it manually by
adding --clangd-completer. So, to install with all language features, ensure
xbuild, go, tsserver, node, npm and tools are
installed and in your PATH, then simply run:
cd ~/.vim/bundle/YouCompleteMe
./install.py --all

That's it. You're done. Refer to the User Guide section on how to use YCM.
Don't forget that if you want the C-family semantic completion engine to work,
you will need to provide the compilation flags for your project to YCM. It's all
in the User Guide.
YCM comes with sane defaults for its options, but you still may want to take a
look at what's available for configuration. There are a few interesting options
that are conservatively turned off by default that you may want to turn on.
Full Installation Guide
These are the steps necessary to get YCM working on a Unix OS and on Windows.
Note to Windows users: we assume that you are running the cmd.exe command
prompt and that the needed executables are in the PATH environment variable. Do
not just copy the shell commands. Replace ~ by %USERPROFILE% in them and use
the right Vim home directory. It should be vimfiles by default instead of
.vim.
See the FAQ if you have any issues.
Remember: YCM is a plugin with a compiled component. If you update YCM
using Vundle and the ycm_core library APIs have changed (happens
rarely), YCM will notify you to recompile it. You should then rerun the install
process.
Please follow the instructions carefully. Read EVERY WORD.


Ensure that your version of Vim is at least 7.4.1578 and that it has
support for Python 2 or Python 3 scripting.
Inside Vim, type :version. Look at the first two to three lines of output;
it should say Vi IMproved X.Y, where X.Y is the major version of vim. If
your version is greater than 7.4, then you're all set. If your version is
7.4 then look below that where it says, Included patches: 1-Z, where Z
will be some number. That number needs to be 1578 or higher.
If your version of Vim is not recent enough, you may need to compile Vim
from source (don't worry, it's easy).
After you have made sure that you have Vim 7.4.1578+, type the following in
Vim: :echo has('python') || has('python3'). The output should be 1. If
it's 0, then get a version of Vim with Python support.
NOTE: For all features, such as signature help, use Vim 8.1.1875 or
later.
On Windows, check also if your Vim architecture is 32 or 64-bit. This is
critical because it must match the Python and the YCM libraries
architectures. We recommend using a 64-bit Vim.


Install YCM with Vundle (or Pathogen, but Vundle is a better
idea). With Vundle, this would mean adding a Plugin 'Valloric/YouCompleteMe' line to your vimrc.
If you don't install YCM with Vundle, make sure you have run
git submodule update --init --recursive after checking out the YCM
repository (Vundle will do this for you) to fetch YCM's dependencies.


Complete this step ONLY if you care about semantic completion support for
C-family languages. Otherwise it's not necessary.
Download the latest version of libclang. Clang is an open-source
compiler that can compile C-family languages. The libclang library it
provides is used to power the YCM semantic completion engine for those
languages. YCM is designed to work with libclang version 9.0.0 or higher.
In addition to libclang, YCM also supports a clangd-based completer.
You can download the latest version of clangd from llvm.org
releases. Follow Step 4 to learn how to tell YCM where to
find clangd binary. Please note that YCM is designed to work with clangd
version 9.0.0 or higher.
You can use the system libclang or clangd only if you are sure it is
version 9.0.0 or higher, otherwise don't. Even if it is, we recommend using
the official binaries from llvm.org if at all possible.
Make sure you download the correct archive file for your OS.
We STRONGLY recommend AGAINST use of the system libclang or clangd
instead of the upstream compiled binaries. Random things may break. Save
yourself the hassle and use the upstream pre-built libclang or clangd.


Compile the ycm_core library that YCM needs. This library
is the C++ engine that YCM uses to get fast completions.
You will need to have cmake installed in order to generate the required
makefiles. Linux users can install cmake with their package manager (sudo apt-get install cmake for Ubuntu) whereas other users can download and
install cmake from its project site. macOS users can also
get it through Homebrew with brew install cmake.
On a Unix OS, you need to make sure you have Python headers installed. On a
Debian-like Linux distro, this would be sudo apt-get install python-dev python3-dev. On macOS they should already be present.
On Windows, you need to download and install Python 2 or
Python 3. Pick the version corresponding to your Vim
architecture. You will also need Microsoft Visual C++ (MSVC) to build YCM.
You can obtain it by installing Visual Studio Build
Tools. MSVC 14 (Visual Studio 2015) and 15 (2017)
are officially supported.
Here we'll assume you installed YCM with Vundle. That means that the
top-level YCM directory is in ~/.vim/bundle/YouCompleteMe.
We'll create a new folder where build files will be placed. Run the
following:
cd ~
mkdir ycm_build
cd ycm_build

Now we need to generate the makefiles. If you DON'T care about semantic
support for C-family languages or plan to use experimental clangd
based completer, run the following command in the ycm_build directory:
cmake -G ""<generator>"" . ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp

where <generator> is Unix Makefiles on Unix systems and one of the
following Visual Studio generators on Windows:

Visual Studio 14 Win64
Visual Studio 15 Win64

Remove the Win64 part in these generators if your Vim architecture is
32-bit.
For those who want to use the system version of boost, you would pass
-DUSE_SYSTEM_BOOST=ON to cmake. This may be necessary on some systems
where the bundled version of boost doesn't compile out of the box.
NOTE: We STRONGLY recommend AGAINST use of the system boost instead
of the bundled version of boost. Random things may break. Save yourself
the hassle and use the bundled version of boost.
If you DO care about semantic support for C-family languages, and want to
use libclang as the provider instead of experimental clangd-based
completer then your cmake call will be a bit more complicated. We'll
assume you downloaded a binary distribution of LLVM+Clang from llvm.org in
step 3 and that you extracted the archive file to folder
~/ycm_temp/llvm_root_dir (with bin, lib, include etc. folders right
inside that folder). On Windows, you can extract the files from the
LLVM+Clang installer using 7-zip.
NOTE: This only works with a downloaded LLVM binary package, not a
custom-built LLVM! See docs below for EXTERNAL_LIBCLANG_PATH when using a
custom LLVM build.
With that in mind, run the following command in the ycm_build directory:
cmake -G ""<generator>"" -DPATH_TO_LLVM_ROOT=~/ycm_temp/llvm_root_dir . ~/.vim/bundle/YouCompleteMe/third_party/ycmd/cpp

where <generator> is replaced like above.
Now that configuration files have been generated, compile the libraries
using this command:
cmake --build . --target ycm_core --config Release

The --config Release part is specific to Windows and will be ignored on a
Unix OS.
For those who want to use the system version of libclang, you would pass
-DUSE_SYSTEM_LIBCLANG=ON to cmake instead of the
-DPATH_TO_LLVM_ROOT=... flag.
NOTE: We STRONGLY recommend AGAINST use of the system libclang instead
of the upstream compiled binaries. Random things may break. Save yourself
the hassle and use the upstream pre-built libclang.
You could also force the use of a custom libclang library with
-DEXTERNAL_LIBCLANG_PATH=/path/to/libclang.so flag (the library would end
with .dylib on macOS). Again, this flag would be used instead of the
other flags. If you compiled LLVM from source, this is the flag you should
be using.
Running the cmake command will also place the libclang.[so|dylib|dll] in
the YouCompleteMe/third_party/ycmd folder for you if you compiled with
clang support (it needs to be there for YCM to work).
If you DO care about semantic support for C-family languages, and want to
use experimental clangd-based completer then you need to add
following line to your vimrc:
let g:ycm_clangd_binary_path = ""/path/to/clangd""
You need to change /path/to/clangd with the path of binary you downloaded
in step 3.


This step is optional.
Build the regex module for improved Unicode support and better
performance with regular expressions. The procedure is similar to compiling
the ycm_core library:
cd ~
mkdir regex_build
cd regex_build
cmake -G ""<generator>"" . ~/.vim/bundle/YouCompleteMe/third_party/ycmd/third_party/cregex
cmake --build . --target _regex --config Release

where <generator> is the same generator used in the previous step.


Set up support for additional languages, as desired:


C# support: install Mono on non-Windows platforms.
Navigate to YouCompleteMe/third_party/ycmd/third_party/omnisharp-roslyn.
Download an Omnisharp-Roslyn release archive and
extract the archive to
YouCompleteMe/third_party/ycmd/third_party/omnisharp-roslyn.
On Windows, be sure that the build utility msbuild is in your
PATH.


Go support: install Go and add it to your path. Navigate to
YouCompleteMe/third_party/ycmd/third_party/go/src/golang.org/x/tools/cmd/gopls
and run
go build



JavaScript and TypeScript support: install Node.js and npm,
navigate to YouCompleteMe/third_party/ycmd and run
npm install -g --prefix third_party/tsserver typescript.


Rust support: install rustup. Export RUSTUP_HOME environment
variable and point it to an empty temporary directory.
Run the following commands:
rustup toolchain install nightly
rustup default nightly
rustup component add rls rust-analysis rust-src

Ensure that YouCompleteMe/third_party/ycmd/third_party/rls directory
exists and is empty. Go into the temporary directory and then into
toolchains/<toolchain>. Finally, move everything from that directory to
YouCompleteMe/third_party/ycmd/third_party/rls.


Java support: install JDK8 (version 8 required). Download a
binary release of eclipse.jdt.ls and extract it to
YouCompleteMe/third_party/ycmd/third_party/eclipse.jdt.ls/target/repository.
Note: this approach is not recommended for most users and is supported
only for advanced users and developers of YCM on a best-efforts basis.
Please use install.py to enable java support.




That's it. You're done. Refer to the User Guide section on how to use YCM.
Don't forget that if you want the C-family semantic completion engine to work,
you will need to provide the compilation flags for your project to YCM. It's all
in the User Guide.
YCM comes with sane defaults for its options, but you still may want to take a
look at what's available for configuration. There are a few interesting options
that are conservatively turned off by default that you may want to turn on.
Quick Feature Summary
General (all languages)

Super-fast identifier completer including tags files and syntax elements
Intelligent suggestion ranking and filtering
File and path suggestions
Suggestions from Vim's OmniFunc
UltiSnips snippet suggestions

C-family languages (C, C++, Objective C, Objective C++, CUDA)

Semantic auto-completion with automatic fixes
Signature help (when using clangd)
Real-time diagnostic display
Go to include/declaration/definition (GoTo, etc.)
View documentation comments for identifiers (GetDoc)
Type information for identifiers (GetType)
Automatically fix certain errors (FixIt)
Reference finding (GoToReferences)
Renaming symbols (RefactorRename <new name>)
Code formatting (Format)

C‚ôØ

Semantic auto-completion
Real-time diagnostic display
Go to declaration/definition (GoTo, etc.)
Go to implementation (GoToImplementation)
View documentation comments for identifiers (GetDoc)
Type information for identifiers (GetType)
Automatically fix certain errors (FixIt)
Management of OmniSharp-Roslyn server instance

Python

Semantic auto-completion
Signature help
Go to definition (GoTo)
Reference finding (GoToReferences)
View documentation comments for identifiers (GetDoc)
Type information for identifiers (GetType)

Go

Semantic auto-completion
Signature help
Real-time diagnostic display
Go to declaration/definition (GoTo, etc.)
Go to type definition (GoToType)
Automatically fix certain errors (FixIt)
View documentation comments for identifiers (GetDoc)
Type information for identifiers (GetType)
Code formatting (Format)
Management of gopls server instance

JavaScript and TypeScript

Semantic auto-completion with automatic import insertion
Signature help
Real-time diagnostic display
Go to definition (GoTo, GoToDefinition, and GoToDeclaration are
identical)
Go to type definition (GoToType)
Reference finding (GoToReferences)
View documentation comments for identifiers (GetDoc)
Type information for identifiers (GetType)
Automatically fix certain errors (FixIt)
Renaming symbols (RefactorRename <new name>)
Code formatting (Format)
Organize imports (OrganizeImports)
Management of TSServer server instance

Rust

Semantic auto-completion
Real-time diagnostic display
Go to declaration/definition (GoTo, etc.)
Go to implementation (GoToImplementation)
Reference finding (GoToReferences)
View documentation comments for identifiers (GetDoc)
Automatically fix certain errors (FixIt)
Type information for identifiers (GetType)
Renaming symbols (RefactorRename <new name>)
Code formatting (Format)
Execute custom server command (ExecuteCommand <args>)
Management of rls server instance

Java

Semantic auto-completion with automatic import insertion
Signature help
Real-time diagnostic display
Go to definition (GoTo, GoToDefinition, and GoToDeclaration are
identical)
Go to type definition (GoToType)
Go to implementation (GoToImplementation)
Reference finding (GoToReferences)
View documentation comments for identifiers (GetDoc)
Type information for identifiers (GetType)
Automatically fix certain errors including code generation (FixIt)
Renaming symbols (RefactorRename <new name>)
Code formatting (Format)
Organize imports (OrganizeImports)
Detection of java projects
Execute custom server command (ExecuteCommand <args>)
Management of jdt.ls server instance

User Guide
General Usage
If the offered completions are too broad, keep typing characters; YCM will
continue refining the offered completions based on your input.
Filtering is ""smart-case"" and ""smart-diacritic"" sensitive; if you are
typing only lowercase letters, then it's case-insensitive. If your input
contains uppercase letters, then the uppercase letters in your query must
match uppercase letters in the completion strings (the lowercase letters still
match both). On top of that, a letter with no diacritic marks will match that
letter with or without marks:



matches
foo
f√¥o
fOo
f√îo


foo
‚úîÔ∏è
‚úîÔ∏è
‚úîÔ∏è
‚úîÔ∏è


f√¥o
‚ùå
‚úîÔ∏è
‚ùå
‚úîÔ∏è


fOo
‚ùå
‚ùå
‚úîÔ∏è
‚úîÔ∏è


f√îo
‚ùå
‚ùå
‚ùå
‚úîÔ∏è



Use the TAB key to accept a completion and continue pressing TAB to cycle
through the completions. Use Shift-TAB to cycle backwards. Note that if you're
using console Vim (that is, not Gvim or MacVim) then it's likely that the
Shift-TAB binding will not work because the console will not pass it to Vim.
You can remap the keys; see the Options section below.
Knowing a little bit about how YCM works internally will prevent confusion. YCM
has several completion engines: an identifier-based completer that collects all
of the identifiers in the current file and other files you visit (and your tags
files) and searches them when you type (identifiers are put into per-filetype
groups).
There are also several semantic engines in YCM. There are libclang-based and
clangd-based completers that provide semantic completion for C-family languages.
There's a Jedi-based completer for semantic completion for Python. There's also
an omnifunc-based completer that uses data from Vim's omnicomplete system to
provide semantic completions when no native completer exists for that language
in YCM.
There are also other completion engines, like the UltiSnips completer and the
filepath completer.
YCM automatically detects which completion engine would be the best in any
situation. On occasion, it queries several of them at once, merges the
outputs and presents the results to you.
Client-Server Architecture
YCM has a client-server architecture; the Vim part of YCM is only a thin client
that talks to the ycmd HTTP+JSON server that has the vast majority of
YCM logic and functionality. The server is started and stopped automatically as
you start and stop Vim.
Completion String Ranking
The subsequence filter removes any completions that do not match the input, but
then the sorting system kicks in. It's actually very complicated and uses lots
of factors, but suffice it to say that ""word boundary"" (WB) subsequence
character matches are ""worth"" more than non-WB matches. In effect, this means
given an input of ""gua"", the completion ""getUserAccount"" would be ranked higher
in the list than the ""Fooguxa"" completion (both of which are subsequence
matches). A word-boundary character are all capital characters, characters
preceded by an underscore and the first letter character in the completion
string.
Signature Help
Signature help is an experimental feature for which we value your feedback.
Valid signatures are displayed in a second popup menu and the current signature
is highlighed along with the current arguemnt.
Signature help is triggered in insert mode automatically when
g:ycm_auto_trigger is enabled and is not supported when it is not enabled.
The signatures popup is hidden when there are no matching signatures or when you
leave insert mode. There is no key binding to clear the popup.
For more details on this feature and a few demos, check out the
PR that proposed it.
General Semantic Completion
You can use Ctrl+Space to trigger the completion suggestions anywhere, even
without a string prefix. This is useful to see which top-level functions are
available for use.
C-family Semantic Completion
In order to perform semantic analysis such as code completion, GoTo and
diagnostics, YouCompleteMe uses libclang or clangd. Both of them make use of
clang compiler, sometimes also referred to as llvm. Like any compiler,
clang also requires a set of compile flags in order to parse your code. Simply
put: If clang can't parse your code, YouCompleteMe can't provide semantic
analysis.
There are 2 methods which can be used to provide compile flags to clang:
Option 1: Use a compilation database
The easiest way to get YCM to compile your code is to use a compilation
database.  A compilation database is usually generated by your build system
(e.g. CMake) and contains the compiler invocation for each compilation unit in
your project.
For information on how to generate a compilation database, see the clang
documentation. In short:

If using CMake, add -DCMAKE_EXPORT_COMPILE_COMMANDS=ON when configuring (or
add set( CMAKE_EXPORT_COMPILE_COMMANDS ON ) to CMakeLists.txt) and copy or
symlink the generated database to the root of your project.
If using Ninja, check out the compdb tool (-t compdb) in its
docs.
If using GNU make, check out compiledb or Bear.
For other build systems, check out
.ycm_extra_conf.py below.

If no .ycm_extra_conf.py is found,
YouCompleteMe automatically tries to load a compilation database if there is
one.
YCM looks for a file named compile_commands.json in the directory of the
opened file or in any directory above it in the hierarchy (recursively); when
the file is found, it is loaded.  YouCompleteMe performs the following lookups
when extracting flags for a particular file:

If the database contains an entry for the file, the flags for that file are
used.
If the file is a header file and a source file with the same root exists in
the database, the flags for the source file are used. For example, if the file
is /home/Test/project/src/lib/something.h and the database contains an entry
for /home/Test/project/src/lib/something.cc, then the flags for
/home/Test/project/src/lib/something.cc are used.
Otherwise, if any flags have been returned from the directory containing the
requested file, those flags are used. This heuristic is intended to provide
potentially working flags for newly created files.

Finally, YCM converts any relative paths in the extracted flags to absolute
paths. This ensures that compilation can be performed from any Vim working
directory.
Option 2: Provide the flags manually
If you don't have a compilation database, or aren't able to generate one,
you have to tell YouCompleteMe how to compile your code some other way.
Every C-family project is different. It is not possible for YCM to guess what
compiler flags to supply for your project. Fortunately, YCM provides a mechanism
for you to generate the flags for a particular file with arbitrary complexity.
This is achieved by requiring you to provide a Python module which implements a
trivial function which, given the file name as argument, returns a list of
compiler flags to use to compile that file.
YCM looks for a .ycm_extra_conf.py file in the directory of the opened file or
in any directory above it in the hierarchy (recursively); when the file is
found, it is loaded (only once!) as a Python module. YCM calls a Settings
method in that module which should provide it with the information necessary to
compile the current file. You can also provide a path to a global configuration
file with the
g:ycm_global_ycm_extra_conf option,
which will be used as a fallback. To prevent the execution of malicious code
from a file you didn't write YCM will ask you once per .ycm_extra_conf.py if
it is safe to load. This can be disabled and you can white-/blacklist files. See
the g:ycm_confirm_extra_conf and
g:ycm_extra_conf_globlist options
respectively.
This system was designed this way so that the user can perform any arbitrary
sequence of operations to produce a list of compilation flags YCM should hand
to Clang.
NOTE: It is highly recommended to include -x <language> flag to libclang.
This is so that the correct language is detected, particularly for header files.
Common values are -x c for C, -x c++ for C++, -x objc for Objective-C, and
-x cuda for CUDA.
To give you an impression, if your C++ project is trivial, and your usual
compilation command is: g++ -Wall -Wextra -Werror -o FILE.o FILE.cc, then the
following .ycm_extra_conf.py is enough to get semantic analysis from
YouCompleteMe:
def Settings( **kwargs ):
  return {
    'flags': [ '-x', 'c++', '-Wall', '-Wextra', '-Werror' ],
  }
As you can see from the trivial example, YCM calls the Settings method which
returns a dictionary with a single element 'flags'. This element is a list
of compiler flags to pass to libclang for the current file. The absolute path of
that file is accessible under the filename key of the kwargs dictionary.
That's it! This is actually enough for most projects, but for complex projects
it is not uncommon to integrate directly with an existing build system using the
full power of the Python language.
For a more elaborate example,
see ycmd's own .ycm_extra_conf.py. You should be able to
use it as a starting point. Don't just copy/paste that file somewhere and
expect things to magically work; your project needs different flags. Hint:
just replace the strings in the flags variable with compilation flags
necessary for your project. That should be enough for 99% of projects.
You could also consider using YCM-Generator to generate the
ycm_extra_conf.py file.
Errors during compilation
If Clang encounters errors when compiling the header files that your file
includes, then it's probably going to take a long time to get completions.  When
the completion menu finally appears, it's going to have a large number of
unrelated completion strings (type/function names that are not actually
members). This is because Clang fails to build a precompiled preamble for your
file if there are any errors in the included headers and that preamble is key to
getting fast completions.
Call the :YcmDiags command to see if any errors or warnings were detected in
your file.
Selecting a C-family completion engine
Currently YCM supports two completion engines for C-family semantic completion.
One libclang-based and an clangd-based completer. When in doubt we recommend
using the libclang-based engine. Here is a quick comparison of the two completer
engines:

Project wide indexing: Clangd has both dynamic and static index support.
The dynamic index stores up-to-date symbols coming from any files you are
currently editing, whereas static index contains project-wide symbol
information. This symbol information is used for code completion and code
navigation. Whereas libclang is limited to the current translation unit(TU).
Code navigation: Clangd provides all the GoTo requests libclang provides and it
improves those using the above mentioned index information to contain
project-wide information rather than just the current TU.
Rename: Clangd can perform semantic rename operations on the current
file, whereas libclang doesn‚Äôt support such functionality.
Code Completion: Clangd can perform code completions at a lower latency
than libclang; also, it has information about all the symbols in your
project so it can suggest items outside your current TU and also provides
proper #include insertions for those items.
Signature help: Clangd provides signature help so that you can see the
names and types of arguments when calling functions.
Format Code: Clangd provides code formatting either for the selected
lines or the whole file, whereas libclang doesn‚Äôt have such functionality.
Performance: Clangd has faster reparse and code completion times
compared to libclang.

To enable:

libclang-based completer pass --clang-completer
clangd-based completer pass --clangd-completer

to install.py while following the installation guide. As
mentioned before, pass --clang-completer when in doubt, since the
clangd-based completer is still in heavy development.
Java Semantic Completion
Java quick Start


Ensure that you have enabled the Java completer. See the
installation guide for details.


Create a project file (gradle or maven) file in the root directory of your
Java project, by following the instructions below.


(Optional) Configure the LSP server. The jdt.ls
configuration options can be found in their codebase.


If you previously used Eclim or Syntastic for Java, disable them for Java.


Edit a Java file from your project.


For the best experience, we highly recommend at least Vim 8.1.1875 when using
Java support with YouCompleteMe.
Java Project Files
In order to provide semantic analysis, the Java completion engine requires
knowledge of your project structure. In particular it needs to know the class
path to use, when compiling your code. Fortunately jdt.ls
supports eclipse project files,
maven projects and gradle projects.
NOTE: Our recommendation is to use either maven or gradle projects.
Diagnostic display - Syntastic
The native support for Java includes YCM's native realtime diagnostics display.
This can conflict with other diagnostics plugins like Syntastic, so when
enabling Java support, please manually disable Syntastic Java diagnostics.
Add the following to your vimrc:
let g:syntastic_java_checkers = []
Diagnostic display - Eclim
The native support for Java includes YCM's native realtime diagnostics display.
This can conflict with other diagnostics plugins like Eclim, so when enabling
Java support, please manually disable Eclim Java diagnostics.
Add the following to your vimrc:
let g:EclimFileTypeValidate = 0
NOTE: We recommend disabling Eclim entirely when editing Java with YCM's
native Java support. This can be done temporarily with :EclimDisable.
Eclipse Projects
Eclipse style projects require two files: .project and
.classpath.
If your project already has these files due to previously being set up within
eclipse, then no setup is required. jdt.ls should load the project just
fine (it's basically eclipse after all).
However, if not, it is possible (easy in fact) to craft them manually, though it
is not recommended. You're better off using gradle or maven (see below).
A simple eclipse style project example can be found in
the ycmd test directory. Normally all that is required is to copy these files to
the root of your project and to edit the .classpath to add additional
libraries, such as:
  <classpathentry kind=""lib"" path=""/path/to/external/jar"" />
  <classpathentry kind=""lib"" path=""/path/to/external/java/source"" />
It may also be necessary to change the directory in which your source files are
located (paths are relative to the .project file itself):
  <classpathentry kind=""src"" output=""target/classes"" path=""path/to/src/"" />
NOTE: The eclipse project and classpath files are not a public interface
and it is highly recommended to use Maven or Gradle project definitions if you
don't already use eclipse to manage your projects.
Maven Projects
Maven needs a file named pom.xml in the root of the project.
Once again a simple pom.xml can be found in ycmd source.
The format of pom.xml files is way beyond the scope of this
document, but we do recommend using the various tools that can generate them for
you, if you're not familiar with them already.
Gradle Projects
Gradle projects require a build.gradle. Again, there is a
trivial example in ycmd's tests.
The format of build.gradle files is way beyond the scope of
this document, but we do recommend using the various tools that can generate
them for you, if you're not familiar with them already.
Troubleshooting
If you're not getting completions or diagnostics, check the server health:

The Java completion engine takes a while to start up and parse your project.
You should be able to see its progress in the command line, and
:YcmDebugInfo. Ensure that the following lines are present:

--   jdt.ls Java Language Server running
--   jdt.ls Java Language Server Startup Status: Ready


If the above lines don't appear after a few minutes, check the jdt.ls and ycmd
log files using :YcmToggleLogs . The jdt.ls
log file is called .log (for some reason).

If you get a message about ""classpath is incomplete"", then make sure you have
correctly configured the project files.
If you get messages about unresolved imports, then make sure you have
correctly configured the project files, in particular
check that the classpath is set correctly.
For anything else, contact us. Java support is experimental at
present so we'd love to hear your feedback! Please do remember to check
CONTRIBUTING.md for the list of diagnostics we'll need.
C# Semantic Completion
YCM relies on OmniSharp-Roslyn to provide completion and code navigation.
OmniSharp-Roslyn needs a solution file for a C# project and there are two ways
of letting YCM know about your solution files.
Automaticly discovered solution files
YCM will scan all parent directories of the file currently being edited and look
for file with .sln extension.
Manually specified solution files
If YCM loads .ycm_extra_conf.py which contains CSharpSolutionFile function,
YCM will try to use that to determine the solution file. This is useful when one
wants to override the default behaviour and specify a solution file that is not
in any of the parent directories of the currently edited file. Example:
def CSharpSolutionFile( filepath ):
  # `filepath` is the path of the file user is editing
  return '/path/to/solution/file' # Can be relative to the `.ycm_extra_conf.py`
If the path returned by CSharpSolutionFile is not an actual file, YCM will
fall back to the other way of finding the file.
Python Semantic Completion
YCM relies on the Jedi engine to provide completion and code navigation. By
default, it will pick the version of Python running the ycmd server and
use its sys.path. While this is fine for simple projects, this needs to be
configurable when working with virtual environments or in a project with
third-party packages. The next sections explain how to do that.
Working with virtual environments
A common practice when working on a Python project is to install its
dependencies in a virtual environment and develop the project inside that
environment. To support this, YCM needs to know the interpreter path of the
virtual environment. You can specify it by creating a .ycm_extra_conf.py file
at the root of your project with the following contents:
def Settings( **kwargs ):
  return {
    'interpreter_path': '/path/to/virtual/environment/python'
  }
where /path/to/virtual/environment/python is the path to the Python used
by the virtual environment you are working in. Typically, the executable can be
found in the Scripts folder of the virtual environment directory on Windows
and in the bin folder on other platforms.
If you don't like having to create a .ycm_extra_conf.py file at the root of
your project and would prefer to specify the interpreter path with a Vim option,
read the Configuring through Vim options
section.
Working with third-party packages
Another common practice is to put the dependencies directly into the project and
add their paths to sys.path at runtime in order to import them. YCM needs to
be told about this path manipulation to support those dependencies. This can be
done by creating a .ycm_extra_conf.py file at the root of the project. This
file must define a Settings( **kwargs ) function returning a dictionary with
the list of paths to prepend to sys.path under the sys_path key. For
instance, the following .ycm_extra_conf.py
def Settings( **kwargs ):
  return {
    'sys_path': [
      '/path/to/some/third_party/package',
      '/path/to/another/third_party/package'
    ]
  }
adds the paths /path/to/some/third_party/package and
/path/to/another/third_party/package at the start of sys.path.
If you would rather prepend paths to sys.path with a Vim option, read the
Configuring through Vim options section.
If you need further control on how to add paths to sys.path, you should define
the PythonSysPath( **kwargs ) function in the .ycm_extra_conf.py file. Its
keyword arguments are sys_path which contains the default sys.path, and
interpreter_path which is the path to the Python interpreter. Here's a trivial
example that insert the /path/to/third_party/package path at the second
position of sys.path:
def PythonSysPath( **kwargs ):
  sys_path = kwargs[ 'sys_path' ]
  sys_path.insert( 1, '/path/to/third_party/package' )
  return sys_path
A more advanced example can be found in YCM's own
.ycm_extra_conf.py.
Configuring through Vim options
You may find inconvenient to have to create a .ycm_extra_conf.py file at the
root of each one of your projects in order to set the path to the Python
interpreter and/or add paths to sys.path and would prefer to be able to
configure those through Vim options. Don't worry, this is possible by using the
g:ycm_extra_conf_vim_data option and
creating a global extra configuration file. Let's take an example. Suppose that
you want to set the interpreter path with the g:ycm_python_interpreter_path
option and prepend paths to sys.path with the g:ycm_python_sys_path option.
Suppose also that you want to name the global extra configuration file
global_extra_conf.py and that you want to put it in your HOME folder. You
should then add the following lines to your vimrc:
let g:ycm_python_interpreter_path = ''
let g:ycm_python_sys_path = []
let g:ycm_extra_conf_vim_data = [
  \  'g:ycm_python_interpreter_path',
  \  'g:ycm_python_sys_path'
  \]
let g:ycm_global_ycm_extra_conf = '~/global_extra_conf.py'
and create the ~/global_extra_conf.py file with the following contents:
def Settings( **kwargs ):
  client_data = kwargs[ 'client_data' ]
  return {
    'interpreter_path': client_data[ 'g:ycm_python_interpreter_path' ],
    'sys_path': client_data[ 'g:ycm_python_sys_path' ]
  }
That's it. You are done. Note that you don't need to restart the server when
setting one of the options. YCM will automatically pick the new values.
Rust Semantic Completion
Completions and GoTo commands within the current crate and its dependencies
should work out of the box with no additional configuration (provided that you
built YCM with the --rust-completer flag; see the Installation
section for details). The install script takes care of
installing the Rust source code, so no configuration is necessary.
In case you are running Python 2.7.8 and older, you will need to manually
install rustup.
To configure RLS look up rls configuration options
Go Semantic Completion
Completions and GoTo commands should work out of the box (provided that you
built YCM with the --go-completer flag; see the Installation
section for details). The server only works for projects with
the ""canonical"" layout.
While YCM can configure a LSP server, currently gopls
doesn't implement the required notification.
JavaScript and TypeScript Semantic Completion
NOTE: YCM originally used the Tern engine for JavaScript but due to
Tern not being maintained anymore by its main author and the TSServer
engine offering more features, YCM is moving to TSServer. This won't affect
you if you were already using Tern but you are encouraged to do the switch
by deleting the third_party/ycmd/third_party/tern_runtime/node_modules
directory in YCM folder. If you are a new user but still want to use Tern,
you should pass the --js-completer option to the install.py script during
installation. Further instructions on how to setup YCM with Tern are
available on the wiki.
All JavaScript and TypeScript features are provided by the TSServer engine,
which is included in the TypeScript SDK. To enable these features, install
Node.js and npm and call the install.py script with the
--ts-completer flag.
TSServer relies on the jsconfig.json file for JavaScript
and the tsconfig.json file for TypeScript to analyze your
project. Ensure the file exists at the root of your project.
To get diagnostics in JavaScript, set the checkJs option to true in your
jsconfig.json file:
{
    ""compilerOptions"": {
        ""checkJs"": true
    }
}
Semantic Completion for Other Languages
C-family, C#, Go, Java, Python, Rust, and JavaScript/TypeScript languages are
supported natively by YouCompleteMe using the Clang, OmniSharp-Roslyn,
Gopls, jdt.ls, Jedi, rls, and TSServer engines,
respectively. Check the installation section for instructions
to enable these features if desired.
Plugging an arbitrary LSP server
Similar to other LSP clients, YCM can use an arbitrary LSP server with the help
of g:ycm_language_server option. An
example of a value of this option would be:
let g:ycm_language_server = 
  \ [ 
  \   {
  \     'name': 'yaml',
  \     'cmdline': [ '/path/to/yaml/server/yaml-language-server', '--stdio' ],
  \     'filetypes': [ 'yaml' ]
  \   },
  \   {
  \     'name': 'rust',
  \     'cmdline': [ 'ra_lsp_server' ],
  \     'filetypes': [ 'rust' ],
  \     'project_root_files': [ 'Cargo.toml' ]
  \   }
  \ ]
project_root_files is an optional key, since not all servers need it.
When configuring a LSP server the value of the name key
will be used as the kwargs[ 'language' ].
See the LSP Examples project for more
examples of configuring the likes of PHP, Ruby, Kotlin, and D.
Using omnifunc for semantic completion
YCM will use your omnifunc (see :h omnifunc in Vim) as a source for semantic
completions if it does not have a native semantic completion engine for your
file's filetype. Vim comes with okayish omnifuncs for various languages like
Ruby, PHP, etc. It depends on the language.
You can get a stellar omnifunc for Ruby with Eclim. Just make sure you have
the latest Eclim installed and configured (this means Eclim >= 2.2.* and
Eclipse >= 4.2.*).
After installing Eclim remember to create a new Eclipse project within your
application by typing :ProjectCreate <path-to-your-project> -n ruby inside vim
and don't forget to have let g:EclimCompletionMethod = 'omnifunc' in your
vimrc. This will make YCM and Eclim play nice; YCM will use Eclim's omnifuncs as
the data source for semantic completions and provide the auto-triggering and
subsequence-based matching (and other YCM features) on top of it.
LSP Configuration
Many LSP servers allow some level of user configuration. YCM enables this with
the help of .ycm_extra_conf.py files. Here's an example of jdt.ls user
configuration.
def Settings( **kwargs ):
  if kwargs[ 'language' ] == 'java':
    return { 'ls': { 'java.format.onType.enabled': True } }
The ls key tells YCM that the dictionary should be passed to thet LSP server.
For each of the LSP server's configuration you should look up the respective
server's documentation.
Writing New Semantic Completers
You have two options here: writing an omnifunc for Vim's omnicomplete system
that YCM will then use through its omni-completer, or a custom completer for YCM
using the Completer API.
Here are the differences between the two approaches:

You have to use VimScript to write the omnifunc, but get to use Python to
write for the Completer API; this by itself should make you want to use the
API.
The Completer API is a much more powerful way to integrate with YCM and it
provides a wider set of features. For instance, you can make your Completer
query your semantic back-end in an asynchronous fashion, thus not blocking
Vim's GUI thread while your completion system is processing stuff. This is
impossible with VimScript. All of YCM's completers use the Completer API.
Performance with the Completer API is better since Python executes faster than
VimScript.

If you want to use the omnifunc system, see the relevant Vim docs with :h complete-functions. For the Completer API, see the API docs.
If you want to upstream your completer into YCM's source, you should use the
Completer API.
Diagnostic Display
YCM will display diagnostic notifications for the C-family, C#, Go, Java,
JavaScript, Rust and TypeScript languages. Since YCM continuously recompiles
your file as you type, you'll get notified of errors and warnings in your file
as fast as possible.
Here are the various pieces of the diagnostic UI:

Icons show up in the Vim gutter on lines that have a diagnostic.
Regions of text related to diagnostics are highlighted (by default, a red
wavy underline in gvim and a red background in vim).
Moving the cursor to a line with a diagnostic echoes the diagnostic text.
Vim's location list is automatically populated with diagnostic data (off by
default, see options).

The new diagnostics (if any) will be displayed the next time you press any key
on the keyboard. So if you stop typing and just wait for the new diagnostics to
come in, that will not work. You need to press some key for the GUI to update.
Having to press a key to get the updates is unfortunate, but cannot be changed
due to the way Vim internals operate; there is no way that a background task can
update Vim's GUI after it has finished running.  You have to press a key. This
will make YCM check for any pending diagnostics updates.
You can force a full, blocking compilation cycle with the
:YcmForceCompileAndDiagnostics command (you may want to map that command to a
key; try putting nnoremap <F5> :YcmForceCompileAndDiagnostics<CR> in your
vimrc). Calling this command will force YCM to immediately recompile your file
and display any new diagnostics it encounters. Do note that recompilation with
this command may take a while and during this time the Vim GUI will be
blocked.
YCM will display a short diagnostic message when you move your cursor to the
line with the error. You can get a detailed diagnostic message with the
<leader>d key mapping (can be changed in the options) YCM provides when your
cursor is on the line with the diagnostic.
You can also see the full diagnostic message for all the diagnostics in the
current file in Vim's locationlist, which can be opened with the :lopen and
:lclose commands (make sure you have set let g:ycm_always_populate_location_list = 1 in your vimrc). A good way to toggle
the display of the locationlist with a single key mapping is provided by
another (very small) Vim plugin called ListToggle (which also makes it
possible to change the height of the locationlist window), also written by
yours truly.
Diagnostic Highlighting Groups
You can change the styling for the highlighting groups YCM uses. For the signs
in the Vim gutter, the relevant groups are:

YcmErrorSign, which falls back to group SyntasticErrorSign and then
error if they exist
YcmWarningSign, which falls back to group SyntasticWarningSign and then
todo if they exist

You can also style the line that has the warning/error with these groups:

YcmErrorLine, which falls back to group SyntasticErrorLine if it exists
YcmWarningLine, which falls back to group SyntasticWarningLine if it
exists

Note that the line highlighting groups only work when the
g:ycm_enable_diagnostic_signs
option is set. If you want highlighted lines but no signs in the Vim gutter,
ensure that your Vim version is 7.4.2201 or later and set the signcolumn
option to off in your vimrc:
set signcolumn=off
The syntax groups used to highlight regions of text with errors/warnings:

YcmErrorSection, which falls back to group SyntasticError if it exists and
then SpellBad
YcmWarningSection, which falls back to group SyntasticWarning if it exists
and then SpellCap

Here's how you'd change the style for a group:
highlight YcmErrorLine guibg=#3f0000
Commands
The :YcmRestartServer command
If the ycmd completion server suddenly stops for some reason, you can
restart it with this command.
The :YcmForceCompileAndDiagnostics command
Calling this command will force YCM to immediately recompile your file
and display any new diagnostics it encounters. Do note that recompilation with
this command may take a while and during this time the Vim GUI will be
blocked.
You may want to map this command to a key; try putting nnoremap <F5> :YcmForceCompileAndDiagnostics<CR> in your vimrc.
The :YcmDiags command
Calling this command will fill Vim's locationlist with errors or warnings if
any were detected in your file and then open it. If a given error or warning can
be fixed by a call to :YcmCompleter FixIt, then  (FixIt available) is
appended to the error or warning text. See the FixIt completer subcommand for
more information.
NOTE: The absence of  (FixIt available) does not strictly imply a fix-it
is not available as not all completers are able to provide this indication. For
example, the c-sharp completer provides many fix-its but does not add this
additional indication.
The g:ycm_open_loclist_on_ycm_diags option can be used to prevent the location
list from opening, but still have it filled with new diagnostic data. See the
Options section for details.
The :YcmShowDetailedDiagnostic command
This command shows the full diagnostic text when the user's cursor is on the
line with the diagnostic.
The :YcmDebugInfo command
This will print out various debug information for the current file. Useful to
see what compile commands will be used for the file if you're using the semantic
completion engine.
The :YcmToggleLogs command
This command presents the list of logfiles created by YCM, the ycmd
server, and the semantic engine server for the current filetype, if any.
One of these logfiles can be opened in the editor (or closed if already open) by
entering the corresponding number or by clicking on it with the mouse.
Additionally, this command can take the logfile names as arguments. Use the
<TAB> key (or any other key defined by the wildchar option) to complete the
arguments or to cycle through them (depending on the value of the wildmode
option). Each logfile given as an argument is directly opened (or closed if
already open) in the editor. Only for debugging purposes.
The :YcmCompleter command
This command gives access to a number of additional IDE-like
features in YCM, for things like semantic GoTo, type
information, FixIt and refactoring.
This command accepts a range that can either be specified through a selection in
one of Vim's visual modes (see :h visual-use) or on the command line. For
instance, :2,5YcmCompleter will apply the command from line 2 to line 5. This
is useful for the Format subcommand.
Call YcmCompleter without further arguments for a list of the commands you can
call for the current completer.
See the file type feature summary for an overview of
the features available for each file type. See the YcmCompleter subcommands
section for more information on the available subcommands and their usage.
YcmCompleter Subcommands
NOTE: See the docs for the YcmCompleter command before tackling this
section.
The invoked subcommand is automatically routed to the currently active semantic
completer, so :YcmCompleter GoToDefinition will invoke the GoToDefinition
subcommand on the Python semantic completer if the currently active file is a
Python one and on the Clang completer if the currently active file is a C-family
language one.
You may also want to map the subcommands to something less verbose; for
instance, nnoremap <leader>jd :YcmCompleter GoTo<CR>
maps the <leader>jd sequence to the longer subcommand invocation.
GoTo Commands
These commands are useful for jumping around and exploring code. When moving
the cursor, the subcommands add entries to Vim's jumplist so you can use
CTRL-O to jump back to where you were before invoking the command (and
CTRL-I to jump forward; see :h jumplist for details). If there is more
than one destination, the quickfix list (see :h quickfix) is populated with
the available locations and opened to full width at the bottom of the screen.
You can change this behavior by using the YcmQuickFixOpened
autocommand.
The GoToInclude subcommand
Looks up the current line for a header and jumps to it.
Supported in filetypes: c, cpp, objc, objcpp, cuda
The GoToDeclaration subcommand
Looks up the symbol under the cursor and jumps to its declaration.
Supported in filetypes: c, cpp, objc, objcpp, cuda, cs, go, java, javascript, python, rust, typescript
The GoToDefinition subcommand
Looks up the symbol under the cursor and jumps to its definition.
NOTE: For C-family languages this only works in certain situations,
namely when the definition of the symbol is in the current translation unit. A
translation unit consists of the file you are editing and all the files you are
including with #include directives (directly or indirectly) in that file.
Supported in filetypes: c, cpp, objc, objcpp, cuda, cs, go, java, javascript, python, rust, typescript
The GoTo subcommand
This command tries to perform the ""most sensible"" GoTo operation it can.
Currently, this means that it tries to look up the symbol under the cursor and
jumps to its definition if possible; if the definition is not accessible from
the current translation unit, jumps to the symbol's declaration. For
C-family languages, it first tries to look up the current line for a header and
jump to it. For C#, implementations are also considered and preferred.
Supported in filetypes: c, cpp, objc, objcpp, cuda, cs, go, java, javascript, python, rust, typescript
The GoToImprecise subcommand
WARNING: This command trades correctness for speed!
Same as the GoTo command except that it doesn't recompile the file with
libclang before looking up nodes in the AST. This can be very useful when you're
editing files that take long to compile but you know that you haven't made any
changes since the last parse that would lead to incorrect jumps. When you're
just browsing around your codebase, this command can spare you quite a bit of
latency.
Supported in filetypes: c, cpp, objc, objcpp, cuda
The GoToReferences subcommand
This command attempts to find all of the references within the project to the
identifier under the cursor and populates the quickfix list with those
locations.
Supported in filetypes: c, cpp, objc, objcpp, cuda, java, javascript, python, typescript, rust
The GoToImplementation subcommand
Looks up the symbol under the cursor and jumps to its implementation (i.e.
non-interface). If there are multiple implementations, instead provides a list
of implementations to choose from.
Supported in filetypes: cs, java, rust
The GoToImplementationElseDeclaration subcommand
Looks up the symbol under the cursor and jumps to its implementation if one,
else jump to its declaration. If there are multiple implementations, instead
provides a list of implementations to choose from.
Supported in filetypes: cs
The GoToType subcommand
Looks up the symbol under the cursor and jumps to the definition of its type
e.g. if the symbol is an object, go to the definition of its class.
Supported in filetypes: go, java, javascript, typescript
Semantic Information Commands
These commands are useful for finding static information about the code, such
as the types of variables, viewing declarations and documentation strings.
The GetType subcommand
Echos the type of the variable or method under the cursor, and where it differs,
the derived type.
For example:
    std::string s;
Invoking this command on s returns std::string => std::basic_string<char>
NOTE: Causes re-parsing of the current translation unit.
Supported in filetypes: c, cpp, objc, objcpp, cuda, java, javascript, go, python, typescript, rust
The GetTypeImprecise subcommand
WARNING: This command trades correctness for speed!
Same as the GetType command except that it doesn't recompile the file with
libclang before looking up nodes in the AST. This can be very useful when you're
editing files that take long to compile but you know that you haven't made any
changes since the last parse that would lead to incorrect type. When you're
just browsing around your codebase, this command can spare you quite a bit of
latency.
Supported in filetypes: c, cpp, objc, objcpp, cuda
The GetParent subcommand
Echos the semantic parent of the point under the cursor.
The semantic parent is the item that semantically contains the given position.
For example:
class C {
    void f();
};

void C::f() {

}
In the out-of-line definition of C::f, the semantic parent is the class C,
of which this function is a member.
In the example above, both declarations of C::f have C as their semantic
context, while the lexical context of the first C::f is C and the lexical
context of the second C::f is the translation unit.
For global declarations, the semantic parent is the translation unit.
NOTE: Causes re-parsing of the current translation unit.
Supported in filetypes: c, cpp, objc, objcpp, cuda
The GetDoc subcommand
Displays the preview window populated with quick info about the identifier
under the cursor. Depending on the file type, this includes things like:

The type or declaration of identifier,
Doxygen/javadoc comments,
Python docstrings,
etc.

Supported in filetypes: c, cpp, objc, objcpp, cuda, cs, go, java, javascript, python, typescript, rust
The GetDocImprecise subcommand
WARNING: This command trades correctness for speed!
Same as the GetDoc command except that it doesn't recompile the file with
libclang before looking up nodes in the AST. This can be very useful when you're
editing files that take long to compile but you know that you haven't made any
changes since the last parse that would lead to incorrect docs. When you're
just browsing around your codebase, this command can spare you quite a bit of
latency.
Supported in filetypes: c, cpp, objc, objcpp, cuda
Refactoring Commands
These commands make changes to your source code in order to perform refactoring
or code correction. YouCompleteMe does not perform any action which cannot be
undone, and never saves or writes files to the disk.
The FixIt subcommand
Where available, attempts to make changes to the buffer to correct diagnostics
on the current line. Where multiple suggestions are available (such as when
there are multiple ways to resolve a given warning, or where multiple
diagnostics are reported for the current line), the options are presented
and one can be selected.
Completers which provide diagnostics may also provide trivial modifications to
the source in order to correct the diagnostic. Examples include syntax errors
such as missing trailing semi-colons, spurious characters, or other errors which
the semantic engine can deterministically suggest corrections.
If no fix-it is available for the current line, or there is no diagnostic on the
current line, this command has no effect on the current buffer. If any
modifications are made, the number of changes made to the buffer is echo'd and
the user may use the editor's undo command to revert.
When a diagnostic is available, and g:ycm_echo_current_diagnostic is set to 1,
then the text  (FixIt) is appended to the echo'd diagnostic when the
completer is able to add this indication. The text  (FixIt available) is
also appended to the diagnostic text in the output of the :YcmDiags command
for any diagnostics with available fix-its (where the completer can provide this
indication).
NOTE: Causes re-parsing of the current translation unit.
Supported in filetypes: c, cpp, objc, objcpp, cuda, cs, go, java, javascript, rust, typescript
The RefactorRename <new name> subcommand
In supported file types, this command attempts to perform a semantic rename of
the identifier under the cursor. This includes renaming declarations,
definitions and usages of the identifier, or any other language-appropriate
action. The specific behavior is defined by the semantic engine in use.
Similar to FixIt, this command applies automatic modifications to your source
files. Rename operations may involve changes to multiple files, which may or may
not be open in Vim buffers at the time. YouCompleteMe handles all of this for
you. The behavior is described in the following section.
Supported in filetypes: c, cpp, objc, objcpp, cuda, java, javascript, typescript, rust
Multi-file Refactor
When a Refactor or FixIt command touches multiple files, YouCompleteMe attempts
to apply those modifications to any existing open, visible buffer in the current
tab. If no such buffer can be found, YouCompleteMe opens the file in a new
small horizontal split at the top of the current window, applies the change,
and then hides the window. NOTE: The buffer remains open, and must be
manually saved. A confirmation dialog is opened prior to doing this to remind
you that this is about to happen.
Once the modifications have been made, the quickfix list (see :help quickfix)
is populated with the locations of all modifications. This can be used to review
all automatic changes made by using :copen. Typically, use the CTRL-W <enter> combination to open the selected file in a new split. It is possible to
customize how the quickfix window is opened by using the YcmQuickFixOpened
autocommand.
The buffers are not saved automatically. That is, you must save the modified
buffers manually after reviewing the changes from the quickfix list. Changes
can be undone using Vim's powerful undo features (see :help undo). Note
that Vim's undo is per-buffer, so to undo all changes, the undo commands must
be applied in each modified buffer separately.
NOTE: While applying modifications, Vim may find files which are already
open and have a swap file. The command is aborted if you select Abort or Quit in
any such prompts. This leaves the Refactor operation partially complete and must
be manually corrected using Vim's undo features. The quickfix list is not
populated in this case. Inspect :buffers or equivalent (see :help buffers)
to see the buffers that were opened by the command.
The Format subcommand
This command formats the whole buffer or some part of it according to the value
of the Vim options shiftwidth and expandtab (see :h 'sw' and :h et
respectively). To format a specific part of your document, you can either select
it in one of Vim's visual modes (see :h visual-use) and run the command or
directly enter the range on the command line, e.g. :2,5YcmCompleter Format to
format it from line 2 to line 5.
Supported in filetypes: c, cpp, objc, objcpp, cuda, java, javascript, go, typescript, rust
The OrganizeImports subcommand
This command removes unused imports and sorts imports in the current file. It
can also group imports from the same module in TypeScript and resolves imports
in Java.
Supported in filetypes: java, javascript, typescript
Miscellaneous Commands
These commands are for general administration, rather than IDE-like features.
They cover things like the semantic engine server instance and compilation
flags.
The ExecuteCommand <args> subcommand
Some LSP completers (currently Rust and Java completers) support executing
server specific commands. Consult the rls and jdt.ls respective
documentations to find out what commands are supported and which arguments are
expected.
The support for ExecuteCommand was implemented to support plugins like
vimspector to debug java, but isn't limited to that specific use case.
The RestartServer subcommand
Restarts the semantic-engine-as-localhost-server for those semantic engines that
work as separate servers that YCM talks to.
Supported in filetypes: c, cpp, objc, objcpp, cuda, cs, go, java, javascript, rust, typescript
The ClearCompilationFlagCache subcommand
YCM caches the flags it gets from the Settings function in your
.ycm_extra_conf.py file unless you return them with the do_cache parameter
set to False. It also caches the flags extracted from the compilation
database. The cache is in memory and is never invalidated (unless you restart
the server with the :YcmRestartServer command).
This command clears that cache entirely. YCM will then re-query your
Settings function or your compilation database as needed in the future.
Supported in filetypes: c, cpp, objc, objcpp, cuda, rust
The ReloadSolution subcommand
Instruct the Omnisharp-Roslyn server to clear its cache and reload all files
from disk.  This is useful when files are added, removed, or renamed in the
solution, files are changed outside of Vim, or whenever Omnisharp-Roslyn cache
is out-of-sync.
Supported in filetypes: cs
Functions
The youcompleteme#GetErrorCount function
Get the number of YCM Diagnostic errors. If no errors are present, this function
returns 0.
For example:
  call youcompleteme#GetErrorCount()
Both this function and youcompleteme#GetWarningCount can be useful when
integrating YCM with other Vim plugins. For example, a lightline user could
add a diagnostics section to their statusline which would display the number of
errors and warnings.
The youcompleteme#GetWarningCount function
Get the number of YCM Diagnostic warnings. If no warnings are present, this
function returns 0.
For example:
  call youcompleteme#GetWarningCount()
Autocommands
The YcmLocationOpened autocommand
This User autocommand is fired when YCM opens the location list window in
response to the YcmDiags command. By default, the location list window is
opened to the bottom of the current window and its height is set to fit all
entries. This behavior can be overridden by using the YcmLocationOpened
autocommand which is triggered while the cursor is in the location list window.
For instance:
function! s:CustomizeYcmLocationWindow()
  "" Move the window to the top of the screen.
  wincmd K
  "" Set the window height to 5.
  5wincmd _
  "" Switch back to working window.
  wincmd p
endfunction

autocmd User YcmLocationOpened call s:CustomizeYcmLocationWindow()
The YcmQuickFixOpened autocommand
This User autocommand is fired when YCM opens the quickfix window in response
to the GoTo* and RefactorRename subcommands. By default, the quickfix window
is opened to full width at the bottom of the screen and its height is set to fit
all entries. This behavior can be overridden by using the YcmQuickFixOpened
autocommand which is triggered while the cursor is in the quickfix window. For
instance:
function! s:CustomizeYcmQuickFixWindow()
  "" Move the window to the top of the screen.
  wincmd K
  "" Set the window height to 5.
  5wincmd _
endfunction

autocmd User YcmQuickFixOpened call s:CustomizeYcmQuickFixWindow()
Options
All options have reasonable defaults so if the plug-in works after installation
you don't need to change any options. These options can be configured in your
vimrc script by including a line like this:
let g:ycm_min_num_of_chars_for_completion = 1
Note that after changing an option in your vimrc script you have to
restart ycmd with the :YcmRestartServer command for the changes to take
effect.
The g:ycm_min_num_of_chars_for_completion option
This option controls the number of characters the user needs to type before
identifier-based completion suggestions are triggered. For example, if the
option is set to 2, then when the user types a second alphanumeric character
after a whitespace character, completion suggestions will be triggered. This
option is NOT used for semantic completion.
Setting this option to a high number like 99 effectively turns off the
identifier completion engine and just leaves the semantic engine.
Default: 2
let g:ycm_min_num_of_chars_for_completion = 2
The g:ycm_min_num_identifier_candidate_chars option
This option controls the minimum number of characters that a completion
candidate coming from the identifier completer must have to be shown in the
popup menu.
A special value of 0 means there is no limit.
NOTE: This option only applies to the identifier completer; it has no effect
on the various semantic completers.
Default: 0
let g:ycm_min_num_identifier_candidate_chars = 0
The g:ycm_max_num_candidates option
This option controls the maximum number of semantic completion suggestions shown
in the completion menu. This only applies to suggestions from semantic
completion engines; see the g:ycm_max_identifier_candidates
option to limit the number of
suggestions from the identifier-based engine.
A special value of 0 means there is no limit.
NOTE: Setting this option to 0 or to a value greater than 100 is not
recommended as it will slow down completion when there are a very large number
of suggestions.
Default: 50
let g:ycm_max_num_candidates = 50
The g:ycm_max_num_identifier_candidates option
This option controls the maximum number of completion suggestions from the
identifier-based engine shown in the completion menu.
A special value of 0 means there is no limit.
NOTE: Setting this option to 0 or to a value greater than 100 is not
recommended as it will slow down completion when there are a very large number
of suggestions.
Default: 10
let g:ycm_max_num_identifier_candidates = 10
The g:ycm_auto_trigger option
When set to 0, this option turns off YCM's identifier completer (the
as-you-type popup) and the semantic triggers (the popup you'd get after typing
. or -> in say C++). You can still force semantic completion with the
<C-Space> shortcut.
If you want to just turn off the identifier completer but keep the semantic
triggers, you should set g:ycm_min_num_of_chars_for_completion to a high
number like 99.
Default: 1
let g:ycm_auto_trigger = 1
The g:ycm_filetype_whitelist option
This option controls for which Vim filetypes (see :h filetype) should YCM be
turned on. The option value should be a Vim dictionary with keys being filetype
strings (like python, cpp, etc.) and values being unimportant (the
dictionary is used like a hash set, meaning that only the keys matter).
The * key is special and matches all filetypes. By default, the whitelist
contains only this * key.
YCM also has a g:ycm_filetype_blacklist option that lists filetypes for which
YCM shouldn't be turned on. YCM will work only in filetypes that both the
whitelist and the blacklist allow (the blacklist ""allows"" a filetype by not
having it as a key).
For example, let's assume you want YCM to work in files with the cpp filetype.
The filetype should then be present in the whitelist either directly (cpp key
in the whitelist) or indirectly through the special * key. It should not be
present in the blacklist.
Filetypes that are blocked by the either of the lists will be completely ignored
by YCM, meaning that neither the identifier-based completion engine nor the
semantic engine will operate in them.
You can get the filetype of the current file in Vim with :set ft?.
Default: {'*': 1}
let g:ycm_filetype_whitelist = {'*': 1}
The g:ycm_filetype_blacklist option
This option controls for which Vim filetypes (see :h filetype) should YCM be
turned off. The option value should be a Vim dictionary with keys being filetype
strings (like python, cpp, etc.) and values being unimportant (the
dictionary is used like a hash set, meaning that only the keys matter).
See the g:ycm_filetype_whitelist option for more details on how this works.
Default: [see next line]
let g:ycm_filetype_blacklist = {
      \ 'tagbar': 1,
      \ 'notes': 1,
      \ 'markdown': 1,
      \ 'netrw': 1,
      \ 'unite': 1,
      \ 'text': 1,
      \ 'vimwiki': 1,
      \ 'pandoc': 1,
      \ 'infolog': 1,
      \ 'leaderf': 1,
      \ 'mail': 1
      \}
The g:ycm_filetype_specific_completion_to_disable option
This option controls for which Vim filetypes (see :h filetype) should the YCM
semantic completion engine be turned off. The option value should be a Vim
dictionary with keys being filetype strings (like python, cpp, etc.) and
values being unimportant (the dictionary is used like a hash set, meaning that
only the keys matter). The listed filetypes will be ignored by the YCM semantic
completion engine, but the identifier-based completion engine will still trigger
in files of those filetypes.
Note that even if semantic completion is not turned off for a specific filetype,
you will not get semantic completion if the semantic engine does not support
that filetype.
You can get the filetype of the current file in Vim with :set ft?.
Default: [see next line]
let g:ycm_filetype_specific_completion_to_disable = {
      \ 'gitcommit': 1
      \}
The g:ycm_filepath_blacklist option
This option controls for which Vim filetypes (see :h filetype) should filepath
completion be disabled. The option value should be a Vim dictionary with keys
being filetype strings (like python, cpp, etc.) and values being unimportant
(the dictionary is used like a hash set, meaning that only the keys matter).
The * key is special and matches all filetypes. Use this key if you want to
completely disable filepath completion:
let g:ycm_filepath_blacklist = {'*': 1}
You can get the filetype of the current file in Vim with :set ft?.
Default: [see next line]
let g:ycm_filepath_blacklist = {
      \ 'html': 1,
      \ 'jsx': 1,
      \ 'xml': 1,
      \}
The g:ycm_show_diagnostics_ui option
When set, this option turns on YCM's diagnostic display features. See the
Diagnostic display section in the User Manual for more details.
Specific parts of the diagnostics UI (like the gutter signs, text highlighting,
diagnostic echo and auto location list population) can be individually turned on
or off. See the other options below for details.
Note that YCM's diagnostics UI is only supported for C-family languages.
When set, this option also makes YCM remove all Syntastic checkers set for the
c, cpp, objc, objcpp, and cuda filetypes since this would conflict
with YCM's own diagnostics UI.
If you're using YCM's identifier completer in C-family languages but cannot use
the clang-based semantic completer for those languages and want to use the GCC
Syntastic checkers, unset this option.
Default: 1
let g:ycm_show_diagnostics_ui = 1
The g:ycm_error_symbol option
YCM will use the value of this option as the symbol for errors in the Vim
gutter.
This option is part of the Syntastic compatibility layer; if the option is not
set, YCM will fall back to the value of the g:syntastic_error_symbol option
before using this option's default.
Default: >>
let g:ycm_error_symbol = '>>'
The g:ycm_warning_symbol option
YCM will use the value of this option as the symbol for warnings in the Vim
gutter.
This option is part of the Syntastic compatibility layer; if the option is not
set, YCM will fall back to the value of the g:syntastic_warning_symbol option
before using this option's default.
Default: >>
let g:ycm_warning_symbol = '>>'
The g:ycm_enable_diagnostic_signs option
When this option is set, YCM will put icons in Vim's gutter on lines that have a
diagnostic set. Turning this off will also turn off the YcmErrorLine and
YcmWarningLine highlighting.
This option is part of the Syntastic compatibility layer; if the option is not
set, YCM will fall back to the value of the g:syntastic_enable_signs option
before using this option's default.
Default: 1
let g:ycm_enable_diagnostic_signs = 1
The g:ycm_enable_diagnostic_highlighting option
When this option is set, YCM will highlight regions of text that are related to
the diagnostic that is present on a line, if any.
This option is part of the Syntastic compatibility layer; if the option is not
set, YCM will fall back to the value of the g:syntastic_enable_highlighting
option before using this option's default.
Default: 1
let g:ycm_enable_diagnostic_highlighting = 1
The g:ycm_echo_current_diagnostic option
When this option is set, YCM will echo the text of the diagnostic present on the
current line when you move your cursor to that line. If a FixIt is available
for the current diagnostic, then  (FixIt) is appended.
This option is part of the Syntastic compatibility layer; if the option is not
set, YCM will fall back to the value of the g:syntastic_echo_current_error
option before using this option's default.
Default: 1
let g:ycm_echo_current_diagnostic = 1
The g:ycm_filter_diagnostics option
This option controls which diagnostics will be rendered by YCM. This option
holds a dictionary of key-values, where the keys are Vim's filetype strings
delimited by commas and values are dictionaries describing the filter.
A filter is a dictionary of key-values, where the keys are the type of filter,
and the value is a list of arguments to that filter. In the case of just a
single item in the list, you may omit the brackets and just provide the argument
directly. If any filter matches a diagnostic, it will be dropped and YCM will
not render it.
The following filter types are supported:

""regex"": Accepts a string regular expression. This type matches
when the regex (treated as case-insensitive) is found in the diagnostic text.
""level"": Accepts a string level, either ""warning"" or ""error."" This type
matches when the diagnostic has the same level.

NOTE: The regex syntax is NOT Vim's, it's Python's.
Default: {}
let g:ycm_filter_diagnostics = {
  \ ""java"": {
  \      ""regex"": [ "".*taco.*"", ... ],
  \      ""level"": ""error"",
  \      ...
  \    }
  \ }
The g:ycm_always_populate_location_list option
When this option is set, YCM will populate the location list automatically every
time it gets new diagnostic data. This option is off by default so as not to
interfere with other data you might have placed in the location list.
See :help location-list in Vim to learn more about the location list.
This option is part of the Syntastic compatibility layer; if the option is not
set, YCM will fall back to the value of the
g:syntastic_always_populate_loc_list option before using this option's
default.
Default: 0
let g:ycm_always_populate_location_list = 0
The g:ycm_open_loclist_on_ycm_diags option
When this option is set, :YcmDiags will automatically open the location list
after forcing a compilation and filling the list with diagnostic data.
See :help location-list in Vim to learn more about the location list.
Default: 1
let g:ycm_open_loclist_on_ycm_diags = 1
The g:ycm_complete_in_comments option
When this option is set to 1, YCM will show the completion menu even when
typing inside comments.
Default: 0
let g:ycm_complete_in_comments = 0
The g:ycm_complete_in_strings option
When this option is set to 1, YCM will show the completion menu even when
typing inside strings.
Note that this is turned on by default so that you can use the filename
completion inside strings. This is very useful for instance in C-family files
where typing #include "" will trigger the start of filename completion. If you
turn off this option, you will turn off filename completion in such situations
as well.
Default: 1
let g:ycm_complete_in_strings = 1
The g:ycm_collect_identifiers_from_comments_and_strings option
When this option is set to 1, YCM's identifier completer will also collect
identifiers from strings and comments. Otherwise, the text in comments and
strings will be ignored.
Default: 0
let g:ycm_collect_identifiers_from_comments_and_strings = 0
The g:ycm_collect_identifiers_from_tags_files option
When this option is set to 1, YCM's identifier completer will also collect
identifiers from tags files. The list of tags files to examine is retrieved from
the tagfiles() Vim function which examines the tags Vim option. See :h 'tags' for details.
YCM will re-index your tags files if it detects that they have been modified.
The only supported tag format is the Exuberant Ctags format. The
format from ""plain"" ctags is NOT supported. Ctags needs to be called with the
--fields=+l option (that's a lowercase L, not a one) because YCM needs the
language:<lang> field in the tags output.
See the FAQ for pointers if YCM does not appear to read your tag files.
This option is off by default because it makes Vim slower if your tags are on a
network directory.
Default: 0
let g:ycm_collect_identifiers_from_tags_files = 0
The g:ycm_seed_identifiers_with_syntax option
When this option is set to 1, YCM's identifier completer will seed its
identifier database with the keywords of the programming language you're
writing.
Since the keywords are extracted from the Vim syntax file for the filetype, all
keywords may not be collected, depending on how the syntax file was written.
Usually at least 95% of the keywords are successfully extracted.
Default: 0
let g:ycm_seed_identifiers_with_syntax = 0
The g:ycm_extra_conf_vim_data option
If you're using semantic completion for C-family files, this option might come
handy; it's a way of sending data from Vim to your Settings function in
your .ycm_extra_conf.py file.
This option is supposed to be a list of VimScript expression strings that are
evaluated for every request to the ycmd server and then passed to your
Settings function as a client_data keyword argument.
For instance, if you set this option to ['v:version'], your Settings
function will be called like this:
# The '801' value is of course contingent on Vim 8.1; in 8.0 it would be '800'
Settings( ..., client_data = { 'v:version': 801 } )
So the client_data parameter is a dictionary mapping Vim expression strings to
their values at the time of the request.
The correct way to define parameters for your Settings function:
def Settings( **kwargs ):
You can then get to client_data with kwargs['client_data'].
Default: []
let g:ycm_extra_conf_vim_data = []
The g:ycm_server_python_interpreter option
YCM will by default search for an appropriate Python interpreter on your system.
You can use this option to override that behavior and force the use of a
specific interpreter of your choosing.
NOTE: This interpreter is only used for the ycmd server. The YCM
client running inside Vim always uses the Python interpreter that's embedded
inside Vim.
Default: ''
let g:ycm_server_python_interpreter = ''
The g:ycm_keep_logfiles option
When this option is set to 1, YCM and the ycmd completion server will
keep the logfiles around after shutting down (they are deleted on shutdown by
default).
To see where the logfiles are, call :YcmDebugInfo.
Default: 0
let g:ycm_keep_logfiles = 0
The g:ycm_log_level option
The logging level that YCM and the ycmd completion server use. Valid
values are the following, from most verbose to least verbose:

debug
info
warning
error
critical

Note that debug is very verbose.
Default: info
let g:ycm_log_level = 'info'
The g:ycm_auto_start_csharp_server option
When set to 1, the OmniSharp-Roslyn server will be automatically started
(once per Vim session) when you open a C# file.
Default: 1
let g:ycm_auto_start_csharp_server = 1
The g:ycm_auto_stop_csharp_server option
When set to 1, the OmniSharp-Roslyn server will be automatically stopped upon
closing Vim.
Default: 1
let g:ycm_auto_stop_csharp_server = 1
The g:ycm_csharp_server_port option
When g:ycm_auto_start_csharp_server is set to 1, specifies the port for
the OmniSharp-Roslyn server to listen on. When set to 0 uses an unused port provided
by the OS.
Default: 0
let g:ycm_csharp_server_port = 0
The g:ycm_csharp_insert_namespace_expr option
By default, when YCM inserts a namespace, it will insert the using statement
under the nearest using statement. You may prefer that the using statement is
inserted somewhere, for example, to preserve sorting. If so, you can set this
option to override this behavior.
When this option is set, instead of inserting the using statement itself, YCM
will set the global variable g:ycm_namespace_to_insert to the namespace to
insert, and then evaluate this option's value as an expression. The option's
expression is responsible for inserting the namespace - the default insertion
will not occur.
Default: ''
let g:ycm_csharp_insert_namespace_expr = ''
The g:ycm_add_preview_to_completeopt option
When this option is set to 1, YCM will add the preview string to Vim's
completeopt option (see :h completeopt). If your completeopt option
already has preview set, there will be no effect. You can see the current
state of your completeopt setting with :set completeopt? (yes, the question
mark is important).
When preview is present in completeopt, YCM will use the preview window at
the top of the file to store detailed information about the current completion
candidate (but only if the candidate came from the semantic engine). For
instance, it would show the full function prototype and all the function
overloads in the window if the current completion is a function name.
Default: 0
let g:ycm_add_preview_to_completeopt = 0
The g:ycm_autoclose_preview_window_after_completion option
When this option is set to 1, YCM will auto-close the preview window after
the user accepts the offered completion string. If there is no preview window
triggered because there is no preview string in completeopt, this option is
irrelevant. See the g:ycm_add_preview_to_completeopt option for more details.
Default: 0
let g:ycm_autoclose_preview_window_after_completion = 0
The g:ycm_autoclose_preview_window_after_insertion option
When this option is set to 1, YCM will auto-close the preview window after
the user leaves insert mode. This option is irrelevant if
g:ycm_autoclose_preview_window_after_completion is set or if no preview
window is triggered. See the g:ycm_add_preview_to_completeopt option for more
details.
Default: 0
let g:ycm_autoclose_preview_window_after_insertion = 0
The g:ycm_max_diagnostics_to_display option
This option controls the maximum number of diagnostics shown to the user when
errors or warnings are detected in the file. This option is only relevant for
the C-family, C#, Java, JavaScript, and TypeScript languages.
A special value of 0 means there is no limit.
Default: 30
let g:ycm_max_diagnostics_to_display = 30
The g:ycm_key_list_select_completion option
This option controls the key mappings used to select the first completion
string.  Invoking any of them repeatedly cycles forward through the completion
list.
Some users like adding <Enter> to this list.
Default: ['<TAB>', '<Down>']
let g:ycm_key_list_select_completion = ['<TAB>', '<Down>']
The g:ycm_key_list_previous_completion option
This option controls the key mappings used to select the previous completion
string. Invoking any of them repeatedly cycles backwards through the completion
list.
Note that one of the defaults is <S-TAB> which means Shift-TAB. That mapping
will probably only work in GUI Vim (Gvim or MacVim) and not in plain console Vim
because the terminal usually does not forward modifier key combinations to Vim.
Default: ['<S-TAB>', '<Up>']
let g:ycm_key_list_previous_completion = ['<S-TAB>', '<Up>']
The g:ycm_key_list_stop_completion option
This option controls the key mappings used to close the completion menu. This is
useful when the menu is blocking the view, when you need to insert the <TAB>
character, or when you want to expand a snippet from UltiSnips and navigate
through it.
Default: ['<C-y>']
let g:ycm_key_list_stop_completion = ['<C-y>']
The g:ycm_key_invoke_completion option
This option controls the key mapping used to invoke the completion menu for
semantic completion. By default, semantic completion is triggered automatically
after typing ., -> and :: in insert mode (if semantic completion support
has been compiled in). This key mapping can be used to trigger semantic
completion anywhere. Useful for searching for top-level functions and classes.
Console Vim (not Gvim or MacVim) passes <Nul> to Vim when the user types
<C-Space> so YCM will make sure that <Nul> is used in the map command when
you're editing in console Vim, and <C-Space> in GUI Vim. This means that you
can just press <C-Space> in both console and GUI Vim and YCM will do the right
thing.
Setting this option to an empty string will make sure no mapping is created.
Default: <C-Space>
let g:ycm_key_invoke_completion = '<C-Space>'
The g:ycm_key_detailed_diagnostics option
This option controls the key mapping used to show the full diagnostic text when
the user's cursor is on the line with the diagnostic. It basically calls
:YcmShowDetailedDiagnostic.
Setting this option to an empty string will make sure no mapping is created.
Default: <leader>d
let g:ycm_key_detailed_diagnostics = '<leader>d'
The g:ycm_global_ycm_extra_conf option
Normally, YCM searches for a .ycm_extra_conf.py file for compilation flags
(see the User Guide for more details on how this works). This option specifies
a fallback path to a config file which is used if no .ycm_extra_conf.py is
found.
You can place such a global file anywhere in your filesystem.
Default: ''
let g:ycm_global_ycm_extra_conf = ''
The g:ycm_confirm_extra_conf option
When this option is set to 1 YCM will ask once per .ycm_extra_conf.py file
if it is safe to be loaded. This is to prevent execution of malicious code
from a .ycm_extra_conf.py file you didn't write.
To selectively get YCM to ask/not ask about loading certain .ycm_extra_conf.py
files, see the g:ycm_extra_conf_globlist option.
Default: 1
let g:ycm_confirm_extra_conf = 1
The g:ycm_extra_conf_globlist option
This option is a list that may contain several globbing patterns. If a pattern
starts with a ! all .ycm_extra_conf.py files matching that pattern will be
blacklisted, that is they won't be loaded and no confirmation dialog will be
shown. If a pattern does not start with a ! all files matching that pattern
will be whitelisted. Note that this option is not used when confirmation is
disabled using g:ycm_confirm_extra_conf and that items earlier in the list
will take precedence over the later ones.
Rules:

*       matches everything
?       matches any single character
[seq]   matches any character in seq
[!seq]  matches any char not in seq

Example:
let g:ycm_extra_conf_globlist = ['~/dev/*','!~/*']

The first rule will match everything contained in the ~/dev directory so
.ycm_extra_conf.py files from there will be loaded.
The second rule will match everything in the home directory so a
.ycm_extra_conf.py file from there won't be loaded.
As the first rule takes precedence everything in the home directory excluding
the ~/dev directory will be blacklisted.

NOTE: The glob pattern is first expanded with Python's
os.path.expanduser() and then resolved with os.path.abspath() before being
matched against the filename.
Default: []
let g:ycm_extra_conf_globlist = []
The g:ycm_filepath_completion_use_working_dir option
By default, YCM's filepath completion will interpret relative paths like ../
as being relative to the folder of the file of the currently active buffer.
Setting this option will force YCM to always interpret relative paths as being
relative to Vim's current working directory.
Default: 0
let g:ycm_filepath_completion_use_working_dir = 0
The g:ycm_semantic_triggers option
This option controls the character-based triggers for the various semantic
completion engines. The option holds a dictionary of key-values, where the keys
are Vim's filetype strings delimited by commas and values are lists of strings,
where the strings are the triggers.
Setting key-value pairs on the dictionary adds semantic triggers to the
internal default set (listed below). You cannot remove the default triggers,
only add new ones.
A ""trigger"" is a sequence of one or more characters that trigger semantic
completion when typed. For instance, C++ (cpp filetype) has . listed as a
trigger. So when the user types foo., the semantic engine will trigger and
serve foo's list of member functions and variables. Since C++ also has ->
listed as a trigger, the same thing would happen when the user typed foo->.
It's also possible to use a regular expression as a trigger. You have to prefix
your trigger with re! to signify it's a regex trigger. For instance,
re!\w+\. would only trigger after the \w+\. regex matches.
NOTE: The regex syntax is NOT Vim's, it's Python's.
Default: [see next line]
let g:ycm_semantic_triggers =  {
  \   'c': ['->', '.'],
  \   'objc': ['->', '.', 're!\[[_a-zA-Z]+\w*\s', 're!^\s*[^\W\d]\w*\s',
  \            're!\[.*\]\s'],
  \   'ocaml': ['.', '#'],
  \   'cpp,cuda,objcpp': ['->', '.', '::'],
  \   'perl': ['->'],
  \   'php': ['->', '::'],
  \   'cs,d,elixir,go,groovy,java,javascript,julia,perl6,python,scala,typescript,vb': ['.'],
  \   'ruby,rust': ['.', '::'],
  \   'lua': ['.', ':'],
  \   'erlang': [':'],
  \ }
The g:ycm_cache_omnifunc option
Some omnicompletion engines do not work well with the YCM cache‚Äîin particular,
they might not produce all possible results for a given prefix. By unsetting
this option you can ensure that the omnicompletion engine is re-queried on every
keypress. That will ensure all completions will be presented, but might cause
stuttering and lagginess if the omnifunc is slow.
Default: 1
let g:ycm_cache_omnifunc = 1
The g:ycm_use_ultisnips_completer option
By default, YCM will query the UltiSnips plugin for possible completions of
snippet triggers. This option can turn that behavior off.
Default: 1
let g:ycm_use_ultisnips_completer = 1
The g:ycm_goto_buffer_command option
Defines where GoTo* commands result should be opened. Can take one of the
following values: 'same-buffer', 'split', or 'split-or-existing-window'.
If this option is set to the 'same-buffer' but current buffer can not be
switched (when buffer is modified and nohidden option is set), then result
will be opened in a split. When the option is set to
'split-or-existing-window', if the result is already open in a window of the
current tab page (or any tab pages with the :tab modifier; see below), it will
jump to that window. Otherwise, the result will be opened in a split as if the
option was set to 'split'.
To customize the way a new window is split, prefix the GoTo* command with one
of the following modifiers: :aboveleft, :belowright, :botright,
:leftabove, :rightbelow, :topleft, and :vertical. For instance, to
split vertically to the right of the current window, run the command:
:rightbelow vertical YcmCompleter GoTo
To open in a new tab page, use the :tab modifier with the 'split' or
'split-or-existing-window' options e.g.:
:tab YcmCompleter GoTo
NOTE: command modifiers were added in Vim 7.4.1898. If you are using an
older version, you can still configure this by setting the option to one of the
deprecated values: 'vertical-split', 'new-tab', or 'new-or-existing-tab'.
Default: 'same-buffer'
let g:ycm_goto_buffer_command = 'same-buffer'
The g:ycm_disable_for_files_larger_than_kb option
Defines the max size (in Kb) for a file to be considered for completion. If this
option is set to 0 then no check is made on the size of the file you're opening.
Default: 1000
let g:ycm_disable_for_files_larger_than_kb = 1000
The g:ycm_use_clangd option
This option controls whether clangd should be used as completion engine for
C-family languages. Can take one of the following values: 1, 0, with
meanings:

1: YCM will use clangd if clangd binary exists in third party or it was
provided with ycm_clangd_binary_path option.
0: YCM will never use clangd completer.

Default: 1
let g:ycm_use_clangd = 1
The g:ycm_clangd_binary_path option
When ycm_use_clangd option is set to 1, this option sets the path to
clangd binary.
Default: ''
let g:ycm_clangd_binary_path = ''
The g:ycm_clangd_args option
This option controls the command line arguments passed to the clangd binary. It
appends new options and overrides the existing ones.
Default: []
let g:ycm_clangd_args = []
The g:ycm_clangd_uses_ycmd_caching option
This option controls which ranking and filtering algorithm to use for completion
items. It can take values:

1: Uses ycmd's caching and filtering logic.
0: Uses clangd's caching and filtering logic.

Default: 1
let g:ycm_clangd_uses_ycmd_caching = 1
The g:ycm_language_server option
This option lets YCM use an arbitrary LSP server, not unlike coc.nvim and others.
However, the officially supported completers are favoured over custom LSP ones,
so overriding an existing completer means first making sure YCM won't choose
that existing completer in the first place.
A simple working example of this option can be found in the section called
""Semantic Completion for Other Languages"".
Default: []
let g:ycm_language_server = []
The g:ycm_disable_signature_help option
This option allows you to disable all signature help for all completion engines.
There is no way to disable it per-completer. This option is reserved, meaning
that while signature help support remains experimental, its values and meaning
may change and it may be removed in a future version.
Default: 0
"" Disable signature help
let g:ycm_disable_signature_help = 1
FAQ
I used to be able to import vim in .ycm_extra_conf.py, but now can't
YCM was rewritten to use a client-server architecture where most of the logic is
in the ycmd server. So the magic vim module you could have previously
imported in your .ycm_extra_conf.py files doesn't exist anymore.
To be fair, importing the magic vim module in extra conf files was never
supported in the first place; it only ever worked by accident and was never a
part of the extra conf API.
But fear not, you should be able to tweak your extra conf files to continue
working by using the g:ycm_extra_conf_vim_data option. See the docs on that
option for details.
I get ImportError exceptions that mention PyInit_ycm_core or initycm_core
These errors are caused by building the YCM native libraries for Python 2 and
trying to load them into a Python 3 process (or the other way around).
For instance, if building for Python 2 but loading in Python 3:
ImportError: dynamic module does not define init function (PyInit_ycm_core)

If building for Python 3 but loading in Python 2:
ImportError: dynamic module does not define init function (initycm_core)

Setting the g:ycm_server_python_interpreter option to force the use of a
specific Python interpreter for ycmd is usually the easiest way to solve the
problem. Common values for that option are /usr/bin/python and
/usr/bin/python3.
I get a linker warning regarding libpython on macOS when compiling YCM
If the warning is ld: warning: path '/usr/lib/libpython2.7.dylib' following -L not a directory, then feel free to ignore it; it's caused by a limitation of
CMake and is not an issue. Everything should still work fine.
I get a weird window at the top of my file when I use the semantic engine
This is Vim's preview window. Vim uses it to show you extra information about
something if such information is available. YCM provides Vim with such extra
information. For instance, when you select a function in the completion list,
the preview window will hold that function's prototype and the prototypes of
any overloads of the function. It will stay there after you select the
completion so that you can use the information about the parameters and their
types to write the function call.
If you would like this window to auto-close after you select a completion
string, set the g:ycm_autoclose_preview_window_after_completion option to 1
in your vimrc file. Similarly, the g:ycm_autoclose_preview_window_after_insertion
option can be set to close the preview window after leaving insert mode.
If you don't want this window to ever show up, add set completeopt-=preview to
your vimrc. Also make sure that the g:ycm_add_preview_to_completeopt option
is set to 0.
It appears that YCM is not working
In Vim, run :messages and carefully read the output. YCM will echo messages to
the message log if it encounters problems. It's likely you misconfigured
something and YCM is complaining about it.
Also, you may want to run the :YcmDebugInfo command; it will make YCM spew out
various debugging information, including the YCM and ycmd logfile paths and
the compile flags for the current file if the file is a C-family language file
and you have compiled in Clang support. Logfiles can be opened in the editor
using the :YcmToggleLogs command.
Sometimes it takes much longer to get semantic completions than normal
This means that libclang (which YCM uses for C-family semantic completion)
failed to pre-compile your file's preamble. In other words, there was an error
compiling some of the source code you pulled in through your header files. I
suggest calling the :YcmDiags command to see what they were.
Bottom line, if libclang can't pre-compile your file's preamble because there
were errors in it, you're going to get slow completions because there's no AST
cache.
YCM auto-inserts completion strings I don't want!
If this happens when Vim automatically wraps text then it's a Vim bug that has
been fixed in version 8.0.0127. Update your Vim to this version or later.
This could also be some mappings that interfere with YCM's internal ones. Make
sure you don't have something mapped to <C-p>, <C-x> or <C-u> (in insert
mode).
YCM never selects something for you; it just shows you a menu and the user has
to explicitly select something. If something is being selected automatically,
this means there's a bug or a misconfiguration somewhere.
I get a E227: mapping already exists for <blah> error when I start Vim
This means that YCM tried to set up a key mapping but failed because you already
had something mapped to that key combination. The <blah> part of the message
will tell you what was the key combination that failed.
Look in the Options section and see if any of the default mappings conflict
with your own. Then change that option value to something else so that the
conflict goes away.
I get 'GLIBC_2.XX' not found (required by libclang.so) when starting Vim
Your system is too old for the precompiled binaries from llvm.org. Compile
Clang on your machine and then link against the libclang.so you just produced.
See the full installation guide for help.
I get LONG_BIT definition appears wrong for platform when compiling
Look at the output of your CMake call. There should be a line in it like the
following (with .dylib in place of .so on macOS):
-- Found PythonLibs: /usr/lib/libpython2.7.so (Required is at least version ""2.5"")

That would be the correct output. An example of incorrect output would
be the following:
-- Found PythonLibs: /usr/lib/libpython2.7.so (found suitable version ""2.5.1"", minimum required is ""2.5"")

Notice how there's an extra bit of output there, the found suitable version ""<version>"" part, where <version> is not the same as the version of the
dynamic library. In the example shown, the library is version 2.7 but the second
string is version 2.5.1.
This means that CMake found one version of Python headers and a different
version for the library. This is wrong. It can happen when you have multiple
versions of Python installed on your machine.
You should probably add the following flags to your cmake call (again, dylib
instead of so on macOS):
-DPYTHON_INCLUDE_DIR=/usr/include/python2.7 -DPYTHON_LIBRARY=/usr/lib/libpython2.7.so

This will force the paths to the Python include directory and the Python library
to use. You may need to set these flags to something else, but you need to make
sure you use the same version of Python that your Vim binary is built against,
which is highly likely to be the system's default Python.
I get libpython2.7.a [...] relocation R_X86_64_32 when compiling
The error is usually encountered when compiling YCM on Centos or RHEL. The full
error looks something like the following:
/usr/bin/ld: /usr/local/lib/libpython2.7.a(abstract.o): relocation R_X86_64_32 against `a local symbol' can not be used when making a shared object; recompile with -fPIC

It's possible to get a slightly different error that's similar to the one above.
Here's the problem and how you solve it:
Your libpython2.7.a was not compiled with -fPIC so it can't be linked into
ycm_core.so.  Use the -DPYTHON_LIBRARY= CMake flag to point it to a .so
version of libpython on your machine (for instance,
-DPYTHON_LIBRARY=/usr/lib/libpython2.7.so). Naturally, this means you'll have
to go through the full installation guide by hand.
I see undefined symbol: clang_getCompletionFixIt in the server logs.
This means that the server is trying to load a version of libclang that is too
old. You need at least libclang 9.0.0. We recommend running the install.py
script without --system-libclang or downloading the latest prebuilt binaries
from llvm.org when going through the full installation
guide.
I get Fatal Python error: PyThreadState_Get: no current thread on startup
This is caused by linking a static version of libpython into ycmd's
ycm_core.so.  This leads to multiple copies of the python interpreter loaded
when python loads ycmd_core.so and this messes up python's global state.
The details aren't important.
The solution is that the version of Python linked and run against must be built
with either --enable-shared or --enable-framework (on OS X).
This is achieved as follows (NOTE: for macOS, replace --enable-shared
with --enable-framework):

When building python from source: ./configure --enable-shared {options}
When building python from pyenv:
PYTHON_CONFIGURE_OPTS=""--enable-shared"" pyenv install {version}

install.py says python must be compiled with --enable-framework. Wat?
See the previous answer for how to ensure your python is built to support
dynamic modules.
YCM does not read identifiers from my tags files
First, put let g:ycm_collect_identifiers_from_tags_files = 1 in your vimrc.
Make sure you are using Exuberant Ctags to produce your tags
files since the only supported tag format is the Exuberant Ctags
format. The format from ""plain"" ctags is NOT supported. The
output of ctags --version should list ""Exuberant Ctags"". See Universal
Ctags for a maintained version.
Ctags needs to be called with the --fields=+l (that's a lowercase L, not a
one) option because YCM needs the language:<lang> field in the tags output.
NOTE: Exuberant Ctags by default sets language tag for
*.h files as C++. If you have C (not C++) project, consider giving parameter
--langmap=c:.c.h to ctags to see tags from *.h files.
NOTE: macOS comes with ""plain"" ctags installed by default. brew install ctags will get you the Exuberant Ctags version.
Also make sure that your Vim tags option is set correctly. See :h 'tags' for
details. If you want to see which tag files YCM will read for a given buffer,
run :echo tagfiles() with the relevant buffer active. Note that that function
will only list tag files that already exist.
CTRL-U in insert mode does not work while the completion menu is visible
YCM uses completefunc completion mode to show suggestions and Vim disables
<C-U> in that mode as a ""feature."" Sadly there's nothing I can do about this.
My CTRL-R mapping does not work while the completion menu is visible
Vim prevents remapping of the <C-R> key in all <C-X> completion modes
(except the <C-X><C-N>/<C-X><C-P> mode which operates in the same mode as
<C-N>/<C-P>) and YCM uses the <C-X><C-U> (completefunc) mode for
completions. This means that adding <C-R> to any of the g:ycm_key_list_*
options has no effect. You need to use another key.
YCM conflicts with UltiSnips TAB key usage
YCM comes with support for UltiSnips (snippet suggestions in the popup menu),
but you'll have to change the UltiSnips mappings. See :h UltiSnips-triggers in
Vim for details. You'll probably want to change some/all of the following
options:
g:UltiSnipsExpandTrigger
g:UltiSnipsJumpForwardTrigger
g:UltiSnipsJumpBackwardTrigger
Snippets added with :UltiSnipsAddFiletypes do not appear in the popup menu
For efficiency, YCM only fetches UltiSnips snippets in specific scenarios like
visiting a buffer or setting its filetype. You can force YCM to retrieve them by
manually triggering the FileType autocommand:
:doautocmd FileType
Why isn't YCM just written in plain VimScript, FFS?
Because of the identifier completion engine and subsequence-based filtering.
Let's say you have many dozens of files open in a single Vim instance (I often
do); the identifier-based engine then needs to store thousands (if not tens of
thousands) of identifiers in its internal data-structures. When the user types,
YCM needs to perform subsequence-based filtering on all of those identifiers
(every single one!) in less than 10 milliseconds.
I'm sorry, but that level of performance is just plain impossible to achieve
with VimScript. I've tried, and the language is just too slow. No, you can't get
acceptable performance even if you limit yourself to just the identifiers in the
current file and simple prefix-based filtering.
Why does YCM demand such a recent version of Vim?
YCM needs a version of Vim with the timers feature to achieve full
asynchronicity. This feature is available since Vim 7.4.1578.
YCM provides powerful new functionality like signature help by using new
features in Vim such as popup windows, and new APIs such as pum_getpos. This
requires Vim 8.1.1875 and we strongly recommend using this version or later.
Nasty bugs happen if I have the vim-autoclose plugin installed
Use the delimitMate plugin instead. It does the same thing without
conflicting with YCM.
Is there some sort of YCM mailing list? I have questions
If you have questions about the plugin or need help, please use the
ycm-users mailing list, don't create issues on the tracker. The tracker is
for bug reports and feature requests.
I get an internal compiler error when installing
This can be a problem on virtual servers with limited memory. A possible
solution is to add more swap memory. A more practical solution would be to force
the build script to run only one compile job at a time. You can do this by
setting the YCM_CORES environment variable to 1. Example:
YCM_CORES=1 ./install.py --clang-completer

I get weird errors when I press Ctrl-C in Vim
Never use Ctrl-C in Vim.
Using Ctrl-C to exit insert mode in Vim is a bad idea. The main issue here is
that Ctrl-C in Vim doesn't just leave insert mode, it leaves it without
triggering InsertLeave autocommands (as per Vim docs). This is a bad idea and
is likely to break many other things and not just YCM.
Bottom line, if you use Ctrl-C to exit insert mode in Vim, you're gonna have a
bad time.
If pressing <esc> is too annoying (agreed, it is), we suggest mapping it to
something more convenient. On a QWERTY keyboard, a good pick for the <esc> map
is inoremap jk <Esc>. This is right on the home row, it's an incredibly rare
digraph in English and if you ever need to type those two chars in sequence in
insert mode, you just type j, then wait 500ms, then type k.
Why did YCM stop using Syntastic for diagnostics display?
Previously, YCM would send any diagnostics it would receive from the libclang
semantic engine to Syntastic for display as signs in the gutter, red squiggles
etc. Today, YCM uses its own code to do that.
Using Syntastic for this was always a kludge. Syntastic assumes its ""checker""
plugins behave in a certain way; those assumptions have never fit YCM. For
instance, YCM continuously recompiles your code in the background for C-family
languages and tries to push new diagnostics to the user as fast as possible,
even while the user types.
Syntastic assumes that a checker only runs on file save (""active"" mode) or even
less frequently, when the user explicitly invokes it (""passive"" mode). This
mismatch in assumptions causes performance problems since Syntastic code isn't
optimized for this use case of constant diagnostic refreshing.
Poor support for this use case also led to crash bugs in Vim caused by
Syntastic-Vim interactions (issue #593) and other problems, like
random Vim flickering. Attempts were made to resolve these issues in
Syntastic, but ultimately some of them failed (for various reasons).
Implementing diagnostic display code directly in YCM resolves all of these
problems. Performance also improved substantially since the relevant code is now
written in Python instead of VimScript (which is very slow) and is tailored only
for YCM's use-cases. We were also able to introduce new features in this area
since we're now not limited to the Syntastic checker API.
We've tried to implement this in the most backwards-compatible way possible; YCM
options that control diagnostic display fall back to Syntastic options that
control the same concepts if the user has those set.
Still, some Syntastic-specific configuration you might have had might not
be supported by the new code. Please file issues on the tracker in such
cases; if we find the request to be reasonable, we'll find a way to address it.
Completion doesn't work with the C++ standard library headers
This is caused by an issue with libclang that only affects some operating
systems. Compiling with clang the binary will use the correct default header
search paths but compiling with libclang.so (which YCM uses) does not.
macOS is normally affected, but there's a workaround in YCM for that specific
OS. If you're not running that OS but still have the same problem, continue
reading.
The workaround is to call echo | clang -v -E -x c++ - and look at the
paths under the #include <...> search starts here: heading. You should take
those paths, prepend -isystem to each individual path and append them all to
the list of flags you return from your Settings function in your
.ycm_extra_conf.py file.
See issue #303 for details.
When I start vim I get a runtime error saying R6034 An application has made an attempt to load the C runtime library incorrectly.
CMake and other things seem to screw up the PATH with their own msvcrXX.dll
versions. Add the following to the very top of your vimrc
to remove these entries from the path.
python << EOF
import os
import re
path = os.environ['PATH'].split(';')

def contains_msvcr_lib(folder):
    try:
        for item in os.listdir(folder):
            if re.match(r'msvcr\d+\.dll', item):
                return True
    except:
        pass
    return False

path = [folder for folder in path if not contains_msvcr_lib(folder)]
os.environ['PATH'] = ';'.join(path)
EOF
I hear that YCM only supports Python 2, is that true?
No. Both the Vim client and the ycmd server run on Python 2 or 3. If
you are talking about code completion in a project, you can configure the Python
used for your project through a .ycm_extra_conf.py file. See the Python
Semantic Completion section for more details.
On Windows I get E887: Sorry, this command is disabled, the Python's site module could not be loaded
If you are running vim on Windows with Python 2.7.11, this is likely caused by a
bug. Follow this
workaround or use a different version
(Python 2.7.12 does not suffer from the bug).
I can't complete Python packages in a virtual environment.
This means that the Python used to run Jedi is not the Python of the virtual
environment you're in. To resolve this you should create a .ycm_extra_conf.py
file at the root of your project that sets the interpreter_path option to the
Python of your virtual environment, e.g.
def Settings(**kwargs):
  return {
    'interpreter_path': '/path/to/virtual/env/bin/python'
  }
See the Python Semantic Completion section for
more details.
I want to defer loading of YouCompleteMe until after Vim finishes booting
In recent versions of Vim, you can install YCM in a folder under
~/.vim/pack/*/opt and then load it once the user is idle via an autocommand:
augroup load_ycm
  autocmd!
  autocmd CursorHold, CursorHoldI * :packadd YouCompleteMe
                                \ | autocmd! load_ycm
augroup END
YCM does not shut down when I quit Vim
YCM relies on the VimLeave event to shut down the ycmd server. Some
plugins prevent this event from triggering by exiting Vim through an autocommand
without using the nested keyword (see :h autocmd-nested). You should
identify which plugin is responsible for the issue and report it to the plugin
author. Note that when this happens, ycmd will automatically shut itself
down after 30 minutes.
YCM does not work with my Anaconda Python setup
Anaconda is often incompatible with the pre-built libclang used by YCM
and therefore is not supported. The recommended way to solve this is to run
/path/to/real/python install.py (for example /usr/bin/python install.py).
If you want completion in Anaconda projects, point the interpreter_path option
in your .ycm_extra_conf.py file to the path of your Anaconda Python e.g.
def Settings(**kwargs):
  return {
    'interpreter_path': '/path/to/anaconda/python'
  }
See the Python Semantic Completion section for
more details.
Automatic import insertion after selecting a completion breaks undo
This is a Vim bug fixed in version 8.1.0256. Update your Vim to this version or
later.
TAB is already mapped to trigger completion in the command-line window
Vim automatically maps the key set by the wildchar option, which is TAB by
default, to complete commands in the command-line window. If you would prefer
using this key to cycle through YCM's suggestions without changing the value of
wildchar, add the following to your vimrc:
autocmd CmdwinEnter * inoremap <expr><buffer> <TAB>
      \ pumvisible() ? ""\<C-n>"" : ""\<TAB>""
Contributor Code of Conduct
Please note that this project is released with a Contributor Code of
Conduct. By participating in this project you agree to abide by its
terms.
Contact
If you have questions about the plugin or need help, please join the Gitter
room or use the ycm-users mailing list.
If you have bug reports or feature suggestions, please use the issue
tracker. Before you do, please carefully read
CONTRIBUTING.md as this asks for important diagnostics which
the team will use to help get you going.
The latest version of the plugin is available at
http://ycm-core.github.io/YouCompleteMe/.
The author's homepage is http://val.markovic.io.
Please do NOT go to #vim on freenode for support. Please contact the
YouCompleteMe maintainers directly using the contact details.
License
This software is licensed under the GPL v3 license.
¬© 2015-2018 YouCompleteMe contributors
",GitHub - ycm-core/YouCompleteMe: A code-completion engine for Vim
13,Python,"



interactive-coding-challenges
120+ continually updated, interactive, and test-driven coding challenges, with Anki flashcards.
Challenges focus on algorithms and data structures found in coding interviews.
Each challenge has one or more reference solutions that are:

Fully functional
Unit tested
Easy-to-understand

Challenges will soon provide on-demand incremental hints to help you arrive at the optimal solution.
Notebooks also detail:

Constraints
Test cases
Algorithms
Big-O time and space complexities

Also included are unit tested reference implementations of various data structures and algorithms.
Challenge Solutions





Anki Flashcards: Coding and Design




The provided Anki flashcard deck uses spaced repetition to help you retain key concepts.

Coding deck

Great for use while on-the-go.
Design Resource: The System Design Primer
Looking for resources to help you prep for the System Design and Object-Oriented Design interviews?




Check out the sister repo The System Design Primer, which contains additional Anki decks:

System design deck
System design exercises deck
Object oriented design exercises deck


Notebook Structure
Each challenge has two notebooks, a challenge notebook with unit tests for you to solve and a solution notebook for reference.
Problem Statement

States the problem to solve.

Constraints

Describes any constraints or assumptions.

Test Cases

Describes the general and edge test cases that will be evaluated in the unit test.

Algorithm

[Challenge Notebook] Empty, refer to the solution notebook algorithm section if you need a hint.
[Solution Notebook] One or more algorithm solution discussions, with Big-O time and space complexities.

Hints

[Challenge Notebook] Provides on-demand incremental hints to help you arrive at the optimal solution.  Coming soon!

Code (Challenge: Implement Me!)

[Challenge Notebook] Skeleton code for you to implement.
[Solution Notebook] One or more reference solutions.

Unit Test

[Challenge Notebook] Unit test for your code.  Expected to fail until you solve the challenge.
[Solution Notebook] Unit test for the reference solution(s).

Index
Challenges Categories
Format: Challenge Category - Number of Challenges

Arrays and Strings - 10
Linked Lists - 8
Stacks and Queues - 8
Graphs and Trees - 21
Sorting - 10
Recursion and Dynamic Programming - 17
Mathematics and Probability - 6
Bit Manipulation - 8
Online Judges - 16
System Design - 8
Object Oriented Design - 8

Total number of challenges: 120
Reference Implementations: Data Structures
Unit tested, fully functional implementations of the following data structures:

Linked List
Stack
Queue
Binary Search Tree
Graph
Min Heap
Trie
Priority Queue
Hash Map

Reference Implementations: Algorithms
Unit tested, fully functional implementations of the following algorithms:

Selection Sort
Insertion Sort
Quick Sort
Merge Sort
Radix Sort
Topological Sort
Tree Depth-First Search (Pre-, In-, Post-Order)
Tree Breadth-First Search
Graph Depth-First Search
Graph Breadth-First Search
Dijkstra's Shortest Path
Unweighted Graph Shortest Path
Knapsack 0/1
Knapsack Unbounded
Sieve of Eratosthenes

Reference Implementations: TODO

A*
Bellman-Ford
Bloom Filter
Convex Hull
Fisher-Yates Shuffle
Kruskal's
Max Flow
Prim's
Rabin-Karp
Traveling Salesman
Union Find
Contribute

Installing and Running Challenges

Repo Structure
Notebook Installation

Nose Installation


Running Challenges

Misc

Contributing
Credits
Contact Info
License

Challenges
Image Credits





Arrays and Strings



Challenge
Static Notebook




Determine if a string contains unique characters
Challenge‚îÇSolution


Determine if a string is a permutation of another
Challenge‚îÇSolution


Determine if a string is a rotation of another
Challenge‚îÇSolution


Compress a string
Challenge‚îÇSolution


Reverse characters in a string
Challenge‚îÇSolution


Given two strings, find the single different char
Challenge‚îÇSolution


Find two indices that sum to a specific value
Challenge‚îÇSolution


Implement a hash table
Challenge‚îÇSolution


Implement fizz buzz
Challenge‚îÇSolution


Find the first non-repeated character in a string
Contribute‚îÇContribute


Remove specified characters in a string
Contribute‚îÇContribute


Reverse words in a string
Contribute‚îÇContribute


Convert a string to an integer
Contribute‚îÇContribute


Convert an integer to a string
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Linked Lists



Challenge
Static Notebook




Remove duplicates from a linked list
Challenge‚îÇSolution


Find the kth to last element of a linked list
Challenge‚îÇSolution


Delete a node in the middle of a linked list
Challenge‚îÇSolution


Partition a linked list around a given value
Challenge‚îÇSolution


Add two numbers whose digits are stored in a linked list
Challenge‚îÇSolution


Find the start of a linked list loop
Challenge‚îÇSolution


Determine if a linked list is a palindrome
Challenge‚îÇSolution


Implement a linked list
Challenge‚îÇSolution


Determine if a list is cyclic or acyclic
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Stacks and Queues



Challenge
Static Notebook




Implement n stacks using a single array
Challenge‚îÇSolution


Implement a stack that keeps track of its minimum element
Challenge‚îÇSolution


Implement a set of stacks class that wraps a list of capacity-bounded stacks
Challenge‚îÇSolution


Implement a queue using two stacks
Challenge‚îÇSolution


Sort a stack using another stack as a buffer
Challenge‚îÇSolution


Implement a stack
Challenge‚îÇSolution


Implement a queue
Challenge‚îÇSolution


Implement a priority queue backed by an array
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute








Graphs and Trees



Challenge
Static Notebooks




Implement depth-first search (pre-, in-, post-order) on a tree
Challenge‚îÇSolution


Implement breadth-first search on a tree
Challenge‚îÇSolution


Determine the height of a tree
Challenge‚îÇSolution


Create a binary search tree with minimal height from a sorted array
Challenge‚îÇSolution


Create a linked list for each level of a binary tree
Challenge‚îÇSolution


Check if a binary tree is balanced
Challenge‚îÇSolution


Determine if a tree is a valid binary search tree
Challenge‚îÇSolution


Find the in-order successor of a given node in a binary search tree
Challenge‚îÇSolution


Find the second largest node in a binary search tree
Challenge‚îÇSolution


Find the lowest common ancestor
Challenge‚îÇSolution


Invert a binary tree
Challenge‚îÇSolution


Implement a binary search tree
Challenge‚îÇSolution


Implement a min heap
Challenge‚îÇSolution


Implement a trie
Challenge‚îÇSolution


Implement depth-first search on a graph
Challenge‚îÇSolution


Implement breadth-first search on a graph
Challenge‚îÇSolution


Determine if there is a path between two nodes in a graph
Challenge‚îÇSolution


Implement a graph
Challenge‚îÇSolution


Find a build order given a list of projects and dependencies.
Challenge‚îÇSolution


Find the shortest path in a weighted graph.
Challenge‚îÇSolution


Find the shortest path in an unweighted graph.
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute








Sorting



Challenge
Static Notebooks




Implement selection sort
Challenge‚îÇSolution


Implement insertion sort
Challenge‚îÇSolution


Implement quick sort
Challenge‚îÇSolution


Implement merge sort
Challenge‚îÇSolution


Implement radix sort
Challenge‚îÇSolution


Sort an array of strings so all anagrams are next to each other
Challenge‚îÇSolution


Find an item in a sorted, rotated array
Challenge‚îÇSolution


Search a sorted matrix for an item
Challenge‚îÇSolution


Find an int not in an input of n integers
Challenge‚îÇSolution


Given sorted arrays A, B, merge B into A in sorted order
Challenge‚îÇSolution


Implement a stable selection sort
Contribute‚îÇContribute


Make an unstable sort stable
Contribute‚îÇContribute


Implement an efficient, in-place version of quicksort
Contribute‚îÇContribute


Given two sorted arrays, merge one into the other in sorted order
Contribute‚îÇContribute


Find an element in a rotated and sorted array of integers
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Recursion and Dynamic Programming



Challenge
Static Notebooks




Implement fibonacci recursively, dynamically, and iteratively
Challenge‚îÇSolution


Maximize items placed in a knapsack
Challenge‚îÇSolution


Maximize unbounded items placed in a knapsack
Challenge‚îÇSolution


Find the longest common subsequence
Challenge‚îÇSolution


Find the longest increasing subsequence
Challenge‚îÇSolution


Minimize the cost of matrix multiplication
Challenge‚îÇSolution


Maximize stock prices given k transactions
Challenge‚îÇSolution


Find the minimum number of ways to represent n cents given an array of coins
Challenge‚îÇSolution


Find the unique number of ways to represent n cents given an array of coins
Challenge‚îÇSolution


Print all valid combinations of n-pairs of parentheses
Challenge‚îÇSolution


Navigate a maze
Challenge‚îÇSolution


Print all subsets of a set
Challenge‚îÇSolution


Print all permutations of a string
Challenge‚îÇSolution


Find the magic index in an array
Challenge‚îÇSolution


Find the number of ways to run up n steps
Challenge‚îÇSolution


Implement the Towers of Hanoi with 3 towers and N disks
Challenge‚îÇSolution


Implement factorial recursively, dynamically, and iteratively
Contribute‚îÇContribute


Perform a binary search on a sorted array of integers
Contribute‚îÇContribute


Print all combinations of a string
Contribute‚îÇContribute


Implement a paint fill function
Contribute‚îÇContribute


Find all permutations to represent n cents, given 1, 5, 10, 25 cent coins
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Mathematics and Probability



Challenge
Static Notebooks




Generate a list of primes
Challenge‚îÇSolution


Find the digital root
Challenge‚îÇSolution


Create a class supporting insert, max, min, mean, mode in O(1)
Challenge‚îÇSolution


Determine if a number is a power of two
Challenge‚îÇSolution


Add two numbers without the + or - sign
Challenge‚îÇSolution


Subtract two numbers without the + or - sign
Challenge‚îÇSolution


Check if a number is prime
Contribute‚îÇContribute


Determine if two lines on a Cartesian plane intersect
Contribute‚îÇContribute


Using only add, implement multiply, subtract, and divide for ints
Contribute‚îÇContribute


Find the kth number such that the only prime factors are 3, 5, and 7
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Bit Manipulation



Challenge
Static Notebooks




Implement common bit manipulation operations
Challenge‚îÇSolution


Determine number of bits to flip to convert a into b
Challenge‚îÇSolution


Draw a line on a screen
Challenge‚îÇSolution


Flip a bit to maximize the longest sequence of 1s
Challenge‚îÇSolution


Get the next largest and next smallest numbers
Challenge‚îÇSolution


Merge two binary numbers
Challenge‚îÇSolution


Swap odd and even bits in an integer
Challenge‚îÇSolution


Print the binary representation of a number between 0 and 1
Challenge‚îÇSolution


Determine the number of 1s in the binary representation of a given integer
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Online Judges



Challenge
Static Notebooks




Find the longest substring with at most k distinct chars
Challenge‚îÇSolution


Find the highest product of three numbers
Challenge‚îÇSolution


Maximize stocks profit from 1 buy and 1 sell
Challenge‚îÇSolution


Move all zeroes in a list to the end
Challenge‚îÇSolution


Find the products of every other int
Challenge‚îÇSolution


Given a list of entries and exits, find the busiest period
Challenge‚îÇSolution


Determine an island's perimeter
Challenge‚îÇSolution


Format license keys
Challenge‚îÇSolution


Find the longest absolute file path
Challenge‚îÇSolution


Merge tuple ranges
Challenge‚îÇSolution


Assign cookies
Challenge‚îÇSolution


Determine if you can win in Nim
Challenge‚îÇSolution


Check if a magazine could have been used to create a ransom note
Challenge‚îÇSolution


Find the number of times a sentence can fit on a screen
Challenge‚îÇSolution


Utopian tree
Challenge‚îÇSolution


Maximizing xor
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute



Repo Structure
interactive-coding-challenges        # Repo
‚îú‚îÄ arrays_strings                    # Category of challenges
‚îÇ  ‚îú‚îÄ rotation                       # Challenge folder
‚îÇ  ‚îÇ  ‚îú‚îÄ rotation_challenge.ipynb    # Challenge notebook
‚îÇ  ‚îÇ  ‚îú‚îÄ rotation_solution.ipynb     # Solution notebook
‚îÇ  ‚îÇ  ‚îú‚îÄ test_rotation.py            # Unit test*
‚îÇ  ‚îú‚îÄ compress
‚îÇ  ‚îÇ  ‚îú‚îÄ compress_challenge.ipynb
‚îÇ  ‚îÇ  ‚îú‚îÄ compress_solution.ipynb
‚îÇ  ‚îÇ  ‚îú‚îÄ test_compress.py
‚îÇ  ‚îú‚îÄ ...
‚îú‚îÄ linked_lists
‚îÇ  ‚îú‚îÄ palindrome
‚îÇ  ‚îÇ  ‚îî‚îÄ ...
‚îÇ  ‚îú‚îÄ ...
‚îú‚îÄ ...

*The notebooks (.ipynb) read/write the associated unit test (.py) file.
Notebook Installation
Jupyter Notebook
Run:
pip install jupyter

For detailed instructions, scripts, and tools to more optimally set up your development environment, check out the dev-setup repo.
For more details on notebook installation, follow the directions here.
More information on IPython/Jupyter Notebooks can be found here.
Nose Tests
Install nose using setuptools/distribute:
easy_install nose

or
pip install nose

More information on Nose can be found here.
Running Challenges
Notebooks
Challenges are provided in the form of IPython/Jupyter Notebooks and have been tested with Python 2.7 and Python 3.x.
If you need to install IPython/Jupyter Notebook, see the Notebook Installation section.

This README contains links to nbviewer, which hosts static notebooks of the repo's contents
To interact with or to modify elements within the dynamic notebooks, refer to the instructions below

Run the notebook of challenges:
$ git clone https://github.com/donnemartin/interactive-coding-challenges.git
$ cd interactive-coding-challenges
$ jupyter notebook

This will launch your web browser with the list of challenge categories:

Navigate to the Challenge Notebook you wish to solve
Run the cells within the challenge notebook (Cell->Run All)

This will result in an expected unit test error


Solve the challenge and verify it passes the unit test
Check out the accompanying Solution Notebook for further discussion

To debug your solution with pdb, refer to the following ticket.
Note: If your solution is different from those listed in the Solution Notebook, consider submitting a pull request so others can benefit from your work.  Review the Contributing Guidelines for details.
Future Development
Challenges, solutions, and unit tests are presented in the form of IPython/Jupyter Notebooks.

Notebooks currently contain mostly Python solutions (tested on both Python 2.7 and Python 3.x), but can be extended to include 40+ supported languages
Repo will be continually updated with new solutions and challenges
Contributions are welcome!

Contributing
Contributions are welcome!
Review the Contributing Guidelines for details on how to:

Submit issues
Add solutions to existing challenges
Add new challenges

Credits
Resources

Cracking the Coding Interview | GitHub Solutions
Programming Interviews Exposed
The Algorithm Design Manual | Solutions
CareerCup
Quora
HackerRank
LeetCode

Images

Arrays and Strings: nltk.org
Linked Lists: wikipedia.org
Stacks: wikipedia.org
Queues: wikipedia.org
Sorting: wikipedia.org
Recursion and Dynamic Programming: wikipedia.org
Graphs and Trees: wikipedia.org
Mathematics and Probability: wikipedia.org
Bit Manipulation: wikipedia.org
Online Judges: topcoder.com

Contact Info
Feel free to contact me to discuss any issues, questions, or comments.
My contact info can be found on my GitHub page.
License
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2015 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",GitHub - donnemartin/interactive-coding-challenges: 120+ interactive Python coding interview challenges (algorithms and data structures).  Includes Anki flashcards.
14,Python,"



















State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch

ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.
Features

As easy to use as pytorch-transformers
As powerful and concise as Keras
High performance on NLU and NLG tasks
Low barrier to entry for educators and practitioners

State-of-the-art NLP for everyone

Deep learning researchers
Hands-on practitioners
AI/ML/NLP teachers and educators

Lower compute costs, smaller carbon footprint

Researchers can share trained models instead of always retraining
Practitioners can reduce compute time and production costs
10 architectures with over 30 pretrained models, some in more than 100 languages

Choose the right framework for every part of a model's lifetime

Train state-of-the-art models in 3 lines of code
Deep interoperability between TensorFlow 2.0 and PyTorch models
Move a single model between TF2.0/PyTorch frameworks at will
Seamlessly pick the right framework for training, evaluation, production




Section
Description




Installation
How to install the package


Model architectures
Architectures (with pretrained weights)


Online demo
Experimenting with this repo‚Äôs text generation capabilities


Quick tour: Usage
Tokenizers & models usage: Bert and GPT-2


Quick tour: TF 2.0 and PyTorch 
Train a TF 2.0 model in 10 lines of code, load it in PyTorch


Quick tour: Fine-tuning/usage scripts
Using provided scripts: GLUE, SQuAD and Text generation


Migrating from pytorch-transformers to transformers
Migrating your code from pytorch-transformers to transformers


Migrating from pytorch-pretrained-bert to pytorch-transformers
Migrating your code from pytorch-pretrained-bert to transformers


[Documentation](v2.2.0/v2.2.1) (v2.1.1) (v2.0.0) (v1.2.0) (v1.1.0) (v1.0.0) (master)
Full API documentation and more



Installation
This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1
With pip
First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to TensorFlow installation page and/or PyTorch installation page regarding the specific install command for your platform.
When TensorFlow 2.0 and/or PyTorch has been installed, ü§ó Transformers can be installed using pip as follows:
pip install transformers
From source
Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to TensorFlow installation page and/or PyTorch installation page regarding the specific install command for your platform.
When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:
pip install [--editable] .
Run the examples
Examples are included in the repository but are not shipped with the library.
Therefore, in order to run the latest versions of the examples you also need to install from source. To do so, create a new virtual environment and follow these steps:
git clone https://github.com/huggingface/transformers
cd transformers
pip install [--editable] .
Tests
A series of tests are included for the library and the example scripts. Library tests can be found in the tests folder and examples tests in the examples folder.
These tests can be run using unittest or pytest (install pytest if needed with pip install pytest).
Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.
You can run the tests from the root of the cloned repository with the commands:
python -m unittest discover -s transformers/tests -p ""*test.py"" -t .
python -m unittest discover -s examples -p ""*test.py"" -t examples
or
python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/
By default, slow tests are skipped. Set the RUN_SLOW environment variable to yes to run them.
Do you want to run a Transformer model on a mobile device?
You should check out our swift-coreml-transformers repo.
It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains GPT-2, DistilGPT-2, BERT, and DistilBERT) to CoreML models that run on iOS devices.
At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!
Model architectures
ü§ó Transformers currently provides 10 NLU/NLG architectures:

BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.
GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
XLNet (from Google/CMU) released with the paper ‚ÄãXLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.
RoBERTa (from Facebook), released together with the paper a Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.
CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.
CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su√°rez*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.
ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.

These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the documentation.
Online demo
Write With Transformer, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo‚Äôs text generation capabilities.
You can use it to experiment with completions generated by GPT2Model, TransfoXLModel, and XLNetModel.

‚Äúü¶Ñ Write with transformer is to writing what calculators are to calculus.‚Äù


Quick tour
Let's do a very quick overview of the model architectures in ü§ó Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the full documentation.
import torch
from transformers import *

# Transformers has a unified API
# for 8 transformer architectures and 30 pretrained weights.
#          Model          | Tokenizer          | Pretrained weights shortcut
MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),
          (GPT2Model,       GPT2Tokenizer,       'gpt2'),
          (CTRLModel,       CTRLTokenizer,       'ctrl'),
          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),
          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),
          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),
          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),
          (RobertaModel,    RobertaTokenizer,    'roberta-base')]

# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`

# Let's encode some text in a sequence of hidden-states using each model:
for model_class, tokenizer_class, pretrained_weights in MODELS:
    # Load pretrained model/tokenizer
    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
    model = model_class.from_pretrained(pretrained_weights)

    # Encode text
    input_ids = torch.tensor([tokenizer.encode(""Here is some text to encode"", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.
    with torch.no_grad():
        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples

# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.
BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]

# All the classes for an architecture can be initiated from pretrained weights for this architecture
# Note that additional weights added for fine-tuning are only initialized
# and need to be trained on the down-stream task
pretrained_weights = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(pretrained_weights)
for model_class in BERT_MODEL_CLASSES:
    # Load pretrained model/tokenizer
    model = model_class.from_pretrained(pretrained_weights)

    # Models can return full list of hidden-states & attentions weights at each layer
    model = model_class.from_pretrained(pretrained_weights,
                                        output_hidden_states=True,
                                        output_attentions=True)
    input_ids = torch.tensor([tokenizer.encode(""Let's see all hidden-states and attentions on this text"")])
    all_hidden_states, all_attentions = model(input_ids)[-2:]

    # Models are compatible with Torchscript
    model = model_class.from_pretrained(pretrained_weights, torchscript=True)
    traced_model = torch.jit.trace(model, (input_ids,))

    # Simple serialization for models and tokenizers
    model.save_pretrained('./directory/to/save/')  # save
    model = model_class.from_pretrained('./directory/to/save/')  # re-load
    tokenizer.save_pretrained('./directory/to/save/')  # save
    tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # re-load

    # SOTA examples for GLUE, SQUAD, text generation...
Quick tour TF 2.0 training and PyTorch interoperability
Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with ü§ó Transformers and then loaded in PyTorch for fast inspection/tests.
import tensorflow as tf
import tensorflow_datasets
from transformers import *

# Load dataset, tokenizer, model from pretrained model/vocabulary
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
data = tensorflow_datasets.load('glue/mrpc')

# Prepare dataset for GLUE as a tf.data.Dataset instance
train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')
valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')
train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)
valid_dataset = valid_dataset.batch(64)

# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule 
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train and evaluate using tf.keras.Model.fit()
history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,
                    validation_data=valid_dataset, validation_steps=7)

# Load the TensorFlow model in PyTorch for inspection
model.save_pretrained('./save/')
pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)

# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task
sentence_0 = ""This research was consistent with his findings.""
sentence_1 = ""His findings were compatible with this research.""
sentence_2 = ""His findings were not compatible with this research.""
inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')
inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')

pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()
pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()

print(""sentence_1 is"", ""a paraphrase"" if pred_1 else ""not a paraphrase"", ""of sentence_0"")
print(""sentence_2 is"", ""a paraphrase"" if pred_2 else ""not a paraphrase"", ""of sentence_0"")
Quick tour of the fine-tuning/usage scripts
Important
Before running the fine-tuning scripts, please read the
instructions on how to
setup your environment to run the examples.
The library comprises several example scripts with SOTA performances for NLU and NLG tasks:

run_glue.py: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (sequence-level classification)
run_squad.py: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (token-level classification)
run_generation.py: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation
other model-specific examples (see the documentation).

Here are three quick usage examples for these scripts:
run_glue.py: Fine-tuning on GLUE tasks for sequence classification
The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.
Before running anyone of these GLUE tasks you should download the
GLUE data by running
this script
and unpack it to some directory $GLUE_DIR.
You should also install the additional packages required by the examples:
pip install -r ./examples/requirements.txt
export GLUE_DIR=/path/to/glue
export TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir $GLUE_DIR/$TASK_NAME \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/$TASK_NAME/
where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.
The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.
Fine-tuning XLNet model on the STS-B regression task
This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).
export GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=${GLUE_DIR}/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120
On this machine we thus have a batch size of 32, please increase gradient_accumulation_steps to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of +0.917 on the development set.
Fine-tuning Bert model on the MRPC classification task
This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 > 92.
python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir $GLUE_DIR/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \
Training with these hyper-parameters gave us the following results:
  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798
run_squad.py: Fine-tuning on SQuAD for question-answering
This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 > 93 on SQuAD:
python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file $SQUAD_DIR/train-v1.1.json \
    --predict_file $SQUAD_DIR/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \
Training with these hyper-parameters gave us the following results:
python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{""exact_match"": 86.91579943235573, ""f1"": 93.1532499015869}
This is the model provided as bert-large-uncased-whole-word-masking-finetuned-squad.
run_generation.py: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet
A conditional generation script is also included to generate text from a prompt.
The generation script includes the tricks proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).
Here is how to run the script with the small version of OpenAI GPT-2 model:
python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \
and from the Salesforce CTRL model:
python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \
Migrating from pytorch-transformers to transformers
Here is a quick summary of what you should take care of when migrating from pytorch-transformers to transformers.
Positional order of some models' keywords inputs (attention_mask, token_type_ids...) changed
To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models keywords inputs (attention_mask, token_type_ids...) has been changed.
If you used to call the models with keyword names for keyword arguments, e.g. model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids), this should not cause any change.
If you used to call the models with positional inputs for keyword arguments, e.g. model(inputs_ids, attention_mask, token_type_ids), you may have to double check the exact order of input arguments.
Migrating from pytorch-pretrained-bert to transformers
Here is a quick summary of what you should take care of when migrating from pytorch-pretrained-bert to transformers.
Models always output tuples
The main breaking change when migrating from pytorch-pretrained-bert to transformers is that every model's forward method always outputs a tuple with various elements depending on the model and the configuration parameters.
The exact content of the tuples for each model is detailed in the models' docstrings and the documentation.
In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in pytorch-pretrained-bert.
Here is a pytorch-pretrained-bert to transformers conversion example for a BertForSequenceClassification classification model:
# Let's load our model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# If you used to have this line in pytorch-pretrained-bert:
loss = model(input_ids, labels=labels)

# Now just use this line in transformers to extract the loss from the output tuple:
outputs = model(input_ids, labels=labels)
loss = outputs[0]

# In transformers you can also have access to the logits:
loss, logits = outputs[:2]

# And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', output_attentions=True)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs
Using hidden states
By enabling the configuration option output_hidden_states, it was possible to retrieve the last hidden states of the encoder. In pytorch-transformers as well as transformers the return value has changed slightly: all_hidden_states now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.
Serialization
Breaking change in the from_pretrained() method:


Models are now set in evaluation mode by default when instantiated with the from_pretrained() method. To train them, don't forget to set them back in training mode (model.train()) to activate the dropout modules.


The additional *input and **kwargs arguments supplied to the from_pretrained() method used to be directly passed to the underlying model's class __init__() method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous BertForSequenceClassification examples. We are working on a way to mitigate this breaking change in #866 by forwarding the the model's __init__() method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.


Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method save_pretrained(save_directory) if you were using any other serialization method before.
Here is an example:
### Let's load a model and tokenizer
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

### Do some stuff to our model and tokenizer
# Ex: add new tokens to the vocabulary and embeddings of our model
tokenizer.add_tokens(['[SPECIAL_TOKEN_1]', '[SPECIAL_TOKEN_2]'])
model.resize_token_embeddings(len(tokenizer))
# Train our model
train(model)

### Now let's save our model and tokenizer to a directory
model.save_pretrained('./my_saved_model_directory/')
tokenizer.save_pretrained('./my_saved_model_directory/')

### Reload the model and the tokenizer
model = BertForSequenceClassification.from_pretrained('./my_saved_model_directory/')
tokenizer = BertTokenizer.from_pretrained('./my_saved_model_directory/')
Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules
The two optimizers previously included, BertAdam and OpenAIAdam, have been replaced by a single AdamW optimizer which has a few differences:

it only implements weights decay correction,
schedules are now externals (see below),
gradient clipping is now also external (see below).

The new optimizer AdamW matches PyTorch Adam optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.
The schedules are now standard PyTorch learning rate schedulers and not part of the optimizer anymore.
Here is a conversion examples from BertAdam with a linear warmup and decay schedule to AdamW and the same schedule:
# Parameters:
lr = 1e-3
max_grad_norm = 1.0
num_training_steps = 1000
num_warmup_steps = 100
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

### Previously BertAdam optimizer was instantiated like this:
optimizer = BertAdam(model.parameters(), lr=lr, schedule='warmup_linear', warmup=warmup_proportion, t_total=num_training_steps)
### and used like this:
for batch in train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

### In Transformers, optimizer and schedules are splitted and instantiated like this:
optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler
### and used like this:
for batch in train_data:
    model.train()
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
Citation
We now have a paper you can cite for the ü§ó Transformers library:
@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

",GitHub - huggingface/transformers: ü§ó Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.
15,Python,"



Apache MXNet (incubating) for Deep Learning



Master
Docs
License




             






Apache MXNet (incubating) is a deep learning framework designed for both efficiency and flexibility.
It allows you to mix symbolic and imperative programming
to maximize efficiency and productivity.
At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.
A graph optimization layer on top of that makes symbolic execution fast and memory efficient.
MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.
MXNet is more than a deep learning project. It is a collection of
blue prints and guidelines for building
deep learning systems, and interesting insights of DL systems for hackers.
Ask Questions

Please use discuss.mxnet.io for asking questions.
Please use mxnet/issues for reporting bugs.
Frequent Asked Questions

How to Contribute

Contribute to MXNet

What's New

Version 1.5.1 Release - MXNet 1.5.1 Patch Release.
Version 1.5.0 Release - MXNet 1.5.0 Release.
Version 1.4.1 Release - MXNet 1.4.1 Patch Release.
Version 1.4.0 Release - MXNet 1.4.0 Release.
Version 1.3.1 Release - MXNet 1.3.1 Patch Release.
Version 1.3.0 Release - MXNet 1.3.0 Release.
Version 1.2.0 Release - MXNet 1.2.0 Release.
Version 1.1.0 Release - MXNet 1.1.0 Release.
Version 1.0.0 Release - MXNet 1.0.0 Release.
Version 0.12.1 Release - MXNet 0.12.1 Patch Release.
Version 0.12.0 Release - MXNet 0.12.0 Release.
Version 0.11.0 Release - MXNet 0.11.0 Release.
Apache Incubator - We are now an Apache Incubator project.
Version 0.10.0 Release - MXNet 0.10.0 Release.
Version 0.9.3 Release - First 0.9 official release.
Version 0.9.1 Release (NNVM refactor) - NNVM branch is merged into master now. An official release will be made soon.
Version 0.8.0 Release
Updated Image Classification with new Pre-trained Models
Notebooks How to Use MXNet
MKLDNN for Faster CPU Performance
MXNet Memory Monger, Training Deeper Nets with Sublinear Memory Cost
Tutorial for NVidia GTC 2016
MXNet.js: Javascript Package for Deep Learning in Browser (without server)
Guide to Creating New Operators (Layers)
Go binding for inference
Amalgamation and Go Binding for Predictors - Outdated
Large Scale Image Classification

Contents

Website
Documentation
Blog
Code Examples
Installation
Features
Ecosystem

Features

Design notes providing useful insights that can re-used by other DL projects
Flexible configuration for arbitrary computation graph
Mix and match imperative and symbolic programming to maximize flexibility and efficiency
Lightweight, memory efficient and portable to smart devices
Scales up to multi GPUs and distributed setting with auto parallelism
Support for Python, Scala, C++, Java, Clojure, R, Go, Javascript, Perl, Matlab, and Julia
Cloud-friendly and directly compatible with AWS S3, AWS Deep Learning AMI, AWS SageMaker, HDFS, and Azure

License
Licensed under an Apache-2.0 license.
Reference Paper
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
Bing Xu, Chiyuan Zhang, and Zheng Zhang.
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015
History
MXNet emerged from a collaboration by the authors of cxxnet, minerva, and purine2. The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.
","GitHub - apache/incubator-mxnet: Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more"
16,Python,"
Unified access to the best community driven cheat sheets repositories of the world.
Let's imagine for a moment that there is such a thing as an ideal cheat sheet.
What should it look like?
What features should it have?

Concise ‚Äî It should only contain the things you need, and nothing else.
Fast ‚Äî It should be possible to use it instantly.
Comprehensive ‚Äî It should contain answers for every possible question.
Universal ‚Äî It should be available everywhere, anytime, without any preparations.
Unobtrusive ‚Äî It should not distract you from your main task.
Tutoring ‚Äî It should help you to learn the subject.
Inconspicuous ‚Äî It should be possible to use it completely unnoticed.

Such a thing exists.

Features
cheat.sh

Has a simple curl/browser interface.
Covers 56 programming languages, several DBMSes, and more than 1000 most important UNIX/Linux commands.
Provides access to the best community driven cheat sheets repositories in the world, on par with StackOverflow.
Available everywhere, no installation needed.
Ultrafast, returns answers within 100 ms, as a rule.
Has a convenient command line client, cht.sh, that is very advantageous and helpful, though not mandatory.
Can be used directly from code editors, without opening a browser and not switching your mental context.
Supports a special stealth mode where it can be used fully invisibly without ever touching a key and and making sounds.




Contents

Features
Usage
Command line client, cht.sh

Installation
Client usage
Tab-completion
Stealth mode


Self-Hosting

Docker


Editors integration

Vim
Emacs
Visual Studio Code
Sublime
IntelliJ IDEA


Special pages
Search
Programming languages cheat sheets
Cheat sheets sources
How to contribute

How to edit a cheat sheet
How to add a cheat sheet
How to add a cheat sheet repository



Usage
To get a cheat sheet for a UNIX/Linux command from a command line, query the service using curl or any other HTTP/HTTPS client
specifying the name of the command in the query:
    curl cheat.sh/tar
    curl cht.sh/curl
    curl https://cheat.sh/rsync
    curl https://cht.sh/tr

As you can see, you can use both HTTPS and HTTP to access the service, and both the long (cheat.sh) and the short (cht.sh) service names.
Here tar, curl, rsync, and tr are names of the UNIX/Linux commands you want to get cheat sheets for.
If you don't know the name of the command you need, you can search for it using the ~KEYWORD notation.
For example, to see how you can make snapshots of a filesystem/volume/something else:
    curl cht.sh/~snapshot




The programming language cheat sheets are located in special namespaces dedicated to them.
    curl cht.sh/go/Pointers
    curl cht.sh/scala/Functions
    curl cht.sh/python/lambda

To get the list of available programming language cheat sheets, use the special query :list:
    curl cht.sh/go/:list

Almost each programming language has a special page named :learn
that describes the language basics (that's a direct mapping from the ""Learn X in Y"" project).
It could be a good starting point if you've just started learning a language.
If there is no cheat sheet for a programming language query (and it is almost always the case),
it is generated on the fly, based on available cheat sheets and answers on StackOverflow.
Of course, there is no guarantee that the returned cheat sheet will be a 100% hit, but it is almost always exactly what you are looking for.
Try these (and your own) queries to get the impression of that, what the answers look like:
    curl cht.sh/go/reverse+a+list
    curl cht.sh/python/random+list+elements
    curl cht.sh/js/parse+json
    curl cht.sh/lua/merge+tables
    curl cht.sh/clojure/variadic+function

If you don't like an answer for your queries, you can pick another one. For that, repeat the query with an additional parameter /1, /2 etc. appended:
    curl cht.sh/python/random+string
    curl cht.sh/python/random+string/1
    curl cht.sh/python/random+string/2

Cheat sheets are formatted as code of the queried programming language (at least we are trying our best to do so)
so they can be pasted into a program in this language directly. Text comments, if there are any, are formatted according to the language syntax.
    $ curl cht.sh/lua/table+keys
    -- lua: retrieve list of keys in a table

    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end

    --[[
       [ Note that you cannot guarantee any order in keyset. If you want the
       [ keys in sorted order, then sort keyset with table.sort(keyset).
       [ 
       [ [lhf] [so/q/12674345] [cc by-sa 3.0]
       ]]

If you don't need text comments in the answer, you can eliminate them
using a special option ?Q:
    $ curl cht.sh/lua/table+keys?Q
    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end
And if you don't need syntax highlighting, switch it off using ?T.
You can combine the options together:
    curl cht.sh/go/reverse+a+list?Q
    curl cht.sh/python/random+list+elements?Q
    curl cht.sh/js/parse+json?Q
    curl cht.sh/lua/merge+tables?QT
    curl cht.sh/clojure/variadic+function?QT

Full list of all options described below and in /:help.
Try your own queries. Follow these rules:

Try to be more specific (/python/append+file is better than /python/file and /python/append).
Ask practical question if possible (yet theoretical question are possible too).
Ask programming language questions only; specify the name of the programming language as the section name.
Separate words with + instead of spaces.
Do not use special characters, they are ignored anyway.
If you want to eliminate cheat sheets containing some word, add it to the query with +-: python/multiply+matrices+-numpy

Read more about the programming languages queries below.
Command line client, cht.sh
The cheat.sh service has its own command line client (cht.sh) that
has several useful features compared to querying the service directly with curl:

Special shell mode with a persistent queries context and readline support.
Queries history.
Clipboard integration.
Tab completion support for shells (bash, fish, zsh).
Stealth mode.

Installation
To install the client:
    mkdir -p ~/bin/
    curl https://cht.sh/:cht.sh > ~/bin/cht.sh
    chmod +x ~/bin/cht.sh

or to install it globally (for all users):
    curl https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh
    chmod +x /usr/local/bin/cht.sh

Note: The package ""rlwrap"" is a required dependency to run in shell mode. Install this using sudo apt install rlwrap
Client usage
Now, you can use cht.sh instead of curl, and write your queries in more natural way,
with spaces instead of +:
    $ cht.sh go reverse a list
    $ cht.sh python random list elements
    $ cht.sh js parse json

It is even more convenient to start the client in a special shell mode:
    $ cht.sh --shell
    cht.sh> go reverse a list

If all your queries are about the same language, you can change the context
and spare repeating the programming language name:
    $ cht.sh --shell
    cht.sh> cd go
    cht.sh/go> reverse a list

or even start the client in this context:
    $ cht.sh --shell go
    cht.sh/go> reverse a list
    ...
    cht.sh/go> join a list
    ...

If you want to change the context, you can do it with the cd command,
or if you want do a single query for some other language, just prepend it with /:
    $ cht.sh --shell go
    ...
    cht.sh/go> /python dictionary comprehension
    ...

If you want to copy the last answer into the clipboard, you can
use the c (copy) command, or C (ccopy, without comments).
    cht.sh/python> append file
    #  python - How do you append to a file?

    with open(""test.txt"", ""a"") as myfile:
        myfile.write(""appended text"")
    cht.sh/python> C
    copy: 2 lines copied to the selection

Type help for other internal cht.sh commands.
	cht.sh> help
	help    - show this help
	hush    - do not show the 'help' string at start anymore
	cd LANG - change the language context
	copy    - copy the last answer in the clipboard (aliases: yank, y, c)
	ccopy   - copy the last answer w/o comments (cut comments; aliases: cc, Y, C)
	exit    - exit the cheat shell (aliases: quit, ^D)
	id [ID] - set/show an unique session id (""reset"" to reset, ""remove"" to remove)
	stealth - stealth mode (automatic queries for selected text)
	update  - self update (only if the scriptfile is writeable)
	version - show current cht.sh version
	/:help  - service help
	QUERY   - space separated query staring (examples are below)
				  cht.sh> python zip list
				  cht.sh/python> zip list
				  cht.sh/go> /python zip list

The cht.sh client has its configuration file which is located at ~/.cht.sh/cht.sh.conf
(location of the file can be overriden by the environment variable CHTSH_CONF).
Use it to specify query options that you would use with each query.
For example, to switch syntax highlighting off create the file with the following
content:
CHTSH_QUERY_OPTIONS=""T""

Or if you want to use a special syntax highlighting theme:
CHTSH_QUERY_OPTIONS=""style=native""

(curl cht.sh/:styles-demo to see all supported styles).
Other cht.sh configuration parameters:
CHTSH_CURL_OPTIONS=""-A curl""        # curl options used for cht.sh queries
CHTSH_URL=https://cht.sh            # URL of the cheat.sh server

Tab completion
Bash Tab completion
To activate tab completion support for cht.sh, add the :bash_completion script to your ~/.bashrc:
    $ curl https://cheat.sh/:bash_completion > ~/.bash.d/cht.sh
    $ . ~/.bash.d/cht.sh
    $ # and add . ~/.bash.d/cht.sh to ~/.bashrc

ZSH Tab completion
To activate tab completion support for cht.sh, add the :zsh script to the fpath in your ~/.zshrc:
    $ curl https://cheat.sh/:zsh > ~/.zsh.d/_cht
    $ echo 'fpath=(~/.zsh.d/ $fpath)' >> ~/.zshrc
    $ # Open a new shell to load the plugin

Stealth mode
Being used fully unnoticed is one of the most important property of any cheat sheet.
cheat.sh can be used completely unnoticed too. The cheat.sh client, cht.sh, has
a special mode, called stealth mode. Using that, you don't even need to touch your
keyboard to open a cheat sheet.
In this mode, as soon as you select some text with the mouse (and thus adding it
into the selection buffer of X Window System or into the clipboard) it's used
as a query string for cheat.sh, and the correspondent cheat sheet is automatically shown.
Let's imagine, that you are having an online interview, where your interviewer asks you
some questions using a shared document (say Google Docs) and you are supposed
to write your coding answers there (it's possible too that you'll type in the questions
on your own, just to show to the interviewer that you've heard it right).
When using the stealth mode of cht.sh, the only thing you need to do in order to see
a cheat sheet for some question, is to select the question using the mouse.
If you don't want any text in the answers and the only thing you need is code,
use the Q option when starting the stealth mode.



You: Hi!                                            | $ cht.sh --shell python
She: Hi!                                            | cht.sh/python> stealth Q
She: Are you ready for a small interview?           | stealth: you are in the stealth mode; select any text
She: Just a couple of questions                     | stealth: selections longer than 5 words are ignored
She: We will talk about python                      | stealth: query arguments: ?Q
She: Let's start from something simple.             | stealth: use ^C to leave this mode
She: Do you know how to reverse a list in python?   |
You: Sure                                           |
You: (selecting ""reverse a list"")                   | stealth: reverse a list
                                                    | reverse_lst = lst[::-1]
You: lst[::-1]?                                     |
She: Good.                                          |
She: Do you know how to chain a list of lists?      |
You: (selecting ""chain a list of lists"")            | stealth: chain a list of lists
                                                    | import itertools
                                                    | a = [[""a"",""b""], [""c""]]
                                                    | print list(itertools.chain.from_iterable(a))
You: May I use external modules?                    |
She: What module do you want to use?                |
You: itertools                                      |
She: Yes, you may use it                            |
You: Ok, then:                                      |
You: itertools.chain.from_iterable(a)               |
She: Good. Let's try something harder.              |
She: What about quicksort implementation?           |
You: (selecting ""quicksort implementation"")         | stealth: quicksort implementation
You: Let me think about it.                         | (some big and clumsy lowlevel implementation shown)
You: Well...(starting typing it in)                 | def sort(array=[12,4,5,6,7,3,1,15]):
                                                    |     less = []
She: (seeing your ugly pascal style)                |     equal = []
She: Could you write it more concise?               |     greater = []
                                                    |     if len(array) > 1:
You: What do you mean?                              |         pivot = array[0]
                                                    |         for x in array:
She: I mean,                                        |             if x < pivot: less.append(x)
She: do you really need all these ifs and fors?     |             if x == pivot: equal.append(x)
She: Could you maybe just use filter instead?       |             if x > pivot: greater.append(x)
                                                    |         return sort(less)+equal+sort(greater)
You: quicksort with filter?                         |     else:
                                                    |         return array
She: Yes                                            |
You: (selecting ""quicksort with filter"")            | stealth: quicksort with filter
You: Ok, I will try.                                | return qsort(filter(lt, L[1:]))+[pivot] \
You: Something like this?                           |     +qsort(filter(ge, L[1:]))
You: qsort(filter(lt, L[1:]))+[pivot] \             |
       + qsort(filter(ge, L[1:]))                   |
                                                    |
She: Yes! Perfect! Exactly what I wanted to see!    |
                                                    |


Of course, this is just for fun, and you should never cheat in your coding interviews,
because you know what happens when you do.

Windows command line client
You can access cheat.sh from Windows command line too.
Use cheat.sh command line client for that: cht.exe.
It supports:

output colorization;
command line options;
its own configuration file.

You can also use scoop command-line installer for Windows to get it:
scoop install cht
Self-Hosting
Docker
Currently, the easiest way to get a self-hosted instance running is by using the docker-compose.yml file provided in the extra/docker folder.
This pulls down the latest image with baked in cheatsheets and starts the app and a Redis instance to back it, making the service available on port 8002 of the local host. This is currently an early implementation and should probably not be used for anything outside of internal/dev/personal use right now.
Editors integration
You can use cheat.sh directly from the editor
(Emacs, Sublime, Vim, and Visual Studio Code are currently supported;
not all features are supported by all plugins though; see below).
Instead of opening your browser, googling, browsing Stack Overflow
and eventually copying the code snippets you need into the clipboard
and later pasting them into the editor,
you can achieve the same instantly and without leaving the editor at all!
Here is what it looks like in Vim:


If you have a question while editing a program, you can just type
your question directly in the buffer and press <leader>KK. You will get
the answer to your question in pager. (with <leader>KB you'll get the answer
in a separate buffer).


If you like the answer, you can manually paste it from the buffer or
the pager, or if you are lazy you can use <leader>KP to paste it below/under
your question (or replace you question using <leader>KR). If you want the
answer without the comments, <leader>KC replays the last query
toggling them.


If you use some static analysis plugin such as syntastic (for Vim), you can use
its warning and error messages as cheat.sh queries: place the cursor on the problem line
and press <leader>KE: explanation for the warning will be opened in a new buffer.
Features supported by cheat.sh plugins for different editors:



Feature
Emacs
Sublime
Vim
VSCode
IDEA




Command queries
‚úì
‚úì
‚úì
‚úì
‚úì


Queries from buffer


‚úì
‚úì



Toggle comments


‚úì
‚úì
‚úì


Prev/next answer


‚úì
‚úì
‚úì


Multiple answers

‚úì


‚úì


Warnings as queries


‚úì




Queries history


‚úì
‚úì



Session id


‚úì




Configurable server
‚úì

‚úì
‚úì




Vim

cheat.sh-vim ‚Äî Vim support

Here is Vim configuration example:
"" some configuration above ...

let mapleader="" ""

call vundle#begin()
Bundle 'gmarik/vundle'
Bundle 'scrooloose/syntastic'
Bundle 'dbeniamine/cheat.sh-vim'
call vundle#end()

let g:syntastic_javascript_checkers = [ 'jshint' ]
let g:syntastic_ocaml_checkers = ['merlin']
let g:syntastic_python_checkers = ['pylint']
let g:syntastic_shell_checkers = ['shellcheck']

"" some configuration below ...
In this example, several Vim plugins are used:

gmarik/vundle ‚Äî Vim plugin manager
scrooloose/syntastic ‚Äî Syntax checking plugin
cheat.sh-vim ‚Äî Vim support

Syntastic shows warnings and errors (found by code analysis tools: jshint, merlin, pylint, shellcheckt etc.), and cheat.sh-vim` shows you explanations for the errors and warnings
and answers on programming languages queries written in the editor.
Watch a demo, where the most important features of the cheat.sh Vim plugin are shown (5 Min):



Or, if you want to scroll and/or pause, the same on YouTube:



Emacs

cheat-sh.el ‚Äî Emacs support (available also at cheat.sh/:emacs)
cheat.sh/:emacs-ivy ‚Äî Emacs support for ivy users


Visual Studio Code

vscode-snippet
Install it from VSCode Marketplace

Usage:

Hit ‚åò Command + ‚áß Shift + p
Run Snippet: Find.
Type your query and hit enter.


(GIF courtesy: Matthias Endler, @mre)
Sublime

cheat.sh-sublime-plugin

Usage:

Write your query string.
Select the query string.
Press Cmd + ‚áß Shift + B to replace the selected query string by the answer generated from cht.sh.


(GIF courtesy: Gaurav Kukreja, @gauravk-in)
IntelliJ IDEA

idea-cheatsh-plugin
Install from idea plugins marketplace

Usage:

Write query string
Select the query string
Press keyboard shortcut Alt + C , S to replace the selected query string by the answer


(GIF courtesy: Szymon Przebierowski, @szymonprz)
Special pages
There are several special pages that are not cheat sheets.
Their names start with colon and have special meaning.
Getting started:
    :help               description of all special pages and options
    :intro              cheat.sh introduction, covering the most important usage questions
    :list               list all cheat sheets (can be used in a subsection too: /go/:list)

Command line client cht.sh and shells support:
    :cht.sh             code of the cht.sh client
    :bash_completion    bash function for tab completion
    :bash               bash function and tab completion setup
    :fish               fish function and tab completion setup
    :zsh                zsh function and tab completion setup

Editors support:
    :vim                cheat.sh support for Vim
    :emacs              cheat.sh function for Emacs
    :emacs-ivy          cheat.sh function for Emacs (uses ivy)

Other pages:
    :post               how to post new cheat sheet
    :styles             list of color styles
    :styles-demo        show color styles usage examples

Search
To search for a keyword, use the query:
    /~keyword

In this case search is not recursive ‚Äî it is conducted only in a page of the specified level.
For example:
    /~snapshot          look for snapshot in the first level cheat sheets
    /scala/~currying     look for currying in scala cheat sheets

For a recursive search in all cheat sheets, use double slash:
    /~snapshot/r         look for snapshot in all cheat sheets

You can use special search options after the closing slash:
    /~shot/bi           case insensitive (i), word boundaries (b)

List of search options:
    i   case insensitive search
    b   word boundaries
    r   recursive search

Programming languages cheat sheets
Cheat sheets related to programming languages
are organized in namespaces (subdirectories), that are named according
to the programming language.
For each supported programming language
there are several special cheat sheets: its own sheet, hello, :list and :learn.
Say for lua it will look like:
    lua
    lua/hello
    lua/:list
    lua/:learn

Some languages has the one-liners-cheat sheet, 1line:
    perl/1line


hello describes how you can start with the language ‚Äî install it if needed, build and run its programs, and it shows the ""Hello world"" program written in the language;
:list shows all topics related to the language
:learn shows a learn-x-in-minutes language cheat sheet perfect for getting started with the language.
1line is a collection of one-liners in this language
weirdness is a collection of examples of weird things in this language


At the moment, cheat.sh covers the 58 following programming languages (alphabetically sorted):



Prefix
Language
Basics
One-liners
Weirdness
StackOverflow




arduino/
Arduino



‚úì


assembly/
Assembly



‚úì


awk/
AWK
‚úì


‚úì


bash/
Bash
‚úì


‚úì


basic/
BASIC



‚úì


bf/
Brainfuck
‚úì


‚úì


c/
C
‚úì


‚úì


chapel/
Chapel
‚úì


‚úì


clean/
Clean



‚úì


clojure/
Clojure
‚úì


‚úì


coffee/
CoffeeScript
‚úì


‚úì


cpp/
C++
‚úì


‚úì


csharp/
C#
‚úì


‚úì


d/
D
‚úì


‚úì


dart/
Dart
‚úì


‚úì


delphi/
Dephi



‚úì


dylan/
Dylan
‚úì


‚úì


eiffel/
Eiffel



‚úì


elixir/
Elixir
‚úì


‚úì


elisp/
ELisp
‚úì


‚úì


elm/
Elm
‚úì


‚úì


erlang/
Erlang
‚úì


‚úì


factor/
Factor
‚úì


‚úì


fortran/
Fortran
‚úì


‚úì


forth/
Forth
‚úì


‚úì


fsharp/
F#
‚úì


‚úì


go/
Go
‚úì


‚úì


groovy/
Groovy
‚úì


‚úì


haskell/
Haskell
‚úì


‚úì


java/
Java
‚úì


‚úì


js/
JavaScript
‚úì
‚úì
‚úì
‚úì


julia/
Julia
‚úì


‚úì


kotlin/
Kotlin
‚úì


‚úì


latex/
LaTeX
‚úì


‚úì


lisp/
Lisp
‚úì


‚úì


lua/
Lua
‚úì


‚úì


matlab/
MATLAB
‚úì


‚úì


nim/
Nim
‚úì


‚úì


ocaml/
OCaml
‚úì


‚úì


octave/
Octave
‚úì


‚úì


perl/
Perl
‚úì
‚úì

‚úì


perl6/
Perl 6
‚úì
‚úì

‚úì


php/
PHP
‚úì


‚úì


pike/
Pike



‚úì


python/
Python
‚úì
‚úì

‚úì


python3/
Python 3
‚úì


‚úì


r/
R
‚úì


‚úì


racket/
Racket
‚úì


‚úì


ruby/
Ruby
‚úì


‚úì


rust/
Rust
‚úì


‚úì


scala/
Scala
‚úì


‚úì


scheme/
Scheme
‚úì


‚úì


solidity/
Solidity
‚úì


‚úì


swift/
Swift
‚úì


‚úì


tcsh/
Tcsh
‚úì


‚úì


tcl/
Tcl
‚úì


‚úì


objective-c/
Objective-C
‚úì


‚úì


vb/
VisualBasic
‚úì


‚úì


vbnet/
VB.Net
‚úì


‚úì



And several other topics, that are though related to programming,
are not programming languages:



Prefix
Topic
Basics
StackOverflow




cmake/
CMake
‚úì
‚úì


django/
Django

‚úì


flask/
Flask

‚úì


git/
Git
‚úì
‚úì



Cheat sheets sources
Instead of creating yet another mediocre cheat sheet repository,
we are concentrating our efforts on creation of a unified
mechanism to access selected existing well developed and good maintained
cheat sheet repositories covering topics of our interest:
programming and operating systems usage.
cheat.sh uses selected community driven cheat sheet repositories
and information sources, maintained by thousands of users, developers and authors
all over the world
(in the Users column number of contributors/number of stars is shown):



Cheat sheets
Repository
Users
Creation Date




UNIX/Linux, programming
cheat.sheets
38/223
May 1, 2017


UNIX/Linux commands
tldr-pages/tldr
760/23158
Dec 8, 2013


UNIX/Linux commands
chrisallenlane/cheat
131/5240
Jul 28, 2013


Programming languages
adambard/learnxinyminutes-docs
1246/6748
Jun 23, 2013


Go
a8m/go-lang-cheat-sheet
31/4039
Feb 9, 2014


Perl
pkrumnis/perl1line.txt
5/190
Nov 4, 2011


Programming languages
StackOverflow
9M
Sep 15, 2008



Pie diagram reflecting cheat sheets sources distribution (by number of cheat sheets on cheat.sh originating from a repository):

How to contribute
How to edit a cheat sheet
If you want to edit a cheat.sh cheat sheet, you should edit it in the upstream repository.
You will find the name of the source repository in a browser when you open a cheat sheet.
There are two github buttons at the bottom of the page: the second one is the button
of the repository, which belongs the current cheat sheet.
You can edit the cheat sheet directly in your browser (you need a github account for it).
There is an edit button in the top right corner. If you click on it, an editor will be open.
There you will change the cheat sheet (under the hood: the upstrem repository is forked, your changes are
committed in the forked repository, a pull request to the upstream repository owner is sent).

How to add a cheat sheet
If you want to add a cheat sheet, you have one of the following
ways:

Add it to one of the external cheat sheets repositories; you should decide on your own what is the best repository for your cheat sheet;
Add it to the local cheat.sh repository (cheat.sheets) on github (fork, commit, pull request);
Post it on cheat.sh using curl or a web browser (cheat.sh/:post).

If you want to change an existing cheat sheet,
you have to find the original repository (when you open a cheat sheet in a browser,
you see the repository's github button in the bottom of the cheat sheet),
the cheat sheet is coming from, and change it there.
After some time the changes will be synchronized on cheat.sh.
How to add a cheat sheet repository
If you want to add a cheat sheet repository to cheat.sh, please open an issue:

Add a new repository

Please specify the name of the repository, and give its short description.
",GitHub - chubin/cheat.sh: the only cheat sheet you need
17,Python,"




Pythonic Data Structures and Algorithms
Minimal and clean example implementations of data structures and algorithms in Python 3.
Contributing
Thanks for your interest in contributing! There are many ways to contribute to this project. Get started here
Tests
Use unittest
For running all tests write down:
$ python3 -m unittest discover tests

For running some specific tests you can do this as following (Ex: sort):
$ python3 -m unittest tests.test_sort

Use pytest
For running all tests write down:
$ python3 -m pytest tests

Install
If you want to use the API algorithms in your code, it is as simple as:
$ pip3 install algorithms

You can test by creating a python file: (Ex: use merge_sort in sort)
from algorithms.sort import merge_sort

if __name__ == ""__main__"":
    my_list = [1, 8, 3, 5, 6]
    my_list = merge_sort(my_list)
    print(my_list)
Uninstall
If you want to uninstall algorithms, it is as simple as:
$ pip3 uninstall -y algorithms

List of Implementations

arrays

delete_nth
flatten
garage
josephus_problem
limit
longest_non_repeat
max_ones_index
merge_intervals
missing_ranges
plus_one
rotate
summarize_ranges
three_sum
trimmean
top_1
two_sum
move_zeros
n_sum


backtrack

general_solution.md
add_operators
anagram
array_sum_combinations
combination_sum
factor_combinations
generate_abbreviations
generate_parenthesis
letter_combination
palindrome_partitioning
pattern_match
permute
permute_unique
subsets
subsets_unique


bfs

maze_search
shortest_distance_from_all_buildings
word_ladder


bit

add_bitwise_operator
bit_operation
bytes_int_conversion
count_flips_to_convert
count_ones
find_difference
find_missing_number
flip_bit_longest_sequence
power_of_two
reverse_bits
single_number
single_number2
single_number3
subsets
swap_pair
has_alternative_bit
insert_bit
remove_bit
binary_gap


calculator

math_parser


compression

huffman_coding
rle_compression
elias


dfs

all_factors
count_islands
pacific_atlantic
sudoku_solver
walls_and_gates


distribution

histogram


dp

buy_sell_stock
climbing_stairs
coin_change
combination_sum
egg_drop
house_robber
int_divide
job_scheduling
knapsack
longest_increasing
matrix_chain_order
max_product_subarray
max_subarray
min_cost_path
num_decodings
regex_matching
rod_cut
word_break
fibonacci
hosoya triangle


graph

check_bipartite
strongly_connected
clone_graph
cycle_detection
find_all_cliques
find_path
graph
dijkstra
markov_chain
minimum_spanning_tree
satisfiability
tarjan
traversal
bellman_ford


heap

merge_sorted_k_lists
skyline
sliding_window_max
binary_heap


iterables

convolved
k_closest_points


linkedlist

add_two_numbers
copy_random_pointer
delete_node
first_cyclic_node
is_cyclic
is_palindrome
kth_to_last
linkedlist
remove_duplicates
reverse
rotate_list
swap_in_pairs
is_sorted
remove_range


map

hashtable
separate_chaining_hashtable
longest_common_subsequence
randomized_set
valid_sudoku
word_pattern
is_isomorphic
is_anagram


maths

base_conversion
combination
cosine_similarity
decimal_to_binary_ip
euler_totient
extended_gcd
factorial
gcd/lcm
generate_strobogrammtic
is_strobogrammatic
modular_exponential
next_bigger
next_perfect_square
nth_digit
prime_check
primes_sieve_of_eratosthenes
pythagoras
rabin_miller
rsa
sqrt_precision_factor
summing_digits
hailstone

find_order
find_primitive_root




matrix

sudoku_validator
bomb_enemy
copy_transform
count_paths
matrix_rotation.txt
matrix_multiplication
rotate_image
search_in_sorted_matrix
sparse_dot_vector
sparse_mul
spiral_traversal
crout_matrix_decomposition
cholesky_matrix_decomposition


queues

max_sliding_window
moving_average
queue
reconstruct_queue
zigzagiterator


search

binary_search
first_occurrence
last_occurrence
linear_search
search_insert
two_sum
search_range
find_min_rotate
search_rotate
jump_search
next_greatest_letter


set

randomized_set
set_covering
find_keyboard_row


sort

bitonic_sort
bogo_sort
bubble_sort
bucket_sort
cocktail_shaker_sort
comb_sort
counting_sort
cycle_sort
gnome_sort
heap_sort
insertion_sort
meeting_rooms
merge_sort
pancake_sort
quick_sort
radix_sort
selection_sort
shell_sort
sort_colors
top_sort
wiggle_sort


stack

longest_abs_path
simplify_path
stack
valid_parenthesis
stutter
switch_pairs
is_consecutive
remove_min
is_sorted


strings

fizzbuzz
delete_reoccurring
strip_url_params
validate_coordinates
domain_extractor
merge_string_checker
add_binary
breaking_bad
decode_string
encode_decode
group_anagrams
int_to_roman
is_palindrome
license_number
make_sentence
multiply_strings
one_edit_distance
rabin_karp
reverse_string
reverse_vowel
reverse_words
roman_to_int
word_squares
unique_morse
judge_circle
strong_password
caesar_cipher
contain_string
count_binary_substring
repeat_string
min_distance
longest_common_prefix
rotate
first_unique_char
repeat_substring
atbash_cipher


tree

bst

array_to_bst
bst_closest_value
BSTIterator
delete_node
is_bst
kth_smallest
lowest_common_ancestor
predecessor
serialize_deserialize
successor
unique_bst
depth_sum
count_left_node
num_empty
height


red_black_tree

red_black_tree


segment_tree

segment_tree
iterative_segment_tree


traversal

inorder
level_order
postorder
preorder
zigzag


trie

add_and_search
trie


b_tree
binary_tree_paths
bin_tree_to_list
deepest_left
invert_tree
is_balanced
is_subtree
is_symmetric
longest_consecutive
lowest_common_ancestor
max_height
max_path_sum
min_height
path_sum
path_sum2
pretty_print
same_tree
tree


unix

path

join_with_slash
full_path
split
simplify_path




union-find

count_islands



Contributors
Thanks to all the contributors
who helped in building the repo.
",GitHub - keon/algorithms: Minimal examples of data structures and algorithms in Python
18,Python,"
What the f*ck Python! üêç
An interesting collection of surprising snippets and lesser-known Python features.

Translations: Chinese ‰∏≠Êñá
Python, being a beautifully designed high-level and interpreter-based programming language, provides us with many features for the programmer's comfort. But sometimes, the outcomes of a Python snippet may not seem obvious to a regular user at first sight.
Here is a fun project to collect such tricky & counter-intuitive examples and lesser-known features in Python, attempting to discuss what exactly is happening under the hood!
While some of the examples you see below may not be WTFs in the truest sense, but they'll reveal some of the interesting parts of Python that you might be unaware of. I find it a nice way to learn the internals of a programming language, and I think you'll find them interesting as well!
If you're an experienced Python programmer, you can take it as a challenge to get most of them right in first attempt. You may be already familiar with some of these examples, and I might be able to revive sweet old memories of yours being bitten by these gotchas üòÖ
PS: If you're a returning reader, you can learn about the new modifications here.
So, here we go...
Table of Contents

Structure of the Examples
Usage
üëÄ Examples

Section: Strain your brain!

‚ñ∂ Strings can be tricky sometimes *
‚ñ∂ Time for some hash brownies!
‚ñ∂ Return return everywhere!
‚ñ∂ Deep down, we're all the same. *
‚ñ∂ For what?
‚ñ∂ Evaluation time discrepancy
‚ñ∂ is is not what it is!
‚ñ∂ A tic-tac-toe where X wins in the first attempt!
‚ñ∂ The sticky output function
‚ñ∂ is not ... is not is (not ...)
‚ñ∂ The surprising comma
‚ñ∂ Backslashes at the end of string
‚ñ∂ not knot!
‚ñ∂ Half triple-quoted strings
‚ñ∂ Midnight time doesn't exist?
‚ñ∂ What's wrong with booleans?
‚ñ∂ Class attributes and instance attributes
‚ñ∂ yielding None
‚ñ∂ Mutating the immutable!
‚ñ∂ The disappearing variable from outer scope
‚ñ∂ When True is actually False
‚ñ∂ From filled to None in one instruction...
‚ñ∂ Subclass relationships *
‚ñ∂ The mysterious key type conversion *
‚ñ∂ Let's see if you can guess this?


Section: Appearances are deceptive!

‚ñ∂ Skipping lines?
‚ñ∂ Teleportation *
‚ñ∂ Well, something is fishy...


Section: Watch out for the landmines!

‚ñ∂ Modifying a dictionary while iterating over it
‚ñ∂ Stubborn del operator *
‚ñ∂ Deleting a list item while iterating
‚ñ∂ Loop variables leaking out!
‚ñ∂ Beware of default mutable arguments!
‚ñ∂ Catching the Exceptions
‚ñ∂ Same operands, different story!
‚ñ∂ The out of scope variable
‚ñ∂ Be careful with chained operations
‚ñ∂ Name resolution ignoring class scope
‚ñ∂ Needle in a Haystack
‚ñ∂ Yielding from... return!


Section: The Hidden treasures!

‚ñ∂ Okay Python, Can you make me fly? *
‚ñ∂ goto, but why? *
‚ñ∂ Brace yourself! *
‚ñ∂ Let's meet Friendly Language Uncle For Life *
‚ñ∂ Even Python understands that love is complicated *
‚ñ∂ Yes, it exists!
‚ñ∂ Inpinity *
‚ñ∂ Mangling time! *


Section: Miscellaneous

‚ñ∂ += is faster
‚ñ∂ Let's make a giant string!
‚ñ∂ Explicit typecast of strings
‚ñ∂ Minor Ones




Contributing
Acknowledgements
üéì License

Help
Want to share wtfpython with friends?
Need a pdf version?



Structure of the Examples
All the examples are structured like below:

‚ñ∂ Some fancy Title *
The asterisk at the end of the title indicates the example was not present in the first release and has been recently added.
# Setting up the code.
# Preparation for the magic...
Output (Python version):
>>> triggering_statement
Probably unexpected output
(Optional): One line describing the unexpected output.
üí° Explanation:

Brief explanation of what's happening and why is it happening.
Setting up examples for clarification (if necessary)
Output:
>>> trigger # some example that makes it easy to unveil the magic
# some justified output



Note: All the examples are tested on Python 3.5.2 interactive interpreter, and they should work for all the Python versions unless explicitly specified in the description.
Usage
A nice way to get the most out of these examples, in my opinion, will be just to read the examples chronologically, and for every example:

Carefully read the initial code for setting up the example. If you're an experienced Python programmer, most of the times you will successfully anticipate what's going to happen next.
Read the output snippets and,

Check if the outputs are the same as you'd expect.
Make sure if you know the exact reason behind the output being the way it is.

If no, take a deep breath, and read the explanation (and if you still don't understand, shout out! and create an issue here).
If yes, give a gentle pat on your back, and you may skip to the next example.





PS: You can also read WTFpython at the command line. There's a pypi package and an npm package (supports colored formatting) for the same.
To install the npm package wtfpython
$ npm install -g wtfpython
Alternatively, to install the pypi package wtfpython
$ pip install wtfpython -U
Now, just run wtfpython at the command line which will open this collection in your selected $PAGER.

üëÄ Examples
Section: Strain your brain!
‚ñ∂ Strings can be tricky sometimes *
1.
>>> a = ""some_string""
>>> id(a)
140420665652016
>>> id(""some"" + ""_"" + ""string"") # Notice that both the ids are same.
140420665652016
2.
>>> a = ""wtf""
>>> b = ""wtf""
>>> a is b
True

>>> a = ""wtf!""
>>> b = ""wtf!""
>>> a is b
False

>>> a, b = ""wtf!"", ""wtf!""
>>> a is b
True
3.
>>> 'a' * 20 is 'aaaaaaaaaaaaaaaaaaaa'
True
>>> 'a' * 21 is 'aaaaaaaaaaaaaaaaaaaaa'
False
Makes sense, right?
üí° Explanation:

Such behavior is due to CPython optimization (called string interning) that tries to use existing immutable objects in some cases rather than creating a new object every time.
After being interned, many variables may point to the same string object in memory (thereby saving memory).
In the snippets above, strings are implicitly interned. The decision of when to implicitly intern a string is implementation dependent. There are some facts that can be used to guess if a string will be interned or not:

All length 0 and length 1 strings are interned.
Strings are interned at compile time ('wtf' will be interned but ''.join(['w', 't', 'f'] will not be interned)
Strings that are not composed of ASCII letters, digits or underscores, are not interned. This explains why 'wtf!' was not interned due to !. Cpython implementation of this rule can be found here



When a and b are set to ""wtf!"" in the same line, the Python interpreter creates a new object, then references the second variable at the same time. If you do it on separate lines, it doesn't ""know"" that there's already wtf! as an object (because ""wtf!"" is not implicitly interned as per the facts mentioned above). It's a compiler optimization and specifically applies to the interactive environment.
Constant folding is a technique for peephole optimization in Python. This means the expression 'a'*20 is replaced by 'aaaaaaaaaaaaaaaaaaaa' during compilation to reduce few clock cycles during runtime. Constant folding only occurs for strings having length less than or equal to 20. (Why? Imagine the size of .pyc file generated as a result of the expression 'a'*10**10). Here's the implementation source for the same.


‚ñ∂ Time for some hash brownies!
1.
some_dict = {}
some_dict[5.5] = ""Ruby""
some_dict[5.0] = ""JavaScript""
some_dict[5] = ""Python""
Output:
>>> some_dict[5.5]
""Ruby""
>>> some_dict[5.0]
""Python""
>>> some_dict[5]
""Python""
""Python"" destroyed the existence of ""JavaScript""?
üí° Explanation

Python dictionaries check for equality and compare the hash value to determine if two keys are the same.
Immutable objects with same value always have the same hash in Python.
>>> 5 == 5.0
True
>>> hash(5) == hash(5.0)
True
Note: Objects with different values may also have same hash (known as hash collision).
When the statement some_dict[5] = ""Python"" is executed, the existing value ""JavaScript"" is overwritten with ""Python"" because Python recognizes 5 and 5.0 as the same keys of the dictionary some_dict.
This StackOverflow answer explains beautifully the rationale behind it.


‚ñ∂ Return return everywhere!
def some_func():
    try:
        return 'from_try'
    finally:
        return 'from_finally'
Output:
>>> some_func()
'from_finally'
üí° Explanation:

When a return, break or continue statement is executed in the try suite of a ""try‚Ä¶finally"" statement, the finally clause is also executed ‚Äòon the way out.
The return value of a function is determined by the last return statement executed. Since the finally clause always executes, a return statement executed in the finally clause will always be the last one executed.


‚ñ∂ Deep down, we're all the same. *
class WTF:
  pass
Output:
>>> WTF() == WTF() # two different instances can't be equal
False
>>> WTF() is WTF() # identities are also different
False
>>> hash(WTF()) == hash(WTF()) # hashes _should_ be different as well
True
>>> id(WTF()) == id(WTF())
True
üí° Explanation:


When id was called, Python created a WTF class object and passed it to the id function. The id function takes its id (its memory location), and throws away the object. The object is destroyed.


When we do this twice in succession, Python allocates the same memory location to this second object as well. Since (in CPython) id uses the memory location as the object id, the id of the two objects is the same.


So, object's id is unique only for the lifetime of the object. After the object is destroyed, or before it is created, something else can have the same id.


But why did the is operator evaluated to False? Let's see with this snippet.
class WTF(object):
  def __init__(self): print(""I"")
  def __del__(self): print(""D"")
Output:
>>> WTF() is WTF()
I
I
D
D
False
>>> id(WTF()) == id(WTF())
I
D
I
D
True
As you may observe, the order in which the objects are destroyed is what made all the difference here.



‚ñ∂ For what?
some_string = ""wtf""
some_dict = {}
for i, some_dict[i] in enumerate(some_string):
    pass
Output:
>>> some_dict # An indexed dict is created.
{0: 'w', 1: 't', 2: 'f'}
üí° Explanation:


A for statement is defined in the Python grammar as:
for_stmt: 'for' exprlist 'in' testlist ':' suite ['else' ':' suite]

Where exprlist is the assignment target. This means that the equivalent of {exprlist} = {next_value} is executed for each item in the iterable.
An interesting example that illustrates this:
for i in range(4):
    print(i)
    i = 10
Output:
0
1
2
3

Did you expect the loop to run just once?
üí° Explanation:

The assignment statement i = 10 never affects the iterations of the loop because of the way for loops work in Python. Before the beginning of every iteration, the next item provided by the iterator (range(4) this case) is unpacked and assigned the target list variables (i in this case).



The enumerate(some_string) function yields a new value i (A counter going up) and a character from the some_string in each iteration. It then sets the (just assigned) i key of the dictionary some_dict to that character. The unrolling of the loop can be simplified as:
>>> i, some_dict[i] = (0, 'w')
>>> i, some_dict[i] = (1, 't')
>>> i, some_dict[i] = (2, 'f')
>>> some_dict



‚ñ∂ Evaluation time discrepancy
1.
array = [1, 8, 15]
g = (x for x in array if array.count(x) > 0)
array = [2, 8, 22]
Output:
>>> print(list(g))
[8]
2.
array_1 = [1,2,3,4]
g1 = (x for x in array_1)
array_1 = [1,2,3,4,5]

array_2 = [1,2,3,4]
g2 = (x for x in array_2)
array_2[:] = [1,2,3,4,5]
Output:
>>> print(list(g1))
[1,2,3,4]

>>> print(list(g2))
[1,2,3,4,5]
üí° Explanation

In a generator expression, the in clause is evaluated at declaration time, but the conditional clause is evaluated at runtime.
So before runtime, array is re-assigned to the list [2, 8, 22], and since out of 1, 8 and 15, only the count of 8 is greater than 0, the generator only yields 8.
The differences in the output of g1 and g2 in the second part is due the way variables array_1 and array_2 are re-assigned values.
In the first case, array_1 is binded to the new object [1,2,3,4,5] and since the in clause is evaluated at the declaration time it still refers to the old object [1,2,3,4] (which is not destroyed).
In the second case, the slice assignment to array_2 updates the same old object [1,2,3,4] to [1,2,3,4,5]. Hence both the g2 and array_2 still have reference to the same object (which has now been updated to [1,2,3,4,5]).


‚ñ∂ is is not what it is!
The following is a very famous example present all over the internet.
>>> a = 256
>>> b = 256
>>> a is b
True

>>> a = 257
>>> b = 257
>>> a is b
False

>>> a = 257; b = 257
>>> a is b
True
üí° Explanation:
The difference between is and ==

is operator checks if both the operands refer to the same object (i.e., it checks if the identity of the operands matches or not).
== operator compares the values of both the operands and checks if they are the same.
So is is for reference equality and == is for value equality. An example to clear things up,
>>> [] == []
True
>>> [] is [] # These are two empty lists at two different memory locations.
False


256 is an existing object but 257 isn't
When you start up python the numbers from -5 to 256 will be allocated. These numbers are used a lot, so it makes sense just to have them ready.
Quoting from https://docs.python.org/3/c-api/long.html

The current implementation keeps an array of integer objects for all integers between -5 and 256, when you create an int in that range you just get back a reference to the existing object. So it should be possible to change the value of 1. I suspect the behavior of Python, in this case, is undefined. :-)

>>> id(256)
10922528
>>> a = 256
>>> b = 256
>>> id(a)
10922528
>>> id(b)
10922528
>>> id(257)
140084850247312
>>> x = 257
>>> y = 257
>>> id(x)
140084850247440
>>> id(y)
140084850247344
Here the interpreter isn't smart enough while executing y = 257 to recognize that we've already created an integer of the value 257, and so it goes on to create another object in the memory.
Both a and b refer to the same object when initialized with same value in the same line.
>>> a, b = 257, 257
>>> id(a)
140640774013296
>>> id(b)
140640774013296
>>> a = 257
>>> b = 257
>>> id(a)
140640774013392
>>> id(b)
140640774013488

When a and b are set to 257 in the same line, the Python interpreter creates a new object, then references the second variable at the same time. If you do it on separate lines, it doesn't ""know"" that there's already 257 as an object.
It's a compiler optimization and specifically applies to the interactive environment. When you enter two lines in a live interpreter, they're compiled separately, therefore optimized separately. If you were to try this example in a .py file, you would not see the same behavior, because the file is compiled all at once.


‚ñ∂ A tic-tac-toe where X wins in the first attempt!
# Let's initialize a row
row = [""""]*3 #row i['', '', '']
# Let's make a board
board = [row]*3
Output:
>>> board
[['', '', ''], ['', '', ''], ['', '', '']]
>>> board[0]
['', '', '']
>>> board[0][0]
''
>>> board[0][0] = ""X""
>>> board
[['X', '', ''], ['X', '', ''], ['X', '', '']]
We didn't assign 3 ""X""s or did we?
üí° Explanation:
When we initialize row variable, this visualization explains what happens in the memory

And when the board is initialized by multiplying the row, this is what happens inside the memory (each of the elements board[0], board[1] and board[2] is a reference to the same list referred by row)

We can avoid this scenario here by not using row variable to generate board. (Asked in this issue).
>>> board = [['']*3 for _ in range(3)]
>>> board[0][0] = ""X""
>>> board
[['X', '', ''], ['', '', ''], ['', '', '']]

‚ñ∂ The sticky output function
funcs = []
results = []
for x in range(7):
    def some_func():
        return x
    funcs.append(some_func)
    results.append(some_func())  # note the function call here

funcs_results = [func() for func in funcs]
Output:
>>> results
[0, 1, 2, 3, 4, 5, 6]
>>> funcs_results
[6, 6, 6, 6, 6, 6, 6]
Even when the values of x were different in every iteration prior to appending some_func to funcs, all the functions return 6.
//OR
>>> powers_of_x = [lambda x: x**i for i in range(10)]
>>> [f(2) for f in powers_of_x]
[512, 512, 512, 512, 512, 512, 512, 512, 512, 512]
üí° Explanation


When defining a function inside a loop that uses the loop variable in its body, the loop function's closure is bound to the variable, not its value. So all of the functions use the latest value assigned to the variable for computation.


To get the desired behavior you can pass in the loop variable as a named variable to the function. Why this works? Because this will define the variable again within the function's scope.
funcs = []
for x in range(7):
    def some_func(x=x):
        return x
    funcs.append(some_func)
Output:
>>> funcs_results = [func() for func in funcs]
>>> funcs_results
[0, 1, 2, 3, 4, 5, 6]



‚ñ∂ is not ... is not is (not ...)
>>> 'something' is not None
True
>>> 'something' is (not None)
False
üí° Explanation

is not is a single binary operator, and has behavior different than using is and not separated.
is not evaluates to False if the variables on either side of the operator point to the same object and True otherwise.


‚ñ∂ The surprising comma
Output:
>>> def f(x, y,):
...     print(x, y)
...
>>> def g(x=4, y=5,):
...     print(x, y)
...
>>> def h(x, **kwargs,):
  File ""<stdin>"", line 1
    def h(x, **kwargs,):
                     ^
SyntaxError: invalid syntax
>>> def h(*args,):
  File ""<stdin>"", line 1
    def h(*args,):
                ^
SyntaxError: invalid syntax
üí° Explanation:

Trailing comma is not always legal in formal parameters list of a Python function.
In Python, the argument list is defined partially with leading commas and partially with trailing commas. This conflict causes situations where a comma is trapped in the middle, and no rule accepts it.
Note: The trailing comma problem is fixed in Python 3.6. The remarks in this post discuss in brief different usages of trailing commas in Python.


‚ñ∂ Backslashes at the end of string
Output:
>>> print(""\\ C:\\"")
\ C:\
>>> print(r""\ C:"")
\ C:
>>> print(r""\ C:\"")

    File ""<stdin>"", line 1
      print(r""\ C:\"")
                     ^
SyntaxError: EOL while scanning string literal

üí° Explanation

In a raw string literal, as indicated by the prefix r, the backslash doesn't have the special meaning.
>>> print(repr(r""wt\""f""))
'wt\\""f'

What the interpreter actually does, though, is simply change the behavior of backslashes, so they pass themselves and the following character through. That's why backslashes don't work at the end of a raw string.


‚ñ∂ not knot!
x = True
y = False
Output:
>>> not x == y
True
>>> x == not y
  File ""<input>"", line 1
    x == not y
           ^
SyntaxError: invalid syntax
üí° Explanation:

Operator precedence affects how an expression is evaluated, and == operator has higher precedence than not operator in Python.
So not x == y is equivalent to not (x == y) which is equivalent to not (True == False) finally evaluating to True.
But x == not y raises a SyntaxError because it can be thought of being equivalent to (x == not) y and not x == (not y) which you might have expected at first sight.
The parser expected the not token to be a part of the not in operator (because both == and not in operators have the same precedence), but after not being able to find an in token following the not token, it raises a SyntaxError.


‚ñ∂ Half triple-quoted strings
Output:
>>> print('wtfpython''')
wtfpython
>>> print(""wtfpython"""""")
wtfpython
>>> # The following statements raise `SyntaxError`
>>> # print('''wtfpython')
>>> # print(""""""wtfpython"")
üí° Explanation:

Python supports implicit string literal concatenation, Example,
>>> print(""wtf"" ""python"")
wtfpython
>>> print(""wtf"" """") # or ""wtf""""""
wtf


''' and """""" are also string delimiters in Python which causes a SyntaxError because the Python interpreter was expecting a terminating triple quote as delimiter while scanning the currently encountered triple quoted string literal.


‚ñ∂ Midnight time doesn't exist?
from datetime import datetime

midnight = datetime(2018, 1, 1, 0, 0)
midnight_time = midnight.time()

noon = datetime(2018, 1, 1, 12, 0)
noon_time = noon.time()

if midnight_time:
    print(""Time at midnight is"", midnight_time)

if noon_time:
    print(""Time at noon is"", noon_time)
Output:
('Time at noon is', datetime.time(12, 0))
The midnight time is not printed.
üí° Explanation:
Before Python 3.5, the boolean value for datetime.time object was considered to be False if it represented midnight in UTC. It is error-prone when using the if obj: syntax to check if the obj is null or some equivalent of ""empty.""

‚ñ∂ What's wrong with booleans?
1.
# A simple example to count the number of boolean and
# integers in an iterable of mixed data types.
mixed_list = [False, 1.0, ""some_string"", 3, True, [], False]
integers_found_so_far = 0
booleans_found_so_far = 0

for item in mixed_list:
    if isinstance(item, int):
        integers_found_so_far += 1
    elif isinstance(item, bool):
        booleans_found_so_far += 1
Output:
>>> integers_found_so_far
4
>>> booleans_found_so_far
0
2.
another_dict = {}
another_dict[True] = ""JavaScript""
another_dict[1] = ""Ruby""
another_dict[1.0] = ""Python""
Output:
>>> another_dict[True]
""Python""
3.
>>> some_bool = True
>>> ""wtf""*some_bool
'wtf'
>>> some_bool = False
>>> ""wtf""*some_bool
''
üí° Explanation:


Booleans are a subclass of int
>>> isinstance(True, int)
True
>>> isinstance(False, int)
True


The integer value of True is 1 and that of False is 0.
>>> True == 1 == 1.0 and False == 0 == 0.0
True


See this StackOverflow answer for the rationale behind it.



‚ñ∂ Class attributes and instance attributes
1.
class A:
    x = 1

class B(A):
    pass

class C(A):
    pass
Output:
>>> A.x, B.x, C.x
(1, 1, 1)
>>> B.x = 2
>>> A.x, B.x, C.x
(1, 2, 1)
>>> A.x = 3
>>> A.x, B.x, C.x
(3, 2, 3)
>>> a = A()
>>> a.x, A.x
(3, 3)
>>> a.x += 1
>>> a.x, A.x
(4, 3)
2.
class SomeClass:
    some_var = 15
    some_list = [5]
    another_list = [5]
    def __init__(self, x):
        self.some_var = x + 1
        self.some_list = self.some_list + [x]
        self.another_list += [x]
Output:
>>> some_obj = SomeClass(420)
>>> some_obj.some_list
[5, 420]
>>> some_obj.another_list
[5, 420]
>>> another_obj = SomeClass(111)
>>> another_obj.some_list
[5, 111]
>>> another_obj.another_list
[5, 420, 111]
>>> another_obj.another_list is SomeClass.another_list
True
>>> another_obj.another_list is some_obj.another_list
True
üí° Explanation:

Class variables and variables in class instances are internally handled as dictionaries of a class object. If a variable name is not found in the dictionary of the current class, the parent classes are searched for it.
The += operator modifies the mutable object in-place without creating a new object. So changing the attribute of one instance affects the other instances and the class attribute as well.


‚ñ∂ yielding None
some_iterable = ('a', 'b')

def some_func(val):
    return ""something""
Output:
>>> [x for x in some_iterable]
['a', 'b']
>>> [(yield x) for x in some_iterable]
<generator object <listcomp> at 0x7f70b0a4ad58>
>>> list([(yield x) for x in some_iterable])
['a', 'b']
>>> list((yield x) for x in some_iterable)
['a', None, 'b', None]
>>> list(some_func((yield x)) for x in some_iterable)
['a', 'something', 'b', 'something']
üí° Explanation:

Source and explanation can be found here: https://stackoverflow.com/questions/32139885/yield-in-list-comprehensions-and-generator-expressions
Related bug report: http://bugs.python.org/issue10544


‚ñ∂ Mutating the immutable!
some_tuple = (""A"", ""tuple"", ""with"", ""values"")
another_tuple = ([1, 2], [3, 4], [5, 6])
Output:
>>> some_tuple[2] = ""change this""
TypeError: 'tuple' object does not support item assignment
>>> another_tuple[2].append(1000) #This throws no error
>>> another_tuple
([1, 2], [3, 4], [5, 6, 1000])
>>> another_tuple[2] += [99, 999]
TypeError: 'tuple' object does not support item assignment
>>> another_tuple
([1, 2], [3, 4], [5, 6, 1000, 99, 999])
But I thought tuples were immutable...
üí° Explanation:


Quoting from https://docs.python.org/2/reference/datamodel.html

Immutable sequences
An object of an immutable sequence type cannot change once it is created. (If the object contains references to other objects, these other objects may be mutable and may be modified; however, the collection of objects directly referenced by an immutable object cannot change.)



+= operator changes the list in-place. The item assignment doesn't work, but when the exception occurs, the item has already been changed in place.



‚ñ∂ The disappearing variable from outer scope
e = 7
try:
    raise Exception()
except Exception as e:
    pass
Output (Python 2.x):
>>> print(e)
# prints nothing
Output (Python 3.x):
>>> print(e)
NameError: name 'e' is not defined
üí° Explanation:


Source: https://docs.python.org/3/reference/compound_stmts.html#except
When an exception has been assigned using as target, it is cleared at the end of the except clause. This is as if
except E as N:
    foo
was translated into
except E as N:
    try:
        foo
    finally:
        del N
This means the exception must be assigned to a different name to be able to refer to it after the except clause. Exceptions are cleared because, with the traceback attached to them, they form a reference cycle with the stack frame, keeping all locals in that frame alive until the next garbage collection occurs.


The clauses are not scoped in Python. Everything in the example is present in the same scope, and the variable e got removed due to the execution of the except clause. The same is not the case with functions which have their separate inner-scopes. The example below illustrates this:
def f(x):
    del(x)
    print(x)

x = 5
y = [5, 4, 3]
Output:
>>>f(x)
UnboundLocalError: local variable 'x' referenced before assignment
>>>f(y)
UnboundLocalError: local variable 'x' referenced before assignment
>>> x
5
>>> y
[5, 4, 3]


In Python 2.x the variable name e gets assigned to Exception() instance, so when you try to print, it prints nothing.
Output (Python 2.x):
>>> e
Exception()
>>> print e
# Nothing is printed!



‚ñ∂ When True is actually False
True = False
if True == False:
    print(""I've lost faith in truth!"")
Output:
I've lost faith in truth!

üí° Explanation:

Initially, Python used to have no bool type (people used 0 for false and non-zero value like 1 for true). Then they added True, False, and a bool type, but, for backward compatibility, they couldn't make True and False constants- they just were built-in variables.
Python 3 was backward-incompatible, so it was now finally possible to fix that, and so this example won't work with Python 3.x!


‚ñ∂ From filled to None in one instruction...
some_list = [1, 2, 3]
some_dict = {
  ""key_1"": 1,
  ""key_2"": 2,
  ""key_3"": 3
}

some_list = some_list.append(4)
some_dict = some_dict.update({""key_4"": 4})
Output:
>>> print(some_list)
None
>>> print(some_dict)
None
üí° Explanation
Most methods that modify the items of sequence/mapping objects like list.append, dict.update, list.sort, etc. modify the objects in-place and return None. The rationale behind this is to improve performance by avoiding making a copy of the object if the operation can be done in-place (Referred from here)

‚ñ∂ Subclass relationships *
Output:
>>> from collections import Hashable
>>> issubclass(list, object)
True
>>> issubclass(object, Hashable)
True
>>> issubclass(list, Hashable)
False
The Subclass relationships were expected to be transitive, right? (i.e., if A is a subclass of B, and B is a subclass of C, the A should a subclass of C)
üí° Explanation:

Subclass relationships are not necessarily transitive in Python. Anyone is allowed to define their own, arbitrary __subclasscheck__ in a metaclass.
When issubclass(cls, Hashable) is called, it simply looks for non-Falsey ""__hash__"" method in cls or anything it inherits from.
Since object is hashable, but list is non-hashable, it breaks the transitivity relation.
More detailed explanation can be found here.


‚ñ∂ The mysterious key type conversion *
class SomeClass(str):
    pass

some_dict = {'s':42}
Output:
>>> type(list(some_dict.keys())[0])
str
>>> s = SomeClass('s')
>>> some_dict[s] = 40
>>> some_dict # expected: Two different keys-value pairs
{'s': 40}
>>> type(list(some_dict.keys())[0])
str
üí° Explanation:


Both the object s and the string ""s"" hash to the same value because SomeClass inherits the __hash__ method of str class.


SomeClass(""s"") == ""s"" evaluates to True because SomeClass also inherits __eq__ method from str class.


Since both the objects hash to the same value and are equal, they are represented by the same key in the dictionary.


For the desired behavior, we can redefine the __eq__ method in SomeClass
class SomeClass(str):
  def __eq__(self, other):
      return (
          type(self) is SomeClass
          and type(other) is SomeClass
          and super().__eq__(other)
      )

  # When we define a custom __eq__, Python stops automatically inheriting the
  # __hash__ method, so we need to define it as well
  __hash__ = str.__hash__

some_dict = {'s':42}
Output:
>>> s = SomeClass('s')
>>> some_dict[s] = 40
>>> some_dict
{'s': 40, 's': 42}
>>> keys = list(some_dict.keys())
>>> type(keys[0]), type(keys[1])
(__main__.SomeClass, str)



‚ñ∂ Let's see if you can guess this?
a, b = a[b] = {}, 5
Output:
>>> a
{5: ({...}, 5)}
üí° Explanation:


According to Python language reference, assignment statements have the form
(target_list ""="")+ (expression_list | yield_expression)

and

An assignment statement evaluates the expression list (remember that this can be a single expression or a comma-separated list, the latter yielding a tuple) and assigns the single resulting object to each of the target lists, from left to right.



The + in (target_list ""="")+ means there can be one or more target lists. In this case, target lists are a, b and a[b] (note the expression list is exactly one, which in our case is {}, 5).


After the expression list is evaluated, it's value is unpacked to the target lists from left to right. So, in our case, first the {}, 5 tuple is unpacked to a, b and we now have a = {} and b = 5.


a is now assigned to {} which is a mutable object.


The second target list is a[b] (you may expect this to throw an error because both a and b have not been defined in the statements before. But remember, we just assigned a to {} and b to 5).


Now, we are setting the key 5 in the dictionary to the tuple ({}, 5) creating a circular reference (the {...} in the output refers to the same object that a is already referencing). Another simpler example of circular reference could be
>>> some_list = some_list[0] = [0]
>>> some_list
[[...]]
>>> some_list[0]
[[...]]
>>> some_list is some_list[0]
True
>>> some_list[0][0][0][0][0][0] == some_list
True
Similar is the case in our example (a[b][0] is the same object as a)


So to sum it up, you can break the example down to
a, b = {}, 5
a[b] = a, b
And the circular reference can be justified by the fact that a[b][0] is the same object as a
>>> a[b][0] is a
True




Section: Appearances are deceptive!
‚ñ∂ Skipping lines?
Output:
>>> value = 11
>>> valu–µ = 32
>>> value
11
Wut?
Note: The easiest way to reproduce this is to simply copy the statements from the above snippet and paste them into your file/shell.
üí° Explanation
Some non-Western characters look identical to letters in the English alphabet but are considered distinct by the interpreter.
>>> ord('–µ') # cyrillic 'e' (Ye)
1077
>>> ord('e') # latin 'e', as used in English and typed using standard keyboard
101
>>> '–µ' == 'e'
False

>>> value = 42 # latin e
>>> valu–µ = 23 # cyrillic 'e', Python 2.x interpreter would raise a `SyntaxError` here
>>> value
42
The built-in ord() function returns a character's Unicode code point, and different code positions of Cyrillic 'e' and Latin 'e' justify the behavior of the above example.

‚ñ∂ Teleportation *
import numpy as np

def energy_send(x):
    # Initializing a numpy array
    np.array([float(x)])

def energy_receive():
    # Return an empty numpy array
    return np.empty((), dtype=np.float).tolist()
Output:
>>> energy_send(123.456)
>>> energy_receive()
123.456
Where's the Nobel Prize?
üí° Explanation:

Notice that the numpy array created in the energy_send function is not returned, so that memory space is free to reallocate.
numpy.empty() returns the next free memory slot without reinitializing it. This memory spot just happens to be the same one that was just freed (usually, but not always).


‚ñ∂ Well, something is fishy...
def square(x):
    """"""
    A simple function to calculate the square of a number by addition.
    """"""
    sum_so_far = 0
    for counter in range(x):
        sum_so_far = sum_so_far + x
  return sum_so_far
Output (Python 2.x):
>>> square(10)
10
Shouldn't that be 100?
Note: If you're not able to reproduce this, try running the file mixed_tabs_and_spaces.py via the shell.
üí° Explanation


Don't mix tabs and spaces! The character just preceding return is a ""tab"",  and the code is indented by multiple of ""4 spaces"" elsewhere in the example.


This is how Python handles tabs:

First, tabs are replaced (from left to right) by one to eight spaces such that the total number of characters up to and including the replacement is a multiple of eight <...>



So the ""tab"" at the last line of square function is replaced with eight spaces, and it gets into the loop.


Python 3 is kind enough to throw an error for such cases automatically.
Output (Python 3.x):
TabError: inconsistent use of tabs and spaces in indentation




Section: Watch out for the landmines!
‚ñ∂ Modifying a dictionary while iterating over it
x = {0: None}

for i in x:
    del x[i]
    x[i+1] = None
    print(i)
Output (Python 2.7- Python 3.5):
0
1
2
3
4
5
6
7

Yes, it runs for exactly eight times and stops.
üí° Explanation:

Iteration over a dictionary that you edit at the same time is not supported.
It runs eight times because that's the point at which the dictionary resizes to hold more keys (we have eight deletion entries, so a resize is needed). This is actually an implementation detail.
How deleted keys are handled and when the resize occurs might be different for different Python implementations.
For more information, you may refer to this StackOverflow thread explaining a similar example in detail.


‚ñ∂ Stubborn del operator *
class SomeClass:
    def __del__(self):
        print(""Deleted!"")
Output:
1.
>>> x = SomeClass()
>>> y = x
>>> del x # this should print ""Deleted!""
>>> del y
Deleted!
Phew, deleted at last. You might have guessed what saved from __del__ being called in our first attempt to delete x. Let's add more twist to the example.
2.
>>> x = SomeClass()
>>> y = x
>>> del x
>>> y # check if y exists
<__main__.SomeClass instance at 0x7f98a1a67fc8>
>>> del y # Like previously, this should print ""Deleted!""
>>> globals() # oh, it didn't. Let's check all our global variables and confirm
Deleted!
{'__builtins__': <module '__builtin__' (built-in)>, 'SomeClass': <class __main__.SomeClass at 0x7f98a1a5f668>, '__package__': None, '__name__': '__main__', '__doc__': None}
Okay, now it's deleted üòï
üí° Explanation:

del x doesn‚Äôt directly call x.__del__().
Whenever del x is encountered, Python decrements the reference count for x by one, and x.__del__() when x‚Äôs reference count reaches zero.
In the second output snippet, y.__del__() was not called because the previous statement (>>> y) in the interactive interpreter created another reference to the same object, thus preventing the reference count to reach zero when del y was encountered.
Calling globals caused the existing reference to be destroyed and hence we can see ""Deleted!"" being printed (finally!).


‚ñ∂ Deleting a list item while iterating
list_1 = [1, 2, 3, 4]
list_2 = [1, 2, 3, 4]
list_3 = [1, 2, 3, 4]
list_4 = [1, 2, 3, 4]

for idx, item in enumerate(list_1):
    del item

for idx, item in enumerate(list_2):
    list_2.remove(item)

for idx, item in enumerate(list_3[:]):
    list_3.remove(item)

for idx, item in enumerate(list_4):
    list_4.pop(idx)
Output:
>>> list_1
[1, 2, 3, 4]
>>> list_2
[2, 4]
>>> list_3
[]
>>> list_4
[2, 4]
Can you guess why the output is [2, 4]?
üí° Explanation:


It's never a good idea to change the object you're iterating over. The correct way to do so is to iterate over a copy of the object instead, and list_3[:] does just that.
>>> some_list = [1, 2, 3, 4]
>>> id(some_list)
139798789457608
>>> id(some_list[:]) # Notice that python creates new object for sliced list.
139798779601192


Difference between del, remove, and pop:

del var_name just removes the binding of the var_name from the local or global namespace (That's why the list_1 is unaffected).
remove removes the first matching value, not a specific index, raises ValueError if the value is not found.
pop removes the element at a specific index and returns it, raises IndexError if an invalid index is specified.

Why the output is [2, 4]?

The list iteration is done index by index, and when we remove 1 from list_2 or list_4, the contents of the lists are now [2, 3, 4]. The remaining elements are shifted down, i.e., 2 is at index 0, and 3 is at index 1. Since the next iteration is going to look at index 1 (which is the 3), the 2 gets skipped entirely. A similar thing will happen with every alternate element in the list sequence.


Refer to this StackOverflow thread explaining the example
See also this nice StackOverflow thread for a similar example related to dictionaries in Python.


‚ñ∂ Loop variables leaking out!
1.
for x in range(7):
    if x == 6:
        print(x, ': for x inside loop')
print(x, ': x in global')
Output:
6 : for x inside loop
6 : x in global
But x was never defined outside the scope of for loop...
2.
# This time let's initialize x first
x = -1
for x in range(7):
    if x == 6:
        print(x, ': for x inside loop')
print(x, ': x in global')
Output:
6 : for x inside loop
6 : x in global
3.
x = 1
print([x for x in range(5)])
print(x, ': x in global')

Output (on Python 2.x):
[0, 1, 2, 3, 4]
(4, ': x in global')

Output (on Python 3.x):
[0, 1, 2, 3, 4]
1 : x in global

üí° Explanation:


In Python, for-loops use the scope they exist in and leave their defined loop-variable behind. This also applies if we explicitly defined the for-loop variable in the global namespace before. In this case, it will rebind the existing variable.


The differences in the output of Python 2.x and Python 3.x interpreters for list comprehension example can be explained by following change documented in What‚Äôs New In Python 3.0 documentation:

""List comprehensions no longer support the syntactic form [... for var in item1, item2, ...]. Use [... for var in (item1, item2, ...)] instead. Also, note that list comprehensions have different semantics: they are closer to syntactic sugar for a generator expression inside a list() constructor, and in particular the loop control variables are no longer leaked into the surrounding scope.""




‚ñ∂ Beware of default mutable arguments!
def some_func(default_arg=[]):
    default_arg.append(""some_string"")
    return default_arg
Output:
>>> some_func()
['some_string']
>>> some_func()
['some_string', 'some_string']
>>> some_func([])
['some_string']
>>> some_func()
['some_string', 'some_string', 'some_string']
üí° Explanation:


The default mutable arguments of functions in Python aren't really initialized every time you call the function. Instead, the recently assigned value to them is used as the default value. When we explicitly passed [] to some_func as the argument, the default value of the default_arg variable was not used, so the function returned as expected.
def some_func(default_arg=[]):
    default_arg.append(""some_string"")
    return default_arg
Output:
>>> some_func.__defaults__ #This will show the default argument values for the function
([],)
>>> some_func()
>>> some_func.__defaults__
(['some_string'],)
>>> some_func()
>>> some_func.__defaults__
(['some_string', 'some_string'],)
>>> some_func([])
>>> some_func.__defaults__
(['some_string', 'some_string'],)


A common practice to avoid bugs due to mutable arguments is to assign None as the default value and later check if any value is passed to the function corresponding to that argument. Example:
def some_func(default_arg=None):
    if not default_arg:
        default_arg = []
    default_arg.append(""some_string"")
    return default_arg



‚ñ∂ Catching the Exceptions
some_list = [1, 2, 3]
try:
    # This should raise an ``IndexError``
    print(some_list[4])
except IndexError, ValueError:
    print(""Caught!"")

try:
    # This should raise a ``ValueError``
    some_list.remove(4)
except IndexError, ValueError:
    print(""Caught again!"")
Output (Python 2.x):
Caught!

ValueError: list.remove(x): x not in list
Output (Python 3.x):
  File ""<input>"", line 3
    except IndexError, ValueError:
                     ^
SyntaxError: invalid syntax
üí° Explanation


To add multiple Exceptions to the except clause, you need to pass them as parenthesized tuple as the first argument. The second argument is an optional name, which when supplied will bind the Exception instance that has been raised. Example,
some_list = [1, 2, 3]
try:
   # This should raise a ``ValueError``
   some_list.remove(4)
except (IndexError, ValueError), e:
   print(""Caught again!"")
   print(e)
Output (Python 2.x):
Caught again!
list.remove(x): x not in list

Output (Python 3.x):
  File ""<input>"", line 4
    except (IndexError, ValueError), e:
                                     ^
IndentationError: unindent does not match any outer indentation level


Separating the exception from the variable with a comma is deprecated and does not work in Python 3; the correct way is to use as. Example,
some_list = [1, 2, 3]
try:
    some_list.remove(4)

except (IndexError, ValueError) as e:
    print(""Caught again!"")
    print(e)
Output:
Caught again!
list.remove(x): x not in list




‚ñ∂ Same operands, different story!
1.
a = [1, 2, 3, 4]
b = a
a = a + [5, 6, 7, 8]
Output:
>>> a
[1, 2, 3, 4, 5, 6, 7, 8]
>>> b
[1, 2, 3, 4]
2.
a = [1, 2, 3, 4]
b = a
a += [5, 6, 7, 8]
Output:
>>> a
[1, 2, 3, 4, 5, 6, 7, 8]
>>> b
[1, 2, 3, 4, 5, 6, 7, 8]
üí° Explanation:


a += b doesn't always behave the same way as a = a + b.  Classes may implement the op= operators differently, and lists do this.


The expression a = a + [5,6,7,8] generates a new list and sets a's reference to that new list, leaving b unchanged.


The expression a += [5,6,7,8] is actually mapped to an ""extend"" function that operates on the list such that a and b still point to the same list that has been modified in-place.



‚ñ∂ The out of scope variable
a = 1
def some_func():
    return a

def another_func():
    a += 1
    return a
Output:
>>> some_func()
1
>>> another_func()
UnboundLocalError: local variable 'a' referenced before assignment
üí° Explanation:


When you make an assignment to a variable in scope, it becomes local to that scope. So a becomes local to the scope of another_func,  but it has not been initialized previously in the same scope which throws an error.


Read this short but an awesome guide to learn more about how namespaces and scope resolution works in Python.


To modify the outer scope variable a in another_func, use global keyword.
def another_func()
    global a
    a += 1
    return a
Output:
>>> another_func()
2



‚ñ∂ Be careful with chained operations
>>> (False == False) in [False] # makes sense
False
>>> False == (False in [False]) # makes sense
False
>>> False == False in [False] # now what?
True

>>> True is False == False
False
>>> False is False is False
True

>>> 1 > 0 < 1
True
>>> (1 > 0) < 1
False
>>> 1 > (0 < 1)
False
üí° Explanation:
As per https://docs.python.org/2/reference/expressions.html#not-in

Formally, if a, b, c, ..., y, z are expressions and op1, op2, ..., opN are comparison operators, then a op1 b op2 c ... y opN z is equivalent to a op1 b and b op2 c and ... y opN z, except that each expression is evaluated at most once.

While such behavior might seem silly to you in the above examples, it's fantastic with stuff like a == b == c and 0 <= x <= 100.

False is False is False is equivalent to (False is False) and (False is False)
True is False == False is equivalent to True is False and False == False and since the first part of the statement (True is False) evaluates to False, the overall expression evaluates to False.
1 > 0 < 1 is equivalent to 1 > 0 and 0 < 1 which evaluates to True.
The expression (1 > 0) < 1 is equivalent to True < 1 and
>>> int(True)
1
>>> True + 1 #not relevant for this example, but just for fun
2
So, 1 < 1 evaluates to False


‚ñ∂ Name resolution ignoring class scope
1.
x = 5
class SomeClass:
    x = 17
    y = (x for i in range(10))
Output:
>>> list(SomeClass.y)[0]
5
2.
x = 5
class SomeClass:
    x = 17
    y = [x for i in range(10)]
Output (Python 2.x):
>>> SomeClass.y[0]
17
Output (Python 3.x):
>>> SomeClass.y[0]
5
üí° Explanation

Scopes nested inside class definition ignore names bound at the class level.
A generator expression has its own scope.
Starting from Python 3.X, list comprehensions also have their own scope.


‚ñ∂ Needle in a Haystack
1.
x, y = (0, 1) if True else None, None
Output:
>>> x, y  # expected (0, 1)
((0, 1), None)
Almost every Python programmer has faced a similar situation.
2.
t = ('one', 'two')
for i in t:
    print(i)

t = ('one')
for i in t:
    print(i)

t = ()
print(t)
Output:
one
two
o
n
e
tuple()
üí° Explanation:

For 1, the correct statement for expected behavior is x, y = (0, 1) if True else (None, None).
For 2, the correct statement for expected behavior is t = ('one',) or t = 'one', (missing comma) otherwise the interpreter considers t to be a str and iterates over it character by character.
() is a special token and denotes empty tuple.


‚ñ∂ Yielding from... return!
1.
def some_func(x):
    if x == 3:
        return [""wtf""]
    else:
        yield from range(x)
Output:
>>> list(some_func(3))
[]
Where did the ""wtf"" go? Is it due to some special effect of yield from? Let's validate that,
2.
def some_func(x):
    if x == 3:
        return [""wtf""]
    else:
        for i in range(x):
          yield i
Output:
>>> list(some_func(3))
[]
Same result, that didn't work either.
üí° Explanation:

From Python 3.3 onwards, it became possible to use return statement with values inside generators (See PEP380). The official docs say that,


""... return expr in a generator causes StopIteration(expr) to be raised upon exit from the generator.""



In case of some_func(3), StopIteration is raised at the beginning because of return statement. The StopIteration exception is automatically catched inside the list(...) wrapper and the for loop. Therefore, the above two snippets result in an empty list.


To get [""wtf""] from the generator some_func we need to catch the StopIteration exception,
try:
    next(some_func(3))
except StopIteration as e:
    some_string = e.value
>>> some_string
[""wtf""]




Section: The Hidden treasures!
This section contains few of the lesser-known interesting things about Python that most beginners like me are unaware of (well, not anymore).
‚ñ∂ Okay Python, Can you make me fly? *
Well, here you go
import antigravity
Output:
Sshh.. It's a super secret.
üí° Explanation:

antigravity module is one of the few easter eggs released by Python developers.
import antigravity opens up a web browser pointing to the classic XKCD comic about Python.
Well, there's more to it. There's another easter egg inside the easter egg. If you look at the code, there's a function defined that purports to implement the XKCD's geohashing algorithm.


‚ñ∂ goto, but why? *
from goto import goto, label
for i in range(9):
    for j in range(9):
        for k in range(9):
            print(""I'm trapped, please rescue!"")
            if k == 2:
                goto .breakout # breaking out from a deeply nested loop
label .breakout
print(""Freedom!"")
Output (Python 2.3):
I'm trapped, please rescue!
I'm trapped, please rescue!
Freedom!
üí° Explanation:

A working version of goto in Python was announced as an April Fool's joke on 1st April 2004.
Current versions of Python do not have this module.
Although it works, but please don't use it. Here's the reason to why goto is not present in Python.


‚ñ∂ Brace yourself! *
If you are one of the people who doesn't like using whitespace in Python to denote scopes, you can use the C-style {} by importing,
from __future__ import braces
Output:
  File ""some_file.py"", line 1
    from __future__ import braces
SyntaxError: not a chance
Braces? No way! If you think that's disappointing, use Java.
üí° Explanation:

The __future__ module is normally used to provide features from future versions of Python. The ""future"" here is however ironic.
This is an easter egg concerned with the community's feelings on this issue.


‚ñ∂ Let's meet Friendly Language Uncle For Life *
Output (Python 3.x)
>>> from __future__ import barry_as_FLUFL
>>> ""Ruby"" != ""Python"" # there's no doubt about it
  File ""some_file.py"", line 1
    ""Ruby"" != ""Python""
              ^
SyntaxError: invalid syntax

>>> ""Ruby"" <> ""Python""
True
There we go.
üí° Explanation:

This is relevant to PEP-401 released on April 1, 2009 (now you know, what it means).
Quoting from the PEP-401

Recognized that the != inequality operator in Python 3.0 was a horrible, finger pain inducing mistake, the FLUFL reinstates the <> diamond operator as the sole spelling.


There were more things that Uncle Barry had to share in the PEP; you can read them here.


‚ñ∂ Even Python understands that love is complicated *
import this
Wait, what's this? this is love ‚ù§Ô∏è
Output:
The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!

It's the Zen of Python!
>>> love = this
>>> this is love
True
>>> love is True
False
>>> love is False
False
>>> love is not True or False
True
>>> love is not True or False; love is love  # Love is complicated
True
üí° Explanation:

this module in Python is an easter egg for The Zen Of Python (PEP 20).
And if you think that's already interesting enough, check out the implementation of this.py. Interestingly, the code for the Zen violates itself (and that's probably the only place where this happens).
Regarding the statement love is not True or False; love is love, ironic but it's self-explanatory.


‚ñ∂ Yes, it exists!
The else clause for loops. One typical example might be:
  def does_exists_num(l, to_find):
      for num in l:
          if num == to_find:
              print(""Exists!"")
              break
      else:
          print(""Does not exist"")
Output:
>>> some_list = [1, 2, 3, 4, 5]
>>> does_exists_num(some_list, 4)
Exists!
>>> does_exists_num(some_list, -1)
Does not exist
The else clause in exception handling. An example,
try:
    pass
except:
    print(""Exception occurred!!!"")
else:
    print(""Try block executed successfully..."")
Output:
Try block executed successfully...
üí° Explanation:

The else clause after a loop is executed only when there's no explicit break after all the iterations.
else clause after try block is also called ""completion clause"" as reaching the else clause in a try statement means that the try block actually completed successfully.


‚ñ∂ Inpinity *
The spelling is intended. Please, don't submit a patch for this.
Output (Python 3.x):
>>> infinity = float('infinity')
>>> hash(infinity)
314159
>>> hash(float('-inf'))
-314159
üí° Explanation:

Hash of infinity is 10‚Åµ x œÄ.
Interestingly, the hash of float('-inf') is ""-10‚Åµ x œÄ"" in Python 3, whereas ""-10‚Åµ x e"" in Python 2.


‚ñ∂ Mangling time! *
class Yo(object):
    def __init__(self):
        self.__honey = True
        self.bitch = True
Output:
>>> Yo().bitch
True
>>> Yo().__honey
AttributeError: 'Yo' object has no attribute '__honey'
>>> Yo()._Yo__honey
True
Why did Yo()._Yo__honey work? Only Indian readers would understand.
üí° Explanation:

Name Mangling is used to avoid naming collisions between different namespaces.
In Python, the interpreter modifies (mangles) the class member names starting with __ (double underscore) and not ending with more than one trailing underscore by adding _NameOfTheClass in front.
So, to access __honey attribute, we are required to append _Yo to the front which would prevent conflicts with the same name attribute defined in any other class.



Section: Miscellaneous
‚ñ∂ += is faster
# using ""+"", three strings:
>>> timeit.timeit(""s1 = s1 + s2 + s3"", setup=""s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000"", number=100)
0.25748300552368164
# using ""+="", three strings:
>>> timeit.timeit(""s1 += s2 + s3"", setup=""s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000"", number=100)
0.012188911437988281
üí° Explanation:

+= is faster than + for concatenating more than two strings because the first string (example, s1 for s1 += s2 + s3) is not destroyed while calculating the complete string.


‚ñ∂ Let's make a giant string!
def add_string_with_plus(iters):
    s = """"
    for i in range(iters):
        s += ""xyz""
    assert len(s) == 3*iters

def add_bytes_with_plus(iters):
    s = b""""
    for i in range(iters):
        s += b""xyz""
    assert len(s) == 3*iters

def add_string_with_format(iters):
    fs = ""{}""*iters
    s = fs.format(*([""xyz""]*iters))
    assert len(s) == 3*iters

def add_string_with_join(iters):
    l = []
    for i in range(iters):
        l.append(""xyz"")
    s = """".join(l)
    assert len(s) == 3*iters

def convert_list_to_string(l, iters):
    s = """".join(l)
    assert len(s) == 3*iters
Output:
>>> timeit(add_string_with_plus(10000))
1000 loops, best of 3: 972 ¬µs per loop
>>> timeit(add_bytes_with_plus(10000))
1000 loops, best of 3: 815 ¬µs per loop
>>> timeit(add_string_with_format(10000))
1000 loops, best of 3: 508 ¬µs per loop
>>> timeit(add_string_with_join(10000))
1000 loops, best of 3: 878 ¬µs per loop
>>> l = [""xyz""]*10000
>>> timeit(convert_list_to_string(l, 10000))
10000 loops, best of 3: 80 ¬µs per loop
Let's increase the number of iterations by a factor of 10.
>>> timeit(add_string_with_plus(100000)) # Linear increase in execution time
100 loops, best of 3: 9.75 ms per loop
>>> timeit(add_bytes_with_plus(100000)) # Quadratic increase
1000 loops, best of 3: 974 ms per loop
>>> timeit(add_string_with_format(100000)) # Linear increase
100 loops, best of 3: 5.25 ms per loop
>>> timeit(add_string_with_join(100000)) # Linear increase
100 loops, best of 3: 9.85 ms per loop
>>> l = [""xyz""]*100000
>>> timeit(convert_list_to_string(l, 100000)) # Linear increase
1000 loops, best of 3: 723 ¬µs per loop
üí° Explanation

You can read more about timeit from here. It is generally used to measure the execution time of snippets.
Don't use + for generating long strings ‚Äî In Python, str is immutable, so the left and right strings have to be copied into the new string for every pair of concatenations. If you concatenate four strings of length 10, you'll be copying (10+10) + ((10+10)+10) + (((10+10)+10)+10) = 90 characters instead of just 40 characters. Things get quadratically worse as the number and size of the string increases (justified with the execution times of add_bytes_with_plus function)
Therefore, it's advised to use .format. or % syntax (however, they are slightly slower than + for short strings).
Or better, if already you've contents available in the form of an iterable object, then use ''.join(iterable_object) which is much faster.
add_string_with_plus didn't show a quadratic increase in execution time unlike add_bytes_with_plus because of the += optimizations discussed in the previous example. Had the statement been s = s + ""x"" + ""y"" + ""z"" instead of s += ""xyz"", the increase would have been quadratic.
def add_string_with_plus(iters):
    s = """"
    for i in range(iters):
        s = s + ""x"" + ""y"" + ""z""
    assert len(s) == 3*iters

>>> timeit(add_string_with_plus(10000))
100 loops, best of 3: 9.87 ms per loop
>>> timeit(add_string_with_plus(100000)) # Quadratic increase in execution time
1 loops, best of 3: 1.09 s per loop



‚ñ∂ Explicit typecast of strings
a = float('inf')
b = float('nan')
c = float('-iNf')  #These strings are case-insensitive
d = float('nan')
Output:
>>> a
inf
>>> b
nan
>>> c
-inf
>>> float('some_other_string')
ValueError: could not convert string to float: some_other_string
>>> a == -c #inf==inf
True
>>> None == None # None==None
True
>>> b == d #but nan!=nan
False
>>> 50/a
0.0
>>> a/a
nan
>>> 23 + b
nan
üí° Explanation:
'inf' and 'nan' are special strings (case-insensitive), which when explicitly typecast-ed to float type, are used to represent mathematical ""infinity"" and ""not a number"" respectively.

‚ñ∂ Minor Ones


join() is a string operation instead of list operation. (sort of counter-intuitive at first usage)
üí° Explanation:
If join() is a method on a string then it can operate on any iterable (list, tuple, iterators). If it were a method on a list, it'd have to be implemented separately by every type. Also, it doesn't make much sense to put a string-specific method on a generic list object API.


Few weird looking but semantically correct statements:

[] = () is a semantically correct statement (unpacking an empty tuple into an empty list)
'a'[0][0][0][0][0] is also a semantically correct statement as strings are sequences(iterables supporting element access using integer indices) in Python.
3 --0-- 5 == 8 and --5 == 5 are both semantically correct statements and evaluate to True.



Given that a is a number, ++a and --a are both valid Python statements but don't behave the same way as compared with similar statements in languages like C, C++ or Java.
>>> a = 5
>>> a
5
>>> ++a
5
>>> --a
5
üí° Explanation:

There is no ++ operator in Python grammar. It is actually two + operators.
++a parses as +(+a) which translates to a. Similarly, the output of the statement --a can be justified.
This StackOverflow thread discusses the rationale behind the absence of increment and decrement operators in Python.



Have you ever heard about the space-invader operator in Python?
>>> a = 42
>>> a -=- 1
>>> a
43
It is used as an alternative incrementation operator, together with another one
>>> a +=+ 1
>>> a
>>> 44
üí° Explanation:
This prank comes from Raymond Hettinger's tweet. The space invader operator is actually just a malformatted a -= (-1). Which is equivalent to a = a - (- 1). Similar for the a += (+ 1) case.


Python uses 2 bytes for local variable storage in functions. In theory, this means that only 65536 variables can be defined in a function. However, python has a handy solution built in that can be used to store more than 2^16 variable names. The following code demonstrates what happens in the stack when more than 65536 local variables are defined (Warning: This code prints around 2^18 lines of text, so be prepared!):
import dis
exec(""""""
def f():
    """""" + """"""
    """""".join([""X"" + str(x) + ""="" + str(x) for x in range(65539)]))

f()

print(dis.dis(f))


Multiple Python threads won't run your Python code concurrently (yes you heard it right!). It may seem intuitive to spawn several threads and let them execute your Python code concurrently, but, because of the Global Interpreter Lock in Python, all you're doing is making your threads execute on the same core turn by turn. Python threads are good for IO-bound tasks, but to achieve actual parallelization in Python for CPU-bound tasks, you might want to use the Python multiprocessing module.


List slicing with out of the bounds indices throws no errors
>>> some_list = [1, 2, 3, 4, 5]
>>> some_list[111:]
[]


int('Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©') returns 123456789 in Python 3. In Python, Decimal characters include digit characters, and all characters that can be used to form decimal-radix numbers, e.g. U+0660, ARABIC-INDIC DIGIT ZERO. Here's an interesting story related to this behavior of Python.


'abc'.count('') == 4. Here's an approximate implementation of count method, which would make the things more clear
def count(s, sub):
    result = 0
    for i in range(len(s) + 1 - len(sub)):
        result += (s[i:i + len(sub)] == sub)
    return result
The behavior is due to the matching of empty substring('') with slices of length 0 in the original string.



Contributing
All patches are Welcome! Please see CONTRIBUTING.md for further details.
For discussions, you can either create a new issue or ping on the Gitter channel
Acknowledgements
The idea and design for this collection were initially inspired by Denys Dovhan's awesome project wtfjs. The overwhelming support by the community gave it the shape it is in right now.
Some nice Links!

https://www.youtube.com/watch?v=sH4XF6pKKmk
https://www.reddit.com/r/Python/comments/3cu6ej/what_are_some_wtf_things_about_python
https://sopython.com/wiki/Common_Gotchas_In_Python
https://stackoverflow.com/questions/530530/python-2-x-gotchas-and-landmines
https://stackoverflow.com/questions/1011431/common-pitfalls-in-python
https://www.python.org/doc/humor/
https://www.codementor.io/satwikkansal/python-practices-for-efficient-code-performance-memory-and-usability-aze6oiq65

üéì License

¬© Satwik Kansal
Help
If you have any wtfs, ideas or suggestions, please share.
Surprise your geeky pythonist friends?
You can use these quick links to recommend wtfpython to your friends,
Twitter
| Linkedin
Need a pdf version?
I've received a few requests for the pdf version of wtfpython. You can add your details here to get the pdf as soon as it is finished.
",GitHub - satwikkansal/wtfpython: A collection of surprising Python snippets and lesser-known features.
19,Python,"Algo VPN



Algo VPN is a set of Ansible scripts that simplify the setup of a personal Wireguard and IPSEC VPN. It uses the most secure defaults available, works with common cloud providers, and does not require client software on most devices. See our release announcement for more information.
Features

Supports only IKEv2 with strong crypto (AES-GCM, SHA2, and P-256) and WireGuard
Generates Apple profiles to auto-configure iOS and macOS devices
Includes a helper script to add and remove users
Blocks ads with a local DNS resolver (optional)
Sets up limited SSH users for tunneling traffic (optional)
Based on current versions of Ubuntu and strongSwan
Installs to DigitalOcean, Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, OpenStack, CloudStack, Hetzner Cloud, or your own Ubuntu server

Anti-features

Does not support legacy cipher suites or protocols like L2TP, IKEv1, or RSA
Does not install Tor, OpenVPN, or other risky servers
Does not depend on the security of TLS
Does not require client software on most platforms
Does not claim to provide anonymity or censorship avoidance
Does not claim to protect you from the FSB, MSS, DGSE, or FSM

Deploy the Algo Server
The easiest way to get an Algo server running is to run it on your local system and let it set up a new virtual machine in the cloud for you.


Setup an account on a cloud hosting provider. Algo supports DigitalOcean (most user friendly), Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, DreamCompute or other OpenStack-based cloud hosting, Exoscale or other CloudStack-based cloud hosting,  or Hetzner Cloud.


Get a copy of Algo. The Algo scripts will be installed on your local system. There are two ways to get a copy:


Download the ZIP file. Unzip the file to create a directory named algo-master containing the Algo scripts.


Run the command git clone https://github.com/trailofbits/algo.git to create a directory named algo containing the Algo scripts.




Install Algo's core dependencies. Algo requires that Python 3.6 or later and at least one supporting package are installed on your system.


macOS: Apple does not provide a suitable version of Python 3 with macOS. Here are two ways to obtain one:


Use the Homebrew package manager. After installing Homebrew install Python 3 by running brew install python3.


Download and install the latest stable Python 3.7 package (currently Python 3.8 will not work). Be sure to run the included Install Certificates command from Finder.


See Deploy from macOS for more detailed information on installing Python 3 on macOS.
Once Python 3 is installed on your Mac, from Terminal run:
python3 -m pip install --upgrade virtualenv


Linux: Recent releases of Ubuntu, Debian, and Fedora come with Python 3 already installed. Make sure your system is up-to-date and install the supporting package(s):

Ubuntu and Debian:

sudo apt install -y python3-virtualenv

Fedora:

sudo dnf install -y python3-virtualenv

Red Hat and CentOS 7 and later (for earlier versions see this documentation):

sudo yum -y install epel-release
sudo yum install -y python36-virtualenv


Windows: Use the Windows Subsystem for Linux (WSL) to create your own copy of Ubuntu running under Windows from which to install and run Algo. See the Windows documentation.




Install Algo's remaining dependencies. You'll need to run these commands from the Algo directory each time you download a new copy of Algo. In a Terminal window cd into the algo-master (ZIP file) or algo (git clone) directory and run:
python3 -m virtualenv --python=""$(command -v python3)"" .env &&
  source .env/bin/activate &&
  python3 -m pip install -U pip virtualenv &&
  python3 -m pip install -r requirements.txt
On Fedora add the option --system-site-packages to the first command above. On macOS install the C compiler if prompted.


List the users to create. Open the file config.cfg in your favorite text editor. Specify the users you wish to create in the users list. Create a unique user for each device you plan to connect to your VPN. If you want to be able to add or delete users later, you must select yes at the Do you want to retain the keys (PKI)? prompt during the deployment.


Start the deployment. Return to your terminal. In the Algo directory, run ./algo and follow the instructions. There are several optional features available. None are required for a fully functional VPN server. These optional features are described in greater detail in here.


That's it! You will get the message below when the server deployment process completes. Take note of the p12 (user certificate) password and the CA key in case you need them later, they will only be displayed this time.
You can now set up clients to connect to your VPN. Proceed to Configure the VPN Clients below.
    ""#                          Congratulations!                            #""
    ""#                     Your Algo server is running.                     #""
    ""#    Config files and certificates are in the ./configs/ directory.    #""
    ""#              Go to https://whoer.net/ after connecting               #""
    ""#        and ensure that all your traffic passes through the VPN.      #""
    ""#                     Local DNS resolver 172.16.0.1                    #""
    ""#        The p12 and SSH keys password for new users is XXXXXXXX       #""
    ""#        The CA key password is XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX       #""
    ""#      Shell access: ssh -i configs/algo.pem root@xxx.xxx.xx.xx        #""

Configure the VPN Clients
Certificates and configuration files that users will need are placed in the configs directory. Make sure to secure these files since many contain private keys. All files are saved under a subdirectory named with the IP address of your new Algo VPN server.
Apple Devices
WireGuard is used to provide VPN services on Apple devices. Algo generates a WireGuard configuration file, wireguard/<username>.conf, and a QR code, wireguard/<username>.png, for each user defined in config.cfg.
On iOS, install the WireGuard app from the iOS App Store. Then, use the WireGuard app to scan the QR code or AirDrop the configuration file to the device.
On macOS Mojave or later, install the WireGuard app from the Mac App Store. WireGuard will appear in the menu bar once you run the app. Click on the WireGuard icon, choose Import tunnel(s) from file..., then select the appropriate WireGuard configuration file.
On either iOS or macOS, you can enable ""Connect on Demand"" and/or exclude certain trusted Wi-Fi networks (such as your home or work) by editing the tunnel configuration in the WireGuard app. (Algo can't do this automatically for you.)
Installing WireGuard is a little more complicated on older version of macOS. See Using macOS as a Client with WireGuard.
If you prefer to use the built-in IPSEC VPN on Apple devices, or need ""Connect on Demand"" or excluded Wi-Fi networks automatically configured, then see Using Apple Devices as a Client with IPSEC.
Android Devices
WireGuard is used to provide VPN services on Android. Install the WireGuard VPN Client. Import the corresponding wireguard/<name>.conf file to your device, then setup a new connection with it. See the Android setup instructions for more detailed walkthrough.
Windows
WireGuard is used to provide VPN services on Windows. Algo generates a WireGuard configuration file, wireguard/<username>.conf, for each user defined in config.cfg.
Install the WireGuard VPN Client. Import the generated wireguard/<username>.conf file to your device, then setup a new connection with it.
Linux WireGuard Clients
WireGuard works great with Linux clients. See this page for an example of how to configure WireGuard on Ubuntu.
Linux strongSwan IPsec Clients (e.g., OpenWRT, Ubuntu Server, etc.)
Please see this page.
Other Devices
Depending on the platform, you may need one or multiple of the following files.

ipsec/manual/cacert.pem: CA Certificate
ipsec/manual/.p12: User Certificate and Private Key (in PKCS#12 format)
ipsec/manual/.conf: strongSwan client configuration
ipsec/manual/.secrets: strongSwan client configuration
ipsec/apple/.mobileconfig: Apple Profile
wireguard/.conf: WireGuard configuration profile
wireguard/.png: WireGuard configuration QR code

Setup an SSH Tunnel
If you turned on the optional SSH tunneling role, then local user accounts will be created for each user in config.cfg and SSH authorized_key files for them will be in the configs directory (user.ssh.pem). SSH user accounts do not have shell access, cannot authenticate with a password, and only have limited tunneling options (e.g., ssh -N is required). This ensures that SSH users have the least access required to setup a tunnel and can perform no other actions on the Algo server.
Use the example command below to start an SSH tunnel by replacing user and ip with your own. Once the tunnel is setup, you can configure a browser or other application to use 127.0.0.1:1080 as a SOCKS proxy to route traffic through the Algo server.
ssh -D 127.0.0.1:1080 -f -q -C -N user@ip -i configs/<server_ip>/ssh-tunnel/<user>.pem
SSH into Algo Server
Your Algo server is configured for key-only SSH access for administrative purposes. Open the Terminal app, cd into the algo-master directory where you originally downloaded Algo, and then use the command listed on the success message:
ssh -i configs/algo.pem user@ip
where user is either root or ubuntu as listed on the success message, and ip is the IP address of your Algo server. If you find yourself regularly logging into the server then it will be useful to load your Algo ssh key automatically. Add the following snippet to the bottom of ~/.bash_profile to add it to your shell environment permanently.
ssh-add ~/.ssh/algo > /dev/null 2>&1
Adding or Removing Users
If you chose to save the CA key during the deploy process, then Algo's own scripts can easily add and remove users from the VPN server.

Update the users list in your config.cfg
Open a terminal, cd to the algo directory, and activate the virtual environment with source .env/bin/activate
Run the command: ./algo update-users

After this process completes, the Algo VPN server will contain only the users listed in the config.cfg file.
Additional Documentation

Deployment instructions, cloud provider setup instructions, and further client setup instructions available here.
FAQ
Troubleshooting

If you read all the documentation and have further questions, join the chat on Gitter.
Endorsements

I've been ranting about the sorry state of VPN svcs for so long, probably about
time to give a proper talk on the subject. TL;DR: use Algo.

-- Kenn White

Before picking a VPN provider/app, make sure you do some research
https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf ... ‚Äì or consider Algo

-- The Register

Algo is really easy and secure.

-- the grugq

I played around with Algo VPN, a set of scripts that let you set up a VPN in the cloud in very little time, even if you don‚Äôt know much about development. I‚Äôve got to say that I was quite impressed with Trail of Bits‚Äô approach.

-- Romain Dillet for TechCrunch

If you‚Äôre uncomfortable shelling out the cash to an anonymous, random VPN provider, this is the best solution.

-- Thorin Klosowski for Lifehacker
Support Algo VPN




All donations support continued development. Thanks!

We accept donations via PayPal, Patreon, and Flattr.
Use our referral code when you sign up to Digital Ocean for a $10 credit.
We also accept and appreciate contributions of new code and bugfixes via Github Pull Requests.

Algo is licensed and distributed under the AGPLv3. If you want to distribute a closed-source modification or service based on Algo, then please consider purchasing an exception . As with the methods above, this will help support continued development.
",GitHub - trailofbits/algo: Set up a personal VPN in the cloud
20,Python,"sqlmap 
     
sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester, and a broad range of switches including database fingerprinting, over data fetching from the database, accessing the underlying file system, and executing commands on the operating system via out-of-band connections.
The sqlmap project is currently searching for sponsor(s).
Screenshots

You can visit the collection of screenshots demonstrating some of the features on the wiki.
Installation
You can download the latest tarball by clicking here or latest zipball by clicking  here.
Preferably, you can download sqlmap by cloning the Git repository:
git clone --depth 1 https://github.com/sqlmapproject/sqlmap.git sqlmap-dev

sqlmap works out of the box with Python version 2.6, 2.7 and 3.x on any platform.
Usage
To get a list of basic options and switches use:
python sqlmap.py -h

To get a list of all options and switches use:
python sqlmap.py -hh

You can find a sample run here.
To get an overview of sqlmap capabilities, a list of supported features, and a description of all options and switches, along with examples, you are advised to consult the user's manual.
Links

Homepage: http://sqlmap.org
Download: .tar.gz or .zip
Commits RSS feed: https://github.com/sqlmapproject/sqlmap/commits/master.atom
Issue tracker: https://github.com/sqlmapproject/sqlmap/issues
User's manual: https://github.com/sqlmapproject/sqlmap/wiki
Frequently Asked Questions (FAQ): https://github.com/sqlmapproject/sqlmap/wiki/FAQ
Twitter: @sqlmap
Demos: http://www.youtube.com/user/inquisb/videos
Screenshots: https://github.com/sqlmapproject/sqlmap/wiki/Screenshots

Translations

Bulgarian
Chinese
Croatian
French
German
Greek
Indonesian
Italian
Japanese
Korean
Polish
Portuguese
Russian
Spanish
Turkish
Ukrainian

",GitHub - sqlmapproject/sqlmap: Automatic SQL injection and database takeover tool
21,Python,"This guide is a collection of techniques for improving the security and privacy of a modern Apple Macintosh computer (""MacBook"") running a recent version of macOS (formerly known as ""OS X"").
This guide is targeted to power users who wish to adopt enterprise-standard security, but is also suitable for novice users with an interest in improving their privacy and security on a Mac.
A system is only as secure as its administrator is capable of making it. There is no one single technology, software, nor technique to guarantee perfect computer security; a modern operating system and computer is very complex, and requires numerous incremental changes to meaningfully improve one's security and privacy posture.
This guide is provided on an 'as is' basis without any warranties of any kind. Only you are responsible if you break anything or get in any sort of trouble by following this guide.
To suggest an improvement, please send a pull request or open an issue.
This guide is also available in ÁÆÄ‰Ωì‰∏≠Êñá.

Basics
Preparing and installing macOS

Verifying installation integrity
Creating a bootable USB installer
Creating an install image

Manual way


Target disk mode
Creating a recovery partition
Virtualization


First boot
System activation
Admin and standard user accounts

Caveats
Setup


Full disk encryption
Firmware
Firewall

Application layer firewall
Third party firewalls
Kernel level packet filtering


Services
Spotlight Suggestions
Homebrew
DNS

Hosts file
dnscrypt
Dnsmasq

Test DNSSEC validation




Captive portal
Certificate authorities
OpenSSL
Curl
Web

Privoxy
Browser

Firefox
Chrome
Safari
Other Web browsers
Web browsers and privacy


Plugins


Tor
VPN
PGP/GPG
OTR
Viruses and malware
System Integrity Protection
Gatekeeper and XProtect
Metadata and artifacts
Passwords
Backup
Wi-Fi
SSH
Physical access
System monitoring

OpenBSM audit
DTrace
Execution
Network


Binary Whitelisting
Miscellaneous
Related software
Additional resources

Basics
Standard security best practices apply:


Create a threat model

What are you trying to protect and from whom? Is your adversary a three letter agency (if so, you may want to consider using OpenBSD instead); a nosy eavesdropper on the network; or a determined apt orchestrating a campaign against you?
Recognize threats and how to reduce attack surface against them.



Keep the system up to date

Patch the base operating system and all third party software.
macOS system updates can be completed using the App Store application, or the softwareupdate command-line utility - neither requires registering an Apple account. Updates can also be downloaded directly from Apple's support site.
Subscribe to announcement mailing lists like Apple security-announce.



Encrypt sensitive data at rest

In addition to full disk encryption, consider creating one or several encrypted partitions or volumes to store passwords, cryptographic keys, personal documents, etc. at rest.
This will mitigate damage in case of compromise and data exfiltration.



Assure data availability

Create regular backups of your data and be ready to format and re-install the operating system in case of compromise.
Always encrypt locally before copying backups to external media or the ""cloud"".
Verify backups work by testing them regularly, for example by accessing certain files or performing a hash based comparison.



Click carefully

Ultimately, the security of a system can be reduced to its administrator.
Care should be taken when installing new software. Always prefer free and open source software (which macOS is not).



Preparing and installing macOS
There are several ways to install macOS.
The simplest way is to boot into Recovery Mode by holding Command and R keys at boot. A system image can be downloaded and applied directly from Apple. However, this way exposes the serial number and other identifying information over the network in plaintext, which may not be desired for privacy reasons.

Packet capture of an unencrypted HTTP conversation during macOS recovery
An alternative way to install macOS is to first download macOS Mojave from the App Store or elsewhere, and create a custom installable system image.
Verifying installation integrity
The macOS installation application is code signed, which should be verified to make sure you received a legitimate copy, using the pkgutil --check-signature or codesign -dvv commands.
To verify the code signature and integrity of macOS application bundles:
$ pkgutil --check-signature /Applications/Install\ macOS\ Catalina.app
Package ""Install macOS Catalina"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Software Signing
       SHA1 fingerprint: 01 3E 27 87 74 8A 74 10 3D 62 D2 CD BF 77 A1 34 55 17 C4 82
       -----------------------------------------------------------------------------
    2. Apple Code Signing Certification Authority
       SHA1 fingerprint: 1D 01 00 78 A6 1F 4F A4 69 4A FF 4D B1 AC 26 6C E1 B4 59 46
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
Use the codesign command to examine an application's code signature:
$ codesign -dvv /Applications/Install\ macOS\ Catalina.app
Executable=/Applications/Install macOS Catalina.app/Contents/MacOS/InstallAssistant_springboard
Identifier=com.apple.InstallAssistant.Catalina
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20100 size=276 flags=0x2000(library-validation) hashes=3+3 location=embedded
Platform identifier=9
Signature size=4628
Authority=Software Signing
Authority=Apple Code Signing Certification Authority
Authority=Apple Root CA
Info.plist entries=33
TeamIdentifier=not set
Sealed Resources version=2 rules=13 files=234
Internal requirements count=1 size=84
Creating a bootable USB installer
Instead of booting from the network or using target disk mode, a bootable macOS installer can be made with the createinstallmedia utility included in Contents/Resources folder of the installer application bundle. See Create a bootable installer for macOS, or run the utility without arguments to see how it works.
To create a bootable USB installer, mount a USB drive, and erase and partition it, then use the createinstallmedia utility:
$ diskutil list
[Find disk matching correct size, usually the last disk, e.g. /dev/disk2]

$ diskutil unmountDisk /dev/disk2

$ diskutil partitionDisk /dev/disk2 1 JHFS+ Installer 100%

$ cd /Applications/Install\ macOS\ Catalina.app

$ sudo ./Contents/Resources/createinstallmedia --volume /Volumes/Installer --nointeraction
Erasing disk: 0%... 10%... 20%... 30%... 100%
Copying to disk: 0%... 10%... 20%... 30%... 40%... 50%... 60%... 70%... 80%... 90%... 100%
Making disk bootable...
Copying boot files...
Install media now available at ""/Volumes/Install macOS Catalina""
Creating an install image
Note Apple's AutoDMG installer does not appear to work across OS versions. If you want to build a 10.14 image, for example, the following steps must be performed on macOS 10.14!
To create a custom install image which can be restored to a Mac (using a USB-C cable and target disk mode, for example), use MagerValp/AutoDMG.
Manual way
Note The following instructions appear to work only on macOS versions before 10.13.
Find InstallESD.dmg which is inside the installation application. Locate it in Terminal or with Finder, right click on the application bundle, select Show Package Contents and navigate to Contents > SharedSupport to find the file InstallESD.dmg
Verify file integrity by comparing its SHA-256 hash with others found in InstallESD_Hashes.csv or notpeter/apple-installer-checksums.
To determine which macOS versions and builds originally shipped with or are available for a Mac, see HT204319.
$ shasum -a 256 InstallESD.dmg
Mount and install the operating system to a temporary image:
$ hdiutil attach -mountpoint /tmp/InstallESD ./InstallESD.dmg

$ hdiutil create -size 32g -type SPARSE -fs HFS+J -volname ""macOS"" -uid 0 -gid 80 -mode 1775 /tmp/macos.sparseimage

$ hdiutil attach -mountpoint /tmp/macos -owners on /tmp/macos.sparseimage

$ sudo installer -pkg /tmp/InstallESD/Packages/OSInstall.mpkg -tgt /tmp/macos -verbose
installer: OS Install started.
#############
[...]
The installation will take a while, so be patient. Use tail -F /var/log/install.log in another terminal to monitor progress and check for errors.
Once the installation is complete, detach, convert and verify the image:
$ hdiutil detach /tmp/macos
""disk4"" unmounted.
""disk4"" ejected.

$ hdiutil detach /tmp/InstallESD
""disk3"" unmounted.
""disk3"" ejected.

$ hdiutil convert -format UDZO /tmp/macos.sparseimage -o ~/sierra.dmg
Preparing imaging engine...
[...]

$ asr imagescan --source ~/sierra.dmg
The file sierra.dmg is now ready to be applied over Target Disk Mode, from a bootable USB installer, booting from the network or recovery mode. The image could be further customized to include provisioned users, installed applications, preferences, for example.
Target disk mode
To use Target Disk Mode, boot up the Mac you wish to image while holding the T key and connect it to another Mac using a USB-C, Thunderbolt or Firewire cable.
If you don't have another Mac, boot to a USB installer, with sierra.dmg and other required files copied to it, by holding the Option key at boot.
Use the command diskutil list to identify the disk of the connected Mac, usually /dev/disk2
Optionally, securely erase the disk with a single pass (if previously FileVault-encrypted, the disk must first be unlocked and mounted as /dev/disk3s2):
$ sudo diskutil secureErase freespace 1 /dev/disk3s2

Partition the disk to Journaled HFS+:
$ sudo diskutil unmountDisk /dev/disk2

$ sudo diskutil partitionDisk /dev/disk2 1 JHFS+ macOS 100%
Restore the image to the new volume, making sure /dev/disk2 is the disk being erased:
$ sudo asr restore --source ~/sierra.dmg --target /Volumes/macOS --erase --buffersize 4m
[...]
Erase contents of /dev/disk2s2 (/Volumes/macOS)? [ny]:y
[...]
The Disk Utility application may also be used to erase the connected disk and restore sierra.dmg to the newly created partition.
To transfer any files, copy them to a shared folder like /Users/Shared on the mounted disk image, e.g. cp Xcode_8.0.dmg /Volumes/macOS/Users/Shared

Finished restore install from USB recovery boot
Creating a recovery partition
Unless you have built the image with AutoDMG, or installed macOS to a second partition on the same Mac, you will need to create a recovery partition in order to use full disk encryption. You can do so using MagerValp/Create-Recovery-Partition-Installer or manually by following these steps:
Download RecoveryHDUpdate.dmg and verify its integrity:
$ shasum -a 256 RecoveryHDUpdate.dmg
f6a4f8ac25eaa6163aa33ac46d40f223f40e58ec0b6b9bf6ad96bdbfc771e12c  RecoveryHDUpdate.dmg
Attach and expand the installer, then run it - again ensuring /Volumes/macOS path is the newly created partition on the connected disk:
$ hdiutil attach RecoveryHDUpdate.dmg

$ pkgutil --expand /Volumes/Mac\ OS\ X\ Lion\ Recovery\ HD\ Update/RecoveryHDUpdate.pkg /tmp/recovery

$ hdiutil attach /tmp/recovery/RecoveryHDUpdate.pkg/RecoveryHDMeta.dmg

$ /tmp/recovery/RecoveryHDUpdate.pkg/Scripts/Tools/dmtest ensureRecoveryPartition /Volumes/macOS/ /Volumes/Recovery\ HD\ Update/BaseSystem.dmg 0 0 /Volumes/Recovery\ HD\ Update/BaseSystem.chunklist
[...]
Creating recovery partition: finished
Run diskutil list again to make sure Recovery HD now exists on /dev/disk2. Eject the disk with hdiutil unmount /Volumes/macOS and power down the target disk mode-booted Mac.
Virtualization
To install macOS as a virtual machine (VM) using VMware Fusion, follow the instructions above to create an image. You will not need to download and create a recovery partition manually.
For the Installation Method, select Install macOS from the recovery partition. Customize any memory or CPU requirements and complete setup. The guest VM should boot into Recovery Mode by default.
Note If the virtual machine does not boot due to a kernel panic, adjust the memory and process resource settings.
In Recovery Mode, select a language, then select Utilities > Terminal from the menu bar.
In the guest VM, type ifconfig | grep inet - you should see a private address like 172.16.34.129
On the host Mac, type ifconfig | grep inet - you should see a private gateway address like 172.16.34.1. From the host Mac, you should be able to ping 172.16.34.129 or the equivalent guest VM address.
From the host Mac, serve the installable image to the guest VM by editing /etc/apache2/httpd.conf and adding the following line to the top (using the gateway address assigned to the host Mac and port 80):
Listen 172.16.34.1:80

On the host Mac, link the image to the default Apache Web server directory:
$ sudo ln ~/sierra.dmg /Library/WebServer/Documents

From the host Mac, start Apache in the foreground:
$ sudo httpd -X

From the guest VM, install the disk image to the volume over the local network using asr:
-bash-3.2# asr restore --source http://172.16.34.1/sierra.dmg --target /Volumes/Macintosh\ HD/ --erase --buffersize 4m
	Validating target...done
	Validating source...done
	Erase contents of /dev/disk0s2 (/Volumes/Macintosh HD)? [ny]: y
	Retrieving scan information...done
	Validating sizes...done
	Restoring  ....10....20....30....40....50....60....70....80....90....100
	Verifying  ....10....20....30....40....50....60....70....80....90....100
	Remounting target volume...done
When it's finished, stop the Apache Web server on the host Mac by pressing Control C at the sudo httpd -X window and remove the image copy with sudo rm /Library/WebServer/Documents/sierra.dmg
In the guest VM, select Startup Disk from the menubar top-left, select the hard drive and restart. You may wish to disable the Network Adapter in VMware to configure the guest VM initially.
Take and Restore from saved guest VM snapshots before and after attempting risky browsing, for example, or use a guest VM to install and operate questionable software.
First boot
Note Before setting up macOS, consider disconnecting networking and configuring a firewall(s) first. However, late 2016 MacBooks with Touch Bar hardware require online OS activation (also see next section).
On first boot, hold Command Option P R keys to clear NVRAM.
When macOS first starts, you'll be greeted by Setup Assistant.
When creating the first account, use a strong password without a hint.
If you enter your real name at the account setup process, be aware that your computer's name and local hostname will comprise that name (e.g., John Appleseed's MacBook) and thus will appear on local networks and in various preference files.
Both should be verified and updated as needed in System Preferences > Sharing or with the following commands after installation:
$ sudo scutil --set ComputerName MacBook
$ sudo scutil --set LocalHostName MacBook

System activation
A few words on the privacy implications of activating ""Touch Bar"" MacBook devices from your friendly anonymous security researcher:

Apple increasingly seems (despite vague claims to the contrary) increasingly interested in merging or ""unifying"" the two OSes, and there are constantly rumors of fundamental changes to macOS that make it far more like iOS than the macOS of old. Apple's introduction of ARM-based coprocessors running iOS/sepOS, first with the T1 processor on the TouchBar MacBook Pros (run the TouchBar, implement NFC/ApplePay, add biometric login using sep, and verify firmware integrity) and the iMac Pro's T2 (implements/verifies embedded device firmware, implements secure boot, etc) seems to cement this concern and basically renders using macOS devices without sending metadata to Apple difficult to impossible.
iOS devices have always required ""activation"" on first boot and when the battery has gone dead which initializes sepOS to proceed with verified boot. First boot activation not only initializes sepOS as discussed below, but sends metadata to Apple (and carriers via Apple with cellular devices) to activate the baseband and SIM. In activation processes after first boot, just as with first boot, a long list of highly sensitive metadata are sent hashed (note hashing does not give you any privacy from Apple here since they link this exact metadata to payment information at purchase) to Apple so it can return the personalized response required for secure boot to complete. What is particularly worrying about this process is that it is a network-linked secure boot process where centralized external servers have the power to dictate what the device should boot. Equally there are significant privacy concerns with devices constantly sending metadata (both during activation and other Apple-linked/-hosted activities) and linking IP addresses very strongly with real identities based on purchase payment information and if a cellular device, metadata collected about SIM, etc unless such connections are blocked at the network level (which is only possible on self-managed infrastructure, i.e. not cellular) and doing this basically renders using the device impossible since simply installing an application requires sending device metadata to Apple.
That the activation verification mechanism is designed specifically to rely on unique device identifiers that are associated with payment information at purchase and actively associated on a continuing basis by Apple for every Apple-hosted service that the device interacts with (Apple ID-based services, softwareupdate, iMessage, FaceTime, etc.) the ability (and invitation) for Apple to silently send targeted malicious updates to devices matching specific unique ID criteria is a valid concern, and something that should not be dismissed as unlikely, especially given Apple's full compliance with recently implemented Chinese (and other authoritarian and ""non-authoritarian"" countries') national security laws.
iOS has from the start been designed with very little end-user control with no way for end-users to configure devices according to their wishes while maintaining security and relies heavily on new, closed source code. While macOS has for most of its history been designed on the surface in a similar fashion, power and enterprise users can (for the moment) still configure their devices relatively securely while maintaining basically zero network interaction with Apple and with the installation of third party software/kernel extensions, completely control the network stack and intercept filesystem events on a per-process basis. macOS, despite having a good deal of closed source code, was designed at a very different period in Apple's history and was designed more in line with open source standards, and designed to be configurable and controllable by enterprise/power users.
The introduction of these coprocessors to Mac devices, while increasing security in many ways, brings with it all the issues with iOS discussed above, and means that running mac devices securely with complete user control, and without forced network interaction with the Apple mothership in highly sensitive corporate and other environments problematic and risky. Given this author is unaware of the exact hardware configuration of the coprocessors, the following may be inaccurate. However, given the low-level nature of these coprocessors, it would not surprise the author if these coprocessors, if not already, will eventually have separate network access of their own, independent of the Intel CPU (indications suggest not currently the case for T1; unclear on T2), which leads to concerns similar to those that many have raised around Intel ME/AMT (and of course mac devices also have ME in the Intel CPU...). One could argue that these coprocessors increase security, and in many ways that is the case, but not the user's security against a malicious Apple.
The lack of configurability is the key issue. Apple could have introduced secure boot and firmware protection without making it require network access, without making verification linked to device-unique IDs and without introducing an enormous amount of potentially exploitable code to protect against a much smaller, but highly exploitable codebase, while running on a coprocessor with a highly privileged position on the board which gives immense power to an adversary with manufacturer compliance for targeted attacks.
This is an ongoing concern and in the worst case scenario could potentially represent the end of macs as independent, end-user controllable and relatively secure systems appropriate for sensitive environments with strict network and security policies.

From iOS, The Future Of macOS, Freedom, Security And Privacy In An Increasingly Hostile Global Environment.
Admin and standard user accounts
The first user account is always an admin account. Admin accounts are members of the admin group and have access to sudo, which allows them to usurp other accounts, in particular root, and gives them effective control over the system. Any program that the admin executes can potentially obtain the same access, making this a security risk.
Utilities like sudo have weaknesses that can be exploited by concurrently running programs and many panes in System Preferences are unlocked by default (pdf) (p. 61‚Äì62) for admin accounts.
It is considered a best practice by Apple and others (pdf) (p. 41‚Äì42) to use a separate standard account for day-to-day work and use the admin account for installations and system configuration.
It is not strictly required to ever log into the admin account via the macOS login screen. The system will prompt for authentication when required and Terminal can do the rest. To that end, Apple provides some recommendations for hiding the admin account and its home directory. This can be an elegant solution to avoid having a visible 'ghost' account. The admin account can also be removed from FileVault for additional hardening.
Caveats

Only administrators can install applications in /Applications (local directory). Finder and Installer will prompt a standard user with an authentication dialog. Many applications can be installed in ~/Applications instead (the directory can be created manually). As a rule of thumb: applications that do not require admin access ‚Äì or do not complain about not being installed in /Applications ‚Äì should be installed in the user directory, the rest in the local directory. Mac App Store applications are still installed in /Applications and require no additional authentication.
sudo is not available in shells of the standard user, which requires using su or login to enter a shell of the admin account. This can make some maneuvers trickier and requires some basic experience with command-line interfaces.
System Preferences and several system utilities (e.g. Wi-Fi Diagnostics) will require root privileges for full functionality. Many panels in System Preferences are locked and need to be unlocked separately by clicking on the lock icon. Some applications will simply prompt for authentication upon opening, others must be opened by an admin account directly to get access to all functions (e.g. Console).
There are third-party applications that will not work correctly because they assume that the user account is an admin. These programs may have to be executed by logging into the admin account, or by using the open utility.
See additional discussion in issue #167.

Setup
Accounts can be created and managed in System Preferences. On settled systems, it is generally easier to create a second admin account and then demote the first account. This avoids data migration. Newly installed systems can also just add a standard account.
Demoting an account can be done either from the the new admin account in System Preferences ‚Äì the other account must be logged out ‚Äì or by executing these commands (it may not be necessary to execute both, see issue #179):
$ sudo dscl . -delete /Groups/admin GroupMembership <username>
$ sudo dscl . -delete /Groups/admin GroupMembers <GeneratedUID>
To find the ‚ÄúGeneratedUID‚Äù of an account:
$ dscl . -read /Users/<username> GeneratedUID
See also this post for more information about how macOS determines group membership.
Full disk encryption
FileVault provides full disk (technically, full volume) encryption on macOS.
FileVault encryption protects data at rest and hardens (but not always prevents) someone with physical access from stealing data or tampering with your Mac.
With much of the cryptographic operations happening efficiently in hardware, the performance penalty for FileVault is not noticeable.
Like all cryptosystems, the security of FileVault greatly depends on the quality of the pseudo random number generator (PRNG).

The random device implements the Yarrow pseudo random number generator algorithm and maintains its entropy pool.  Additional entropy is fed to the generator regularly by the SecurityServer daemon from random jitter measurements of the kernel.

See man 4 random for more information.
Turning on FileVault in System Preferences after installing macOS, rather than creating an encrypted partition for the installation first, is more secure, because more PRNG entropy is available then.
Additionally, the PRNG can be manually seeded with entropy by writing to /dev/random before enabling FileVault. This can be done by simply using the Mac for a little while before activating FileVault.
It may also be possible to increase entropy with an external source, like OneRNG. See Entropy and Random Number Generators and Fun with encryption and randomness for more information.
Enable FileVault with sudo fdesetup enable or through System Preferences > Security & Privacy and reboot.
If you can remember the password, there's no reason to save the recovery key. However, all encrypted data will be lost forever if without either the password or recovery key.
To learn about how FileVault works, see the paper Infiltrate the Vault: Security Analysis and Decryption of Lion Full Disk Encryption (pdf) and related presentation (pdf). Also see IEEE Std 1619-2007: The XTS-AES Tweakable Block Cipher (pdf).
Optional Enforce system hibernation and evict FileVault keys from memory instead of traditional sleep to memory:
$ sudo pmset -a destroyfvkeyonstandby 1
$ sudo pmset -a hibernatemode 25

All computers have firmware of some type - EFI, BIOS - to help in the discovery of hardware components and ultimately to properly bootstrap the computer using the desired OS instance. In the case of Apple hardware and the use of EFI, Apple stores relevant information within EFI to aid in the functionality of macOS. For example, the FileVault key is stored in EFI to transparently come out of standby mode.


Organizations especially sensitive to a high-attack environment, or potentially exposed to full device access when the device is in standby mode, should mitigate this risk by destroying the FileVault key in firmware. Doing so doesn't destroy the use of FileVault, but simply requires the user to enter the password in order for the system to come out of standby mode.

If you choose to evict FileVault keys in standby mode, you should also modify your standby and power nap settings. Otherwise, your machine may wake while in standby mode and then power off due to the absence of the FileVault key. See issue #124 for more information. These settings can be changed with:
$ sudo pmset -a powernap 0
$ sudo pmset -a standby 0
$ sudo pmset -a standbydelay 0
$ sudo pmset -a autopoweroff 0
For more information, see Best Practices for
Deploying FileVault 2 (pdf) and paper Lest We Remember: Cold Boot Attacks on Encryption Keys (pdf)
Note APFS may make evicting FileVault keys redundant - see discussion and links in issue #283.
Firmware
Setting a firmware password prevents a Mac from starting up from any device other than the startup disk. It may also be set to be required on each boot. This may be useful for mitigating some attacks which require physical access to hardware.  See How to set a firmware password on your Mac for official documentation.
This feature can be helpful if your laptop is lost or stolen, protects against Direct Memory Access (DMA) attacks which can read your FileVault passwords and inject kernel modules such as pcileech, as the only way to reset the firmware password is through an Apple Store, or by using an SPI programmer, such as Bus Pirate or other flash IC programmer.

Start up pressing Command and R keys to boot to Recovery Mode mode.
When the Recovery window appears, choose Firmware Password Utility from the Utilities menu.
In the Firmware Utility window that appears, select Turn On Firmware Password.
Enter a new password, then enter the same password in the Verify field.
Select Set Password.
Select Quit Firmware Utility to close the Firmware Password Utility.
Select Restart or Shutdown from the Apple menu in the top-left corner.

The firmware password will activate at next boot. To validate the password, hold Alt during boot - you should be prompted to enter the password.
The firmware password can also be managed with the firmwarepasswd utility while booted into the OS. For example, to prompt for the firmware password when attempting to boot from a different volume:
$ sudo firmwarepasswd -setpasswd -setmode command
To verify the firmware password:
$ sudo firmwarepasswd -verify
Verifying Firmware Password
Enter password:
Correct
Note, a firmware password may be bypassed by a determined attacker or Apple, with physical access to the computer.

Using a Dediprog SF600 to dump and flash a 2013 MacBook SPI Flash chip to remove a firmware password, sans Apple
Newer Mac models (Mac Pro, iMac Pro, Macbook with TouchBar) with Apple T2 chips, which provide a secure enclave for encrypted keys, lessen the risk of EFI firmware attacks. See this blog post for more information.
See LongSoft/UEFITool, chipsec/chipsec and discussion in issue #213 for more information.
Firewall
There are several types of firewalls available for macOS which should be enabled.
Application layer firewall
Built-in, basic firewall which blocks incoming connections only. This firewall does not have the ability to monitor, nor block outgoing connections.
It can be controlled by the Firewall tab of Security & Privacy in System Preferences, or with the following commands.
Enable the firewall with logging and stealth mode:
$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on
Firewall is enabled. (State = 1)

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setloggingmode on
Turning on log mode

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setstealthmode on
Stealth mode enabled

Computer hackers scan networks so they can attempt to identify computers to attack. You can prevent your computer from responding to some of these scans by using stealth mode. When stealth mode is enabled, your computer does not respond to ICMP ping requests, and does not answer to connection attempts from a closed TCP or UDP port. This makes it more difficult for attackers to find your computer.

To prevent built-in software as well as code-signed, downloaded software from being whitelisted automatically:
$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsigned off
Disabled allow signed built-in applications automatically

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsignedapp off
Disabled allow signed downloaded applications automatically

Applications that are signed by a valid certificate authority are automatically added to the list of allowed apps, rather than prompting the user to authorize them. Apps included in macOS are signed by Apple and are allowed to receive incoming connections when this setting is enabled. For example, since iTunes is already signed by Apple, it is automatically allowed to receive incoming connections through the firewall.


If you run an unsigned app that is not listed in the firewall list, a dialog appears with options to Allow or Deny connections for the app. If you choose ""Allow"", macOS signs the application and automatically adds it to the firewall list. If you choose ""Deny"", macOS adds it to the list but denies incoming connections intended for this app.

After interacting with socketfilterfw, restart the process by sending a line hangup signal:
$ sudo pkill -HUP socketfilterfw
Third party firewalls
Programs such as Little Snitch, Hands Off, Radio Silence, LuLu and Security Growler provide a good balance of usability and security.
These programs are capable of monitoring and blocking incoming and outgoing network connections. However, they may require the use of a closed source kernel extension.
If the number of choices of allowing/blocking network connections is overwhelming, use Silent Mode with connections allowed, then periodically check the configuration to gain understanding of applications and what they are doing.
It is worth noting that these firewalls can be bypassed by programs running as root or through OS vulnerabilities (pdf), but they are still worth having - just don't expect absolute protection. However, some malware actually deletes itself and doesn't execute if Little Snitch, or other security software, is installed.
For more on how Little Snitch works, see the Network Kernel Extensions Programming Guide and Shut up snitch! ‚Äì reverse engineering and exploiting a critical Little Snitch vulnerability.
Kernel level packet filtering
A highly customizable, powerful, but also most complicated firewall exists in the kernel. It can be controlled with pfctl and various configuration files.
pf can also be controlled with a GUI application such as IceFloor or Murus.
There are many books and articles on the subject of pf firewall. Here's is just one example of blocking traffic by IP address.
Add the following into a file called pf.rules:
wifi = ""en0""
ether = ""en7""
set block-policy drop
set fingerprints ""/etc/pf.os""
set ruleset-optimization basic
set skip on lo0
scrub in all no-df
table <blocklist> persist
block in log
block in log quick from no-route to any
block log on $wifi from { <blocklist> } to any
block log on $wifi from any to { <blocklist> }
antispoof quick for { $wifi $ether }
pass out proto tcp from { $wifi $ether } to any keep state
pass out proto udp from { $wifi $ether } to any keep state
pass out proto icmp from $wifi to any keep state

Then use the following commands to manipulate the firewall:

sudo pfctl -e -f pf.rules to enable the firewall and load the configuration
sudo pfctl -d to disable the firewall
sudo pfctl -t blocklist -T add 1.2.3.4 to add an IP address to the blocklist
sudo pfctl -t blocklist -T show to view the blocklist
sudo ifconfig pflog0 create to create an interface for logging
sudo tcpdump -ni pflog0 to view filtered packets

Unless you're already familiar with packet filtering, spending too much time configuring pf is not recommended. It is also probably unnecessary if your Mac is behind a NAT on a secure home network.
It is possible to use the pf firewall to block network access to entire ranges of network addresses, for example to a whole organization:
Query Merit RADb for the list of networks in use by an autonomous system, like Facebook:
$ whois -h whois.radb.net '!gAS32934'
Copy and paste the list of networks returned into the blocklist command:
$ sudo pfctl -t blocklist -T add 31.13.24.0/21 31.13.64.0/24 157.240.0.0/16
Confirm the addresses were added:
$ sudo pfctl -t blocklist -T show
No ALTQ support in kernel
ALTQ related functions disabled
   31.13.24.0/21
   31.13.64.0/24
   157.240.0.0/16
Confirm network traffic is blocked to those addresses (note that DNS requests will still work):
$ dig a +short facebook.com
157.240.2.35

$ curl --connect-timeout 5 -I http://facebook.com/
*   Trying 157.240.2.35...
* TCP_NODELAY set
* Connection timed out after 5002 milliseconds
* Closing connection 0
curl: (28) Connection timed out after 5002 milliseconds

$ sudo tcpdump -tqni pflog0 'host 157.240.2.35'
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.162771 > 157.240.2.35.80: tcp 0
Outgoing TCP SYN packets are blocked, so a TCP connection is not established and thus a Web site is effectively blocked at the IP layer.
To use pf to audit ""phone home"" behavior of user and system-level processes, see fix-macosx/net-monitor. See drduh/config/scripts/pf-blocklist.sh for more inspiration.
Services
Note System Integrity Protection does not allow disabling system services on recent macOS versions. Either temporarily disable SIP or disable services from Recovery Mode.
See fix-macosx/yosemite-phone-home, l1k/osxparanoia and karek314/macOS-home-call-drop for further recommendations.
Services on macOS are managed by launchd. See launchd.info, as well as Apple's Daemons and Services Programming Guide and Technical Note TN2083
You can also run KnockKnock that shows more information about startup items.

Use launchctl list to view running user agents
Use sudo launchctl list to view running system daemons
Specify the service name to examine it, e.g. launchctl list com.apple.Maps.mapspushd
Use defaults read to examine job plists in /System/Library/LaunchDaemons and /System/Library/LaunchAgents
Use man and strings to find out more about what an agent/daemon does

For example, to learn what a system launch daemon or agent does, start with:
$ defaults read /System/Library/LaunchDaemons/com.apple.apsd.plist
Look at the Program or ProgramArguments section to see which binary is run, in this case apsd. To find more information about that, look at the man page with man apsd
For example, if you're not interested in Apple Push Notifications, disable the service:
$ sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.apsd.plist
Note Unloading services may break usability of some applications. Read the manual pages and use Google to make sure you understand what you're doing first.
Be careful about disabling any system daemons you don't understand, as it may render your system unbootable. If you break your Mac, use single user mode to fix it.
Use Console and Activity Monitor applications if you notice your Mac heating up, feeling sluggish, or generally misbehaving, as it may have resulted from your tinkering.
To view the status of services:
$ find /var/db/com.apple.xpc.launchd/ -type f -print -exec defaults read {} \; 2>/dev/null
Annotated lists of launch daemons and agents, the respective program executed, and the programs' hash sums are included in this repository.
(Optional) Run the read_launch_plists.py script and diff output to check for any discrepancies on your system, e.g.:
$ diff <(python read_launch_plists.py) <(cat 16A323_launchd.csv)
See also cirrusj.github.io/Yosemite-Stop-Launch for descriptions of services and Provisioning OS X and Disabling Unnecessary Services for another explanation.
Persistent login items may also exist in these directories:

/Library/LaunchAgents
/Library/LaunchDaemons
/Library/ScriptingAdditions
/Library/StartupItems
/System/Library/LaunchAgents
/System/Library/LaunchDaemons
/System/Library/ScriptingAdditions
/System/Library/StartupItems
~/Library/LaunchAgents
~/Library/Preferences/com.apple.loginitems.plist

See Mac OSX Startup (pdf) for more information.
Spotlight Suggestions
Disable Spotlight Suggestions in both the Spotlight preferences and Safari's Search preferences to avoid your search queries being sent to Apple.
Also disable Bing Web Searches in the Spotlight preferences to avoid your search queries being sent to Microsoft.
See fix-macosx.com for detailed instructions.

If you've upgraded to OS X 10.10 ""Yosemite"" and you're using the default settings, each time you start typing in Spotlight (to open an application or search for a file on your computer), your local search terms and location are sent to Apple and third parties (including Microsoft).

Note This Web site and instructions may no longer work on macOS Sierra - see issue 164.
For comparison to Windows 10, see https://fix10.isleaked.com/
Homebrew
Consider using Homebrew to make software installations easier and to update userland tools (see Apple's great GPL purge).
Note If you have not already installed Xcode or Command Line Tools, use xcode-select --install to download and install them, or check Apple's developer site.
Install Homebrew:
$ mkdir homebrew && curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew
Edit PATH in your shell or shell rc file to use ~/homebrew/bin and ~/homebrew/sbin. For example, echo 'PATH=$PATH:~/homebrew/sbin:~/homebrew/bin' >> .zshrc, then change your login shell to Z shell with chsh -s /bin/zsh, open a new Terminal window and run brew update.
Homebrew uses SSL/TLS to talk with GitHub and verifies integrity of downloaded packages, so it's fairly secure.
Remember to periodically run brew update and brew upgrade on trusted and secure networks to download and install software updates. To get information on a package before installation, run brew info <package> and check its recipe online.
According to Homebrew's Anonymous Aggregate User Behaviour Analytics, Homebrew gathers anonymous aggregate user behaviour analytics and reporting these to Google Analytics.
To opt out of Homebrew's analytics, you can set export HOMEBREW_NO_ANALYTICS=1 in your environment or shell rc file, or use brew analytics off.
You may also wish to enable additional security options, such as HOMEBREW_NO_INSECURE_REDIRECT=1 and HOMEBREW_CASK_OPTS=--require-sha.
DNS
Hosts file
Use the hosts file to block known malware, advertising or otherwise unwanted domains.
Edit the hosts file as root, for example with sudo vi /etc/hosts. The hosts file can also be managed with the GUI app 2ndalpha/gasmask.
To block a domain by A record, append any one of the following lines to /etc/hosts:
0 example.com
0.0.0.0 example.com
127.0.0.1 example.com

Note IPv6 uses the AAAA DNS record type, rather than A record type, so you may also want to block those connections by also including ::1 example.com entries, like shown here.
There are many lists of domains available online which you can paste in, just make sure each line starts with 0, 0.0.0.0, 127.0.0.1, and the line 127.0.0.1 localhost is included.
For hosts lists, see someonewhocares.org, l1k/osxparanoia/blob/master/hosts and StevenBlack/hosts.
Append a list of hosts with the tee command and confirm only non-routable addresses or comments were added:
$ curl https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts | sudo tee -a /etc/hosts

$ wc -l /etc/hosts
65580

$ egrep -ve ""^#|^255.255.255|^0.0.0.0|^127.0.0.1|^0 "" /etc/hosts | sort | uniq | sort
::1 localhost
fe80::1%lo0 localhost
[should not return any other IP addresses]
See man hosts and FreeBSD Configuration Files for more information.
See the dnsmasq section of this guide for more hosts blocking options.
dnscrypt
To encrypt outgoing DNS traffic, consider using jedisct1/dnscrypt-proxy. In combination with dnsmasq and DNSSEC, the integrity and authenticity of DNS traffic is greatly improved.
JayBrown/DNSCrypt-Menu and jedisct1/bitbar-dnscrypt-proxy-switcher provide a graphical user interface to dnscrypt.
Install dnscrypt from Homebrew and follow the instructions to configure and start dnscrypt-proxy:
$ brew install dnscrypt-proxy
If using in combination with Dnsmasq, find the file homebrew.mxcl.dnscrypt-proxy.plist by running
$ brew info dnscrypt-proxy
which will show a location like /usr/local/etc/dnscrypt-proxy.toml
Open it in a text editor, find the line starting with listen_addresses = and edit that line to use DNScrypt on a port other than 53, like 5355:
listen_addresses = ['127.0.0.1:5355', '[::1]:5355']

Start DNSCrypt:
$ sudo brew services restart dnscrypt-proxy
Make sure DNSCrypt is running:
$ sudo lsof +c 15 -Pni UDP:5355
COMMAND          PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
dnscrypt-proxy 15244 nobody    7u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   10u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355
dnscrypt-proxy 15244 nobody   12u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   14u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355

By default, dnscrypt-proxy runs on localhost (127.0.0.1), port 53,
and under the ""nobody"" user using the resolvers specified in https://raw.githubusercontent.com/DNSCrypt/dnscrypt-resolvers/master/v2/public-resolvers.md. If you would like to change these settings, you will have to edit the configuration file (e.g. listen_addresses, user_name, urls, etc.)

This can be accomplished by editing /usr/local/etc/dnscrypt-proxy.toml as described above.
You can run your own dnscrypt server (see also drduh/Debian-Privacy-Server-Guide#dnscrypt) from a trusted location or use one of many public servers instead.
Confirm outgoing DNS traffic is encrypted:
$ sudo tcpdump -qtni en0
IP 10.8.8.8.59636 > 107.181.168.52: UDP, length 512
IP 107.181.168.52 > 10.8.8.8.59636: UDP, length 368

$ dig +short -x 128.180.155.106.49321
d0wn-us-ns4
dnscrypt-proxy also has the capability to blacklist domains, including the use of wild-cards. See the Sample configuration file for dnscrypt-proxy for the options.
Note Applications and programs may resolve DNS using their own provided servers. If dnscrypt-proxy is used, it is possible to disable all other, non-dnscrypt DNS traffic with the following pf rules:
block drop quick on !lo0 proto udp from any to any port = 53
block drop quick on !lo0 proto tcp from any to any port = 53
See also What is a DNS leak, the mDNSResponder manual page and ipv6-test.com.
Dnsmasq
Among other features, dnsmasq is able to cache replies, prevent upstream queries for unqualified names, and block entire top-level domain names.
Use in combination with DNSCrypt to additionally encrypt outgoing DNS traffic.
If you don't wish to use DNSCrypt, you should at least use DNS not provided by your ISP. Two popular alternatives are Google DNS and OpenDNS.
(Optional) DNSSEC is a set of extensions to DNS which provide to DNS clients (resolvers) origin authentication of DNS data, authenticated denial of existence, and data integrity. All answers from DNSSEC protected zones are digitally signed. The signed records are authenticated via a chain of trust, starting with a set of verified public keys for the DNS root-zone. The current root-zone trust anchors may be downloaded from IANA website. There are a number of resources on DNSSEC, but probably the best one is dnssec.net website.
Install Dnsmasq (DNSSEC is optional):
$ brew install dnsmasq --with-dnssec
Download drduh/config/dnsmasq.conf:
$ curl -o homebrew/etc/dnsmasq.conf https://raw.githubusercontent.com/drduh/config/master/dnsmasq.conf

Edit the file and examine all the options. To block entire levels of domains, append drduh/config/domains or your own rules.
Install and start the program (sudo is required to bind to privileged port 53):
$ sudo brew services start dnsmasq
To set Dnsmasq as your local DNS server, open System Preferences > Network and select the active interface, then the DNS tab, select + and add 127.0.0.1, or use:
$ sudo networksetup -setdnsservers ""Wi-Fi"" 127.0.0.1
Make sure Dnsmasq is correctly configured:
$ scutil --dns | head
DNS configuration

resolver #1
  search domain[0] : whatever
  nameserver[0] : 127.0.0.1
  flags    : Request A records, Request AAAA records
  reach    : 0x00030002 (Reachable,Local Address,Directly Reachable Address)

$ networksetup -getdnsservers ""Wi-Fi""
127.0.0.1
Note Some VPN software overrides DNS settings on connect. See issue #24 for more information.
Test DNSSEC validation
Test DNSSEC validation succeeds for signed zones - the reply should have NOERROR status and contain ad flag:
$ dig +dnssec icann.org
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 47039
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
Test DNSSEC validation fails for zones that are signed improperly - the reply should have SERVFAIL status:
$ dig www.dnssec-failed.org
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 15190
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1
Captive portal
When macOS connects to new networks, it checks for Internet connectivity and may launch a Captive Portal assistant utility application.
An attacker could trigger the utility and direct a Mac to a site with malware without user interaction, so it's best to disable this feature and log in to captive portals using your regular Web browser by navigating to a non-secure HTTP page and accepting a redirect to the captive portal login interface (after disabling any custom proxy or DNS settings).
$ sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control.plist Active -bool false
Also see Apple's secret ""wispr"" request, How to disable the captive portal window in Mac OS Lion and An undocumented change to Captive Network Assistant settings in OS X 10.10 Yosemite.
Certificate authorities
macOS comes with over 200 root authority certificates installed from for-profit corporations like Apple, Verisign, Thawte, Digicert and government agencies from China, Japan, Netherlands, U.S., and more! These Certificate Authorities (CAs) are capable of issuing SSL/TLS certificates for any domain, code signing certificates, etc.
For more information, see Certification Authority Trust Tracker, Analysis of the HTTPS certificate ecosystem (pdf), and You Won‚Äôt Be Needing These Any More: On Removing Unused Certificates From Trust Stores (pdf).
Inspect system root certificates in Keychain Access, under the System Roots tab or by using the security command line tool and /System/Library/Keychains/SystemRootCertificates.keychain file.
Disable certificate authorities through Keychain Access by marking them as Never Trust and closing the window:

The risk of a man in the middle attack in which a coerced or compromised certificate authority trusted by your system issues a fake/rogue SSL certificate is quite low, but still possible.
OpenSSL
The version of OpenSSL in Sierra is 0.9.8zh which is not current. It doesn't support TLS 1.1 or newer, elliptic curve ciphers, and more.
Since Apple's official supported TLS library on macOS is Secure Transport, OpenSSL deprecated is considered deprecated (according to the Cryptographic Services Guide. Apple's version of OpenSSL may also have patches which may surprise you.
If you're going to use OpenSSL on your Mac, download and install a recent version of OpenSSL with brew install openssl. Note, linking brew to be used in favor of /usr/bin/openssl may interfere with built-in software. See issue #39.
Compare the TLS protocol and cipher between the homebrew version and the system version of OpenSSL:
$ ~/homebrew/bin/openssl version; echo | ~/homebrew/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 1.0.2j  26 Sep 2016
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-GCM-SHA256

$ /usr/bin/openssl version; echo | /usr/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 0.9.8zh 14 Jan 2016
SSL-Session:
    Protocol  : TLSv1
    Cipher    : AES128-SHA
See also Comparison of TLS implementations, How's My SSL and Qualys SSL Labs Tools.
Curl
The version of Curl which comes with macOS uses Secure Transport for SSL/TLS validation.
If you prefer to use OpenSSL, install with brew install curl --with-openssl and ensure it's the default with brew link --force curl
Download drduh/config/curlrc or see the man page:
$ curl -o ~/.curlrc https://raw.githubusercontent.com/drduh/config/master/curlrc
Web
Privoxy
Consider using Privoxy as a local proxy to filter Web browsing traffic.
Note macOS proxy settings are not universal; apps and services may not honor system proxy settings. Ensure the application you wish to proxy is correctly configured and manually verify connections don't leak. Additionally, it may be possible to configure the pf firewall to transparently proxy all traffic.
A signed installation package for privoxy can be downloaded from silvester.org.uk or Sourceforge. The signed package is more secure than the Homebrew version, and attracts full support from the Privoxy project.
Alternatively, install and start privoxy using Homebrew:
$ brew install privoxy

$ brew services start privoxy
By default, privoxy listens on localhost, TCP port 8118.
Set the system HTTP proxy for your active network interface 127.0.0.1 and 8118 (This can be done through System Preferences > Network > Advanced > Proxies):
$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 8118
(Optional) Set the system HTTPS proxy, which still allows for domain name filtering, with:
$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 8118
Confirm the proxy is set:
$ scutil --proxy
<dictionary> {
  ExceptionsList : <array> {
    0 : *.local
    1 : 169.254/16
  }
  FTPPassive : 1
  HTTPEnable : 1
  HTTPPort : 8118
  HTTPProxy : 127.0.0.1
}
Visit http://p.p/ in a browser, or with Curl:
$ ALL_PROXY=127.0.0.1:8118 curl -I http://p.p/
HTTP/1.1 200 OK
Content-Length: 2401
Content-Type: text/html
Cache-Control: no-cache
Privoxy already comes with many good rules, however you can also write your own.
Download drduh/config/privoxy/config and drduh/config/privoxy/user.action to get started:
$ curl -o homebrew/etc/privoxy/config https://raw.githubusercontent.com/drduh/config/master/privoxy/config

$ curl -o homebrew/etc/privoxy/user.action https://raw.githubusercontent.com/drduh/config/master/privoxy/user.action
Restart Privoxy: and verify it's blocking and redirecting traffic:
$ sudo brew services restart privoxy

$ ALL_PROXY=127.0.0.1:8118 curl ads.foo.com/ -IL
HTTP/1.1 403 Request blocked by Privoxy
Content-Type: image/gif
Content-Length: 64
Cache-Control: no-cache

$ ALL_PROXY=127.0.0.1:8118 curl imgur.com/ -IL
HTTP/1.1 302 Local Redirect from Privoxy
Location: https://imgur.com/
Content-Length: 0
Date: Sun, 09 Oct 2016 18:48:19 GMT

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
You can replace ad images with pictures of kittens, for example, by starting a local Web server and redirecting blocked requests to localhost.
Browser
The Web browser poses the largest security and privacy risk, as its fundamental job is to download and execute untrusted code from the Internet. This is an important statement. The unique use case of Web Browsers of operation in hostile environments, has forced them to adopt certain impressive security features. The cornerstone of Web Browser security is the Same Origin Policy (SOP). In a few words, SOP prevents a malicious script on one page from obtaining access to sensitive data on another web page through that page's Document Object Model (DOM). If SOP is compromised, the security of the whole Web Browser is compromised.
The best tip to ensure secure browsing regardless your choice of Web Browser is proper security hygiene. The majority of Web Browser exploits require social engineering attacks to achieve native code execution. Always be mindful of the links you click and be extra careful when websites ask you to download and install software. 99% percent of the time that software is malware.
Another important consideration about Web Browser security is Web Extensions. Web Extensions greatly increase the attack surface of the Web Browser. This is an issue that plagues Firefox and Chrome alike. Luckily, Web Extensions can only access specific browser APIs that are being governed by their manifest. That means we can quickly audit their behavior and remove them if they request access to information they shouldn't (why would an Ad blocker require camera access?). In the interest of security, it is best to limit your use of Web Extensions.
Mozilla Firefox, Google Chrome, Safari, and Tor Browser are covered in this guide. Each Web Browser offers certain benefits and drawbacks regarding their security and privacy. It is best to make an informed choice and not necessarily commit to only one.
Firefox
Mozilla Firefox is an excellent browser as well as being completely open source. Currently, Firefox is in a renaissance period. It replaces major parts of its infrastructure and code base under projects Quantum and Photon. Part of the Quantum project is to replace C++ code with Rust. Rust is a systems programming language with a focus on security and thread safety. It is expected that Rust adoption will greatly improve the overall security posture of Firefox.
Firefox offers a similar security model to Chrome: it has a bug bounty program, although it is not a lucrative as Chrome's. Firefox follows a six-week release cycle similar to Chrome. See discussion in issues #2 and #90 for more information about certain differences in Firefox and Chrome.
Firefox supports user-supplied configuration files. See drduh/config/user.js, pyllyukko/user.js and ghacksuserjs/ghacks-user.js for recommended preferences and hardening measures. Also see NoScript, an extension which allows whitelist-based, pre-emptive script blocking.
Firefox is focused on user privacy. It supports tracking protection in Private Browsing mode. The tracking protection can be enabled for the default account, although it may break the browsing experience on some websites. Another feature for added privacy unique to Firefox is Containers, similar to Chrome profiles.
Previous versions of Firefox used a Web Extension SDK that was quite invasive and offered immense freedom to developers. Sadly, that freedom also introduced a number of vulnerabilities in Firefox that greatly affected its users. You can find more information about vulnerabilities introduced by Firefox's legacy extensions in this paper (pdf). Currently, Firefox only supports Web Extensions through the Web Extension Api, which is very similar to Chrome's.
Submission of Web Extensions in Firefox is free. Web Extensions in Firefox most of the time are open source, although certain Web Extensions are proprietary.
Note Similar to Chrome and Safari, Firefox allows account sync across multiple devices. While stored login passwords are encrypted, Firefox does not require a password to reveal their plain text format. Firefox only displays as yes/no prompt. This is an important security issue. Keep that in mind if you sign in to your Firefox account from devices that do not belong to you and leave them unattended. The issue has been raised among the Firefox community and hopefully will be resolved in the coming versions.
See drduh/config/firefox.user.js for additional Firefox configuration options to improve security and privacy.
Chrome
Google Chrome is based on the open source Chromium project with certain proprietary components:

Automatic updates with GoogleSoftwareUpdateDaemon.
Usage tracking and crash reporting, which can be disabled through Chrome's settings.
Chrome Web Store.
Adobe Flash Plugin - supports a Pepper API version of Adobe Flash which gets updated automatically with Chrome.
Media Codec support - adds support for proprietary codecs.
Chrome PDF viewer.
Non-optional tracking. Google Chrome installer includes a randomly generated token. The token is sent to Google after the installation completes in order to measure the success rate. The RLZ identifier stores information ‚Äì in the form of encoded strings ‚Äì like the source of chrome download and installation week. It doesn‚Äôt include any personal information and it‚Äôs used to measure the effectiveness of a promotional campaign. Chrome downloaded from Google‚Äôs website doesn‚Äôt have the RLZ identifier. The source code to decode the strings is made open by Google.

Chrome offers account sync between multiple devices. Part of the sync data are stored website credentials. The login passwords are encrypted and in order to access them, a user's Google account password is required. You can use your Google account to sign to your Chrome customized settings from other devices while retaining your the security of your passwords.
Chrome's Web store for extensions requires a 5 dollar lifetime fee in order to submit extensions. The low cost allows the development of many quality Open Source Web Extensions that do not aim to monetize through usage.
Chrome has the largest share of global usage and is the preferred target platform for the majority of developers. Major technologies are based on Chrome's Open Source components, such as node.js which uses Chrome's V8 Engine and the Electron framework, which is based on Chromium and node.js. Chrome's vast user base makes it the most attractive target for threat actors and security researchers. Despite under constants attacks, Chrome has retained an impressive security track record over the years. This is not a small feat.
Chrome offers separate profiles, sandboxing, frequent updates (including Flash, although you should disable it - see below), and carries impressive credentials. In addition, Google offers a very lucrative bounty program for reporting vulnerabilities along with its own Project Zero. This means that a large number of highly talented and motivated people are constantly auditing Chrome's code base.
Create separate Chrome profiles to reduce XSS risk and compartmentalize cookies/identities. In each profile, either disable Javascript in Chrome settings and manually whitelist allowed origins - or use uBlock Origin to manage Javascript and/or disable third-party scripts/frames. Also install HTTPSEverywhere to upgrade insecure connections.
Change the default search engine from Google to reduce additional tracking.
Disable DNS prefetching (see also DNS Prefetching and Its Privacy Implications (pdf)).
Read Chromium Security and Chromium Privacy for more detailed, technical information.
Read Google's privacy policy and learn which Google services collect personal information. Google is open about the data it stores and how it used them. Users can opt out from many of those services and see what type of information Google has stored from their account settings.
Safari
Safari is the default Web browser of macOS. It is also the most optimized browser for reducing battery use. Safari, like Chrome, has both Open Source and proprietary components. Safari is based on the open source Web Engine WebKit, which is ubiquitous among the macOS ecosystem. WebKit is used by Apple apps such as Mail, iTunes, iBooks, and the App Store. Chrome's Blink engine is a fork of WebKit and both engines share a number of similarities.
Safari supports certain unique features that benefit user security and privacy. Content blockers enables the creation of content blocking rules without using Javascript. This rule based approach greatly improves memory user, security, and privacy. Safari 11 introduced an Intelligent Tracking Prevention system. This feature automatically removes tracking data stored in Safari after a period of non-interaction by the user from the tracker's website.
Similar to Chrome and Firefox, Safari offers an invite only bounty program for bug reporting to a select number of security researchers. The bounty program was announced during Apple's presentation at BlackHat 2016.
Web Extensions in Safari have an additional option to use native code in the Safari's sandbox environment, in addition to Web Extension APIs. Web Extensions in Safari are also distributed through Apple's App store. App store submission comes with the added benefit of Web Extension code being audited by Apple. On the other hand App store submission comes at a steep cost. Yearly developer subscription fee costs 100 USD (in contrast to Chrome's 5 dollar lifetime fee and Firefox's free submission). The high cost is prohibitive for the majority of Open Source developers. As a result, Safari has very few extensions to choose from. However, you should keep the high cost in mind when installing extensions. It is expected that most Web Extensions will have some way of monetizing usage in order to cover developer costs. And be extra careful when the Web Extension's source code is not Open Source. On a side note, some Safari extensions are Open Source and freely available. Be grateful to those developers.
Safari syncs user preferences and saved passwords with iCloud Keychain. In order to be viewed in plain text, a user must input the account password of the current device. This means that users can sync data across devices with added security.
Safari follows a slower release cycle than Chrome and Firefox (3-4 minor releases, 1 major release, per year). Newer features are slower to be adopted to the stable channel. Although security updates in Safari are handled independent of the stable release schedule and issued automatically through the App store. The Safari channel that follows a six-week release cycle (similar to as Chrome and Firefox) is called Safari Technology Preview and it is the recommended option instead of the stable channel of Safari.
An excellent open source ad blocker for Safari that fully leverages Content blockers is dgraham/Ka-Block. Ka-Block is focussed on user privacy. The only time the extension makes a network connection is when a new version of the extension is released. See also el1t/uBlock-Safari to disable hyperlink auditing beacons.
Other Web browsers
Many Chromium-derived browsers are not recommended. They are usually closed source, poorly maintained, have bugs, and make dubious claims to protect privacy. See The Private Life of Chromium Browsers.
Other miscellaneous browsers, such as Brave, are not evaluated in this guide, so are neither recommended nor actively discouraged from use.
Web browsers and privacy
All Web Browsers retain certain information about our browsing habits. That information is used for a number of reasons. One of them is to improve the overall performance of the Web Browser. Most Web Browsers offer prediction services to resolve typos or URL redirections, store analytics data of browsing patterns, crash reports and black listing of known malicious servers. Those options can be turned on and off from each Web browser's settings panel.
Since Web browsers execute untrusted code from the server, it is important to understand what type of information can be accessed. The Navigator interface gives access to information about the Web Browser's user agent. Those include information such as the operating system, Web sites' permissions, and the device's battery level. For more information about security conscious browsing and what type of information is being ""leaked"" by your browser, see HowTo: Privacy & Security Conscious Browsing, browserleaks.com and EFF Panopticlick.
To hinder third party trackers, it is recommended to disable third-party cookies in Web browser settings. A third party cookie is a cookie associated with a file requested by a different domain than the one the user is currently viewing. Most of the time third-party cookies are used to create browsing profiles by tracking a user's movement on the web. Disabling third-party cookies prevents HTTP responses and scripts from other domains from setting cookies. Moreover, cookies are removed from requests to domains that are not the document origin domain, so cookies are only sent to the current site that is being viewed.
Also be aware of WebRTC, which may reveal your local or public (if connected to VPN) IP address(es). In Firefox and Chrome/Chromium this can be disabled with extensions such as uBlock Origin and rentamob/WebRTC-Leak-Prevent. Disabling WebRTC in Safari is only possible with a system hack.
Plugins
Adobe Flash, Oracle Java, Adobe Reader, Microsoft Silverlight (Netflix now works with HTML5) and other plugins are security risks and should not be installed.
If they are necessary, only use them in a disposable virtual machine and subscribe to security announcements to make sure you're always patched.
See Hacking Team Flash Zero-Day, Java Trojan BackDoor.Flashback, Acrobat Reader: Security Vulnerabilities, and Angling for Silverlight Exploits for examples.
Tor
Tor is an anonymizing proxy which can be used for browsing the Web.
Download Tor Browser from Tor Project.
Do not attempt to configure other browsers or applications to use Tor as you may make a mistake which will compromise anonymity.
Download both the dmg and asc signature files, then verify the disk image has been signed by Tor developers:
$ cd ~/Downloads

$ file Tor*
TorBrowser-8.0.4-osx64_en-US.dmg:     bzip2 compressed data, block size = 900k
TorBrowser-8.0.4-osx64_en-US.dmg.asc: PGP signature Signature (old)

$ gpg Tor*asc
[...]
gpg: Can't check signature: No public key

$ gpg --recv 0x4E2C6E8793298290
gpg: key 0x4E2C6E8793298290: public key ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" imported
gpg: no ultimately trusted keys found
gpg: Total number processed: 1
gpg:               imported: 1

$ gpg --verify Tor*asc
gpg: assuming signed data in 'TorBrowser-8.0.4-osx64_en-US.dmg'
gpg: Signature made Mon Dec 10 07:16:22 2018 PST
gpg:                using RSA key 0xEB774491D9FF06E2
gpg: Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: EF6E 286D DA85 EA2A 4BA7  DE68 4E2C 6E87 9329 8290
     Subkey fingerprint: 1107 75B5 D101 FB36 BC6C  911B EB77 4491 D9FF 06E2
Make sure Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" appears in the output. The warning about the key not being certified is benign, as it has not yet been manually assigned trust.
See How to verify signatures for packages for more information.
To finish installing Tor Browser, open the disk image and drag the it into the Applications folder, or with:
$ hdiutil mount TorBrowser-8.0.4-osx64_en-US.dmg

$ cp -r /Volumes/Tor\ Browser/Tor\ Browser.app/ ~/Applications/

Verify the Tor application's code signature was made by with The Tor Project's Apple developer ID MADPSAYN6T, using the spctl -a -v and/or pkgutil --check-signature commands:
$ spctl -a -vv ~/Applications/Tor\ Browser.app
/Users/drduh/Applications/Tor Browser.app: accepted
source=Developer ID
origin=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)

$ pkgutil --check-signature ~/Applications/Tor\ Browser.app
Package ""Tor Browser.app"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
       SHA1 fingerprint: 95 80 54 F1 54 66 F3 9C C2 D8 27 7A 29 21 D9 61 11 93 B3 E8
       -----------------------------------------------------------------------------
    2. Developer ID Certification Authority
       SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
You can also use the codesign command to examine an application's code signature:
$ codesign -dvv ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox
Identifier=org.torproject.torbrowser
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20200 size=229 flags=0x0(none) hashes=4+3 location=embedded
Library validation warning=OS X SDK version before 10.9 does not support Library Validation
Signature size=4247
Authority=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
Authority=Developer ID Certification Authority
Authority=Apple Root CA
Signed Time=Dec 10, 2018 at 12:18:45 AM
Info.plist entries=24
TeamIdentifier=MADPSAYN6T
Sealed Resources version=2 rules=12 files=128
Internal requirements count=1 size=188
To view full certificate details for a signed application, extract them with codesign and decode it with openssl:
$ codesign -d --extract-certificates ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox

$ file codesign*
codesign0: data
codesign1: data
codesign2: data

$ openssl x509 -inform der -in codesign0 -subject -issuer -startdate -enddate -noout
subject= /UID=MADPSAYN6T/CN=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)/OU=MADPSAYN6T/O=The Tor Project, Inc/C=US
issuer= /CN=Developer ID Certification Authority/OU=Apple Certification Authority/O=Apple Inc./C=US
notBefore=Apr 12 22:40:13 2016 GMT
notAfter=Apr 13 22:40:13 2021 GMT

$ openssl x509 -inform der -in codesign0  -fingerprint -noout
SHA1 Fingerprint=95:80:54:F1:54:66:F3:9C:C2:D8:27:7A:29:21:D9:61:11:93:B3:E8

$ openssl x509 -inform der -in codesign0 -fingerprint -sha256 -noout
SHA256 Fingerprint=B5:0D:47:F0:3E:CB:42:B6:68:1C:6F:38:06:2B:C2:9F:41:FA:D6:54:F1:29:D3:E4:DD:9C:C7:49:35:FF:F5:D9
Tor traffic is encrypted to the exit node (i.e., cannot be read by a passive network eavesdropper), but Tor use can be identified - for example, TLS handshake ""hostnames"" will show up in plaintext:
$ sudo tcpdump -An ""tcp"" | grep ""www""
listening on pktap, link-type PKTAP (Apple DLT_PKTAP), capture size 262144 bytes
............."". ...www.odezz26nvv7jeqz1xghzs.com.........
.............#.!...www.bxbko3qi7vacgwyk4ggulh.com.........
.6....m.....>...:.........|../*	Z....W....X=..6...C../....................................0...0..0.......'....F./0..	*.H........0%1#0!..U....www.b6zazzahl3h3faf4x2.com0...160402000000Z..170317000000Z0'1%0#..U....www.tm3ddrghe22wgqna5u8g.net0..0..
See Tor Protocol Specification and Tor/TLSHistory for more information.
You may wish to additionally obfuscate Tor traffic using a pluggable transport, such as Yawning/obfs4proxy or SRI-CSL/stegotorus.
This can be done by setting up your own Tor relay or finding an existing private or public bridge to serve as an obfuscating entry node.
For extra security, use Tor inside a VirtualBox or VMware virtualized GNU/Linux or BSD machine.
Finally, remember the Tor network provides anonymity, which is not necessarily synonymous with privacy. The Tor network does not guarantee protection against a global observer capable of traffic analysis and correlation. See also Seeking Anonymity in an Internet Panopticon (pdf) and Traffic Correlation on Tor by Realistic Adversaries (pdf).
Also see Invisible Internet Project (I2P) and its Tor comparison.
VPN
Unencrypted network traffic is being actively monitored and possibly tampered with. Encrypted traffic still exposes connection metadata and could be used to infer behavior or specific actions.
It is a good idea to use a VPN with outgoing network traffic (not split tunnel) together with a trustworthy provider. drduh/Debian-Privacy-Server-Guide is one of many available guides for setting up a personal VPN server.
Don't just blindly sign up for a VPN service without understanding the full implications and how your traffic will be routed. If you don't understand how the VPN works or are not familiar with the software used, you are probably better off without it.
When choosing a VPN service or setting up your own, be sure to research the protocols, key exchange algorithms, authentication mechanisms, and type of encryption being used. Some protocols, such as PPTP, should be avoided in favor of OpenVPN, for example. Strong cryptographic algorithms like AES-256, RSA-4096, SHA-256 should be preferred.
Some clients may send traffic over the next available interface when VPN is interrupted or disconnected. See scy/8122924 for an example on how to allow traffic only over VPN.
Another set of scripts to lock down your system so it will only access the internet via a VPN can be found as part of the Voodoo Privacy project - sarfata/voodooprivacy and there is an updated guide to setting up an IPSec VPN on a virtual machine (hwdsl2/setup-ipsec-vpn) or a docker container (hwdsl2/docker-ipsec-vpn-server).
It may be worthwhile to consider the geographical location of the VPN provider. See further discussion in issue #114.
Also see this technical overview of the macOS built-in VPN L2TP/IPSec and IKEv2 client.
Further, it is possible to run the contemporary Linux-based Wireguard VPN either from a Linux VM or via a set of cross platform tools.
Other Open Source OpenVPN clients/GUI: Eddie, Pritunl are not evaluated in this guide, so are neither recommended nor actively discouraged from use.
PGP/GPG
PGP is a standard for encrypting email end to end. That means only the chosen recipients can decrypt a message, unlike regular email which is read and forever archived by providers.
GPG, or GNU Privacy Guard, is a GPL-licensed open source program compliant with the PGP standard.
GPG is used to verify signatures of software you download and install, as well as symmetrically or asymmetrically encrypt files and text.
Install from Homebrew with brew install gnupg.
If you prefer a graphical application, download and install GPG Suite.
Download drduh/config/gpg.conf to use recommended settings:
$ curl -o ~/.gnupg/gpg.conf https://raw.githubusercontent.com/drduh/config/master/gpg.conf
See drduh/YubiKey-Guide to securely generate and store GPG keys.
Read online guides and practice encrypting and decrypting email to yourself and your friends. Get them interested in this stuff!
OTR
OTR stands for off-the-record and is a cryptographic protocol for encrypting and authenticating conversations over instant messaging.
You can use OTR on top of any existing XMPP chat service, even Google Hangouts (which only encrypts conversations between users and the server using TLS).
The first time you start a conversation with someone new, you'll be asked to verify their public key fingerprint. Make sure to do this in person or by some other secure means (e.g. GPG encrypted mail).
A popular macOS GUI client for XMPP and other chat protocols is Adium.
Other XMPP clients include profanity and agl/xmpp-client. Another relatively new XMPP chat client is CoyIM, it's focused and security and has built-in support for OTR and Tor.
If you want to know how OTR works, read the paper Off-the-Record Communication, or, Why Not To Use PGP (pdf)
Viruses and malware
There is an ever-increasing amount of Mac malware in the wild. Macs aren't immune from viruses and malicious software!
Some malware comes bundled with both legitimate software, such as the Java bundling Ask Toolbar, and some with illegitimate software, such as Mac.BackDoor.iWorm bundled with pirated programs. Malwarebytes Anti-Malware for Mac is an excellent program for ridding oneself of ""garden-variety"" malware and other ""crapware"".
See Methods of malware persistence on Mac OS X (pdf) and Malware Persistence on OS X Yosemite to learn about how garden-variety malware functions.
You could periodically run a tool like Knock Knock to examine persistent applications (e.g. scripts, binaries). But by then, it is probably too late. Maybe applications such as Block Block and Ostiarius will help. See warnings and caveats in issue #90 first, however. An open-source alternative could be maclaunch.sh.
Anti-virus programs are a double-edged sword -- not so useful for advanced users and will likely increase attack surface against sophisticated threats; however possibly useful for catching ""garden variety"" malware on novice users' Macs. There is also the additional processing overhead to consider when using ""active"" scanning features.
See Sophail: Applied attacks against Antivirus (pdf), Analysis and Exploitation of an ESET Vulnerability, a trivial Avast RCE, Popular Security Software Came Under Relentless NSA and GCHQ Attacks, How Israel Caught Russian Hackers Scouring the World for U.S. Secrets and AVG: ""Web TuneUP"" extension multiple critical vulnerabilities.
Therefore, the best anti-virus is Common Sense 2019. See discussion in issue #44.
Local privilege escalation bugs are plenty on macOS, so always be careful when downloading and running untrusted programs or trusted programs from third party websites or downloaded over HTTP (example).
Subscribe to updates at The Safe Mac and Malwarebytes Blog for current Mac security news.
To scan an application with multiple AV products and examine its behavior, upload it to VirusTotal.
Also check out Hacking Team malware for macOS: root installation for MacOS, Support driver for Mac Agent and RCS Agent for Mac, which is a good example of advanced malware with capabilities to hide from userland (e.g., ps, ls), for example. For more, see A Brief Analysis of an RCS Implant Installer and reverse.put.as
System Integrity Protection
System Integrity Protection (SIP) is a security feature since OS X 10.11 ""El Capitan"". It is enabled by default, but can be disabled, which may be necessary to change some system settings, such as deleting root certificate authorities or unloading certain launch daemons. Keep this feature on, as it is by default.
From What's New in OS X 10.11:

A new security policy that applies to every running process, including privileged code and code that runs out of the sandbox. The policy extends additional protections to components on disk and at run-time, only allowing system binaries to be modified by the system installer and software updates. Code injection and runtime attachments to system binaries are no longer permitted.

Also see What is the ‚Äúrootless‚Äù feature in El Capitan, really?
Some MacBook hardware has shipped with SIP disabled. To verify SIP is enabled, use the command csrutil status, which should return: System Integrity Protection status: enabled. Otherwise, enable SIP through Recovery Mode.
Gatekeeper and XProtect
Gatekeeper and the quarantine system try to prevent unsigned or ""bad"" programs and files from running and opening.
XProtect prevents the execution of known bad files and outdated plugin versions, but does nothing to cleanup or stop existing malware.
Both offer trivial protection against common risks and are fine at default settings.
See also Mac Malware Guide : How does Mac OS X protect me? and Gatekeeper, XProtect and the Quarantine attribute.
Note Quarantine stores information about downloaded files in ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2, which may pose a privacy risk. To examine the file, simply use strings or the following command:
$ echo 'SELECT datetime(LSQuarantineTimeStamp + 978307200, ""unixepoch"") as LSQuarantineTimeStamp, ' \
  'LSQuarantineAgentName, LSQuarantineOriginURLString, LSQuarantineDataURLString from LSQuarantineEvent;' | \
  sqlite3 /Users/$USER/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
See here for more information.
To permanently disable this feature, clear the file and make it immutable:
$ :>~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2

$ sudo chflags schg ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
Metadata and artifacts
macOS attaches metadata (HFS+ extended attributes) to downloaded files, which can be viewed with the mdls and xattr commands:
$ ls -l@ ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
-rw-r--r--@ 1 drduh staff 63M Jan 1 12:00 TorBrowser-8.0.4-osx64_en-US.dmg
	com.apple.metadata:kMDItemWhereFroms	  46B
	com.apple.quarantine	  57B

$ mdls ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
kMDItemContentCreationDate         = 2019-01-01 00:00:00 +0000
kMDItemContentCreationDate_Ranking = 2019-01-01 00:00:00 +0000
kMDItemContentModificationDate     = 2019-01-01 00:00:00 +0000
kMDItemContentType                 = ""com.apple.disk-image-udif""
kMDItemContentTypeTree             = (
    ""public.archive"",
    ""public.item"",
    ""public.data"",
    ""public.disk-image"",
    ""com.apple.disk-image"",
    ""com.apple.disk-image-udif""
)
kMDItemDateAdded                   = 2019-01-01 00:00:00 +0000
kMDItemDateAdded_Ranking           = 2019-01-01 00:00:00 +0000
kMDItemDisplayName                 = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSContentChangeDate         = 2019-01-01 00:00:00 +0000
kMDItemFSCreationDate              = 2019-01-01 00:00:00 +0000
kMDItemFSCreatorCode               = """"
kMDItemFSFinderFlags               = 0
kMDItemFSHasCustomIcon             = (null)
kMDItemFSInvisible                 = 0
kMDItemFSIsExtensionHidden         = 0
kMDItemFSIsStationery              = (null)
kMDItemFSLabel                     = 0
kMDItemFSName                      = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSNodeCount                 = (null)
kMDItemFSOwnerGroupID              = 5000
kMDItemFSOwnerUserID               = 501
kMDItemFSSize                      = 65840402
kMDItemFSTypeCode                  = """"
kMDItemInterestingDate_Ranking     = 2019-01-01 00:00:00 +0000
kMDItemKind                        = ""Disk Image""
kMDItemWhereFroms                  = (
    ""https://dist.torproject.org/torbrowser/8.0.4/TorBrowser-8.0.4-osx64_en-US.dmg"",
    ""https://www.torproject.org/projects/torbrowser.html.en""
)

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
com.apple.metadata:kMDItemWhereFroms:
00000000 ¬†62 70 6C 69 73 74 30 30 A2 01 02 5F 10 4D 68 74 ¬†|bplist00..._.Mht|
00000010 ¬†74 70 73 3A 2F 2F 64 69 73 74 2E 74 6F 72 70 72 ¬†|tps://dist.torpr|
00000020 ¬†6F 6A 65 63 74 2E 6F 72 67 2F 74 6F 72 62 72 6F ¬†|oject.org/torbro|
[...]
com.apple.quarantine: 0081;58519ffa;Google Chrome.app;1F032CAB-F5A1-4D92-84EB-CBECA971B7BC
Metadata attributes can also be removed with the -d flag:
$ xattr -d com.apple.metadata:kMDItemWhereFroms ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -d com.apple.quarantine ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
[No output expected]
Other metadata and artifacts may be found in the directories including, but not limited to, ~/Library/Preferences/, ~/Library/Containers/<APP>/Data/Library/Preferences, /Library/Preferences, some of which is detailed below.
~/Library/Preferences/com.apple.sidebarlists.plist contains historical list of volumes attached. To clear it, use the command /usr/libexec/PlistBuddy -c ""delete :systemitems:VolumesList"" ~/Library/Preferences/com.apple.sidebarlists.plist
/Library/Preferences/com.apple.Bluetooth.plist contains Bluetooth metadata, including device history. If Bluetooth is not used, the metadata can be cleared with:
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist DeviceCache
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist IDSPairedDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANInterfaces
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist SCOAudioDevices
/var/spool/cups contains the CUPS printer job cache. To clear it, use the commands:
$ sudo rm -rfv /var/spool/cups/c0*
$ sudo rm -rfv /var/spool/cups/tmp/*
$ sudo rm -rfv /var/spool/cups/cache/job.cache*
To clear the list of iOS devices connected, use:
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist Devices
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist Devices
$ sudo rm -rfv /var/db/lockdown/*
Quicklook thumbnail data can be cleared using the qlmanage -r cache command, but this writes to the file resetreason in the Quicklook directories, and states that the Quicklook cache was manually cleared. Disable the thumbnail cache with qlmanage -r disablecache
It can also be manually cleared by getting the directory names with getconf DARWIN_USER_CACHE_DIR and sudo getconf DARWIN_USER_CACHE_DIR, then removing them:
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
Similarly, for the root user:
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
Also see 'quicklook' cache may leak encrypted data.
To clear Finder preferences:
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXDesktopVolumePositions
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXRecentFolders
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentMoveAndCopyDestinations
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentSearches
$ defaults delete ~/Library/Preferences/com.apple.finder.plist SGTRecentFileSearches
Additional diagnostic files may be found in the following directories - but caution should be taken before removing any, as it may break logging or cause other issues:
/var/db/CoreDuet/
/var/db/diagnostics/
/var/db/systemstats/
/var/db/uuidtext/
/var/log/DiagnosticMessages/

macOS stored preferred Wi-Fi data (including credentials) in NVRAM. To clear it, use the following commands:
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:current-network
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-networks
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-count
macOS may collect sensitive information about what you type, even if user dictionary and suggestions are off. To remove them, and prevent them from being created again, use the following commands:
$ rm -rfv ""~/Library/LanguageModeling/*"" ""~/Library/Spelling/*"" ""~/Library/Suggestions/*""
$ chmod -R 000 ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
$ chflags -R uchg ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
QuickLook application support metadata can be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Application Support/Quick Look/*""
$ chmod -R 000 ""~/Library/Application Support/Quick Look""
$ chflags -R uchg ""~/Library/Application Support/Quick Look""
Document revision metadata is stored in /.DocumentRevisions-V100 and can be cleared and locked with the following commands - caution should be taken as this may break some core Apple applications:
$ sudo rm -rfv /.DocumentRevisions-V100/*
$ sudo chmod -R 000 /.DocumentRevisions-V100
$ sudo chflags -R uchg /.DocumentRevisions-V100
Saved application state metadata may be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Saved Application State/*""
$ rm -rfv ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chmod -R 000 ""~/Library/Saved Application State/""
$ chmod -R 000 ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chflags -R uchg ""~/Library/Saved Application State/""
$ chflags -R uchg ""~/Library/Containers/<APPNAME>/Saved Application State""
Autosave metadata can be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ rm -rfv ""~/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Autosave Information""
The Siri analytics database, which is created even if the Siri launch agent disabled, can be cleared and locked with the following commands:
$ rm -rfv ~/Library/Assistant/SiriAnalytics.db
$ chmod -R 000 ~/Library/Assistant/SiriAnalytics.db
$ chflags -R uchg ~/Library/Assistant/SiriAnalytics.db
~/Library/Preferences/com.apple.iTunes.plist contains iTunes metadata. Recent iTunes search data may be cleared with the following command:
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist recentSearches
If you do not use Apple ID-linked services, the following keys may be cleared, too, using the following commands:
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist StoreUserInfo
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist WirelessBuddyID
All media played in QuickTime Player can be found in:
~/Library/Containers/com.apple.QuickTimePlayerX/Data/Library/Preferences/com.apple.QuickTimePlayerX.plist

Additional metadata may exist in the following files:
~/Library/Containers/com.apple.appstore/Data/Library/Preferences/com.apple.commerce.knownclients.plist
~/Library/Preferences/com.apple.commerce.plist
~/Library/Preferences/com.apple.QuickTimePlayerX.plist

Passwords
Generate strong passwords with several programs or directly from /dev/urandom:
$ openssl rand -base64 30
qb8ZWbUU2Ri3FOAPY/1wKSFAJwMXmpQM4mZU4YbO

$ gpg --gen-random -a 0 90 | fold -w 40
3e+kfHOvovHVXxZYPgu+OOWQ1g1ttbljr+kNGv7f
loD//RsjUXYGIjfPM/bT0itsoEstyGLVUsFns8wP
zYM8VRBga+TsnxWrS7lWKfH1uvVPowzkq9kXCdvJ

$ LANG=C tr -dc 'A-F0-9' < /dev/urandom | fold -w 40 | head -n 5
45D0371481EE5E5A5C1F68EA59E69F9CA52CB321
A30B37A00302643921F205621B145E7EAF520164
B6EF38A2DA1D0586D20105502AFFF0468EA5F16A
029D6EA9F76CD64D3356E342EA154BEFEBE23387
07F468F0569579A0A06471247CABC4F4C1386E24

$ tr -dc '[:alnum:]' < /dev/urandom | fold -w 40 | head -n5
zmj8S0iuxud8y8YHjzdg7Hefu6U1KAYBiLl3aE8v
nCNpuMkWohTjQHntTzbiLQJG5zLzEHWSWaYSwjtm
R2L6M909S3ih852IkJqQFMDawCiHcpPBxlllAPrt
aZOXKVUmxhzQwVSYb6nqAbGTVMFSJOLf094bFZAb
HfgwSNlkVBXwIPQST6E6x6vDNCCasMLSSOoTUfSK

$ tr -dc '[:lower:]' < /dev/urandom | fold -w 40 | head -n5
gfvkanntxutzwxficgvavbwdvttexdezdftvvtmn
lgrsuiugwkqbtbkyggcbpbqlynwbiyxzlabstqcf
ufctdlsbyonkowzpmotxiksnsbwdzkjrjsupoqvr
hjwibdjxtmuvqricljayzkgdfztcmapsgwsubggr
bjstlmvwjczakgeetkbmwbjnidbeaerhaonpkacg

$ tr -dc '[:upper:]' < /dev/urandom | fold -w 40 | head -n5
EUHZMAOBOLNFXUNNDSTLJTPDCPVQBPUEQOLRZUQZ
HVNVKBEPAAYMXRCGVCNEZLFHNUYMRYPTWPWOOZVM
TAHEUPQJTSYQVJVYSKLURESMKWEZONXLUDHWQODB
PRDITWMAXXZLTRXEEOGOSGAWUXYDGDRJYRHUWICM
VHERIQBLBPHSIUZSGYZRDHTNAPUGJMRODIKBWZRJ

$ tr -dc '[:graph:]' < /dev/urandom | fold -w 40 | head -n5
n\T2|zUz:\C,@z9!#p3!B/[t6m:B94}q&t(^)Ol~
J%MMDbAgGdP}zrSQO!3mrP3$w!.[Ng_xx-_[C<3g
^)6V&*<2""ZOgU.mBd]iInvFKiT<dq~y\O[cdDK`V
+RE]UYPIf3:StX`y#w,.iG~g""urD)'FnDIFI_q^)
6?HRillpgvvFDBAr4[:H{^oAL<`Em7$roF=2w;1~
You can also generate passwords, even memorable ones, using Keychain Access password assistant, or a command line equivalent like anders/pwgen.
Keychains are encrypted with a PBKDF2 derived key and are a pretty safe place to store credentials. See also Breaking into the OS X keychain. Also be aware that Keychain does not encrypt the names corresponding to password entries.
Alternatively, you can manage an encrypted passwords file yourself with GnuPG (see drduh/Purse and drduh/pwd.sh for example).
In addition to passwords, ensure eligible online accounts, such as GitHub, Google accounts, banking, have two factor authentication enabled.
Look to Yubikey for a two factor and private key (e.g., ssh, gpg) hardware token. See drduh/YubiKey-Guide and trmm.net/Yubikey. One of two Yubikey's slots can also be programmed to emit a long, static password (which can be used in combination with a short, memorized password, for example).
In Addition to Login and other PAMs, you can use Yubikey to secure your login and sudo, here is a pdf guide from Yubico. Yubikey are a bit pricey, there is cheaper alternative, but not as capable, U2F Zero. Here is a great guide to set it up
Backup
Always encrypt files locally before backing them up to external media or online services.
One way is to use a symmetric cipher with GPG and a password of your choosing. Files can also be encrypted to a public key with GPG, with the private key stored on YubiKey.
To compress and encrypt a directory:
$ tar zcvf - ~/Downloads | gpg -c > ~/Desktop/backup-$(date +%F-%H%M).tar.gz.gpg
tar: Removing leading '/' from member names
a Users/drduh/Downloads
a Users/drduh/Downloads/.DS_Store
a Users/drduh/Downloads/.localized
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
To decrypt and decompress the directory:
$ gpg -o ~/Desktop/decrypted-backup.tar.gz -d ~/Desktop/backup-2015-01-01-0000.tar.gz.gpg
gpg: AES256 encrypted data
gpg: encrypted with 1 passphrase

$ tar zxvf ~/Desktop/decrypted-backup.tar.gz
tar: Removing leading '/' from member names
x Users/drduh/._Downloads
x Users/drduh/Downloads/
x Users/drduh/Downloads/._.DS_Store
x Users/drduh/Downloads/.DS_Store
x Users/drduh/Downloads/.localized
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
You can also create and use encrypted volumes using Disk Utility or hdiutil:
$ hdiutil create ~/Desktop/encrypted.dmg -encryption -size 50M -volname ""secretStuff"" -fs JHFS+
Enter a new password to secure ""encrypted.dmg"":
Re-enter new password:
....................................
Created: /Users/drduh/Desktop/encrypted.img

$ hdiutil mount ~/Desktop/encrypted.dmg
Enter password to access ""encrypted.dmg"":
[...]
/Volumes/secretStuff

$ cp -v ~/Documents/passwords.txt /Volumes/secretStuff
[...]

$ hdiutil eject /Volumes/secretStuff
""disk4"" unmounted.
""disk4"" ejected.
See also the following applications and services: Tresorit, SpiderOak, Arq, Espionage, and restic.
Wi-Fi
macOS remembers access points it has connected to. Like all wireless devices, the Mac will broadcast all access point names it remembers (e.g., MyHomeNetwork) each time it looks for a network, such as when waking from sleep.
This is a privacy risk, so remove networks from the list in System Preferences > Network > Advanced when they are no longer needed.
Also see Signals from the Crowd: Uncovering Social Relationships through Smartphone Probes (pdf) and Wi-Fi told me everything about you (pdf).
Saved Wi-Fi information (SSID, last connection, etc.) can be found in:
/Library/Preferences/SystemConfiguration/com.apple.airport.preferences.plist

You may want to spoof the MAC address of the network card before connecting to new and untrusted wireless networks to mitigate passive fingerprinting:
$ sudo ifconfig en0 ether $(openssl rand -hex 6 | sed 's%\(..\)%\1:%g; s%.$%%')
It is also good to know that macOS will store Wi-Fi SSIDs and passwords in NVRAM, because Recovery Mode needs access to restore from the Internet. Be sure to either clear NVRAM or de-authenticate your Mac from your Apple account, which will clear the NVRAM, before passing a Mac along. (Resetting the SMC will clear some of the NVRAM, but not all.)
Note MAC addresses will reset to hardware defaults on each boot.
Also see feross/SpoofMAC.
Finally, WEP protection on wireless networks is not secure and you should favor connecting to WPA2 protected networks only to mitigate the risk of passive eavesdroppers.
SSH
For outgoing SSH connections, use hardware or password-protected keys, set up remote hosts and consider hashing them for added privacy. See drduh/config/ssh_config for recommended client options.
You can also use ssh to create an encrypted tunnel to send traffic through, similar to a VPN.
For example, to use Privoxy running on a remote host port 8118:
$ ssh -C -L 5555:127.0.0.1:8118 you@remote-host.tld

$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 5555

$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 5555
Or to use an ssh connection as a SOCKS proxy:
$ ssh -NCD 3000 you@remote-host.tld
By default, macOS does not have sshd or Remote Login enabled.
To enable sshd and allow incoming ssh connections:
$ sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist
Or use the System Preferences > Sharing menu.
If enabling sshd, be sure to disable password authentication and consider further hardening your configuration. See drduh/config/sshd_config for recommended options.
Confirm whether sshd is running:
$ sudo lsof -Pni TCP:22
Physical access
Keep your Mac physically secure at all times. Don't leave it unattended in hotels and such.
A skilled attacker with unsupervised physical access to your computer can infect the boot ROM to install a keylogger and steal your password - see Thunderstrike for an example.
A helpful tool is usbkill, which is an anti-forensic kill-switch that waits for a change on your USB ports and then immediately shuts down your computer.
Consider purchasing a privacy filter for your screen to thwart shoulder surfers.
Superglues or epoxy resins can also be used to disable physical access to computer ports. Nail polish and tamper-evidence seals can be applied to components to detect tampering.
System monitoring
OpenBSM audit
macOS has a powerful OpenBSM (Basic Security Module) auditing capability. You can use it to monitor process execution, network activity, and much more.
To tail audit logs, use the praudit utility:
$ sudo praudit -l /dev/auditpipe
header,201,11,execve(2),0,Thu Sep  1 12:00:00 2015, + 195 msec,exec arg,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,attribute,100755,root,wheel,16777220,986535,0,subject,drduh,root,wheel,root,wheel,412,100005,50511731,0.0.0.0,return,success,0,trailer,201,
header,88,11,connect(2),0,Thu Sep  1 12:00:00 2015, + 238 msec,argument,1,0x5,fd,socket-inet,2,443,173.194.74.104,subject,drduh,root,wheel,root,wheel,326,100005,50331650,0.0.0.0,return,failure : Operation now in progress,4354967105,trailer,88
header,111,11,OpenSSH login,0,Thu Sep  1 12:00:00 2015, + 16 msec,subject_ex,drduh,drduh,staff,drduh,staff,404,404,49271,::1,text,successful login drduh,return,success,0,trailer,111,
See the manual pages for audit, praudit, audit_control and other files in /etc/security
Note although man audit says the -s flag will synchronize the audit configuration, it appears necessary to reboot for changes to take effect.
See articles on ilostmynotes.blogspot.com and derflounder.wordpress.com for more information.
DTrace
Note System Integrity Protection interferes with DTrace, so it is not possible to use it in recent macOS versions without disabling SIP.

iosnoop monitors disk I/O
opensnoop monitors file opens
execsnoop monitors execution of processes
errinfo monitors failed system calls
dtruss monitors all system calls

See man -k dtrace for more information.
Execution
ps -ef lists information about all running processes.
You can also view processes with Activity Monitor.
launchctl list and sudo launchctl list list loaded and running user and system launch daemons and agents.
Network
List open network files:
$ sudo lsof -Pni
List contents of various network-related data structures:
$ sudo netstat -atln
Wireshark can be used from the command line with tshark.
Monitor DNS queries and replies:
$ tshark -Y ""dns.flags.response == 1"" -Tfields \
  -e frame.time_delta \
  -e dns.qry.name \
  -e dns.a \
  -Eseparator=,
Monitor HTTP requests and responses:
$ tshark -Y ""http.request or http.response"" -Tfields \
  -e ip.dst \
  -e http.request.full_uri \
  -e http.request.method \
  -e http.response.code \
  -e http.response.phrase \
  -Eseparator=/s
Monitor x509 (SSL/TLS) certificates:
$ tshark -Y ""ssl.handshake.certificate"" -Tfields \
  -e ip.src \
  -e x509sat.uTF8String \
  -e x509sat.printableString \
  -e x509sat.universalString \
  -e x509sat.IA5String \
  -e x509sat.teletexString \
  -Eseparator=/s -Equote=d
Also see the simple networking monitoring application BonzaiThePenguin/Loading.
Binary Whitelisting
google/santa is a security software developed for Google's corporate Macintosh fleet and open sourced.

Santa is a binary whitelisting/blacklisting system for macOS. It consists of a kernel extension that monitors for executions, a userland daemon that makes execution decisions based on the contents of a SQLite database, a GUI agent that notifies the user in case of a block decision and a command-line utility for managing the system and synchronizing the database with a server.

Santa uses the Kernel Authorization API to monitor and allow/disallow binaries from executing in the kernel. Binaries can be white- or black-listed by unique hash or signing developer certificate. Santa can be used to only allow trusted code execution, or to blacklist known malware from executing on a Mac, similar to Bit9 software for Windows.
Note Santa does not currently have a graphical user interface for managing rules. The following instructions are for advanced users only!
To install Santa, visit the Releases page and download the latest disk image, the mount it and install the contained package:
$ hdiutil mount ~/Downloads/santa-0.9.20.dmg

$ sudo installer -pkg /Volumes/santa-0.9.20/santa-0.9.20.pkg -tgt /
By default, Santa installs in ""Monitor"" mode (meaning, nothing gets blocked, only logged) and comes with two rules: one for Apple binaries and another for Santa software itself.
Verify Santa is running and its kernel module is loaded:
$ santactl status
>>> Daemon Info
  Mode                   | Monitor
  File Logging           | No
  Watchdog CPU Events    | 0  (Peak: 0.00%)
  Watchdog RAM Events    | 0  (Peak: 0.00MB)
>>> Kernel Info
  Kernel cache count     | 0
>>> Database Info
  Binary Rules           | 0
  Certificate Rules      | 2
  Events Pending Upload  | 0

$ ps -ef | grep ""[s]anta""
    0   786     1   0 10:01AM ??         0:00.39 /Library/Extensions/santa-driver.kext/Contents/MacOS/santad --syslog

$ kextstat | grep santa
  119    0 0xffffff7f822ff000 0x6000     0x6000     com.google.santa-driver (0.9.14) 693D8E4D-3161-30E0-B83D-66A273CAE026 <5 4 3 1>
Create a blacklist rule to prevent iTunes from executing:
$ sudo santactl rule --blacklist --path /Applications/iTunes.app/
Added rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Try to launch iTunes - it will be blocked.
$ open /Applications/iTunes.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/iTunes.app.

To remove the rule:
$ sudo santactl rule --remove --path /Applications/iTunes.app/
Removed rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Open iTunes:
$ open /Applications/iTunes.app/
[iTunes will open successfully]
Create a new, example C program:
$ cat <<EOF > foo.c
> #include <stdio.h>
> main() { printf(""Hello World\n‚Äù); }
> EOF
Compile the program with GCC (requires installation of Xcode or command-line tools):
$ gcc -o foo foo.c

$ file foo
foo: Mach-O 64-bit executable x86_64

$ codesign -d foo
foo: code object is not signed at all
Run it:
$ ./foo
Hello World
Toggle Santa into ""Lockdown"" mode, which only allows whitelisted binaries to run:
$ sudo defaults write /var/db/santa/config.plist ClientMode -int 2

Try to run the unsigned binary:
$ ./foo
bash: ./foo: Operation not permitted

Santa

The following application has been blocked from executing
because its trustworthiness cannot be determined.

Path:       /Users/demouser/foo
Identifier: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Parent:     bash (701)
To whitelist a specific binary, determine its SHA-256 sum:
$ santactl fileinfo /Users/demouser/foo
Path                 : /Users/demouser/foo
SHA-256              : 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
SHA-1                : 4506f3a8c0a5abe4cacb98e6267549a4d8734d82
Type                 : Executable (x86-64)
Code-signed          : No
Rule                 : Blacklisted (Unknown)
Add a whitelist rule:
$ sudo santactl rule --whitelist --sha256 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Added rule for SHA-256: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed.
Run it:
$ ./foo
Hello World
It's allowed and works!
Applications can also be whitelisted by developer certificate (so that new binary versions will not need to be manually whitelisted on each update). For example, download and run Google Chrome - it will be blocked by Santa in ""Lockdown"" mode:
$ curl -sO https://dl.google.com/chrome/mac/stable/GGRO/googlechrome.dmg

$ hdiutil mount googlechrome.dmg

$ cp -r /Volumes/Google\ Chrome/Google\ Chrome.app /Applications/

$ open /Applications/Google\ Chrome.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/Google Chrome.app.
Whitelist the application by its developer certificate (first item in the Signing Chain):
$ santactl fileinfo /Applications/Google\ Chrome.app/
Path                 : /Applications/Google Chrome.app/Contents/MacOS/Google Chrome
SHA-256              : 0eb08224d427fb1d87d2276d911bbb6c4326ec9f74448a4d9a3cfce0c3413810
SHA-1                : 9213cbc7dfaaf7580f3936a915faa56d40479f6a
Bundle Name          : Google Chrome
Bundle Version       : 2883.87
Bundle Version Str   : 55.0.2883.87
Type                 : Executable (x86-64)
Code-signed          : Yes
Rule                 : Blacklisted (Unknown)
Signing Chain:
     1. SHA-256             : 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
        SHA-1               : 85cee8254216185620ddc8851c7a9fc4dfe120ef
        Common Name         : Developer ID Application: Google Inc.
        Organization        : Google Inc.
        Organizational Unit : EQHXZ8M8AV
        Valid From          : 2012/04/26 07:10:10 -0700
        Valid Until         : 2017/04/27 07:10:10 -0700

     2. SHA-256             : 7afc9d01a62f03a2de9637936d4afe68090d2de18d03f29c88cfb0b1ba63587f
        SHA-1               : 3b166c3b7dc4b751c9fe2afab9135641e388e186
        Common Name         : Developer ID Certification Authority
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2012/02/01 14:12:15 -0800
        Valid Until         : 2027/02/01 14:12:15 -0800

     3. SHA-256             : b0b1730ecbc7ff4505142c49f1295e6eda6bcaed7e2c68c5be91b5a11001f024
        SHA-1               : 611e5b662c593a08ff58d14ae22452d198df6c60
        Common Name         : Apple Root CA
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2006/04/25 14:40:36 -0700
        Valid Until         : 2035/02/09 13:40:36 -0800
In this case, 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153 is the SHA-256 of Google‚Äôs Apple developer certificate (team ID EQHXZ8M8AV). To whitelist it:
$ sudo santactl rule --whitelist --certificate --sha256 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
Added rule for SHA-256: 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153.
Google Chrome should now launch, and subsequent updates to the application will continue to work as long as the code signing certificate doesn‚Äôt change or expire.
To disable ""Lockdown"" mode:
$ sudo defaults delete /var/db/santa/config.plist ClientMode
See /var/log/santa.log to monitor ALLOW and DENY execution decisions.
A log and configuration server for Santa is available in Zentral, an open source event monitoring solution and TLS server for osquery and Santa.
Zentral will support Santa in both MONITORING and LOCKDOWN operation mode. Clients need to be enrolled with a TLS connection to sync Santa Rules, all Santa events from endpoints are aggregated and logged back in Zentral. Santa events can trigger actions and notifications from within the Zentral Framework.
Note Python, Bash and other interpreters are whitelisted (since they are signed by Apple's developer certificate), so Santa will not be able to block such scripts from executing. Thus, a potential non-binary program which disables Santa is a weakness (not vulnerability, since it is so by design) to take note of.
Miscellaneous
Disable Diagnostics & Usage Data.
If you want to play music or watch videos, use VLC media player which is free and open source.
If you want to use torrents, use Transmission which is free and open source (note: like all software, even open source projects, malware may still find its way in). You may also wish to use a block list to avoid peering with known bad hosts - see Which is the best blocklist for Transmission and johntyree/3331662.
Manage default file handlers with duti, which can be installed with brew install duti. One reason to manage extensions is to prevent auto-mounting of remote filesystems in Finder (see Protecting Yourself From Sparklegate). Here are several recommended file handlers to manage:
$ duti -s com.apple.Safari afp

$ duti -s com.apple.Safari ftp

$ duti -s com.apple.Safari nfs

$ duti -s com.apple.Safari smb

$ duti -s com.apple.TextEdit public.unix-executable
Monitor system logs with the Console application or syslog -w or /usr/bin/log stream commands.
In systems prior to macOS Sierra (10.12), enable the tty_tickets flag in /etc/sudoers to restrict the sudo session to the Terminal window/tab that started it. To do so, use sudo visudo and add the line Defaults    tty_tickets.
Set your screen to lock as soon as the screensaver starts:
$ defaults write com.apple.screensaver askForPassword -int 1

$ defaults write com.apple.screensaver askForPasswordDelay -int 0
Expose hidden files and Library folder in Finder:
$ defaults write com.apple.finder AppleShowAllFiles -bool true

$ chflags nohidden ~/Library
Show all filename extensions (so that ""Evil.jpg.app"" cannot masquerade easily).
$ defaults write NSGlobalDomain AppleShowAllExtensions -bool true
Don't default to saving documents to iCloud:
$ defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false
Enable Secure Keyboard Entry in Terminal (unless you use YubiKey or applications such as TextExpander).
Disable crash reporter (the dialog which appears after an application crashes and prompts to report the problem to Apple):
$ defaults write com.apple.CrashReporter DialogType none
Disable Bonjour multicast advertisements:
$ sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES
Disable Handoff and Bluetooth features, if they aren't necessary.
Consider sandboxing your applications. See fG! Sandbox Guide (pdf) and s7ephen/OSX-Sandbox--Seatbelt--Profiles.
Did you know Apple has not shipped a computer with TPM since 2006?
macOS comes with this line in /etc/sudoers:
Defaults env_keep += ""HOME MAIL""

Which stops sudo from changing the HOME variable when you elevate privileges. This means it will execute as root the bash dotfiles in the non-root user's home directory when you run ""sudo bash"". It is advisable to comment this line out to avoid a potentially easy way for malware or a local attacker to escalate privileges to root.
If you want to retain the convenience of the root user having a non-root user's home directory, you can append an export line to /var/root/.bashrc, e.g.:
export HOME=/Users/blah
Set a custom umask:
$ sudo launchctl config user umask 077
Reboot, create a file in Finder and verify its permissions (macOS default allows 'group/other' read access):
$ ls -ld umask*
drwx------  2 kevin  staff       64 Dec  4 12:27 umask_testing_dir
-rw-------@ 1 kevin  staff  2026566 Dec  4 12:28 umask_testing_file
Related software

CISOfy/lynis - Cross-platform security auditing tool and assists with compliance testing and system hardening.
Dylib Hijack Scanner - Scan for applications that are either susceptible to dylib hijacking or have been hijacked.
F-Secure XFENCE (formerly Little Flocker) - ""Little Snitch for files""; prevents applications from accessing files.
Lockdown - Audits and remediates security configuration settings.
Zentral - A log and configuration server for santa and osquery. Run audit and probes on inventory, events, logfiles, combine with point-in-time alerting. A full Framework and Django web server build on top of the elastic stack (formerly known as ELK stack).
facebook/osquery - Can be used to retrieve low level system information.  Users can write SQL queries to retrieve system information.
google/grr - Incident response framework focused on remote live forensics.
jipegit/OSXAuditor - Analyzes artifacts on a running system, such as quarantined files, Safari, Chrome and Firefox history, downloads, HTML5 databases and localstore, social media and email accounts, and Wi-Fi access point names.
kristovatlas/osx-config-check - Checks your OSX machine against various hardened configuration settings.
libyal/libfvde - Library to access FileVault Drive Encryption (FVDE) (or FileVault2) encrypted volumes.
stronghold - Securely and easily configure your Mac from the terminal. Inspired by this guide.
yelp/osxcollector - Forensic evidence collection & analysis toolkit for OS X.
The Eclectic Light Company - Downloads - A collection of useful diagnostics and control applications and utilities for macOS.

Additional resources

Apple Open Source
Auditing and Exploiting Apple IPC
CIS Benchmarks
Demystifying the DMG File Format
Demystifying the i-Device NVMe NAND (New storage used by Apple)
Developing Mac OSX kernel rootkits
DoD Security Technical Implementation Guides for Mac OS
EFF Surveillance Self-Defense Guide
Extracting FileVault 2 Keys with Volatility
Fuzzing the macOS WindowServer for Exploitable Vulnerabilities
Hacker News discussion 2
Hacker News discussion
Harden the World: Mac OSX 10.11 El Capitan
Hidden backdoor API to root privileges in Apple OS X
How to Switch to the Mac
How to make macOS Spotlight fuck the fuck off and do your bidding
IOKit kernel code execution exploit
IPv6 Hardening Guide for OS X
Mac Developer Library: Secure Coding Guide
Mac Forensics: Mac OS X and the HFS+ File System (pdf)
Mac OS X Forensics - Technical Report (pdf)
Mac OS X and iOS Internals: To the Apple's Core by Jonathan Levin
MacAdmins on Slack
MacOS Hardening Guide - Appendix of *OS Internals: Volume III - Security & Insecurity Internals (pdf)
Managing Macs at Google Scale (LISA '13)
OS X 10.10 Yosemite: The Ars Technica Review
OS X Core Technologies Overview White Paper (pdf)
OS X Hardening: Securing a Large Global Mac Fleet (LISA '13)
OSX.Pirrit Mac Adware Part III: The DaVinci Code
Over The Air - Vol. 2, Pt. 1: Exploiting The Wi-Fi Stack on Apple Devices
Patrick Wardle's Objective-See blog
Remote code execution, git, and OS X
Reverse Engineering Mac OS X blog
Reverse Engineering Resources
Security Configuration For Mac OS X Version 10.6 Snow Leopard (pdf)
The EFI boot process
The Great DOM Fuzz-off of 2017
The Intel Mac boot process
The macOS Phishing Easy Button: AppleScript Dangers
There's a lot of vulnerable OS X applications out there (Sparkle Framework RCE)
Userland Persistence on Mac OS X
iCloud security and privacy overview
iSeeYou: Disabling the MacBook Webcam Indicator LED

",GitHub - drduh/macOS-Security-and-Privacy-Guide: Guide to securing and improving privacy on macOS
22,Python,"
spaCy: Industrial-strength NLP
spaCy is a library for advanced Natural Language Processing in Python and
Cython. It's built on the very latest research, and was designed from day one to
be used in real products. spaCy comes with
pretrained statistical models and word vectors, and
currently supports tokenization for 50+ languages. It features
state-of-the-art speed, convolutional neural network models for tagging,
parsing and named entity recognition and easy deep learning integration.
It's commercial open-source software, released under the MIT license.
üí´ Version 2.2 out now!
Check out the release notes here.











üìñ Documentation



Documentation





spaCy 101
New to spaCy? Here's everything you need to know!


Usage Guides
How to use spaCy and its features.


New in v2.2
New features, backwards incompatibilities and migration guide.


API Reference
The detailed reference for spaCy's API.


Models
Download statistical language models for spaCy.


Universe
Libraries, extensions, demos, books and courses.


Changelog
Changes and version history.


Contribute
How to contribute to the spaCy project and code base.



üí¨ Where to ask questions
The spaCy project is maintained by @honnibal and
@ines, along with core contributors
@svlandeg and
@adrianeboyd. Please understand that we won't
be able to provide individual support via email. We also believe that help is
much more valuable if it's shared publicly, so that more people can benefit from
it.



Type
Platforms




üö® Bug Reports
GitHub Issue Tracker


üéÅ Feature Requests
GitHub Issue Tracker


üë©‚Äçüíª Usage Questions
Stack Overflow ¬∑ Gitter Chat ¬∑ Reddit User Group


üóØ General Discussion
Gitter Chat ¬∑ Reddit User Group



Features

Non-destructive tokenization
Named entity recognition
Support for 50+ languages
pretrained statistical models and word vectors
State-of-the-art speed
Easy deep learning integration
Part-of-speech tagging
Labelled dependency parsing
Syntax-driven sentence segmentation
Built in visualizers for syntax and NER
Convenient string-to-hash mapping
Export to numpy data arrays
Efficient binary serialization
Easy model packaging and deployment
Robust, rigorously evaluated accuracy

üìñ For more details, see the
facts, figures and benchmarks.
Install spaCy
For detailed installation instructions, see the
documentation.

Operating system: macOS / OS X ¬∑ Linux ¬∑ Windows (Cygwin, MinGW, Visual
Studio)
Python version: Python 2.7, 3.5+ (only 64 bit)
Package managers: pip ¬∑ conda (via conda-forge)

pip
Using pip, spaCy releases are available as source packages and binary wheels (as
of v2.0.13).
pip install spacy
To install additional data tables for lemmatization in spaCy v2.2+ you can
run pip install spacy[lookups] or install
spacy-lookups-data
separately. The lookups package is needed to create blank models with
lemmatization data, and to lemmatize in languages that don't yet come with
pretrained models and aren't powered by third-party libraries.
When using pip it is generally recommended to install packages in a virtual
environment to avoid modifying system state:
python -m venv .env
source .env/bin/activate
pip install spacy
conda
Thanks to our great community, we've finally re-added conda support. You can now
install spaCy via conda-forge:
conda install -c conda-forge spacy
For the feedstock including the build recipe and configuration, check out
this repository. Improvements
and pull requests to the recipe and setup are always appreciated.
Updating spaCy
Some updates to spaCy may require downloading new statistical models. If you're
running spaCy v2.0 or higher, you can use the validate command to check if
your installed models are compatible and if not, print details on how to update
them:
pip install -U spacy
python -m spacy validate
If you've trained your own models, keep in mind that your training and runtime
inputs must match. After updating spaCy, we recommend retraining your models
with the new version.
üìñ For details on upgrading from spaCy 1.x to spaCy 2.x, see the
migration guide.
Download models
As of v1.7.0, models for spaCy can be installed as Python packages. This
means that they're a component of your application, just like any other module.
Models can be installed using spaCy's download command, or manually by
pointing pip to a path or URL.



Documentation





Available Models
Detailed model descriptions, accuracy figures and benchmarks.


Models Documentation
Detailed usage instructions.



# download best-matching version of specific model for your spaCy installation
python -m spacy download en_core_web_sm

# pip install .tar.gz archive from path or URL
pip install /Users/you/en_core_web_sm-2.2.0.tar.gz
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz
Loading and using models
To load a model, use spacy.load() with the model name, a shortcut link or a
path to the model data directory.
import spacy
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is a sentence."")
You can also import a model directly via its full name and then call its
load() method with no arguments.
import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()
doc = nlp(""This is a sentence."")
üìñ For more info and examples, check out the
models documentation.
Compile from source
The other way to install spaCy is to clone its
GitHub repository and build it from
source. That is the common way if you want to make changes to the code base.
You'll need to make sure that you have a development environment consisting of a
Python distribution including header files, a compiler,
pip,
virtualenv and
git installed. The compiler part is the trickiest. How to
do that depends on your system. See notes on Ubuntu, OS X and Windows for
details.
# make sure you are using the latest pip
python -m pip install -U pip
git clone https://github.com/explosion/spaCy
cd spaCy

python -m venv .env
source .env/bin/activate
export PYTHONPATH=`pwd`
pip install -r requirements.txt
python setup.py build_ext --inplace
Compared to regular install via pip, requirements.txt
additionally installs developer dependencies such as Cython. For more details
and instructions, see the documentation on
compiling spaCy from source and the
quickstart widget to get the right
commands for your platform and Python version.
Ubuntu
Install system-level dependencies via apt-get:
sudo apt-get install build-essential python-dev git
macOS / OS X
Install a recent version of XCode,
including the so-called ""Command Line Tools"". macOS and OS X ship with Python
and git preinstalled.
Windows
Install a version of the
Visual C++ Build Tools
or Visual Studio Express that
matches the version that was used to compile your Python interpreter. For
official distributions these are VS 2008 (Python 2.7), VS 2010 (Python 3.4) and
VS 2015 (Python 3.5).
Run tests
spaCy comes with an extensive test suite. In order to run the
tests, you'll usually want to clone the repository and build spaCy from source.
This will also install the required development dependencies and test utilities
defined in the requirements.txt.
Alternatively, you can find out where spaCy is installed and run pytest on
that directory. Don't forget to also install the test utilities via spaCy's
requirements.txt:
python -c ""import os; import spacy; print(os.path.dirname(spacy.__file__))""
pip install -r path/to/requirements.txt
python -m pytest <spacy-directory>
See the documentation for more details and
examples.
",GitHub - explosion/spaCy: üí´ Industrial-strength Natural Language Processing (NLP) with Python and Cython
23,Python,"This repository is archived.
This repository is archived and will not receive any updates or accept issues or pull requests.
To report bugs in reddit.com please make a post in /r/bugs.
If you have found a bug that can in some way compromise the security of the
site or its users, please exercise responsible
disclosure and e-mail
security@reddit.com.

API
For notices about reddit API changes and discussion of reddit API client development, subscribe to the /r/redditdev and /r/changelog subreddits.
To learn more about reddit's API, check out our automated API documentation and the API wiki page. Please use a unique User-Agent string and take care to abide by our API rules.
Quickstart
To set up your own instance of reddit see the install guide.
",GitHub - reddit-archive/reddit: historical code from reddit.com
24,Python,"Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†

Êú¨‰π¶ÁΩëÂùÄÔºözh.d2l.ai | 1.0.0Áâàrc0ÂèëÂ∏É | Â¶Ç‰ΩïÂÆâË£ÖÂíå‰ΩøÁî®‰π¶‰∏≠Ê∫ê‰ª£Á†Å
Êõ¥Êñ∞
Ëã±ÊñáÁâàÂÖ®Èù¢ÊîπËøõ‰∫ÜÈ¢ÑÂ§áÁü•ËØÜ‰∏ÄÁ´†Ôºå
Êñ∞Â¢û‰∫ÜÊé®ËçêÁ≥ªÁªü‰∏ÄÁ´†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊï∞Â≠¶‰∏ÄÁ´†„ÄÇ
Ê¨¢ËøéÂÖ≥Ê≥®Ëã±ÊñáÁâàÂºÄÊ∫êÈ°πÁõÆÔºöhttps://github.com/d2l-ai/d2l-en
Ëã±ÊñáÁâà Dive into Deep Learning
Âä†Â∑ûÂ§ßÂ≠¶‰ºØÂÖãÂà©ÂàÜÊ†° 2019 Âπ¥Êò•Â≠¶Êúü Introduction to Deep Learning ËØæÁ®ãÊïôÊùêÔºà‰∏≠ÊñáÁâàËØæ‰ª∂ÔºàÂÜÖÂê´ÊïôÂ≠¶ËßÜÈ¢ëÂú∞ÂùÄÔºâÔºâ„ÄÇ
Ëã±ÊñáÁâàÂºïÁî®
BibTeX entry:
@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}

Ë¥°ÁåÆ
ÊÑüË∞¢Á§æÂå∫Ë¥°ÁåÆËÄÖ‰ª¨‰∏∫ÊØè‰∏Ä‰ΩçËØªËÄÖÊîπËøõËøôÊú¨ÂºÄÊ∫ê‰π¶„ÄÇ
Â¶Ç‰ΩïË¥°ÁåÆ | Ëá¥Ë∞¢ | ËÆ®ËÆ∫ÊàñÊä•ÂëäÈóÆÈ¢ò | ÂÖ∂‰ªñ
",GitHub - d2l-ai/d2l-zh: „ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„ÄãÔºöÈù¢Âêë‰∏≠ÊñáËØªËÄÖ„ÄÅËÉΩËøêË°å„ÄÅÂèØËÆ®ËÆ∫„ÄÇËã±ÊñáÁâàÂç≥‰ºØÂÖãÂà©‚ÄúÊ∑±Â∫¶Â≠¶‰π†ÂØºËÆ∫‚ÄùÊïôÊùê„ÄÇ
25,Python,"ZeroNet   
Decentralized websites using Bitcoin crypto and the BitTorrent network - https://zeronet.io
Why?

We believe in open, free, and uncensored network and communication.
No single point of failure: Site remains online so long as at least 1 peer is
serving it.
No hosting costs: Sites are served by visitors.
Impossible to shut down: It's nowhere because it's everywhere.
Fast and works offline: You can access the site even if Internet is
unavailable.

Features

Real-time updated sites
Namecoin .bit domains support
Easy to setup: unpack & run
Clone websites in one click
Password-less BIP32
based authorization: Your account is protected by the same cryptography as your Bitcoin wallet
Built-in SQL server with P2P data synchronization: Allows easier site development and faster page load times
Anonymity: Full Tor network support with .onion hidden services instead of IPv4 addresses
TLS encrypted connections
Automatic uPnP port opening
Plugin for multiuser (openproxy) support
Works with any browser/OS

How does it work?

After starting zeronet.py you will be able to visit zeronet sites using
http://127.0.0.1:43110/{zeronet_address} (eg.
http://127.0.0.1:43110/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D).
When you visit a new zeronet site, it tries to find peers using the BitTorrent
network so it can download the site files (html, css, js...) from them.
Each visited site is also served by you.
Every site contains a content.json file which holds all other files in a sha512 hash
and a signature generated using the site's private key.
If the site owner (who has the private key for the site address) modifies the
site, then he/she signs the new content.json and publishes it to the peers.
Afterwards, the peers verify the content.json integrity (using the
signature), they download the modified files and publish the new content to
other peers.

Slideshow about ZeroNet cryptography, site updates, multi-user sites ¬ª
Frequently asked questions ¬ª
ZeroNet Developer Documentation ¬ª
Screenshots


More screenshots in ZeroNet docs ¬ª
How to join
Windows

Download ZeroNet-py3-win64.zip (18MB)
Unpack anywhere
Run ZeroNet.exe

macOS

Download ZeroNet-dist-mac.zip (13.2MB)
Unpack anywhere
Run ZeroNet.app

Linux (x86-64bit)

wget https://github.com/HelloZeroNet/ZeroNet-linux/archive/dist-linux64/ZeroNet-py3-linux64.tar.gz
tar xvpfz ZeroNet-py3-linux64.tar.gz
cd ZeroNet-linux-dist-linux64/
Start with: ./ZeroNet.sh
Open the ZeroHello landing page in your browser by navigating to: http://127.0.0.1:43110/

Tip: Start with ./ZeroNet.sh --ui_ip '*' --ui_restrict your.ip.address to allow remote connections on the web interface.
Install from source

wget https://github.com/HelloZeroNet/ZeroNet/archive/py3/ZeroNet-py3.tar.gz
tar xvpfz ZeroNet-py3.tar.gz
cd ZeroNet-py3
sudo apt-get update
sudo apt-get install python3-pip
sudo python3 -m pip install -r requirements.txt
Start with: python3 zeronet.py
Open the ZeroHello landing page in your browser by navigating to: http://127.0.0.1:43110/

Current limitations

No torrent-like file splitting for big file support (big file support added)
No more anonymous than Bittorrent (built-in full Tor support added)
File transactions are not compressed or encrypted yet (TLS encryption added)
No private sites

How can I create a ZeroNet site?
Shut down zeronet if you are running it already
$ zeronet.py siteCreate
...
- Site private key: 23DKQpzxhbVBrAtvLEc2uvk7DZweh4qL3fn3jpM3LgHDczMK2TtYUq
- Site address: 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
...
- Site created!
$ zeronet.py
...
Congratulations, you're finished! Now anyone can access your site using
http://127.0.0.1:43110/13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
Next steps: ZeroNet Developer Documentation
How can I modify a ZeroNet site?

Modify files located in data/13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2 directory.
After you're finished:

$ zeronet.py siteSign 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
- Signing site: 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2...
Private key (input hidden):

Enter the private key you got when you created the site, then:

$ zeronet.py sitePublish 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
...
Site:13DNDk..bhC2 Publishing to 3/10 peers...
Site:13DNDk..bhC2 Successfuly published to 3 peers
- Serving files....

That's it! You've successfully signed and published your modifications.

Help keep this project alive

Bitcoin: 1QDhxQ6PraUZa21ET5fYUCPgdrwBomnFgX
Paypal: https://zeronet.io/docs/help_zeronet/donate/

Sponsors

Better macOS/Safari compatibility made possible by BrowserStack.com

Thank you!

More info, help, changelog, zeronet sites: https://www.reddit.com/r/zeronet/
Come, chat with us: #zeronet @ FreeNode or on gitter
Email: hello@zeronet.io (PGP: CB9613AE)

",GitHub - HelloZeroNet/ZeroNet: ZeroNet - Decentralized websites using Bitcoin crypto and BitTorrent network
26,Python,"Glances - An eye on your system












Summary
Glances is a cross-platform monitoring tool which aims to present a
large amount of monitoring information through a curses or Web
based interface. The information dynamically adapts depending on the
size of the user interface.

It can also work in client/server mode. Remote monitoring could be done
via terminal, Web interface or API (XML-RPC and RESTful). Stats can also
be exported to files or external time/value databases.

Glances is written in Python and uses libraries to grab information from
your system. It is based on an open architecture where developers can
add new plugins or exports modules.

Requirements

python 2.7,>=3.4
psutil>=5.3.0 (better with latest version)

Optional dependencies:

bernhard (for the Riemann export module)
bottle (for Web server mode)
cassandra-driver (for the Cassandra export module)
couchdb (for the CouchDB export module)
docker (for the Docker monitoring support) [Linux/macOS-only]
elasticsearch (for the Elastic Search export module)
hddtemp (for HDD temperature monitoring support) [Linux-only]
influxdb (for the InfluxDB export module)
kafka-python (for the Kafka export module)
netifaces (for the IP plugin)
nvidia-ml-py3 (for the GPU plugin)
pika (for the RabbitMQ/ActiveMQ export module)
potsdb (for the OpenTSDB export module)
prometheus_client (for the Prometheus export module)
py-cpuinfo (for the Quicklook CPU info module)
pygal (for the graph export module)
pymdstat (for RAID support) [Linux-only]
pySMART.smartx (for HDD Smart support) [Linux-only]
pysnmp (for SNMP support)
pystache (for the action script feature)
pyzmq (for the ZeroMQ export module)
requests (for the Ports, Cloud plugins and RESTful export module)
scandir (for the Folders plugin) [Only for Python < 3.5]
statsd (for the StatsD export module)
wifi (for the wifi plugin) [Linux-only]
zeroconf (for the autodiscover mode)

Note for Python 2.6 users
Glances no longer supports Python 2.6. Please upgrade
to a minimum Python version of 2.7/3.4+ or downgrade to Glances 2.6.2 (last version
with Python 2.6 support).
Note for CentOS Linux 6 and 7 users
Python 2.7 and 3.4 are now available via SCL repositories. See:
https://lists.centos.org/pipermail/centos-announce/2015-December/021555.html.

Installation
There are several methods to test/install Glances on your system. Choose your weapon!

Glances Auto Install script: the total way
To install both dependencies and the latest Glances production ready version
(aka master branch), just enter the following command line:
curl -L https://bit.ly/glances | /bin/bash
or
wget -O- https://bit.ly/glances | /bin/bash
Note: This is only supported on some GNU/Linux distributions and Mac OS X.
If you want to support other distributions, please contribute to glancesautoinstall.

PyPI: The simple way
Glances is on PyPI. By using PyPI, you will be using the latest
stable version.
To install, simply use pip:
pip install glances
Note: Python headers are required to install psutil. For example,
on Debian/Ubuntu you need to install first the python-dev package.
For Fedora/CentOS/RHEL install first python-devel package. For Windows,
just install psutil from the binary installation file.
Note 2 (for the Wifi plugin): If you want to use the Wifi plugin, you need
to install the wireless-tools package on your system.
You can also install the following libraries in order to use optional
features (like the Web interface, exports modules...):
pip install 'glances[action,browser,cloud,cpuinfo,docker,export,folders,gpu,graph,ip,raid,snmp,web,wifi]'
To upgrade Glances to the latest version:
pip install --upgrade glances
pip install --upgrade glances[...]
If you need to install Glances in a specific user location, use:
export PYTHONUSERBASE=~/mylocalpath
pip install --user glances

Docker: the funny way
A Glances container is available. It includes the latest development
HEAD version. You can use it to monitor your server and all your other
containers!
Get the Glances container:
docker pull nicolargo/glances
Run the container in console mode:
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host --network host -it docker.io/nicolargo/glances
Additionally, if you want to use your own glances.conf file, you can
create your own Dockerfile:
FROM nicolargo/glances
COPY glances.conf /glances/conf/glances.conf
CMD python -m glances -C /glances/conf/glances.conf $GLANCES_OPT
Alternatively, you can specify something along the same lines with
docker run options:
docker run -v `pwd`/glances.conf:/glances/conf/glances.conf -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host -it docker.io/nicolargo/glances
Where `pwd`/glances.conf is a local directory containing your glances.conf file.
Run the container in Web server mode (notice the GLANCES_OPT environment
variable setting parameters for the glances startup command):
docker run -d --restart=""always"" -p 61208-61209:61208-61209 -e GLANCES_OPT=""-w"" -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host docker.io/nicolargo/glances

GNU/Linux
Glances is available on many Linux distributions, so you should be
able to install it using your favorite package manager. Be aware that
when you use this method the operating system package for Glances
may not be the latest version.

FreeBSD
To install the binary package:
# pkg install py27-glances
To install Glances from ports:
# cd /usr/ports/sysutils/py-glances/
# make install clean

macOS
If you do not want to use the glancesautoinstall script, follow this procedure.
macOS users can install Glances using Homebrew or MacPorts.

Homebrew
$ brew install glances

MacPorts
$ sudo port install glances

Windows
Install Python for Windows (Python 2.7.9+ and 3.4+ ship with pip) and
then run the following command:
$ pip install glances
Alternatively, you could clone the repository and install with the following command.
$ git clone https://github.com/nicolargo/glances.git
$ cd glances
$ python setup.py install

Android
You need a rooted device and the Termux application (available on the
Google Play Store).
Start Termux on your device and enter:
$ apt update
$ apt upgrade
$ apt install clang python python-dev
$ pip install bottle
$ pip install glances
And start Glances:
$ glances
You can also run Glances in server mode (-s or -w) in order to remotely
monitor your Android device.

Source
To install Glances from source:
$ wget https://github.com/nicolargo/glances/archive/vX.Y.tar.gz -O - | tar xz
$ cd glances-*
# python setup.py install
Note: Python headers are required to install psutil.

Chef
An awesome Chef cookbook is available to monitor your infrastructure:
https://supermarket.chef.io/cookbooks/glances (thanks to Antoine Rouyer)

Puppet
You can install Glances using Puppet: https://github.com/rverchere/puppet-glances

Ansible
A Glances Ansible role is available: https://galaxy.ansible.com/zaxos/glances-ansible-role/

Usage
For the standalone mode, just run:
$ glances
For the Web server mode, run:
$ glances -w
and enter the URL http://<ip>:61208 in your favorite web browser.
For the client/server mode, run:
$ glances -s
on the server side and run:
$ glances -c <ip>
on the client one.
You can also detect and display all Glances servers available on your
network or defined in the configuration file:
$ glances --browser
You can also display raw stats on stdout:
$ glances --stdout cpu.user,mem.used,load
cpu.user: 30.7
mem.used: 3278204928
load: {'cpucore': 4, 'min1': 0.21, 'min5': 0.4, 'min15': 0.27}
cpu.user: 3.4
mem.used: 3275251712
load: {'cpucore': 4, 'min1': 0.19, 'min5': 0.39, 'min15': 0.27}
...
or in a CSV format thanks to the stdout-csv option:
$ glances --stdout-csv now,cpu.user,mem.used,load
now,cpu.user,mem.used,load.cpucore,load.min1,load.min5,load.min15
2018-12-08 22:04:20 CEST,7.3,5948149760,4,1.04,0.99,1.04
2018-12-08 22:04:23 CEST,5.4,5949136896,4,1.04,0.99,1.04
...
and RTFM, always.

Documentation
For complete documentation have a look at the readthedocs website.
If you have any question (after RTFM!), please post it on the official Q&A forum.

Gateway to other services
Glances can export stats to: CSV file, JSON file, InfluxDB, Cassandra, CouchDB,
OpenTSDB, Prometheus, StatsD, ElasticSearch, RabbitMQ/ActiveMQ,
ZeroMQ, Kafka, Riemann and RESTful server.

How to contribute ?
If you want to contribute to the Glances project, read this wiki page.
There is also a chat dedicated to the Glances developers:



Donation
If this project help you, you can give me a tip ;)


Author
Nicolas Hennion (@nicolargo) <nicolas@nicolargo.com>


License
Glances is distributed under the LGPL version 3 license. See COPYING for more details.
","GitHub - nicolargo/glances: Glances an Eye on your system. A top/htop alternative for GNU/Linux, BSD, Mac OS and Windows operating systems."
27,Python,"
  
 
 
 
 
 



Version:4.4.0rc5 (cliffs)

Web:http://celeryproject.org/

Download:https://pypi.org/project/celery/

Source:https://github.com/celery/celery/

Keywords:task, queue, job, async, rabbitmq, amqp, redis,
python, distributed, actors




Donations
This project relies on your generous donations.
If you are using Celery to create a commercial product, please consider becoming our backer or our sponsor to ensure Celery's future.

For enterprise
Available as part of the Tidelift Subscription.
The maintainers of celery and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. Learn more.

What's a Task Queue?
Task queues are used as a mechanism to distribute work across threads or
machines.
A task queue's input is a unit of work, called a task, dedicated worker
processes then constantly monitor the queue for new work to perform.
Celery communicates via messages, usually using a broker
to mediate between clients and workers. To initiate a task a client puts a
message on the queue, the broker then delivers the message to a worker.
A Celery system can consist of multiple workers and brokers, giving way
to high availability and horizontal scaling.
Celery is written in Python, but the protocol can be implemented in any
language. In addition to Python there's node-celery for Node.js,
a PHP client and gocelery for golang.
Language interoperability can also be achieved by using webhooks
in such a way that the client enqueues an URL to be requested by a worker.

What do I need?
Celery version 4.3 runs on,

Python (2.7, 3.4, 3.5, 3.6, 3.7)
PyPy2.7 (6.0)
PyPy3.5 (6.0)

This is the last version to support Python 2.7,
and from the next version (Celery 5.x) Python 3.5 or newer is required.
If you're running an older version of Python, you need to be running
an older version of Celery:

Python 2.6: Celery series 3.1 or earlier.
Python 2.5: Celery series 3.0 or earlier.
Python 2.4 was Celery series 2.2 or earlier.

Celery is a project with minimal funding,
so we don't support Microsoft Windows.
Please don't open any issues related to that platform.
Celery is usually used with a message broker to send and receive messages.
The RabbitMQ, Redis transports are feature complete,
but there's also experimental support for a myriad of other solutions, including
using SQLite for local development.
Celery can run on a single machine, on multiple machines, or even
across datacenters.

Get Started
If this is the first time you're trying to use Celery, or you're
new to Celery 4.2 coming from previous versions then you should read our
getting started tutorials:

First steps with Celery

Tutorial teaching you the bare minimum needed to get started with Celery.


Next steps

A more complete overview, showing more features.




Celery is...

Simple

Celery is easy to use and maintain, and does not need configuration files.
It has an active, friendly community you can talk to for support,
like at our mailing-list, or the IRC channel.
Here's one of the simplest applications you can make:
from celery import Celery

app = Celery('hello', broker='amqp://guest@localhost//')

@app.task
def hello():
    return 'hello world'



Highly Available

Workers and clients will automatically retry in the event
of connection loss or failure, and some brokers support
HA in way of Primary/Primary or Primary/Replica replication.


Fast

A single Celery process can process millions of tasks a minute,
with sub-millisecond round-trip latency (using RabbitMQ,
py-librabbitmq, and optimized settings).


Flexible

Almost every part of Celery can be extended or used on its own,
Custom pool implementations, serializers, compression schemes, logging,
schedulers, consumers, producers, broker transports, and much more.




It supports...


Message Transports


RabbitMQ, Redis, Amazon SQS



Concurrency


Prefork, Eventlet, gevent, single threaded (solo)



Result Stores


AMQP, Redis
memcached
SQLAlchemy, Django ORM
Apache Cassandra, IronCache, Elasticsearch



Serialization


pickle, json, yaml, msgpack.
zlib, bzip2 compression.
Cryptographic message signing.






Framework Integration
Celery is easy to integrate with web frameworks, some of which even have
integration packages:



Django
not needed

Pyramid
pyramid_celery

Pylons
celery-pylons

Flask
not needed

web2py
web2py-celery

Tornado
tornado-celery




The integration packages aren't strictly necessary, but they can make
development easier, and sometimes they add important hooks like closing
database connections at fork.

Documentation
The latest documentation is hosted at Read The Docs, containing user guides,
tutorials, and an API reference.
ÊúÄÊñ∞ÁöÑ‰∏≠ÊñáÊñáÊ°£ÊâòÁÆ°Âú® https://www.celerycn.io/ ‰∏≠ÔºåÂåÖÂê´Áî®Êà∑ÊåáÂçó„ÄÅÊïôÁ®ã„ÄÅAPIÊé•Âè£Á≠â„ÄÇ

Installation
You can install Celery either via the Python Package Index (PyPI)
or from source.
To install using pip:
$ pip install -U Celery


Bundles
Celery also defines a group of bundles that can be used
to install Celery and the dependencies for a given feature.
You can specify these in your requirements or on the pip
command-line by using brackets. Multiple bundles can be specified by
separating them by commas.
$ pip install ""celery[librabbitmq]""

$ pip install ""celery[librabbitmq,redis,auth,msgpack]""

The following bundles are available:

Serializers


celery[auth]:for using the auth security serializer.

celery[msgpack]:for using the msgpack serializer.

celery[yaml]:for using the yaml serializer.




Concurrency


celery[eventlet]:for using the eventlet pool.

celery[gevent]:for using the gevent pool.




Transports and Backends


celery[librabbitmq]:for using the librabbitmq C library.


celery[redis]:for using Redis as a message transport or as a result backend.


celery[sqs]:for using Amazon SQS as a message transport.


celery[tblib]:for using the task_remote_tracebacks feature.


celery[memcache]:for using Memcached as a result backend (using pylibmc)


celery[pymemcache]:for using Memcached as a result backend (pure-Python implementation).


celery[cassandra]:for using Apache Cassandra as a result backend with DataStax driver.


celery[azureblockblob]:for using Azure Storage as a result backend (using azure-storage)


celery[s3]:for using S3 Storage as a result backend.


celery[couchbase]:for using Couchbase as a result backend.


celery[arangodb]:for using ArangoDB as a result backend.


celery[elasticsearch]:for using Elasticsearch as a result backend.


celery[riak]:for using Riak as a result backend.


celery[cosmosdbsql]:for using Azure Cosmos DB as a result backend (using pydocumentdb)


celery[zookeeper]:for using Zookeeper as a message transport.


celery[sqlalchemy]:for using SQLAlchemy as a result backend (supported).


celery[pyro]:for using the Pyro4 message transport (experimental).


celery[slmq]:for using the SoftLayer Message Queue transport (experimental).


celery[consul]:for using the Consul.io Key/Value store as a message transport or result backend (experimental).


celery[django]:specifies the lowest version possible for Django support.
You should probably not use this in your requirements, it's here
for informational purposes only.





Downloading and installing from source
Download the latest version of Celery from PyPI:
https://pypi.org/project/celery/
You can install it by doing the following,:
$ tar xvfz celery-0.0.0.tar.gz
$ cd celery-0.0.0
$ python setup.py build
# python setup.py install

The last command must be executed as a privileged user if
you aren't currently using a virtualenv.

Using the development version

With pip
The Celery development version also requires the development
versions of kombu, amqp, billiard, and vine.
You can install the latest snapshot of these using the following
pip commands:
$ pip install https://github.com/celery/celery/zipball/master#egg=celery
$ pip install https://github.com/celery/billiard/zipball/master#egg=billiard
$ pip install https://github.com/celery/py-amqp/zipball/master#egg=amqp
$ pip install https://github.com/celery/kombu/zipball/master#egg=kombu
$ pip install https://github.com/celery/vine/zipball/master#egg=vine


With git
Please see the Contributing section.

Getting Help

Mailing list
For discussions about the usage, development, and future of Celery,
please join the celery-users mailing list.

IRC
Come chat with us on IRC. The #celery channel is located at the Freenode
network.

Bug tracker
If you have any suggestions, bug reports, or annoyances please report them
to our issue tracker at https://github.com/celery/celery/issues/

Wiki
https://github.com/celery/celery/wiki

Credits

Contributors
This project exists thanks to all the people who contribute. Development of
celery happens at GitHub: https://github.com/celery/celery
You're highly encouraged to participate in the development
of celery. If you don't like GitHub (for some reason) you're welcome
to send regular patches.
Be sure to also read the Contributing to Celery section in the
documentation.


Backers
Thank you to all our backers! üôè [Become a backer]


Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a
link to your website. [Become a sponsor]



License
This software is licensed under the New BSD License. See the LICENSE
file in the top distribution directory for the full license text.
",GitHub - celery/celery: Distributed Task Queue (development branch)
28,Python,"pyspider   
A Powerful Spider(Web Crawler) System in Python. TRY IT NOW!

Write script in Python
Powerful WebUI with script editor, task monitor, project manager and result viewer
MySQL, CouchDB, MongoDB, Redis, SQLite, Elasticsearch; PostgreSQL with SQLAlchemy as database backend
RabbitMQ, Redis and Kombu as message queue
Task priority, retry, periodical, recrawl by age, etc...
Distributed architecture, Crawl Javascript pages, Python 2.{6,7}, 3.{3,4,5,6} support, etc...

Tutorial: http://docs.pyspider.org/en/latest/tutorial/
Documentation: http://docs.pyspider.org/
Release notes: https://github.com/binux/pyspider/releases
Sample Code
from pyspider.libs.base_handler import *


class Handler(BaseHandler):
    crawl_config = {
    }

    @every(minutes=24 * 60)
    def on_start(self):
        self.crawl('http://scrapy.org/', callback=self.index_page)

    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('a[href^=""http""]').items():
            self.crawl(each.attr.href, callback=self.detail_page)

    def detail_page(self, response):
        return {
            ""url"": response.url,
            ""title"": response.doc('title').text(),
        }

Installation

pip install pyspider
run command pyspider, visit http://localhost:5000/

WARNING: WebUI is open to the public by default, it can be used to execute any command which may harm your system. Please use it in an internal network or enable need-auth for webui.
Quickstart: http://docs.pyspider.org/en/latest/Quickstart/
Contribute

Use It
Open Issue, send PR
User Group
‰∏≠ÊñáÈóÆÁ≠î

TODO
v0.4.0

 a visual scraping interface like portia

License
Licensed under the Apache License, Version 2.0
",GitHub - binux/pyspider: A Powerful Spider(Web Crawler) System in Python.
29,Python,"TensorFlow Course



This repository aims to provide simple and ready-to-use tutorials for TensorFlow.
Each tutorial includes source code and most of them are associated with a documentation.

Table of Contents


Download Free TensorFlow Roadmap EBook
What is TensorFlow?


Motivation
Why use TensorFlow?
What's the point of this repository?


TensorFlow Installation and Setup the Environment
TensorFlow Tutorials
Warm-up
Basics
Basic Machine Learning
Neural Networks


Some Useful Tutorials
Contributing
Pull Request Process
Final Note


Developers
Acknowledgement



Download Free TensorFlow Roadmap EBook



What is TensorFlow?
TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.
TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015.
The current stable release as of September 27, 2018 is 1.11.0

Motivation
There are different motivations for this open source project. TensorFlow (as we write this document) is one of / the best deep learning frameworks available. The question that should be asked is why has this repository been created when there are so many other tutorials about TensorFlow available on the web?

Why use TensorFlow?
Deep Learning is in very high interest these days - there's a crucial need for rapid and optimized implementations of the algorithms and architectures. TensorFlow is designed to facilitate this goal.
The strong advantage of TensorFlow is it flexibility in designing highly modular models which can also be a disadvantage for beginners since a lot of the pieces must be considered together when creating the model.
This issue has been facilitated as well by developing high-level APIs such as Keras and Slim which abstract a lot of the pieces used in designing machine learning algorithms.
The interesting thing about TensorFlow is that it can be found anywhere these days. Lots of the researchers and developers are using it and its community is growing at the speed of light! So many issues can be dealt with easily since they're usually the same issues that a lot of other people run into considering the large number of people involved in the TensorFlow community.

What's the point of this repository?
Developing open source projects for the sake of just developing something is not the reason behind this effort.
Considering the large number of tutorials that are being added to this large community, this repository has been created to break the jump-in and jump-out process that usually happens to most of the open source projects, but why and how?
First of all, what's the point of putting effort into something that most of the people won't stop by and take a look? What's the point of creating something that does not help anyone in the developers and researchers community? Why spend time for something that can easily be forgotten? But how we try to do it? Even up to this
very moment there are countless tutorials on TensorFlow whether on the model design or TensorFlow
workflow.
Most of them are too complicated or suffer from a lack of documentation. There are only a few available tutorials which are concise and well-structured and provide enough insight for their specific implemented models.
The goal of this project is to help the community with structured tutorials and simple and optimized code implementations to provide better insight about how to use TensorFlow quick and effectively.
It is worth noting that, the main goal of this project is to provide well-documented tutorials and less-complicated code!

TensorFlow Installation and Setup the Environment

In order to install TensorFlow please refer to the following link:


TensorFlow Installation



The virtual environment installation is recommended in order to prevent package conflict and having the capacity to customize the working environment.

TensorFlow Tutorials
The tutorials in this repository are partitioned into relevant categories.


Warm-up



#
topic
Source Code
¬†



1
Start-up
Welcome  / IPython
Documentation





Basics



#
topic
Source Code
¬†



2
TensorFLow Basics
Basic Math Operations   / IPython
Documentation

3
TensorFLow Basics
TensorFlow Variables   / IPython
Documentation





Basic Machine Learning



#
topic
Source Code
¬†



4
Linear Models
Linear Regression  / IPython
Documentation

5
Predictive Models
Logistic Regression  / IPython
Documentation

6
Support Vector Machines
Linear SVM  / IPython
¬†

7
Support Vector Machines
MultiClass Kernel SVM  / IPython
¬†





Neural Networks



#
topic
Source Code
¬†



8
Multi Layer Perceptron
Simple Multi Layer Perceptron   / IPython
¬†

9
Convolutional Neural Network
Simple Convolutional Neural Networks
Documentation

10
Recurrent Neural Network
RNN  / IPython
¬†




Some Useful Tutorials


TensorFlow Examples - TensorFlow tutorials and code examples for beginners
Sungjoon's TensorFlow-101 - TensorFlow tutorials written in Python with Jupyter Notebook
Terry Um‚Äôs TensorFlow Exercises - Re-create the codes from other TensorFlow examples
Classification on time series - Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data



Contributing
When contributing to this repository, please first discuss the change you wish to make via issue,
email, or any other method with the owners of this repository before making a change. For typos, please
do not create a pull request. Instead, declare them in issues or email the repository owner.
Please note we have a code of conduct, please follow it in all your interactions with the project.

Pull Request Process
Please consider the following criterions in order to help us in a better way:


The pull request is mainly expected to be a code script suggestion or improvement.
A pull request related to non-code-script sections is expected to make a significant difference in the documentation. Otherwise, it is expected to be announced in the issues section.
Ensure any install or build dependencies are removed before the end of the layer when doing a build and creating a pull request.
Add comments with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.
You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.



Final Note
We are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.
For contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate
your kind feedback and elaborate code inspections.

Developers
Creator: Machine Learning Mindset [Blog, GitHub, Twitter]
Developer: Amirsina Torfi [GitHub, Personal Website, Linkedin ]

Acknowledgement
I have taken huge efforts in this project for hopefully being a small part of TensorFlow world. However, it would not have been plausible without the kind support and help of my friend and colleague Domenick Poster for his valuable advices. He helped me for having a better understanding of TensorFlow and my special appreciation goes to him.
",GitHub - machinelearningmindset/TensorFlow-Course: Simple and ready-to-use tutorials for TensorFlow
30,Python,"



interactive-coding-challenges
120+ continually updated, interactive, and test-driven coding challenges, with Anki flashcards.
Challenges focus on algorithms and data structures found in coding interviews.
Each challenge has one or more reference solutions that are:

Fully functional
Unit tested
Easy-to-understand

Challenges will soon provide on-demand incremental hints to help you arrive at the optimal solution.
Notebooks also detail:

Constraints
Test cases
Algorithms
Big-O time and space complexities

Also included are unit tested reference implementations of various data structures and algorithms.
Challenge Solutions





Anki Flashcards: Coding and Design




The provided Anki flashcard deck uses spaced repetition to help you retain key concepts.

Coding deck

Great for use while on-the-go.
Design Resource: The System Design Primer
Looking for resources to help you prep for the System Design and Object-Oriented Design interviews?




Check out the sister repo The System Design Primer, which contains additional Anki decks:

System design deck
System design exercises deck
Object oriented design exercises deck


Notebook Structure
Each challenge has two notebooks, a challenge notebook with unit tests for you to solve and a solution notebook for reference.
Problem Statement

States the problem to solve.

Constraints

Describes any constraints or assumptions.

Test Cases

Describes the general and edge test cases that will be evaluated in the unit test.

Algorithm

[Challenge Notebook] Empty, refer to the solution notebook algorithm section if you need a hint.
[Solution Notebook] One or more algorithm solution discussions, with Big-O time and space complexities.

Hints

[Challenge Notebook] Provides on-demand incremental hints to help you arrive at the optimal solution.  Coming soon!

Code (Challenge: Implement Me!)

[Challenge Notebook] Skeleton code for you to implement.
[Solution Notebook] One or more reference solutions.

Unit Test

[Challenge Notebook] Unit test for your code.  Expected to fail until you solve the challenge.
[Solution Notebook] Unit test for the reference solution(s).

Index
Challenges Categories
Format: Challenge Category - Number of Challenges

Arrays and Strings - 10
Linked Lists - 8
Stacks and Queues - 8
Graphs and Trees - 21
Sorting - 10
Recursion and Dynamic Programming - 17
Mathematics and Probability - 6
Bit Manipulation - 8
Online Judges - 16
System Design - 8
Object Oriented Design - 8

Total number of challenges: 120
Reference Implementations: Data Structures
Unit tested, fully functional implementations of the following data structures:

Linked List
Stack
Queue
Binary Search Tree
Graph
Min Heap
Trie
Priority Queue
Hash Map

Reference Implementations: Algorithms
Unit tested, fully functional implementations of the following algorithms:

Selection Sort
Insertion Sort
Quick Sort
Merge Sort
Radix Sort
Topological Sort
Tree Depth-First Search (Pre-, In-, Post-Order)
Tree Breadth-First Search
Graph Depth-First Search
Graph Breadth-First Search
Dijkstra's Shortest Path
Unweighted Graph Shortest Path
Knapsack 0/1
Knapsack Unbounded
Sieve of Eratosthenes

Reference Implementations: TODO

A*
Bellman-Ford
Bloom Filter
Convex Hull
Fisher-Yates Shuffle
Kruskal's
Max Flow
Prim's
Rabin-Karp
Traveling Salesman
Union Find
Contribute

Installing and Running Challenges

Repo Structure
Notebook Installation

Nose Installation


Running Challenges

Misc

Contributing
Credits
Contact Info
License

Challenges
Image Credits





Arrays and Strings



Challenge
Static Notebook




Determine if a string contains unique characters
Challenge‚îÇSolution


Determine if a string is a permutation of another
Challenge‚îÇSolution


Determine if a string is a rotation of another
Challenge‚îÇSolution


Compress a string
Challenge‚îÇSolution


Reverse characters in a string
Challenge‚îÇSolution


Given two strings, find the single different char
Challenge‚îÇSolution


Find two indices that sum to a specific value
Challenge‚îÇSolution


Implement a hash table
Challenge‚îÇSolution


Implement fizz buzz
Challenge‚îÇSolution


Find the first non-repeated character in a string
Contribute‚îÇContribute


Remove specified characters in a string
Contribute‚îÇContribute


Reverse words in a string
Contribute‚îÇContribute


Convert a string to an integer
Contribute‚îÇContribute


Convert an integer to a string
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Linked Lists



Challenge
Static Notebook




Remove duplicates from a linked list
Challenge‚îÇSolution


Find the kth to last element of a linked list
Challenge‚îÇSolution


Delete a node in the middle of a linked list
Challenge‚îÇSolution


Partition a linked list around a given value
Challenge‚îÇSolution


Add two numbers whose digits are stored in a linked list
Challenge‚îÇSolution


Find the start of a linked list loop
Challenge‚îÇSolution


Determine if a linked list is a palindrome
Challenge‚îÇSolution


Implement a linked list
Challenge‚îÇSolution


Determine if a list is cyclic or acyclic
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Stacks and Queues



Challenge
Static Notebook




Implement n stacks using a single array
Challenge‚îÇSolution


Implement a stack that keeps track of its minimum element
Challenge‚îÇSolution


Implement a set of stacks class that wraps a list of capacity-bounded stacks
Challenge‚îÇSolution


Implement a queue using two stacks
Challenge‚îÇSolution


Sort a stack using another stack as a buffer
Challenge‚îÇSolution


Implement a stack
Challenge‚îÇSolution


Implement a queue
Challenge‚îÇSolution


Implement a priority queue backed by an array
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute








Graphs and Trees



Challenge
Static Notebooks




Implement depth-first search (pre-, in-, post-order) on a tree
Challenge‚îÇSolution


Implement breadth-first search on a tree
Challenge‚îÇSolution


Determine the height of a tree
Challenge‚îÇSolution


Create a binary search tree with minimal height from a sorted array
Challenge‚îÇSolution


Create a linked list for each level of a binary tree
Challenge‚îÇSolution


Check if a binary tree is balanced
Challenge‚îÇSolution


Determine if a tree is a valid binary search tree
Challenge‚îÇSolution


Find the in-order successor of a given node in a binary search tree
Challenge‚îÇSolution


Find the second largest node in a binary search tree
Challenge‚îÇSolution


Find the lowest common ancestor
Challenge‚îÇSolution


Invert a binary tree
Challenge‚îÇSolution


Implement a binary search tree
Challenge‚îÇSolution


Implement a min heap
Challenge‚îÇSolution


Implement a trie
Challenge‚îÇSolution


Implement depth-first search on a graph
Challenge‚îÇSolution


Implement breadth-first search on a graph
Challenge‚îÇSolution


Determine if there is a path between two nodes in a graph
Challenge‚îÇSolution


Implement a graph
Challenge‚îÇSolution


Find a build order given a list of projects and dependencies.
Challenge‚îÇSolution


Find the shortest path in a weighted graph.
Challenge‚îÇSolution


Find the shortest path in an unweighted graph.
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute








Sorting



Challenge
Static Notebooks




Implement selection sort
Challenge‚îÇSolution


Implement insertion sort
Challenge‚îÇSolution


Implement quick sort
Challenge‚îÇSolution


Implement merge sort
Challenge‚îÇSolution


Implement radix sort
Challenge‚îÇSolution


Sort an array of strings so all anagrams are next to each other
Challenge‚îÇSolution


Find an item in a sorted, rotated array
Challenge‚îÇSolution


Search a sorted matrix for an item
Challenge‚îÇSolution


Find an int not in an input of n integers
Challenge‚îÇSolution


Given sorted arrays A, B, merge B into A in sorted order
Challenge‚îÇSolution


Implement a stable selection sort
Contribute‚îÇContribute


Make an unstable sort stable
Contribute‚îÇContribute


Implement an efficient, in-place version of quicksort
Contribute‚îÇContribute


Given two sorted arrays, merge one into the other in sorted order
Contribute‚îÇContribute


Find an element in a rotated and sorted array of integers
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Recursion and Dynamic Programming



Challenge
Static Notebooks




Implement fibonacci recursively, dynamically, and iteratively
Challenge‚îÇSolution


Maximize items placed in a knapsack
Challenge‚îÇSolution


Maximize unbounded items placed in a knapsack
Challenge‚îÇSolution


Find the longest common subsequence
Challenge‚îÇSolution


Find the longest increasing subsequence
Challenge‚îÇSolution


Minimize the cost of matrix multiplication
Challenge‚îÇSolution


Maximize stock prices given k transactions
Challenge‚îÇSolution


Find the minimum number of ways to represent n cents given an array of coins
Challenge‚îÇSolution


Find the unique number of ways to represent n cents given an array of coins
Challenge‚îÇSolution


Print all valid combinations of n-pairs of parentheses
Challenge‚îÇSolution


Navigate a maze
Challenge‚îÇSolution


Print all subsets of a set
Challenge‚îÇSolution


Print all permutations of a string
Challenge‚îÇSolution


Find the magic index in an array
Challenge‚îÇSolution


Find the number of ways to run up n steps
Challenge‚îÇSolution


Implement the Towers of Hanoi with 3 towers and N disks
Challenge‚îÇSolution


Implement factorial recursively, dynamically, and iteratively
Contribute‚îÇContribute


Perform a binary search on a sorted array of integers
Contribute‚îÇContribute


Print all combinations of a string
Contribute‚îÇContribute


Implement a paint fill function
Contribute‚îÇContribute


Find all permutations to represent n cents, given 1, 5, 10, 25 cent coins
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Mathematics and Probability



Challenge
Static Notebooks




Generate a list of primes
Challenge‚îÇSolution


Find the digital root
Challenge‚îÇSolution


Create a class supporting insert, max, min, mean, mode in O(1)
Challenge‚îÇSolution


Determine if a number is a power of two
Challenge‚îÇSolution


Add two numbers without the + or - sign
Challenge‚îÇSolution


Subtract two numbers without the + or - sign
Challenge‚îÇSolution


Check if a number is prime
Contribute‚îÇContribute


Determine if two lines on a Cartesian plane intersect
Contribute‚îÇContribute


Using only add, implement multiply, subtract, and divide for ints
Contribute‚îÇContribute


Find the kth number such that the only prime factors are 3, 5, and 7
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Bit Manipulation



Challenge
Static Notebooks




Implement common bit manipulation operations
Challenge‚îÇSolution


Determine number of bits to flip to convert a into b
Challenge‚îÇSolution


Draw a line on a screen
Challenge‚îÇSolution


Flip a bit to maximize the longest sequence of 1s
Challenge‚îÇSolution


Get the next largest and next smallest numbers
Challenge‚îÇSolution


Merge two binary numbers
Challenge‚îÇSolution


Swap odd and even bits in an integer
Challenge‚îÇSolution


Print the binary representation of a number between 0 and 1
Challenge‚îÇSolution


Determine the number of 1s in the binary representation of a given integer
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Online Judges



Challenge
Static Notebooks




Find the longest substring with at most k distinct chars
Challenge‚îÇSolution


Find the highest product of three numbers
Challenge‚îÇSolution


Maximize stocks profit from 1 buy and 1 sell
Challenge‚îÇSolution


Move all zeroes in a list to the end
Challenge‚îÇSolution


Find the products of every other int
Challenge‚îÇSolution


Given a list of entries and exits, find the busiest period
Challenge‚îÇSolution


Determine an island's perimeter
Challenge‚îÇSolution


Format license keys
Challenge‚îÇSolution


Find the longest absolute file path
Challenge‚îÇSolution


Merge tuple ranges
Challenge‚îÇSolution


Assign cookies
Challenge‚îÇSolution


Determine if you can win in Nim
Challenge‚îÇSolution


Check if a magazine could have been used to create a ransom note
Challenge‚îÇSolution


Find the number of times a sentence can fit on a screen
Challenge‚îÇSolution


Utopian tree
Challenge‚îÇSolution


Maximizing xor
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute



Repo Structure
interactive-coding-challenges        # Repo
‚îú‚îÄ arrays_strings                    # Category of challenges
‚îÇ  ‚îú‚îÄ rotation                       # Challenge folder
‚îÇ  ‚îÇ  ‚îú‚îÄ rotation_challenge.ipynb    # Challenge notebook
‚îÇ  ‚îÇ  ‚îú‚îÄ rotation_solution.ipynb     # Solution notebook
‚îÇ  ‚îÇ  ‚îú‚îÄ test_rotation.py            # Unit test*
‚îÇ  ‚îú‚îÄ compress
‚îÇ  ‚îÇ  ‚îú‚îÄ compress_challenge.ipynb
‚îÇ  ‚îÇ  ‚îú‚îÄ compress_solution.ipynb
‚îÇ  ‚îÇ  ‚îú‚îÄ test_compress.py
‚îÇ  ‚îú‚îÄ ...
‚îú‚îÄ linked_lists
‚îÇ  ‚îú‚îÄ palindrome
‚îÇ  ‚îÇ  ‚îî‚îÄ ...
‚îÇ  ‚îú‚îÄ ...
‚îú‚îÄ ...

*The notebooks (.ipynb) read/write the associated unit test (.py) file.
Notebook Installation
Jupyter Notebook
Run:
pip install jupyter

For detailed instructions, scripts, and tools to more optimally set up your development environment, check out the dev-setup repo.
For more details on notebook installation, follow the directions here.
More information on IPython/Jupyter Notebooks can be found here.
Nose Tests
Install nose using setuptools/distribute:
easy_install nose

or
pip install nose

More information on Nose can be found here.
Running Challenges
Notebooks
Challenges are provided in the form of IPython/Jupyter Notebooks and have been tested with Python 2.7 and Python 3.x.
If you need to install IPython/Jupyter Notebook, see the Notebook Installation section.

This README contains links to nbviewer, which hosts static notebooks of the repo's contents
To interact with or to modify elements within the dynamic notebooks, refer to the instructions below

Run the notebook of challenges:
$ git clone https://github.com/donnemartin/interactive-coding-challenges.git
$ cd interactive-coding-challenges
$ jupyter notebook

This will launch your web browser with the list of challenge categories:

Navigate to the Challenge Notebook you wish to solve
Run the cells within the challenge notebook (Cell->Run All)

This will result in an expected unit test error


Solve the challenge and verify it passes the unit test
Check out the accompanying Solution Notebook for further discussion

To debug your solution with pdb, refer to the following ticket.
Note: If your solution is different from those listed in the Solution Notebook, consider submitting a pull request so others can benefit from your work.  Review the Contributing Guidelines for details.
Future Development
Challenges, solutions, and unit tests are presented in the form of IPython/Jupyter Notebooks.

Notebooks currently contain mostly Python solutions (tested on both Python 2.7 and Python 3.x), but can be extended to include 40+ supported languages
Repo will be continually updated with new solutions and challenges
Contributions are welcome!

Contributing
Contributions are welcome!
Review the Contributing Guidelines for details on how to:

Submit issues
Add solutions to existing challenges
Add new challenges

Credits
Resources

Cracking the Coding Interview | GitHub Solutions
Programming Interviews Exposed
The Algorithm Design Manual | Solutions
CareerCup
Quora
HackerRank
LeetCode

Images

Arrays and Strings: nltk.org
Linked Lists: wikipedia.org
Stacks: wikipedia.org
Queues: wikipedia.org
Sorting: wikipedia.org
Recursion and Dynamic Programming: wikipedia.org
Graphs and Trees: wikipedia.org
Mathematics and Probability: wikipedia.org
Bit Manipulation: wikipedia.org
Online Judges: topcoder.com

Contact Info
Feel free to contact me to discuss any issues, questions, or comments.
My contact info can be found on my GitHub page.
License
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2015 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",GitHub - donnemartin/interactive-coding-challenges: 120+ interactive Python coding interview challenges (algorithms and data structures).  Includes Anki flashcards.
31,Python,"Tornado Web Server


Tornado is a Python web framework and
asynchronous networking library, originally developed at FriendFeed.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
long polling,
WebSockets, and other
applications that require a long-lived connection to each user.

Hello, world
Here is a simple ""Hello, world"" example web app for Tornado:
import tornado.ioloop
import tornado.web

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write(""Hello, world"")

def make_app():
    return tornado.web.Application([
        (r""/"", MainHandler),
    ])

if __name__ == ""__main__"":
    app = make_app()
    app.listen(8888)
    tornado.ioloop.IOLoop.current().start()
This example does not use any of Tornado's asynchronous features; for
that see this simple chat room.

Documentation
Documentation and links to additional resources are available at
https://www.tornadoweb.org
","GitHub - tornadoweb/tornado: Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed."
32,Python,"12306 Ë¥≠Á•®Â∞èÂä©Êâã
pythonÁâàÊú¨

 2.7.10 - 2.7.15
 3.6 - 3.7.4
 2.7.9

Â∑≤ÊúâÂäüËÉΩ

 Ëá™Âä®ÊâìÁ†Å
 Ëá™Âä®ÁôªÂΩï
 ÂáÜÁÇπÈ¢ÑÂîÆÂíåÊç°Êºè
 Êô∫ËÉΩÂÄôË°•
 ÈÇÆ‰ª∂ÈÄöÁü•
 serverÈÖ±ÈÄöÁü•

‰æùËµñÂ∫ì

È™åËØÅÁ†ÅÁõÆÂâçÂèØ‰ª•Êú¨Âú∞ËØÜÂà´ÔºåÈúÄË¶Å‰∏ãËΩΩÊ®°ÂûãÔºåÊîæ‰∫éÈ°πÁõÆÊ†πÁõÆÂΩïÔºåÂÖ®ÈÉ®‰ª£Á†ÅÊù•Ê∫ê‰∫éÊ≠§È°πÁõÆ ‰º†ÈÄÅÈó®ÔºåË°®Á§∫ÊÑüË∞¢
  PS: 
  1. Ê®°Âûã‰∏ãËΩΩÈìæÊé•:https://pan.baidu.com/s/1rS155VjweWVWIJogakechA  ÂØÜÁ†Å:bmlm
     Áæ§ÈáåÈù¢‰πüÂèØ‰ª•‰∏ãËΩΩ
  2. git‰ªìÂ∫ì‰∏ãËΩΩÔºöhttps://github.com/testerSunshine/12306model.git


Ëá™ÊâòÁÆ°‰∫ëÊâìÁ†ÅÊúçÂä°Âô®Êê≠Âª∫Ôºö12306_code_server

Â¶ÇÊûúÂ§ßÂÆ∂ÊúâÁ©∫Èó≤ÁöÑÊúçÂä°Âô®ÔºåÂèØÊê≠Âª∫‰πãÂêéÂÜçËøô‰∏™ issues ÈáåÈù¢Â°´ÂÖ•Ëá™Â∑±ÁöÑÊúçÂä°Âô®(ËØ∑Ê≥®ÊÑèÊúçÂä°Âô®ÂÆâÂÖ®ÔºÅ)


È°πÁõÆ‰æùËµñÂåÖÊü•Áúã requirements.txt
ÂÆâË£ÖÊñπÊ≥ïx:

rootÁî®Êà∑(ÈÅøÂÖçÂ§öpythonÁéØÂ¢É‰∫ßÁîüÈóÆÈ¢ò): pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt
ÈùûrootÁî®Êà∑ÔºàÈÅøÂÖçÂÆâË£ÖÂíåËøêË°åÊó∂‰ΩøÁî®‰∫Ü‰∏çÂêåÁéØÂ¢ÉÔºâ: pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt



È°πÁõÆ‰ΩøÁî®ËØ¥Êòé

ÊúçÂä°Âô®ÂêØÂä®:

‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂

ÂèØ‰ª•ÈÖçÁΩÆÈÇÆÁÆ±,ÈÖçÁΩÆÈÇÆÁÆ±ÁöÑÊ†ºÂºèÂú®ÈÖçÁΩÆÈáåÈù¢ÂèØ‰ª•ÁúãÂà∞ex
ÂèØ‰ª•ÈÖçÁΩÆserverÈÖ±ÊèêÈÜíÔºàÊé®ËçêÔºâÈÖçÁΩÆÊïôÁ®ã
ÈÖçÁΩÆÈÖçÁΩÆÊñá‰ª∂ÁöÑÊó∂ÂÄôÔºåÈúÄÊ≥®ÊÑèÁ©∫Ê†ºÂíåÈÅµÂæ™pythonËØ≠Ê≥ïÊ†ºÂºè


ËøêË°åÊ†πÁõÆÂΩïsudo python run.pyÔºåÂç≥ÂèØÂºÄÂßã


Â¶ÇÊûú‰Ω†ÁöÑÊúçÂä°Âô®ÂÆâË£Ö‰∫Üdocker‰∏édocker-compose, ÈÇ£‰πàÂ∞±ÂèØ‰ª•ÈÄöËøádocker-composeËøõË°åÂêØÂä®,docker.shËÑöÊú¨ÂØπÊ≠§ËøõË°å‰∫ÜÂ∞ÅË£ÖÔºåÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ãÂëΩ‰ª§ËøõË°åÂêØÂä®

1„ÄÅsudo ./docker.sh run #ÂàõÂª∫‰∏Ä‰∏™ÈïúÂÉèÂπ∂ÂêØÂä®ÂÆπÂô®ÔºåÂ¶ÇÊûúÈïúÂÉèÂ∑≤ÁªèÂàõÂª∫Ëøá‰∫Ü‰ºöÁõ¥Êé•ÂêØÂä®ÂÆπÂô®„ÄÇ
2„ÄÅsudo ./docker.sh restart #‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÂêéÔºåÈÄöËøáÊ≠§ÂêçÂëΩ‰ª§ÂèØÈáçÊñ∞Âä†ËΩΩÂÆπÂô®ËøêË°å
3„ÄÅsudo ./docker.sh rm #Âà†Èô§ÂÆπÂô®
4„ÄÅsudo ./docker.sh drun #ÂêéÂè∞ËøêË°åÂÆπÂô®
5„ÄÅsudo ./docker.sh logs #Âú®ÂêéÂè∞ËøêË°åÊó∂ÔºåÈÄöËøáÊ≠§ÂëΩ‰ª§Êü•ÁúãËøêË°åÁöÑÂÜÖÂÆπ
Ê≥®: ÈíàÂØπÊ≤°ÊúâdockerÁéØÂ¢ÉÁöÑÂêåÂ≠¶Êèê‰æõ‰∫ÜdockerÂÆâË£ÖËÑöÊú¨(centos7)
- sudo ./docker_install_centos.sh
Ê≥®: Ëã•Âè™ÊúâdockerÊ≤°Êúâdocker-compose. ÂèØÈÄöËøápip install docker-composeËøõË°å‰∏ãËΩΩ



ÁõÆÂΩïÂØπÂ∫îËØ¥Êòé

agency - cdn‰ª£ÁêÜ
config - È°πÁõÆÈÖçÁΩÆ
verify - Ëá™Âä®ÊâìÁ†Å
init - È°πÁõÆ‰∏ªËøêË°åÁõÆÂΩï
inter - Êé•Âè£
myException - ÂºÇÂ∏∏
myUrllib  requestÁΩëÁªúËØ∑Ê±ÇÂ∫ì

ÊÄùË∑ØÂõæ



È°πÁõÆÂ£∞ÊòéÔºö

Êú¨ËΩØ‰ª∂Âè™‰æõÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®ÔºåÂãø‰Ωú‰∏∫ÂïÜ‰∏öÁî®ÈÄîÔºå‰∫§ÊµÅÁæ§Âè∑

1Áæ§Ôºö286271084(Â∑≤Êª°)
2Áæ§Ôºö649992274(Â∑≤Êª°)
3Áæ§Ôºö632501142(Â∑≤Êª°)
4Áæ§: 606340519(Â∑≤Êª°)
5Áæ§: 948526733(Â∑≤Êª°)
7Áæ§: 660689659(Â∑≤Êª°)
8Áæ§: 620629239(Â∑≤Êª°)
6Áæ§: 608792930(Êú™Êª°)
9Áæ§: 693035807(Êú™Êª°)


ËØ∑‰∏çË¶ÅÈáçÂ§çÂä†Áæ§Ôºå‰∏Ä‰∏™Áæ§Â∞±ÂèØ‰ª•‰∫ÜÔºåÊääÊú∫‰ºöÁïôÁªôÊõ¥Â§ö‰∫∫
ËøõÁæ§ÂÖàÁúãÂÖ¨ÂëäÔºÅÔºÅÔºÅËøõÁæ§ÂÖàÁúãÂÖ¨ÂëäÔºÅÔºÅÔºÅËøõÁæ§ÂÖàÁúãÂÖ¨ÂëäÔºÅÔºÅÔºÅ ÈáçË¶ÅÁöÑ‰∫ãÊÉÖËØ¥‰∏âÈÅç
ËÉΩ‰∏∫‰Ω†Êä¢Âà∞‰∏ÄÂº†ÂõûÂÆ∂ÁöÑÁ•®ÔºåÊòØÊàëÊúÄÂ§ßÁöÑÂøÉÊÑø

Êó•ÂøóÂàóÂ≠ê

ÊàêÂäülogÔºåÂ¶ÇÊûúÊòØË¥≠Á•®Â§±Ë¥•ÁöÑÔºåËØ∑Â∏¶‰∏äÂ§±Ë¥•ÁöÑlogÁªôÊàëÔºåÊàëÂ∞ΩÂäõÂ∏Æ‰Ω†Ë∞ÉÔºå‰πüÂèØÂä†Áæ§‰∏ÄËµ∑‰∫§ÊµÅÔºåÁ®ãÂ∫èÂè™ÊòØÂä†ÈÄü‰π∞Á•®ÁöÑËøáÁ®ãÔºåÂπ∂‰∏ç‰∏ÄÂÆöËÉΩ‰π∞Âà∞Á•®
Ê≠£Âú®Á¨¨355Ê¨°Êü•ËØ¢  ‰πòËΩ¶Êó•Êúü: 2018-02-12  ËΩ¶Ê¨°G4741,G2365,G1371,G1377,G1329 Êü•ËØ¢Êó†Á•®  ‰ª£ÁêÜËÆæÁΩÆ Êó†  ÊÄªËÄóÊó∂429ms
ËΩ¶Ê¨°: G4741 ÂßãÂèëËΩ¶Á´ô: ‰∏äÊµ∑ ÁªàÁÇπÁ´ô: ÈÇµÈò≥ ‰∫åÁ≠âÂ∫ß:Êúâ
Ê≠£Âú®Â∞ùËØïÊèê‰∫§ËÆ¢Á•®...
Â∞ùËØïÊèê‰∫§ËÆ¢Âçï...
Âá∫Á•®ÊàêÂäü
ÊéíÈòüÊàêÂäü, ÂΩìÂâç‰ΩôÁ•®ËøòÂâ©‰Ωô: 359 Âº†
Ê≠£Âú®‰ΩøÁî®Ëá™Âä®ËØÜÂà´È™åËØÅÁ†ÅÂäüËÉΩ
È™åËØÅÁ†ÅÈÄöËøá,Ê≠£Âú®Êèê‰∫§ËÆ¢Âçï
Êèê‰∫§ËÆ¢ÂçïÊàêÂäüÔºÅ
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -12 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -6 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -7 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -4 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -4 ms
ÊÅ≠ÂñúÊÇ®ËÆ¢Á•®ÊàêÂäüÔºåËÆ¢ÂçïÂè∑‰∏∫ÔºöEB52743573, ËØ∑Á´ãÂç≥ÊâìÂºÄÊµèËßàÂô®ÁôªÂΩï12306ÔºåËÆøÈóÆ‚ÄòÊú™ÂÆåÊàêËÆ¢Âçï‚ÄôÔºåÂú®30ÂàÜÈíüÂÜÖÂÆåÊàêÊîØ‰ªòÔºÅ



‰ΩøÁî®Â∏ÆÂä©(‰∏Ä‰∫õÂÆâË£ÖÈóÆÈ¢òÂíå‰ΩøÁî®ÂèçÈ¶àËæÉÂ§öÁöÑÈóÆÈ¢ò)Ôºö


ÊµãËØïÈÇÆÁÆ±ÊòØÂê¶ÂèØÁî® ÈÇÆÁÆ±ÈÖçÁΩÆÈóÆÈ¢òÁúãissues


Â≠¶ÁîüÁ•®issues Â≠¶ÁîüÁ•®‰øÆÊîπ


‰æùËµñÂÆâË£Ö‰∏çÂØπÁöÑÈóÆÈ¢òÔºàImportErrorÔºârequirements.txtÈóÆÈ¢ò


Ëã•Âø´Ë±ÜÂ≠êÁñëÈóÆ ÁÇπÊàë


IOError: „ÄêErrno 0„Äë Error ÈóÆÈ¢ò ÁÇπÊàë


ÊµãËØï‰∏ãÂçïÊé•Âè£ÊòØÂê¶ÂèØÁî®ÔºåÊúâ‰∏§‰∏™‰∏ãÂçïÊé•Âè£ÔºåÈöè‰æøÁî®Âì™‰∏™ÈÉΩok


Â¶ÇÊûú‰∏ãËΩΩÈ™åËØÅÁ†ÅËøáÊúüÊàñËÄÖ‰∏ãËΩΩÂ§±Ë¥•ÁöÑÈóÆÈ¢òÔºåÂ∫îËØ•ÊòØ12306Â∞ÅipÁöÑÁ≠ñÁï•ÔºåÂ§öÈáçËØïÂá†Ê¨°Ôºå12306Áé∞Âú®Â∞ÅÊúçÂä°Âô®(ÈòøÈáå‰∫ëÂíåËÖæËÆØ‰∫ë)ipÊØîËæÉ‰∏•ÈáçÔºåÂ∞ΩÈáè‰∏çË¶ÅÊîæÂú®ÊúçÂä°Âô®ÈáåÈù¢


ÁõÆÂâç12306ÂØπÊúçÂä°Âô®ipÊØîËæÉÊïèÊÑüÔºåÂ§ßÂÆ∂ËøòÊòØÂú®Ëá™Â∑±ÂÆ∂ÈáåÊåÇÁùÄÂêß


Ëá™Âä®Êõ¥Êç¢ipËΩØ‰ª∂ÁõÆÂâçÂ∑≤ÊîØÊåÅTPLINKÂíåÂ∞èÁ±≥Ë∑ØÁî±Âô®ÔºåÂè™ÈôêÂÆ∂Â∫≠ÁΩëÁªúÁÇπÊàëË∑≥ËΩ¨


ÊÑüË∞¢‰∏Ä‰∏ãÂ∞è‰ºô‰º¥ÂØπÊú¨È°πÁõÆÊèê‰æõÁöÑÂ∏ÆÂä©

@sun7127@126.com
@ Êâç
@MonsterTan
‰ª•ÂèäÊâÄÊúâ‰∏∫Ê≠§È°πÁõÆÊèê‰æõprÁöÑÂêåÂ≠¶

Êõ¥Êñ∞Êó•Âøó

Êõ¥Êñ∞Êó•Âøó

",GitHub - testerSunshine/12306: 12306Êô∫ËÉΩÂà∑Á•®ÔºåËÆ¢Á•®
33,Python,"



Apache MXNet (incubating) for Deep Learning



Master
Docs
License




             






Apache MXNet (incubating) is a deep learning framework designed for both efficiency and flexibility.
It allows you to mix symbolic and imperative programming
to maximize efficiency and productivity.
At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.
A graph optimization layer on top of that makes symbolic execution fast and memory efficient.
MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.
MXNet is more than a deep learning project. It is a collection of
blue prints and guidelines for building
deep learning systems, and interesting insights of DL systems for hackers.
Ask Questions

Please use discuss.mxnet.io for asking questions.
Please use mxnet/issues for reporting bugs.
Frequent Asked Questions

How to Contribute

Contribute to MXNet

What's New

Version 1.5.1 Release - MXNet 1.5.1 Patch Release.
Version 1.5.0 Release - MXNet 1.5.0 Release.
Version 1.4.1 Release - MXNet 1.4.1 Patch Release.
Version 1.4.0 Release - MXNet 1.4.0 Release.
Version 1.3.1 Release - MXNet 1.3.1 Patch Release.
Version 1.3.0 Release - MXNet 1.3.0 Release.
Version 1.2.0 Release - MXNet 1.2.0 Release.
Version 1.1.0 Release - MXNet 1.1.0 Release.
Version 1.0.0 Release - MXNet 1.0.0 Release.
Version 0.12.1 Release - MXNet 0.12.1 Patch Release.
Version 0.12.0 Release - MXNet 0.12.0 Release.
Version 0.11.0 Release - MXNet 0.11.0 Release.
Apache Incubator - We are now an Apache Incubator project.
Version 0.10.0 Release - MXNet 0.10.0 Release.
Version 0.9.3 Release - First 0.9 official release.
Version 0.9.1 Release (NNVM refactor) - NNVM branch is merged into master now. An official release will be made soon.
Version 0.8.0 Release
Updated Image Classification with new Pre-trained Models
Notebooks How to Use MXNet
MKLDNN for Faster CPU Performance
MXNet Memory Monger, Training Deeper Nets with Sublinear Memory Cost
Tutorial for NVidia GTC 2016
MXNet.js: Javascript Package for Deep Learning in Browser (without server)
Guide to Creating New Operators (Layers)
Go binding for inference
Amalgamation and Go Binding for Predictors - Outdated
Large Scale Image Classification

Contents

Website
Documentation
Blog
Code Examples
Installation
Features
Ecosystem

Features

Design notes providing useful insights that can re-used by other DL projects
Flexible configuration for arbitrary computation graph
Mix and match imperative and symbolic programming to maximize flexibility and efficiency
Lightweight, memory efficient and portable to smart devices
Scales up to multi GPUs and distributed setting with auto parallelism
Support for Python, Scala, C++, Java, Clojure, R, Go, Javascript, Perl, Matlab, and Julia
Cloud-friendly and directly compatible with AWS S3, AWS Deep Learning AMI, AWS SageMaker, HDFS, and Azure

License
Licensed under an Apache-2.0 license.
Reference Paper
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
Bing Xu, Chiyuan Zhang, and Zheng Zhang.
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015
History
MXNet emerged from a collaboration by the authors of cxxnet, minerva, and purine2. The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.
","GitHub - apache/incubator-mxnet: Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more"
34,Python,"Docker Compose

‚ùóÔ∏è The docker-compose project announces that as Python 2 reaches it's EOL, versions 1.25.x will be the last to support it. For more information, please refer to this issue.
Compose is a tool for defining and running multi-container Docker applications.
With Compose, you use a Compose file to configure your application's services.
Then, using a single command, you create and start all the services
from your configuration. To learn more about all the features of Compose
see the list of features.
Compose is great for development, testing, and staging environments, as well as
CI workflows. You can learn more about each case in
Common Use Cases.
Using Compose is basically a three-step process.

Define your app's environment with a Dockerfile so it can be
reproduced anywhere.
Define the services that make up your app in docker-compose.yml so
they can be run together in an isolated environment.
Lastly, run docker-compose up and Compose will start and run your entire app.

A docker-compose.yml looks like this:
version: '2'

services:
  web:
    build: .
    ports:
     - ""5000:5000""
    volumes:
     - .:/code
  redis:
    image: redis

For more information about the Compose file, see the
Compose file reference.
Compose has commands for managing the whole lifecycle of your application:

Start, stop and rebuild services
View the status of running services
Stream the log output of running services
Run a one-off command on a service

Installation and documentation

Full documentation is available on Docker's website.
Code repository for Compose is on GitHub.
If you find any problems please fill out an issue. Thank you!

Contributing

Want to help build Compose? Check out our contributing documentation.
Releasing
Releases are built by maintainers, following an outline of the release process.
",GitHub - docker/compose: Define and run multi-container applications with Docker
35,Python,"







data-science-ipython-notebooks
Index

deep-learning

tensorflow
theano
keras
caffe


scikit-learn
statistical-inference-scipy
pandas
matplotlib
numpy
python-data
kaggle-and-business-analyses
spark
mapreduce-python
amazon web services
command lines
misc
notebook-installation
credits
contributing
contact-info
license





deep-learning
IPython Notebook(s) demonstrating deep learning functionality.




tensor-flow-tutorials
Additional TensorFlow tutorials:

pkmital/tensorflow_tutorials
nlintz/TensorFlow-Tutorials
alrojo/tensorflow-tutorial
BinRoot/TensorFlow-Book
tuanavu/tensorflow-basic-tutorials




Notebook
Description




tsf-basics
Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google.


tsf-linear
Implement linear regression in TensorFlow.


tsf-logistic
Implement logistic regression in TensorFlow.


tsf-nn
Implement nearest neighboars in TensorFlow.


tsf-alex
Implement AlexNet in TensorFlow.


tsf-cnn
Implement convolutional neural networks in TensorFlow.


tsf-mlp
Implement multilayer perceptrons in TensorFlow.


tsf-rnn
Implement recurrent neural networks in TensorFlow.


tsf-gpu
Learn about basic multi-GPU computation in TensorFlow.


tsf-gviz
Learn about graph visualization in TensorFlow.


tsf-lviz
Learn about loss visualization in TensorFlow.



tensor-flow-exercises



Notebook
Description




tsf-not-mnist
Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow.


tsf-fully-connected
Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow.


tsf-regularization
Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow.


tsf-convolutions
Create convolutional neural networks in TensorFlow.


tsf-word2vec
Train a skip-gram model over Text8 data in TensorFlow.


tsf-lstm
Train a LSTM character model over Text8 data in TensorFlow.







theano-tutorials



Notebook
Description




theano-intro
Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.


theano-scan
Learn scans, a mechanism to perform loops in a Theano graph.


theano-logistic
Implement logistic regression in Theano.


theano-rnn
Implement recurrent neural networks in Theano.


theano-mlp
Implement multilayer perceptrons in Theano.







keras-tutorials



Notebook
Description




keras
Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano.


setup
Learn about the tutorial goals and how to set up your Keras environment.


intro-deep-learning-ann
Get an intro to deep learning with Keras and Artificial Neural Networks (ANN).


theano
Learn about Theano by working with weights matrices and gradients.


keras-otto
Learn about Keras by looking at the Kaggle Otto challenge.


ann-mnist
Review a simple implementation of ANN for MNIST using Keras.


conv-nets
Learn about Convolutional Neural Networks (CNNs) with Keras.


conv-net-1
Recognize handwritten digits from MNIST using Keras - Part 1.


conv-net-2
Recognize handwritten digits from MNIST using Keras - Part 2.


keras-models
Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras.


auto-encoders
Learn about Autoencoders with Keras.


rnn-lstm
Learn about Recurrent Neural Networks (RNNs) with Keras.


lstm-sentence-gen
Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras.



deep-learning-misc



Notebook
Description




deep-dream
Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images.







scikit-learn
IPython Notebook(s) demonstrating scikit-learn functionality.



Notebook
Description




intro
Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.


knn
Implement k-nearest neighbors in scikit-learn.


linear-reg
Implement linear regression in scikit-learn.


svm
Implement support vector machine classifiers with and without kernels in scikit-learn.


random-forest
Implement random forest classifiers and regressors in scikit-learn.


k-means
Implement k-means clustering in scikit-learn.


pca
Implement principal component analysis in scikit-learn.


gmm
Implement Gaussian mixture models in scikit-learn.


validation
Implement validation and model selection in scikit-learn.







statistical-inference-scipy
IPython Notebook(s) demonstrating statistical inference with SciPy functionality.



Notebook
Description




scipy
SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.


effect-size
Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States.


sampling
Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data.


hypothesis
Explore hypothesis testing by analyzing the difference of first-born babies compared with others.







pandas
IPython Notebook(s) demonstrating pandas functionality.



Notebook
Description




pandas
Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.


github-data-wrangling
Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the Viz repo.


Introduction-to-Pandas
Introduction to Pandas.


Introducing-Pandas-Objects
Learn about Pandas objects.


Data Indexing and Selection
Learn about data indexing and selection in Pandas.


Operations-in-Pandas
Learn about operating on data in Pandas.


Missing-Values
Learn about handling missing data in Pandas.


Hierarchical-Indexing
Learn about hierarchical indexing in Pandas.


Concat-And-Append
Learn about combining datasets: concat and append in Pandas.


Merge-and-Join
Learn about combining datasets: merge and join in Pandas.


Aggregation-and-Grouping
Learn about aggregation and grouping in Pandas.


Pivot-Tables
Learn about pivot tables in Pandas.


Working-With-Strings
Learn about vectorized string operations in Pandas.


Working-with-Time-Series
Learn about working with time series in pandas.


Performance-Eval-and-Query
Learn about high-performance Pandas: eval() and query() in Pandas.







matplotlib
IPython Notebook(s) demonstrating matplotlib functionality.



Notebook
Description




matplotlib
Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.


matplotlib-applied
Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots.


Introduction-To-Matplotlib
Introduction to Matplotlib.


Simple-Line-Plots
Learn about simple line plots in Matplotlib.


Simple-Scatter-Plots
Learn about simple scatter plots in Matplotlib.


Errorbars.ipynb
Learn about visualizing errors in Matplotlib.


Density-and-Contour-Plots
Learn about density and contour plots in Matplotlib.


Histograms-and-Binnings
Learn about histograms, binnings, and density in Matplotlib.


Customizing-Legends
Learn about customizing plot legends in Matplotlib.


Customizing-Colorbars
Learn about customizing colorbars in Matplotlib.


Multiple-Subplots
Learn about multiple subplots in Matplotlib.


Text-and-Annotation
Learn about text and annotation in Matplotlib.


Customizing-Ticks
Learn about customizing ticks in Matplotlib.


Settings-and-Stylesheets
Learn about customizing Matplotlib: configurations and stylesheets.


Three-Dimensional-Plotting
Learn about three-dimensional plotting in Matplotlib.


Geographic-Data-With-Basemap
Learn about geographic data with basemap in Matplotlib.


Visualization-With-Seaborn
Learn about visualization with Seaborn.







numpy
IPython Notebook(s) demonstrating NumPy functionality.



Notebook
Description




numpy
Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.


Introduction-to-NumPy
Introduction to NumPy.


Understanding-Data-Types
Learn about data types in Python.


The-Basics-Of-NumPy-Arrays
Learn about the basics of NumPy arrays.


Computation-on-arrays-ufuncs
Learn about computations on NumPy arrays: universal functions.


Computation-on-arrays-aggregates
Learn about aggregations: min, max, and everything in between in NumPy.


Computation-on-arrays-broadcasting
Learn about computation on arrays: broadcasting in NumPy.


Boolean-Arrays-and-Masks
Learn about comparisons, masks, and boolean logic in NumPy.


Fancy-Indexing
Learn about fancy indexing in NumPy.


Sorting
Learn about sorting arrays in NumPy.


Structured-Data-NumPy
Learn about structured data: NumPy's structured arrays.







python-data
IPython Notebook(s) demonstrating Python functionality geared towards data analysis.



Notebook
Description




data structures
Learn Python basics with tuples, lists, dicts, sets.


data structure utilities
Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions.


functions
Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools.


datetime
Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta.


logging
Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler.


pdb
Learn how to debug in Python with the interactive source code debugger.


unit tests
Learn how to test in Python with Nose unit tests.







kaggle-and-business-analyses
IPython Notebook(s) used in kaggle competitions and business analyses.



Notebook
Description




titanic
Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning.


churn-analysis
Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.







spark
IPython Notebook(s) demonstrating spark and HDFS functionality.



Notebook
Description




spark
In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.


hdfs
Reliably stores very large files across machines in a large cluster.







mapreduce-python
IPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.



Notebook
Description




mapreduce-python
Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and mrjob config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  Disco is another python-based alternative.







aws
IPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.
Also check out:

SAWS: A Supercharged AWS command line interface (CLI).
Awesome AWS: A curated list of libraries, open source repos, guides, blogs, and other resources.




Notebook
Description




boto
Official AWS SDK for Python.


s3cmd
Interacts with S3 through the command line.


s3distcp
Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.


s3-parallel-put
Uploads multiple files to S3 in parallel.


redshift
Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP).


kinesis
Streams data in real time with the ability to process thousands of data streams per second.


lambda
Runs code in response to events, automatically managing compute resources.







commands
IPython Notebook(s) demonstrating various command lines for Linux, Git, etc.



Notebook
Description




linux
Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.


anaconda
Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment.


ipython notebook
Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.


git
Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows.


ruby
Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages.


jekyll
Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server.


pelican
Python-based alternative to Jekyll.


django
High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include Pyramid, Flask, Tornado, and Bottle.



misc
IPython Notebook(s) demonstrating miscellaneous functionality.



Notebook
Description




regex
Regular expression cheat sheet useful in data wrangling.


algorithmia
Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.



notebook-installation
anaconda
Anaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.
Follow instructions to install Anaconda or the more lightweight miniconda.
dev-setup
For detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the dev-setup repo.
running-notebooks
To view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found here.
$ git clone https://github.com/donnemartin/data-science-ipython-notebooks.git
$ cd data-science-ipython-notebooks
$ jupyter notebook

Notebooks tested with Python 2.7.x.
credits

Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython by Wes McKinney
PyCon 2015 Scikit-learn Tutorial by Jake VanderPlas
Python Data Science Handbook by Jake VanderPlas
Parallel Machine Learning with scikit-learn and IPython by Olivier Grisel
Statistical Interference Using Computational Methods in Python by Allen Downey
TensorFlow Examples by Aymeric Damien
TensorFlow Tutorials by Parag K Mital
TensorFlow Tutorials by Nathan Lintz
TensorFlow Tutorials by Alexander R Johansen
TensorFlow Book by Nishant Shukla
Summer School 2015 by mila-udem
Keras tutorials by Valerio Maggio
Kaggle
Yhat Blog

contributing
Contributions are welcome!  For bug reports or requests please submit an issue.
contact-info
Feel free to contact me to discuss any issues, questions, or comments.

Email: donne.martin@gmail.com
Twitter: @donne_martin
GitHub: donnemartin
LinkedIn: donnemartin
Website: donnemartin.com

license
This repository contains a variety of content; some developed by Donne Martin, and some from third-parties.  The third-party content is distributed under the license provided by those parties.
The content developed by Donne Martin is distributed under the following license:
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2015 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

","GitHub - donnemartin/data-science-ipython-notebooks: Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines."
36,Python,"Algo VPN



Algo VPN is a set of Ansible scripts that simplify the setup of a personal Wireguard and IPSEC VPN. It uses the most secure defaults available, works with common cloud providers, and does not require client software on most devices. See our release announcement for more information.
Features

Supports only IKEv2 with strong crypto (AES-GCM, SHA2, and P-256) and WireGuard
Generates Apple profiles to auto-configure iOS and macOS devices
Includes a helper script to add and remove users
Blocks ads with a local DNS resolver (optional)
Sets up limited SSH users for tunneling traffic (optional)
Based on current versions of Ubuntu and strongSwan
Installs to DigitalOcean, Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, OpenStack, CloudStack, Hetzner Cloud, or your own Ubuntu server

Anti-features

Does not support legacy cipher suites or protocols like L2TP, IKEv1, or RSA
Does not install Tor, OpenVPN, or other risky servers
Does not depend on the security of TLS
Does not require client software on most platforms
Does not claim to provide anonymity or censorship avoidance
Does not claim to protect you from the FSB, MSS, DGSE, or FSM

Deploy the Algo Server
The easiest way to get an Algo server running is to run it on your local system and let it set up a new virtual machine in the cloud for you.


Setup an account on a cloud hosting provider. Algo supports DigitalOcean (most user friendly), Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, DreamCompute or other OpenStack-based cloud hosting, Exoscale or other CloudStack-based cloud hosting,  or Hetzner Cloud.


Get a copy of Algo. The Algo scripts will be installed on your local system. There are two ways to get a copy:


Download the ZIP file. Unzip the file to create a directory named algo-master containing the Algo scripts.


Run the command git clone https://github.com/trailofbits/algo.git to create a directory named algo containing the Algo scripts.




Install Algo's core dependencies. Algo requires that Python 3.6 or later and at least one supporting package are installed on your system.


macOS: Apple does not provide a suitable version of Python 3 with macOS. Here are two ways to obtain one:


Use the Homebrew package manager. After installing Homebrew install Python 3 by running brew install python3.


Download and install the latest stable Python 3.7 package (currently Python 3.8 will not work). Be sure to run the included Install Certificates command from Finder.


See Deploy from macOS for more detailed information on installing Python 3 on macOS.
Once Python 3 is installed on your Mac, from Terminal run:
python3 -m pip install --upgrade virtualenv


Linux: Recent releases of Ubuntu, Debian, and Fedora come with Python 3 already installed. Make sure your system is up-to-date and install the supporting package(s):

Ubuntu and Debian:

sudo apt install -y python3-virtualenv

Fedora:

sudo dnf install -y python3-virtualenv

Red Hat and CentOS 7 and later (for earlier versions see this documentation):

sudo yum -y install epel-release
sudo yum install -y python36-virtualenv


Windows: Use the Windows Subsystem for Linux (WSL) to create your own copy of Ubuntu running under Windows from which to install and run Algo. See the Windows documentation.




Install Algo's remaining dependencies. You'll need to run these commands from the Algo directory each time you download a new copy of Algo. In a Terminal window cd into the algo-master (ZIP file) or algo (git clone) directory and run:
python3 -m virtualenv --python=""$(command -v python3)"" .env &&
  source .env/bin/activate &&
  python3 -m pip install -U pip virtualenv &&
  python3 -m pip install -r requirements.txt
On Fedora add the option --system-site-packages to the first command above. On macOS install the C compiler if prompted.


List the users to create. Open the file config.cfg in your favorite text editor. Specify the users you wish to create in the users list. Create a unique user for each device you plan to connect to your VPN. If you want to be able to add or delete users later, you must select yes at the Do you want to retain the keys (PKI)? prompt during the deployment.


Start the deployment. Return to your terminal. In the Algo directory, run ./algo and follow the instructions. There are several optional features available. None are required for a fully functional VPN server. These optional features are described in greater detail in here.


That's it! You will get the message below when the server deployment process completes. Take note of the p12 (user certificate) password and the CA key in case you need them later, they will only be displayed this time.
You can now set up clients to connect to your VPN. Proceed to Configure the VPN Clients below.
    ""#                          Congratulations!                            #""
    ""#                     Your Algo server is running.                     #""
    ""#    Config files and certificates are in the ./configs/ directory.    #""
    ""#              Go to https://whoer.net/ after connecting               #""
    ""#        and ensure that all your traffic passes through the VPN.      #""
    ""#                     Local DNS resolver 172.16.0.1                    #""
    ""#        The p12 and SSH keys password for new users is XXXXXXXX       #""
    ""#        The CA key password is XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX       #""
    ""#      Shell access: ssh -i configs/algo.pem root@xxx.xxx.xx.xx        #""

Configure the VPN Clients
Certificates and configuration files that users will need are placed in the configs directory. Make sure to secure these files since many contain private keys. All files are saved under a subdirectory named with the IP address of your new Algo VPN server.
Apple Devices
WireGuard is used to provide VPN services on Apple devices. Algo generates a WireGuard configuration file, wireguard/<username>.conf, and a QR code, wireguard/<username>.png, for each user defined in config.cfg.
On iOS, install the WireGuard app from the iOS App Store. Then, use the WireGuard app to scan the QR code or AirDrop the configuration file to the device.
On macOS Mojave or later, install the WireGuard app from the Mac App Store. WireGuard will appear in the menu bar once you run the app. Click on the WireGuard icon, choose Import tunnel(s) from file..., then select the appropriate WireGuard configuration file.
On either iOS or macOS, you can enable ""Connect on Demand"" and/or exclude certain trusted Wi-Fi networks (such as your home or work) by editing the tunnel configuration in the WireGuard app. (Algo can't do this automatically for you.)
Installing WireGuard is a little more complicated on older version of macOS. See Using macOS as a Client with WireGuard.
If you prefer to use the built-in IPSEC VPN on Apple devices, or need ""Connect on Demand"" or excluded Wi-Fi networks automatically configured, then see Using Apple Devices as a Client with IPSEC.
Android Devices
WireGuard is used to provide VPN services on Android. Install the WireGuard VPN Client. Import the corresponding wireguard/<name>.conf file to your device, then setup a new connection with it. See the Android setup instructions for more detailed walkthrough.
Windows
WireGuard is used to provide VPN services on Windows. Algo generates a WireGuard configuration file, wireguard/<username>.conf, for each user defined in config.cfg.
Install the WireGuard VPN Client. Import the generated wireguard/<username>.conf file to your device, then setup a new connection with it.
Linux WireGuard Clients
WireGuard works great with Linux clients. See this page for an example of how to configure WireGuard on Ubuntu.
Linux strongSwan IPsec Clients (e.g., OpenWRT, Ubuntu Server, etc.)
Please see this page.
Other Devices
Depending on the platform, you may need one or multiple of the following files.

ipsec/manual/cacert.pem: CA Certificate
ipsec/manual/.p12: User Certificate and Private Key (in PKCS#12 format)
ipsec/manual/.conf: strongSwan client configuration
ipsec/manual/.secrets: strongSwan client configuration
ipsec/apple/.mobileconfig: Apple Profile
wireguard/.conf: WireGuard configuration profile
wireguard/.png: WireGuard configuration QR code

Setup an SSH Tunnel
If you turned on the optional SSH tunneling role, then local user accounts will be created for each user in config.cfg and SSH authorized_key files for them will be in the configs directory (user.ssh.pem). SSH user accounts do not have shell access, cannot authenticate with a password, and only have limited tunneling options (e.g., ssh -N is required). This ensures that SSH users have the least access required to setup a tunnel and can perform no other actions on the Algo server.
Use the example command below to start an SSH tunnel by replacing user and ip with your own. Once the tunnel is setup, you can configure a browser or other application to use 127.0.0.1:1080 as a SOCKS proxy to route traffic through the Algo server.
ssh -D 127.0.0.1:1080 -f -q -C -N user@ip -i configs/<server_ip>/ssh-tunnel/<user>.pem
SSH into Algo Server
Your Algo server is configured for key-only SSH access for administrative purposes. Open the Terminal app, cd into the algo-master directory where you originally downloaded Algo, and then use the command listed on the success message:
ssh -i configs/algo.pem user@ip
where user is either root or ubuntu as listed on the success message, and ip is the IP address of your Algo server. If you find yourself regularly logging into the server then it will be useful to load your Algo ssh key automatically. Add the following snippet to the bottom of ~/.bash_profile to add it to your shell environment permanently.
ssh-add ~/.ssh/algo > /dev/null 2>&1
Adding or Removing Users
If you chose to save the CA key during the deploy process, then Algo's own scripts can easily add and remove users from the VPN server.

Update the users list in your config.cfg
Open a terminal, cd to the algo directory, and activate the virtual environment with source .env/bin/activate
Run the command: ./algo update-users

After this process completes, the Algo VPN server will contain only the users listed in the config.cfg file.
Additional Documentation

Deployment instructions, cloud provider setup instructions, and further client setup instructions available here.
FAQ
Troubleshooting

If you read all the documentation and have further questions, join the chat on Gitter.
Endorsements

I've been ranting about the sorry state of VPN svcs for so long, probably about
time to give a proper talk on the subject. TL;DR: use Algo.

-- Kenn White

Before picking a VPN provider/app, make sure you do some research
https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf ... ‚Äì or consider Algo

-- The Register

Algo is really easy and secure.

-- the grugq

I played around with Algo VPN, a set of scripts that let you set up a VPN in the cloud in very little time, even if you don‚Äôt know much about development. I‚Äôve got to say that I was quite impressed with Trail of Bits‚Äô approach.

-- Romain Dillet for TechCrunch

If you‚Äôre uncomfortable shelling out the cash to an anonymous, random VPN provider, this is the best solution.

-- Thorin Klosowski for Lifehacker
Support Algo VPN




All donations support continued development. Thanks!

We accept donations via PayPal, Patreon, and Flattr.
Use our referral code when you sign up to Digital Ocean for a $10 credit.
We also accept and appreciate contributions of new code and bugfixes via Github Pull Requests.

Algo is licensed and distributed under the AGPLv3. If you want to distribute a closed-source modification or service based on Algo, then please consider purchasing an exception . As with the methods above, this will help support continued development.
",GitHub - trailofbits/algo: Set up a personal VPN in the cloud
37,Python,"



Odoo
Odoo is a suite of web based open source business apps.
The main Odoo Apps include an Open Source CRM,
Website Builder,
eCommerce,
Warehouse Management,
Project Management,
Billing & Accounting,
Point of Sale,
Human Resources,
Marketing,
Manufacturing,
...
Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured Open Source ERP when you install several Apps.
Getting started with Odoo
For a standard installation please follow the Setup instructions
from the documentation.
To learn the software, we recommend the Odoo eLearning, or Scale-up, the business game. Developers can start with the developer tutorials
",GitHub - odoo/odoo: Odoo. Open Source Apps To Grow Your Business.
38,Python,"Python Fire  
Python Fire is a library for automatically generating command line interfaces
(CLIs) from absolutely any Python object.

Python Fire is a simple way to create a CLI in Python. [1]
Python Fire is a helpful tool for developing and debugging Python code. [2]
Python Fire helps with exploring existing code or turning other people's code
into a CLI. [3]
Python Fire makes transitioning between Bash and Python easier. [4]
Python Fire makes using a Python REPL easier by setting up the REPL with the
modules and variables you'll need already imported and created. [5]

Installation
To install Python Fire with pip, run: pip install fire
To install Python Fire with conda, run: conda install fire -c conda-forge
To install Python Fire from source, first clone the repository and then run:
python setup.py install
Basic Usage
You can call Fire on any Python object:
functions, classes, modules, objects, dictionaries, lists, tuples, etc.
They all work!
Here's an example of calling Fire on a function.
import fire

def hello(name=""World""):
  return ""Hello %s!"" % name

if __name__ == '__main__':
  fire.Fire(hello)
Then, from the command line, you can run:
python hello.py  # Hello World!
python hello.py --name=David  # Hello David!
python hello.py --help  # Shows usage information.
Here's an example of calling Fire on a class.
import fire

class Calculator(object):
  """"""A simple calculator class.""""""

  def double(self, number):
    return 2 * number

if __name__ == '__main__':
  fire.Fire(Calculator)
Then, from the command line, you can run:
python calculator.py double 10  # 20
python calculator.py double --number=15  # 30
To learn how Fire behaves on functions, objects, dicts, lists, etc, and to learn
about Fire's other features, see the Using a Fire CLI page.
For additional examples, see The Python Fire Guide.
Why is it called Fire?
When you call Fire, it fires off (executes) your command.
Where can I learn more?
Please see The Python Fire Guide.
Reference



Setup
Command
Notes




install
pip install fire







Creating a CLI
Command
Notes




import
import fire



Call
fire.Fire()
Turns the current module into a Fire CLI.


Call
fire.Fire(component)
Turns component into a Fire CLI.






Using a CLI
Command
Notes




Help
command --help or command -- --help



REPL
command -- --interactive
Enters interactive mode.


Separator
command -- --separator=X
Sets the separator to X. The default separator is -.


Completion
command -- --completion [shell]
Generates a completion script for the CLI.


Trace
command -- --trace
Gets a Fire trace for the command.


Verbose
command -- --verbose




Note that these flags are separated from the Fire command by an isolated --.
License
Licensed under the
Apache 2.0 License.
Disclaimer
This is not an official Google product.
",GitHub - google/python-fire: Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python object.
39,Python,"My Python Examples
I do not consider myself a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me at craig@geekcomputers.co.uk.
These scripts contain important functions which help reduce human workload.
Code documentation is aligned correctly when the files are viewed in Notepad++.


batch_file_rename.py - This batch renames a group of files in a given directory, once you pass the current and the new extensions.


create_dir_if_not_there.py - Checks to see if a directory exists in the users home directory. If a directory does not exist, then one will be created.


Fast Youtube Downloader - Downloads YouTube videos quickly with parallel threads using aria2c.


Google Image Downloader - Query a given term and retrieve images from the Google Image database.


dir_test.py - Tests to see if the directory testdir exists, if not it will create the directory for you.


env_check.py - This script will check to see if all of the environment variables required are set.


blackjack.py - This script contains the Casino BlackJack-21 Game in Python.


fileinfo.py - Shows file information for a given file.


folder_size.py - Scans the current directory and all subdirectories and displays the size.


logs.py - This script will search for all *.log files in the given directory, zip them using the program you specify, and then date stamp them.


move_files_over_x_days.py - Moves all files over a specified age (in days) from the source directory to the destination directory.


nslookup_check.py - This simple script opens the file server_list.txt and then does an nslookup for each one to check the DNS entry.


osinfo.py - Displays some information about the OS on which you are running this script.


ping_servers.py - This script, depending on the arguments supplied, will ping the servers associated with that application group.


ping_subnet.py - After supplying the first 3 octets this file scans the final range for available addresses.


powerdown_startup.py - This file goes through the server list and pings the machine, if it is up it will load the putty session, if it is not it will notify you.


puttylogs.py -  This file zips up all the logs in the given directory.


script_count.py - This file scans the scripts directory and gives a count of the different types of scripts.


[get_youtube_view.py] - This is a simple python script used to get more views on your youtube videos. This script may also be used to repeat songs on Youtube.


script_listing.py - This file will list all the files in the given directory, and go through all the subdirectories as well.


testlines.py - This simple script opens a file and prints out 100 lines of whatever is the set for the line variable.


tweeter.py - Allows you to tweet text or a picture from the terminal.


serial_scanner.py contains a method called ListAvailablePorts which returns a list with the names of the serial ports that are in use in the computer. This method works only on Linux and Windows (can be extended for mac osx). If no port is found, an empty list is returned.


get_youtube_view.py - A simple python script to get more views for your YouTube videos. Useful for repeating songs on YouTube.


CountMillionCharacter.py And CountMillionCharacter2.0.py - Gets character count of a text file.


xkcd_downloader.py - Downloads the latest XKCD comic and places them in a new folder called ""comics"".


timymodule.py - A great alternative to Pythons 'timeit' module and easier to use.


calculator.py - Uses Python's eval() function to implement a calculator.


Google_News.py - Uses BeautifulSoup to provide Latest news headline along with news link.


cricket_live_score - Uses BeautifulSoup to provide live cricket score.


youtube.py - Takes a song name as input and fetches the YouTube URL of the best matching song and plays it.


site_health.py - Checks the health of a remote server


SimpleStopWatch.py - Simple Stop Watch implementation using Python's time module.


Changemac.py - This script change your MAC address , generate random MAC address or enter input as new MAC address in your linux(Successfully Tested in Ubuntu 18.04).


whatsapp-monitor.py - Uses Selenium to give online status about your contacts when your contacts become online in whatsapp you will get an update about it on terminal.


whatsapp-chat-analyzer.py - This is whatsapp group/individual chat analyzer .
This script is able to analyse all activity happened in whatsapp group and visualize all thing through matplotlib library(In Graph form).


JARVIS.py - Control windows programs with your voice.


",GitHub - geekcomputers/Python: My Python Examples
40,Python,"itchat
   English version
itchatÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂæÆ‰ø°‰∏™‰∫∫Âè∑Êé•Âè£Ôºå‰ΩøÁî®pythonË∞ÉÁî®ÂæÆ‰ø°‰ªéÊú™Â¶ÇÊ≠§ÁÆÄÂçï„ÄÇ
‰ΩøÁî®‰∏çÂà∞‰∏âÂçÅË°åÁöÑ‰ª£Á†ÅÔºå‰Ω†Â∞±ÂèØ‰ª•ÂÆåÊàê‰∏Ä‰∏™ËÉΩÂ§üÂ§ÑÁêÜÊâÄÊúâ‰ø°ÊÅØÁöÑÂæÆ‰ø°Êú∫Âô®‰∫∫„ÄÇ
ÂΩìÁÑ∂ÔºåËØ•apiÁöÑ‰ΩøÁî®Ëøú‰∏çÊ≠¢‰∏Ä‰∏™Êú∫Âô®‰∫∫ÔºåÊõ¥Â§öÁöÑÂäüËÉΩÁ≠âÁùÄ‰Ω†Êù•ÂèëÁé∞ÔºåÊØîÂ¶ÇËøô‰∫õ„ÄÇ
ËØ•Êé•Âè£‰∏éÂÖ¨‰ºóÂè∑Êé•Âè£itchatmpÂÖ±‰∫´Á±ª‰ººÁöÑÊìç‰ΩúÊñπÂºèÔºåÂ≠¶‰π†‰∏ÄÊ¨°ÊéåÊè°‰∏§‰∏™Â∑•ÂÖ∑„ÄÇ
Â¶Ç‰ªäÂæÆ‰ø°Â∑≤ÁªèÊàê‰∏∫‰∫Ü‰∏™‰∫∫Á§æ‰∫§ÁöÑÂæàÂ§ß‰∏ÄÈÉ®ÂàÜÔºåÂ∏åÊúõËøô‰∏™È°πÁõÆËÉΩÂ§üÂ∏ÆÂä©‰Ω†Êâ©Â±ï‰Ω†ÁöÑ‰∏™‰∫∫ÁöÑÂæÆ‰ø°Âè∑„ÄÅÊñπ‰æøËá™Â∑±ÁöÑÁîüÊ¥ª„ÄÇ
ÂÆâË£Ö
ÂèØ‰ª•ÈÄöËøáÊú¨ÂëΩ‰ª§ÂÆâË£ÖitchatÔºö
pip install itchat
ÁÆÄÂçïÂÖ•Èó®ÂÆû‰æã
Êúâ‰∫ÜitchatÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÁªôÊñá‰ª∂‰º†ËæìÂä©ÊâãÂèë‰∏ÄÊù°‰ø°ÊÅØÔºåÂè™ÈúÄË¶ÅËøôÊ†∑Ôºö
import itchat

itchat.auto_login()

itchat.send('Hello, filehelper', toUserName='filehelper')
Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÂõûÂ§çÂèëÁªôËá™Â∑±ÁöÑÊñáÊú¨Ê∂àÊÅØÔºåÂè™ÈúÄË¶ÅËøôÊ†∑Ôºö
import itchat

@itchat.msg_register(itchat.content.TEXT)
def text_reply(msg):
    return msg.text

itchat.auto_login()
itchat.run()
‰∏Ä‰∫õËøõÈò∂Â∫îÁî®ÂèØ‰ª•Âú®‰∏ãÈù¢ÁöÑÂºÄÊ∫êÊú∫Âô®‰∫∫ÁöÑÊ∫êÁ†ÅÂíåËøõÈò∂Â∫îÁî®‰∏≠ÁúãÂà∞ÔºåÊàñËÄÖ‰Ω†‰πüÂèØ‰ª•ÈòÖËßàÊñáÊ°£„ÄÇ
ËØï‰∏ÄËØï
ËøôÊòØ‰∏Ä‰∏™Âü∫‰∫éËøô‰∏ÄÈ°πÁõÆÁöÑÂºÄÊ∫êÂ∞èÊú∫Âô®‰∫∫ÔºåÁôæÈóª‰∏çÂ¶Ç‰∏ÄËßÅÔºåÊúâÂÖ¥Ë∂£ÂèØ‰ª•Â∞ùËØï‰∏Ä‰∏ã„ÄÇ
Áî±‰∫éÂ•ΩÂèãÊï∞ÈáèÂÆûÂú®Â¢ûÈïøËøáÂø´ÔºåËá™Âä®ÈÄöËøáÂ•ΩÂèãÈ™åËØÅÁöÑÂäüËÉΩÊºîÁ§∫ÊöÇÊó∂ÂÖ≥Èó≠„ÄÇ

Êà™Â±è
 
ËøõÈò∂Â∫îÁî®
ÁâπÊÆäÁöÑÂ≠óÂÖ∏‰ΩøÁî®ÊñπÂºè
ÈÄöËøáÊâìÂç∞itchatÁöÑÁî®Êà∑‰ª•ÂèäÊ≥®ÂÜåÊ∂àÊÅØÁöÑÂèÇÊï∞ÔºåÂèØ‰ª•ÂèëÁé∞Ëøô‰∫õÂÄºÈÉΩÊòØÂ≠óÂÖ∏„ÄÇ
‰ΩÜÂÆûÈôÖ‰∏äitchatÁ≤æÂøÉÊûÑÈÄ†‰∫ÜÁõ∏Â∫îÁöÑÊ∂àÊÅØ„ÄÅÁî®Êà∑„ÄÅÁæ§ËÅä„ÄÅÂÖ¨‰ºóÂè∑Á±ª„ÄÇ
ÂÖ∂ÊâÄÊúâÁöÑÈîÆÂÄºÈÉΩÂèØ‰ª•ÈÄöËøáËøô‰∏ÄÊñπÂºèËÆøÈóÆÔºö
@itchat.msg_register(TEXT)
def _(msg):
    # equals to print(msg['FromUserName'])
    print(msg.fromUserName)
Â±ûÊÄßÂêç‰∏∫ÈîÆÂÄºÈ¶ñÂ≠óÊØçÂ∞èÂÜôÂêéÁöÑÂÜÖÂÆπ„ÄÇ
author = itchat.search_friends(nickName='LittleCoder')[0]
author.send('greeting, littlecoder!')
ÂêÑÁ±ªÂûãÊ∂àÊÅØÁöÑÊ≥®ÂÜå
ÈÄöËøáÂ¶Ç‰∏ã‰ª£Á†ÅÔºåÂæÆ‰ø°Â∑≤ÁªèÂèØ‰ª•Â∞±Êó•Â∏∏ÁöÑÂêÑÁßç‰ø°ÊÅØËøõË°åËé∑Âèñ‰∏éÂõûÂ§ç„ÄÇ
import itchat, time
from itchat.content import *

@itchat.msg_register([TEXT, MAP, CARD, NOTE, SHARING])
def text_reply(msg):
    msg.user.send('%s: %s' % (msg.type, msg.text))

@itchat.msg_register([PICTURE, RECORDING, ATTACHMENT, VIDEO])
def download_files(msg):
    msg.download(msg.fileName)
    typeSymbol = {
        PICTURE: 'img',
        VIDEO: 'vid', }.get(msg.type, 'fil')
    return '@%s@%s' % (typeSymbol, msg.fileName)

@itchat.msg_register(FRIENDS)
def add_friend(msg):
    msg.user.verify()
    msg.user.send('Nice to meet you!')

@itchat.msg_register(TEXT, isGroupChat=True)
def text_reply(msg):
    if msg.isAt:
        msg.user.send(u'@%s\u2005I received: %s' % (
            msg.actualNickName, msg.text))

itchat.auto_login(True)
itchat.run(True)
ÂëΩ‰ª§Ë°å‰∫åÁª¥Á†Å
ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§ÂèØ‰ª•Âú®ÁôªÈôÜÁöÑÊó∂ÂÄô‰ΩøÁî®ÂëΩ‰ª§Ë°åÊòæÁ§∫‰∫åÁª¥Á†ÅÔºö
itchat.auto_login(enableCmdQR=True)
ÈÉ®ÂàÜÁ≥ªÁªüÂèØËÉΩÂ≠óÂπÖÂÆΩÂ∫¶ÊúâÂá∫ÂÖ•ÔºåÂèØ‰ª•ÈÄöËøáÂ∞ÜenableCmdQRËµãÂÄº‰∏∫ÁâπÂÆöÁöÑÂÄçÊï∞ËøõË°åË∞ÉÊï¥Ôºö
# Â¶ÇÈÉ®ÂàÜÁöÑlinuxÁ≥ªÁªüÔºåÂùóÂ≠óÁ¨¶ÁöÑÂÆΩÂ∫¶‰∏∫‰∏Ä‰∏™Â≠óÁ¨¶ÔºàÊ≠£Â∏∏Â∫î‰∏∫‰∏§Â≠óÁ¨¶ÔºâÔºåÊïÖËµãÂÄº‰∏∫2
itchat.auto_login(enableCmdQR=2)
ÈªòËÆ§ÊéßÂà∂Âè∞ËÉåÊôØËâ≤‰∏∫ÊöóËâ≤ÔºàÈªëËâ≤ÔºâÔºåËã•ËÉåÊôØËâ≤‰∏∫ÊµÖËâ≤ÔºàÁôΩËâ≤ÔºâÔºåÂèØ‰ª•Â∞ÜenableCmdQRËµãÂÄº‰∏∫Ë¥üÂÄºÔºö
itchat.auto_login(enableCmdQR=-1)
ÈÄÄÂá∫Á®ãÂ∫èÂêéÊöÇÂ≠òÁôªÈôÜÁä∂ÊÄÅ
ÈÄöËøáÂ¶Ç‰∏ãÂëΩ‰ª§ÁôªÈôÜÔºåÂç≥‰ΩøÁ®ãÂ∫èÂÖ≥Èó≠Ôºå‰∏ÄÂÆöÊó∂Èó¥ÂÜÖÈáçÊñ∞ÂºÄÂêØ‰πüÂèØ‰ª•‰∏çÁî®ÈáçÊñ∞Êâ´Á†Å„ÄÇ
itchat.auto_login(hotReload=True)
Áî®Êà∑ÊêúÁ¥¢
‰ΩøÁî®search_friendsÊñπÊ≥ïÂèØ‰ª•ÊêúÁ¥¢Áî®Êà∑ÔºåÊúâÂõõÁßçÊêúÁ¥¢ÊñπÂºèÔºö

‰ªÖËé∑ÂèñËá™Â∑±ÁöÑÁî®Êà∑‰ø°ÊÅØ
Ëé∑ÂèñÁâπÂÆöUserNameÁöÑÁî®Êà∑‰ø°ÊÅØ
Ëé∑ÂèñÂ§áÊ≥®„ÄÅÂæÆ‰ø°Âè∑„ÄÅÊòµÁß∞‰∏≠ÁöÑ‰ªª‰Ωï‰∏ÄÈ°πÁ≠â‰∫énameÈîÆÂÄºÁöÑÁî®Êà∑
Ëé∑ÂèñÂ§áÊ≥®„ÄÅÂæÆ‰ø°Âè∑„ÄÅÊòµÁß∞ÂàÜÂà´Á≠â‰∫éÁõ∏Â∫îÈîÆÂÄºÁöÑÁî®Êà∑

ÂÖ∂‰∏≠‰∏â„ÄÅÂõõÈ°πÂèØ‰ª•‰∏ÄÂêå‰ΩøÁî®Ôºå‰∏ãÈù¢ÊòØÁ§∫‰æãÁ®ãÂ∫èÔºö
# Ëé∑ÂèñËá™Â∑±ÁöÑÁî®Êà∑‰ø°ÊÅØÔºåËøîÂõûËá™Â∑±ÁöÑÂ±ûÊÄßÂ≠óÂÖ∏
itchat.search_friends()
# Ëé∑ÂèñÁâπÂÆöUserNameÁöÑÁî®Êà∑‰ø°ÊÅØ
itchat.search_friends(userName='@abcdefg1234567')
# Ëé∑Âèñ‰ªª‰Ωï‰∏ÄÈ°πÁ≠â‰∫énameÈîÆÂÄºÁöÑÁî®Êà∑
itchat.search_friends(name='littlecodersh')
# Ëé∑ÂèñÂàÜÂà´ÂØπÂ∫îÁõ∏Â∫îÈîÆÂÄºÁöÑÁî®Êà∑
itchat.search_friends(wechatAccount='littlecodersh')
# ‰∏â„ÄÅÂõõÈ°πÂäüËÉΩÂèØ‰ª•‰∏ÄÂêå‰ΩøÁî®
itchat.search_friends(name='LittleCoderÊú∫Âô®‰∫∫', wechatAccount='littlecodersh')
ÂÖ≥‰∫éÂÖ¨‰ºóÂè∑„ÄÅÁæ§ËÅäÁöÑËé∑Âèñ‰∏éÊêúÁ¥¢Âú®ÊñáÊ°£‰∏≠ÊúâÊõ¥Âä†ËØ¶ÁªÜÁöÑ‰ªãÁªç„ÄÇ
ÈôÑ‰ª∂ÁöÑ‰∏ãËΩΩ‰∏éÂèëÈÄÅ
itchatÁöÑÈôÑ‰ª∂‰∏ãËΩΩÊñπÊ≥ïÂ≠òÂÇ®Âú®msgÁöÑTextÈîÆ‰∏≠„ÄÇ
ÂèëÈÄÅÁöÑÊñá‰ª∂ÁöÑÊñá‰ª∂ÂêçÔºàÂõæÁâáÁªôÂá∫ÁöÑÈªòËÆ§Êñá‰ª∂ÂêçÔºâÈÉΩÂ≠òÂÇ®Âú®msgÁöÑFileNameÈîÆ‰∏≠„ÄÇ
‰∏ãËΩΩÊñπÊ≥ïÊé•Âèó‰∏Ä‰∏™ÂèØÁî®ÁöÑ‰ΩçÁΩÆÂèÇÊï∞ÔºàÂåÖÊã¨Êñá‰ª∂ÂêçÔºâÔºåÂπ∂Â∞ÜÊñá‰ª∂Áõ∏Â∫îÁöÑÂ≠òÂÇ®„ÄÇ
@itchat.msg_register([PICTURE, RECORDING, ATTACHMENT, VIDEO])
def download_files(msg):
    msg.download(msg.fileName)
    itchat.send('@%s@%s' % (
        'img' if msg['Type'] == 'Picture' else 'fil', msg['FileName']),
        msg['FromUserName'])
    return '%s received' % msg['Type']
Â¶ÇÊûú‰Ω†‰∏çÈúÄË¶Å‰∏ãËΩΩÂà∞Êú¨Âú∞Ôºå‰ªÖÊÉ≥Ë¶ÅËØªÂèñ‰∫åËøõÂà∂‰∏≤ËøõË°åËøõ‰∏ÄÊ≠•Â§ÑÁêÜÂèØ‰ª•‰∏ç‰º†ÂÖ•ÂèÇÊï∞ÔºåÊñπÊ≥ïÂ∞Ü‰ºöËøîÂõûÂõæÁâáÁöÑ‰∫åËøõÂà∂‰∏≤„ÄÇ
@itchat.msg_register([PICTURE, RECORDING, ATTACHMENT, VIDEO])
def download_files(msg):
    with open(msg.fileName, 'wb') as f:
        f.write(msg.download())
Áî®Êà∑Â§öÂºÄ
‰ΩøÁî®Â¶Ç‰∏ãÂëΩ‰ª§ÂèØ‰ª•ÂÆåÊàêÂ§öÂºÄÁöÑÊìç‰ΩúÔºö
import itchat

newInstance = itchat.new_instance()
newInstance.auto_login(hotReload=True, statusStorageDir='newInstance.pkl')

@newInstance.msg_register(itchat.content.TEXT)
def reply(msg):
    return msg.text

newInstance.run()
ÈÄÄÂá∫ÂèäÁôªÈôÜÂÆåÊàêÂêéË∞ÉÁî®ÁâπÂÆöÊñπÊ≥ï
ÁôªÈôÜÂÆåÊàêÂêéÁöÑÊñπÊ≥ïÈúÄË¶ÅËµãÂÄºÂú®loginCallback‰∏≠„ÄÇ
ËÄåÈÄÄÂá∫ÂêéÁöÑÊñπÊ≥ïÈúÄË¶ÅËµãÂÄºÂú®exitCallback‰∏≠„ÄÇ
import time

import itchat

def lc():
    print('finish login')
def ec():
    print('exit')

itchat.auto_login(loginCallback=lc, exitCallback=ec)
time.sleep(3)
itchat.logout()
Ëã•‰∏çËÆæÁΩÆloginCallbackÁöÑÂÄºÔºåÂàôÂ∞Ü‰ºöËá™Âä®Âà†Èô§‰∫åÁª¥Á†ÅÂõæÁâáÂπ∂Ê∏ÖÁ©∫ÂëΩ‰ª§Ë°åÊòæÁ§∫„ÄÇ
Â∏∏ËßÅÈóÆÈ¢ò‰∏éËß£Á≠î
Q: Â¶Ç‰ΩïÈÄöËøáËøô‰∏™ÂåÖÂ∞ÜËá™Â∑±ÁöÑÂæÆ‰ø°Âè∑Âèò‰∏∫ÊéßÂà∂Âô®Ôºü
A: Êúâ‰∏§ÁßçÊñπÂºèÔºöÂèëÈÄÅ„ÄÅÊé•ÂèóËá™Â∑±UserNameÁöÑÊ∂àÊÅØÔºõÂèëÈÄÅÊé•Êî∂Êñá‰ª∂‰º†ËæìÂä©ÊâãÔºàfilehelperÔºâÁöÑÊ∂àÊÅØ
Q: ‰∏∫‰ªÄ‰πàÊàëÂèëÈÄÅ‰ø°ÊÅØÁöÑÊó∂ÂÄôÈÉ®ÂàÜ‰ø°ÊÅØÊ≤°ÊúâÊàêÂäüÂèëÂá∫Êù•Ôºü
A: Êúâ‰∫õË¥¶Âè∑ÊòØÂ§©ÁîüÊó†Ê≥ïÁªôËá™Â∑±ÁöÑË¥¶Âè∑ÂèëÈÄÅ‰ø°ÊÅØÁöÑÔºåÂª∫ËÆÆ‰ΩøÁî®filehelper‰ª£Êõø„ÄÇ
‰ΩúËÄÖ
LittleCoder: ÊûÑÊû∂ÂèäÁª¥Êä§Python2 Python3ÁâàÊú¨„ÄÇ
tempdban: ÂçèËÆÆ„ÄÅÊûÑÊû∂ÂèäÊó•Â∏∏Áª¥Êä§„ÄÇ
Chyroc: ÂÆåÊàêÁ¨¨‰∏ÄÁâàÊú¨ÁöÑPython3ÊûÑÊû∂„ÄÇ
Á±ª‰ººÈ°πÁõÆ
youfou/wxpy: ‰ºòÁßÄÁöÑapiÂåÖË£ÖÂíåÈÖçÂ•óÊèí‰ª∂ÔºåÂæÆ‰ø°Êú∫Âô®‰∫∫/‰ºòÈõÖÁöÑÂæÆ‰ø°‰∏™‰∫∫Âè∑API
liuwons/wxBot: Á±ª‰ººÁöÑÂü∫‰∫éPythonÁöÑÂæÆ‰ø°Êú∫Âô®‰∫∫
zixia/wechaty: Âü∫‰∫éJavascript(ES6)ÁöÑÂæÆ‰ø°‰∏™‰∫∫Ë¥¶Âè∑Êú∫Âô®‰∫∫NodeJSÊ°ÜÊû∂/Â∫ì
sjdy521/Mojo-Weixin: ‰ΩøÁî®PerlËØ≠Ë®ÄÁºñÂÜôÁöÑÂæÆ‰ø°ÂÆ¢Êà∑Á´ØÊ°ÜÊû∂ÔºåÂèØÈÄöËøáÊèí‰ª∂Êèê‰æõÂü∫‰∫éHTTPÂçèËÆÆÁöÑapiÊé•Âè£‰æõÂÖ∂‰ªñËØ≠Ë®ÄË∞ÉÁî®
HanSon/vbot: Âü∫‰∫éPHP7ÁöÑÂæÆ‰ø°‰∏™‰∫∫Âè∑Êú∫Âô®‰∫∫ÔºåÈÄöËøáÂÆûÁé∞ÂåøÂêçÂáΩÊï∞ÂèØ‰ª•Êñπ‰æøÂú∞ÂÆûÁé∞ÂêÑÁßçËá™ÂÆö‰πâÁöÑÂäüËÉΩ
yaphone/itchat4j: Áî®JavaÊâ©Â±ï‰∏™‰∫∫ÂæÆ‰ø°Âè∑ÁöÑËÉΩÂäõ
kanjielu/jeeves: ‰ΩøÁî®springbootÂºÄÂèëÁöÑÂæÆ‰ø°Êú∫Âô®‰∫∫
ÈóÆÈ¢òÂíåÂª∫ËÆÆ
Â¶ÇÊûúÊúâ‰ªÄ‰πàÈóÆÈ¢òÊàñËÄÖÂª∫ËÆÆÈÉΩÂèØ‰ª•Âú®Ëøô‰∏™IssueÂíåÊàëËÆ®ËÆ∫
ÊàñËÄÖ‰πüÂèØ‰ª•Âú®gitter‰∏ä‰∫§ÊµÅÔºö
ÂΩìÁÑ∂‰πüÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨Êñ∞Âª∫ÁöÑQQÁæ§ËÆ®ËÆ∫Ôºö549762872, 205872856
",GitHub - littlecodersh/ItChat: A complete and graceful API for Wechat. ÂæÆ‰ø°‰∏™‰∫∫Âè∑Êé•Âè£„ÄÅÂæÆ‰ø°Êú∫Âô®‰∫∫ÂèäÂëΩ‰ª§Ë°åÂæÆ‰ø°Ôºå‰∏âÂçÅË°åÂç≥ÂèØËá™ÂÆö‰πâ‰∏™‰∫∫Âè∑Êú∫Âô®‰∫∫„ÄÇ
41,Python,"Status: Maintenance (expect bug fixes and minor updates)

OpenAI Gym
OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms. This is the gym open-source library, which gives you access to a standardized set of environments.

See What's New section below
gym makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. You can use it from Python code, and soon from other languages.
If you're not sure where to start, we recommend beginning with the
docs on our site. See also the FAQ.
A whitepaper for OpenAI Gym is available at http://arxiv.org/abs/1606.01540, and here's a BibTeX entry that you can use to cite it in a publication:
@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


Contents of this document

OpenAI Gym
Basics
Installation
Environments
Examples
Testing
What's new





Basics
There are two basic concepts in reinforcement learning: the
environment (namely, the outside world) and the agent (namely, the
algorithm you are writing). The agent sends actions to the
environment, and the environment replies with observations and
rewards (that is, a score).
The core gym interface is Env, which is
the unified environment interface. There is no interface for agents;
that part is left to you. The following are the Env methods you
should know:

reset(self): Reset the environment's state. Returns observation.
step(self, action): Step the environment by one timestep. Returns observation, reward, done, info.
render(self, mode='human'): Render one frame of the environment. The default mode will do something human friendly, such as pop up a window.


Supported systems
We currently support Linux and OS X running Python 2.7 or 3.5 -- 3.7.
Windows support is experimental - algorithmic, toy_text, classic_control and atari should work on Windows (see next section for installation instructions); nevertheless, proceed at your own risk.

Installation
You can perform a minimal install of gym with:
git clone https://github.com/openai/gym.git
cd gym
pip install -e .
If you prefer, you can do a minimal install of the packaged version directly from PyPI:
pip install gym
You'll be able to run a few environments right away:

algorithmic
toy_text
classic_control (you'll need pyglet to render though)

We recommend playing with those environments at first, and then later
installing the dependencies for the remaining environments.

Installing everything
To install the full set of environments, you'll need to have some system
packages installed. We'll build out the list here over time; please let us know
what you end up installing on your platform. Also, take a look at the docker files (py.Dockerfile) to
see the composition of our CI-tested images.
On Ubuntu 16.04 and 18.04:
MuJoCo has a proprietary dependency we can't set up for you. Follow
the
instructions
in the mujoco-py package for help.  As an alternative to mujoco-py, consider PyBullet which uses the open source Bullet physics engine and has no license requirement.
Once you're ready to install everything, run pip install -e '.[all]' (or pip install 'gym[all]').

Pip version
To run pip install -e '.[all]', you'll need a semi-recent pip.
Please make sure your pip is at least at version 1.5.0. You can
upgrade using the following: pip install --ignore-installed
pip. Alternatively, you can open setup.py and
install the dependencies by hand.

Rendering on a server
If you're trying to render video on a server, you'll need to connect a
fake display. The easiest way to do this is by running under
xvfb-run (on Ubuntu, install the xvfb package):
xvfb-run -s ""-screen 0 1400x900x24"" bash

Installing dependencies for specific environments
If you'd like to install the dependencies for only specific
environments, see setup.py. We
maintain the lists of dependencies on a per-environment group basis.

Environments
See List of Environments and the gym site.
For information on creating your own environments, see Creating your own Environments.

Examples
See the examples directory.

Run examples/agents/random_agent.py to run a simple random agent.
Run examples/agents/cem.py to run an actual learning agent (using the cross-entropy method).
Run examples/scripts/list_envs to generate a list of all environments.


Testing
We are using pytest for tests. You can run them via:
pytest

What's new


2019-11-08 (v0.15.4)

Added multiple env wrappers (thanks @zuoxingdong and @hartikainen!)


Removed mujoco >= 2.0 support due to lack of tests





2019-10-09 (v0.15.3)

VectorEnv modifications - unified the VectorEnv api (added reset_async, reset_wait, step_async, step_wait methods to SyncVectorEnv); more flexibility in AsyncVectorEnv workers





2019-08-23 (v0.15.2)

More Wrappers - AtariPreprocessing, FrameStack, GrayScaleObservation, FilterObservation,  FlattenDictObservationsWrapper, PixelObservationWrapper, TransformReward (thanks @zuoxingdong, @hartikainen)
Remove rgb_rendering_tracking logic from mujoco environments (default behavior stays the same for the -v3 environments, rgb rendering returns a view from tracking camera)
Velocity goal constraint for MountainCar (thanks @abhinavsagar)
Taxi-v2 -> Taxi-v3 (add missing wall in the map to replicate env as describe in the original paper, thanks @kobotics)





2019-07-26 (v0.14.0)

Wrapper cleanup
Spec-related bug fixes
VectorEnv fixes





2019-06-21 (v0.13.1)

Bug fix for ALE 0.6 difficulty modes
Use narrow range for pyglet versions





2019-06-21 (v0.13.0)

Upgrade to ALE 0.6 (atari-py 0.2.0) (thanks @JesseFarebro!)





2019-06-21 (v0.12.6)

Added vectorized environments (thanks @tristandeleu!). Vectorized environment runs multiple copies of an environment in parallel. To create a vectorized version of an environment, use gym.vector.make(env_id, num_envs, **kwargs), for instance, gym.vector.make('Pong-v4',16).





2019-05-28 (v0.12.5)

fixed Fetch-slide environment to be solvable.





2019-05-24 (v0.12.4)

remove pyopengl dependency and use more narrow atari-py and box2d-py versions





2019-03-25 (v0.12.1)

rgb rendering in MuJoCo locomotion -v3 environments now comes from tracking camera (so that agent does not run away from the field of view). The old behaviour can be restored by passing rgb_rendering_tracking=False kwarg. Also, a potentially breaking change!!! Wrapper class now forwards methods and attributes to wrapped env.





2019-02-26 (v0.12.0)

release mujoco environments v3 with support for gym.make kwargs such as xml_file, ctrl_cost_weight, reset_noise_scale etc





2019-02-06 (v0.11.0)

remove gym.spaces.np_random common PRNG; use per-instance PRNG instead.
support for kwargs in gym.make
lots of bugfixes




2018-02-28: Release of a set of new robotics environments.

2018-01-25: Made some aesthetic improvements and removed unmaintained parts of gym. This may seem like a downgrade in functionality, but it is actually a long-needed cleanup in preparation for some great new things that will be released in the next month.


Now your Env and Wrapper subclasses should define step, reset, render, close, seed rather than underscored method names.
Removed the board_game, debugging, safety, parameter_tuning environments since they're not being maintained by us at OpenAI. We encourage authors and users to create new repositories for these environments.
Changed MultiDiscrete action space to range from [0, ..., n-1] rather than [a, ..., b-1].
No more render(close=True), use env-specific methods to close the rendering.
Removed scoreboard directory, since site doesn't exist anymore.
Moved gym/monitoring to gym/wrappers/monitoring
Add dtype to Space.
Not using python's built-in module anymore, using gym.logger



2018-01-24: All continuous control environments now use mujoco_py >= 1.50.
Versions have been updated accordingly to -v2, e.g. HalfCheetah-v2. Performance
should be similar (see https://github.com/openai/gym/pull/834) but there are likely
some differences due to changes in MuJoCo.

2017-06-16: Make env.spec into a property to fix a bug that occurs
when you try to print out an unregistered Env.

2017-05-13: BACKWARDS INCOMPATIBILITY: The Atari environments are now at
v4. To keep using the old v3 environments, keep gym <= 0.8.2 and atari-py
<= 0.0.21. Note that the v4 environments will not give identical results to
existing v3 results, although differences are minor. The v4 environments
incorporate the latest Arcade Learning Environment (ALE), including several
ROM fixes, and now handle loading and saving of the emulator state. While
seeds still ensure determinism, the effect of any given seed is not preserved
across this upgrade because the random number generator in ALE has changed.
The *NoFrameSkip-v4 environments should be considered the canonical Atari
environments from now on.

2017-03-05: BACKWARDS INCOMPATIBILITY: The configure method has been removed
from Env. configure was not used by gym, but was used by some dependent
libraries including universe. These libraries will migrate away from the
configure method by using wrappers instead. This change is on master and will be released with 0.8.0.

2016-12-27: BACKWARDS INCOMPATIBILITY: The gym monitor is now a
wrapper. Rather than starting monitoring as
env.monitor.start(directory), envs are now wrapped as follows:
env = wrappers.Monitor(env, directory). This change is on master
and will be released with 0.7.0.

2016-11-1: Several experimental changes to how a running monitor interacts
with environments. The monitor will now raise an error if reset() is called
when the env has not returned done=True. The monitor will only record complete
episodes where done=True. Finally, the monitor no longer calls seed() on the
underlying env, nor does it record or upload seed information.

2016-10-31: We're experimentally expanding the environment ID format
to include an optional username.

2016-09-21: Switch the Gym automated logger setup to configure the
root logger rather than just the 'gym' logger.

2016-08-17: Calling close on an env will also close the monitor
and any rendering windows.

2016-08-17: The monitor will no longer write manifest files in
real-time, unless write_upon_reset=True is passed.

2016-05-28: For controlled reproducibility, envs now support seeding
(cf #91 and #135). The monitor records which seeds are used. We will
soon add seed information to the display on the scoreboard.


",GitHub - openai/gym: A toolkit for developing and comparing reinforcement learning algorithms.
42,Python,"Pipenv: Python Development Workflow for Humans






Pipenv is a tool that aims to bring the best of all packaging worlds
(bundler, composer, npm, cargo, yarn, etc.) to the Python world.
Windows is a first-class citizen, in our world.
It automatically creates and manages a virtualenv for your projects, as
well as adds/removes packages from your Pipfile as you
install/uninstall packages. It also generates the ever-important
Pipfile.lock, which is used to produce deterministic builds.

The problems that Pipenv seeks to solve are multi-faceted:

You no longer need to use pip and virtualenv separately. They
work together.
Managing a requirements.txt file can be
problematic,
so Pipenv uses the upcoming Pipfile and Pipfile.lock instead,
which is superior for basic use cases.
Hashes are used everywhere, always. Security. Automatically expose
security vulnerabilities.
Give you insight into your dependency graph (e.g. $ pipenv graph).
Streamline development workflow by loading .env files.

You can quickly play with Pipenv right in your browser:

Installation
If you're on MacOS, you can install Pipenv easily with Homebrew:
$ brew install pipenv

Or, if you're using Debian Buster+:
$ sudo apt install pipenv

Or, if you're using Fedora:
$ sudo dnf install pipenv

Or, if you're using FreeBSD:
# pkg install py36-pipenv

Otherwise, refer to the documentation for instructions.
‚ú®üç∞‚ú®
‚ò§ User Testimonials
Jannis Leidel, former pip maintainer---
:   Pipenv is the porcelain I always wanted to build for pip. It fits
my brain and mostly replaces virtualenvwrapper and manual pip calls
for me. Use it.
David Gang---
:   This package manager is really awesome. For the first time I know
exactly what my dependencies are which I installed and what the
transitive dependencies are. Combined with the fact that installs
are deterministic, makes this package manager first class, like
cargo.
Justin Myles Holmes---
:   Pipenv is finally an abstraction meant to engage the mind instead
of merely the filesystem.
‚ò§ Features

Enables truly deterministic builds, while easily specifying only
what you want.
Generates and checks file hashes for locked dependencies.
Automatically install required Pythons, if pyenv is available.
Automatically finds your project home, recursively, by looking for a
Pipfile.
Automatically generates a Pipfile, if one doesn't exist.
Automatically creates a virtualenv in a standard location.
Automatically adds/removes packages to a Pipfile when they are
un/installed.
Automatically loads .env files, if they exist.

The main commands are install, uninstall, and lock, which
generates a Pipfile.lock. These are intended to replace
$ pip install usage, as well as manual virtualenv management (to
activate a virtualenv, run $ pipenv shell).
Basic Concepts

A virtualenv will automatically be created, when one doesn't exist.
When no parameters are passed to install, all packages
[packages] specified will be installed.
To initialize a Python 3 virtual environment, run
$ pipenv --three.
To initialize a Python 2 virtual environment, run $ pipenv --two.
Otherwise, whatever virtualenv defaults to will be the default.

Other Commands

shell will spawn a shell with the virtualenv activated.
run will run a given command from the virtualenv, with any
arguments forwarded (e.g. $ pipenv run python).
check asserts that PEP 508 requirements are being met by the
current environment.
graph will print a pretty graph of all your installed
dependencies.

Shell Completion
For example, with fish, put this in your
~/.config/fish/completions/pipenv.fish:
eval (pipenv --completion)

Alternatively, with bash, put this in your .bashrc or .bash_profile:
eval ""$(pipenv --completion)""

Magic shell completions are now enabled! There is also a fish
plugin, which will automatically
activate your subshells for you!
Fish is the best shell. You should use it.
‚ò§ Usage
$ pipenv
Usage: pipenv [OPTIONS] COMMAND [ARGS]...

Options:
  --where          Output project home information.
  --venv           Output virtualenv information.
  --py             Output Python interpreter information.
  --envs           Output Environment Variable options.
  --rm             Remove the virtualenv.
  --bare           Minimal output.
  --completion     Output completion (to be eval'd).
  --man            Display manpage.
  --three / --two  Use Python 3/2 when creating virtualenv.
  --python TEXT    Specify which version of Python virtualenv should use.
  --site-packages  Enable site-packages for the virtualenv.
  --version        Show the version and exit.
  -h, --help       Show this message and exit.


Usage Examples:
   Create a new project using Python 3.7, specifically:
   $ pipenv --python 3.7

   Remove project virtualenv (inferred from current directory):
   $ pipenv --rm

   Install all dependencies for a project (including dev):
   $ pipenv install --dev

   Create a lockfile containing pre-releases:
   $ pipenv lock --pre

   Show a graph of your installed dependencies:
   $ pipenv graph

   Check your installed dependencies for security vulnerabilities:
   $ pipenv check

   Install a local setup.py into your virtual environment/Pipfile:
   $ pipenv install -e .

   Use a lower-level pip command:
   $ pipenv run pip freeze

Commands:
  check      Checks for security vulnerabilities and against PEP 508 markers
             provided in Pipfile.
  clean      Uninstalls all packages not specified in Pipfile.lock.
  graph      Displays currently‚Äìinstalled dependency graph information.
  install    Installs provided packages and adds them to Pipfile, or (if no
             packages are given), installs all packages from Pipfile.
  lock       Generates Pipfile.lock.
  open       View a given module in your editor.
  run        Spawns a command installed into the virtualenv.
  shell      Spawns a shell within the virtualenv.
  sync       Installs all packages specified in Pipfile.lock.
  uninstall  Un-installs a provided package and removes it from Pipfile.

Locate the project:
$ pipenv --where
/Users/kennethreitz/Library/Mobile Documents/com~apple~CloudDocs/repos/kr/pipenv/test

Locate the virtualenv:
$ pipenv --venv
/Users/kennethreitz/.local/share/virtualenvs/test-Skyy4vre

Locate the Python interpreter:
$ pipenv --py
/Users/kennethreitz/.local/share/virtualenvs/test-Skyy4vre/bin/python

Install packages:
$ pipenv install
Creating a virtualenv for this project...
...
No package provided, installing all dependencies.
Virtualenv location: /Users/kennethreitz/.local/share/virtualenvs/test-EJkjoYts
Installing dependencies from Pipfile.lock...
...

To activate this project's virtualenv, run the following:
$ pipenv shell

Installing from git:
You can install packages with pipenv from git and other version control systems using URLs formatted according to the following rule:
<vcs_type>+<scheme>://<location>/<user_or_organization>/<repository>@<branch_or_tag>#<package_name>

The only optional section is the @<branch_or_tag> section.  When using git over SSH, you may use the shorthand vcs and scheme alias git+git@<location>:<user_or_organization>/<repository>@<branch_or_tag>#<package_name>. Note that this is translated to git+ssh://git@<location> when parsed.
Valid values for <vcs_type> include git, bzr, svn, and hg.  Valid values for <scheme> include http,, https, ssh, and file.  In specific cases you also have access to other schemes: svn may be combined with svn as a scheme, and bzr can be combined with sftp and lp.
Note that it is strongly recommended that you install any version-controlled dependencies in editable mode, using pipenv install -e, in order to ensure that dependency resolution can be performed with an up to date copy of the repository each time it is performed, and that it includes all known dependencies.
Below is an example usage which installs the git repository located at https://github.com/requests/requests.git from tag v2.19.1 as package name requests:
$ pipenv install -e git+https://github.com/requests/requests.git@v2.19#egg=requests
Creating a Pipfile for this project...
Installing -e git+https://github.com/requests/requests.git@v2.19.1#egg=requests...
[...snipped...]
Adding -e git+https://github.com/requests/requests.git@v2.19.1#egg=requests to Pipfile's [packages]...
[...]

You can read more about pip's implementation of vcs support here.
Install a dev dependency:
$ pipenv install pytest --dev
Installing pytest...
...
Adding pytest to Pipfile's [dev-packages]...

Show a dependency graph:
$ pipenv graph
requests==2.18.4
  - certifi [required: >=2017.4.17, installed: 2017.7.27.1]
  - chardet [required: >=3.0.2,<3.1.0, installed: 3.0.4]
  - idna [required: >=2.5,<2.7, installed: 2.6]
  - urllib3 [required: <1.23,>=1.21.1, installed: 1.22]

Generate a lockfile:
$ pipenv lock
Assuring all dependencies from Pipfile are installed...
Locking [dev-packages] dependencies...
Locking [packages] dependencies...
Note: your project now has only default [packages] installed.
To install [dev-packages], run: $ pipenv install --dev

Install all dev dependencies:
$ pipenv install --dev
Pipfile found at /Users/kennethreitz/repos/kr/pip2/test/Pipfile. Considering this to be the project home.
Pipfile.lock out of date, updating...
Assuring all dependencies from Pipfile are installed...
Locking [dev-packages] dependencies...
Locking [packages] dependencies...

Uninstall everything:
$ pipenv uninstall --all
No package provided, un-installing all dependencies.
Found 25 installed package(s), purging...
...
Environment now purged and fresh!

Use the shell:
$ pipenv shell
Loading .env environment variables‚Ä¶
Launching subshell in virtual environment. Type 'exit' or 'Ctrl+D' to return.
$ ‚ñØ

‚ò§ Documentation
Documentation resides over at pipenv.org.
",GitHub - pypa/pipenv: Python Development Workflow for Humans.
43,Python,"



interactive-coding-challenges
120+ continually updated, interactive, and test-driven coding challenges, with Anki flashcards.
Challenges focus on algorithms and data structures found in coding interviews.
Each challenge has one or more reference solutions that are:

Fully functional
Unit tested
Easy-to-understand

Challenges will soon provide on-demand incremental hints to help you arrive at the optimal solution.
Notebooks also detail:

Constraints
Test cases
Algorithms
Big-O time and space complexities

Also included are unit tested reference implementations of various data structures and algorithms.
Challenge Solutions





Anki Flashcards: Coding and Design




The provided Anki flashcard deck uses spaced repetition to help you retain key concepts.

Coding deck

Great for use while on-the-go.
Design Resource: The System Design Primer
Looking for resources to help you prep for the System Design and Object-Oriented Design interviews?




Check out the sister repo The System Design Primer, which contains additional Anki decks:

System design deck
System design exercises deck
Object oriented design exercises deck


Notebook Structure
Each challenge has two notebooks, a challenge notebook with unit tests for you to solve and a solution notebook for reference.
Problem Statement

States the problem to solve.

Constraints

Describes any constraints or assumptions.

Test Cases

Describes the general and edge test cases that will be evaluated in the unit test.

Algorithm

[Challenge Notebook] Empty, refer to the solution notebook algorithm section if you need a hint.
[Solution Notebook] One or more algorithm solution discussions, with Big-O time and space complexities.

Hints

[Challenge Notebook] Provides on-demand incremental hints to help you arrive at the optimal solution.  Coming soon!

Code (Challenge: Implement Me!)

[Challenge Notebook] Skeleton code for you to implement.
[Solution Notebook] One or more reference solutions.

Unit Test

[Challenge Notebook] Unit test for your code.  Expected to fail until you solve the challenge.
[Solution Notebook] Unit test for the reference solution(s).

Index
Challenges Categories
Format: Challenge Category - Number of Challenges

Arrays and Strings - 10
Linked Lists - 8
Stacks and Queues - 8
Graphs and Trees - 21
Sorting - 10
Recursion and Dynamic Programming - 17
Mathematics and Probability - 6
Bit Manipulation - 8
Online Judges - 16
System Design - 8
Object Oriented Design - 8

Total number of challenges: 120
Reference Implementations: Data Structures
Unit tested, fully functional implementations of the following data structures:

Linked List
Stack
Queue
Binary Search Tree
Graph
Min Heap
Trie
Priority Queue
Hash Map

Reference Implementations: Algorithms
Unit tested, fully functional implementations of the following algorithms:

Selection Sort
Insertion Sort
Quick Sort
Merge Sort
Radix Sort
Topological Sort
Tree Depth-First Search (Pre-, In-, Post-Order)
Tree Breadth-First Search
Graph Depth-First Search
Graph Breadth-First Search
Dijkstra's Shortest Path
Unweighted Graph Shortest Path
Knapsack 0/1
Knapsack Unbounded
Sieve of Eratosthenes

Reference Implementations: TODO

A*
Bellman-Ford
Bloom Filter
Convex Hull
Fisher-Yates Shuffle
Kruskal's
Max Flow
Prim's
Rabin-Karp
Traveling Salesman
Union Find
Contribute

Installing and Running Challenges

Repo Structure
Notebook Installation

Nose Installation


Running Challenges

Misc

Contributing
Credits
Contact Info
License

Challenges
Image Credits





Arrays and Strings



Challenge
Static Notebook




Determine if a string contains unique characters
Challenge‚îÇSolution


Determine if a string is a permutation of another
Challenge‚îÇSolution


Determine if a string is a rotation of another
Challenge‚îÇSolution


Compress a string
Challenge‚îÇSolution


Reverse characters in a string
Challenge‚îÇSolution


Given two strings, find the single different char
Challenge‚îÇSolution


Find two indices that sum to a specific value
Challenge‚îÇSolution


Implement a hash table
Challenge‚îÇSolution


Implement fizz buzz
Challenge‚îÇSolution


Find the first non-repeated character in a string
Contribute‚îÇContribute


Remove specified characters in a string
Contribute‚îÇContribute


Reverse words in a string
Contribute‚îÇContribute


Convert a string to an integer
Contribute‚îÇContribute


Convert an integer to a string
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Linked Lists



Challenge
Static Notebook




Remove duplicates from a linked list
Challenge‚îÇSolution


Find the kth to last element of a linked list
Challenge‚îÇSolution


Delete a node in the middle of a linked list
Challenge‚îÇSolution


Partition a linked list around a given value
Challenge‚îÇSolution


Add two numbers whose digits are stored in a linked list
Challenge‚îÇSolution


Find the start of a linked list loop
Challenge‚îÇSolution


Determine if a linked list is a palindrome
Challenge‚îÇSolution


Implement a linked list
Challenge‚îÇSolution


Determine if a list is cyclic or acyclic
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Stacks and Queues



Challenge
Static Notebook




Implement n stacks using a single array
Challenge‚îÇSolution


Implement a stack that keeps track of its minimum element
Challenge‚îÇSolution


Implement a set of stacks class that wraps a list of capacity-bounded stacks
Challenge‚îÇSolution


Implement a queue using two stacks
Challenge‚îÇSolution


Sort a stack using another stack as a buffer
Challenge‚îÇSolution


Implement a stack
Challenge‚îÇSolution


Implement a queue
Challenge‚îÇSolution


Implement a priority queue backed by an array
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute








Graphs and Trees



Challenge
Static Notebooks




Implement depth-first search (pre-, in-, post-order) on a tree
Challenge‚îÇSolution


Implement breadth-first search on a tree
Challenge‚îÇSolution


Determine the height of a tree
Challenge‚îÇSolution


Create a binary search tree with minimal height from a sorted array
Challenge‚îÇSolution


Create a linked list for each level of a binary tree
Challenge‚îÇSolution


Check if a binary tree is balanced
Challenge‚îÇSolution


Determine if a tree is a valid binary search tree
Challenge‚îÇSolution


Find the in-order successor of a given node in a binary search tree
Challenge‚îÇSolution


Find the second largest node in a binary search tree
Challenge‚îÇSolution


Find the lowest common ancestor
Challenge‚îÇSolution


Invert a binary tree
Challenge‚îÇSolution


Implement a binary search tree
Challenge‚îÇSolution


Implement a min heap
Challenge‚îÇSolution


Implement a trie
Challenge‚îÇSolution


Implement depth-first search on a graph
Challenge‚îÇSolution


Implement breadth-first search on a graph
Challenge‚îÇSolution


Determine if there is a path between two nodes in a graph
Challenge‚îÇSolution


Implement a graph
Challenge‚îÇSolution


Find a build order given a list of projects and dependencies.
Challenge‚îÇSolution


Find the shortest path in a weighted graph.
Challenge‚îÇSolution


Find the shortest path in an unweighted graph.
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute








Sorting



Challenge
Static Notebooks




Implement selection sort
Challenge‚îÇSolution


Implement insertion sort
Challenge‚îÇSolution


Implement quick sort
Challenge‚îÇSolution


Implement merge sort
Challenge‚îÇSolution


Implement radix sort
Challenge‚îÇSolution


Sort an array of strings so all anagrams are next to each other
Challenge‚îÇSolution


Find an item in a sorted, rotated array
Challenge‚îÇSolution


Search a sorted matrix for an item
Challenge‚îÇSolution


Find an int not in an input of n integers
Challenge‚îÇSolution


Given sorted arrays A, B, merge B into A in sorted order
Challenge‚îÇSolution


Implement a stable selection sort
Contribute‚îÇContribute


Make an unstable sort stable
Contribute‚îÇContribute


Implement an efficient, in-place version of quicksort
Contribute‚îÇContribute


Given two sorted arrays, merge one into the other in sorted order
Contribute‚îÇContribute


Find an element in a rotated and sorted array of integers
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Recursion and Dynamic Programming



Challenge
Static Notebooks




Implement fibonacci recursively, dynamically, and iteratively
Challenge‚îÇSolution


Maximize items placed in a knapsack
Challenge‚îÇSolution


Maximize unbounded items placed in a knapsack
Challenge‚îÇSolution


Find the longest common subsequence
Challenge‚îÇSolution


Find the longest increasing subsequence
Challenge‚îÇSolution


Minimize the cost of matrix multiplication
Challenge‚îÇSolution


Maximize stock prices given k transactions
Challenge‚îÇSolution


Find the minimum number of ways to represent n cents given an array of coins
Challenge‚îÇSolution


Find the unique number of ways to represent n cents given an array of coins
Challenge‚îÇSolution


Print all valid combinations of n-pairs of parentheses
Challenge‚îÇSolution


Navigate a maze
Challenge‚îÇSolution


Print all subsets of a set
Challenge‚îÇSolution


Print all permutations of a string
Challenge‚îÇSolution


Find the magic index in an array
Challenge‚îÇSolution


Find the number of ways to run up n steps
Challenge‚îÇSolution


Implement the Towers of Hanoi with 3 towers and N disks
Challenge‚îÇSolution


Implement factorial recursively, dynamically, and iteratively
Contribute‚îÇContribute


Perform a binary search on a sorted array of integers
Contribute‚îÇContribute


Print all combinations of a string
Contribute‚îÇContribute


Implement a paint fill function
Contribute‚îÇContribute


Find all permutations to represent n cents, given 1, 5, 10, 25 cent coins
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Mathematics and Probability



Challenge
Static Notebooks




Generate a list of primes
Challenge‚îÇSolution


Find the digital root
Challenge‚îÇSolution


Create a class supporting insert, max, min, mean, mode in O(1)
Challenge‚îÇSolution


Determine if a number is a power of two
Challenge‚îÇSolution


Add two numbers without the + or - sign
Challenge‚îÇSolution


Subtract two numbers without the + or - sign
Challenge‚îÇSolution


Check if a number is prime
Contribute‚îÇContribute


Determine if two lines on a Cartesian plane intersect
Contribute‚îÇContribute


Using only add, implement multiply, subtract, and divide for ints
Contribute‚îÇContribute


Find the kth number such that the only prime factors are 3, 5, and 7
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Bit Manipulation



Challenge
Static Notebooks




Implement common bit manipulation operations
Challenge‚îÇSolution


Determine number of bits to flip to convert a into b
Challenge‚îÇSolution


Draw a line on a screen
Challenge‚îÇSolution


Flip a bit to maximize the longest sequence of 1s
Challenge‚îÇSolution


Get the next largest and next smallest numbers
Challenge‚îÇSolution


Merge two binary numbers
Challenge‚îÇSolution


Swap odd and even bits in an integer
Challenge‚îÇSolution


Print the binary representation of a number between 0 and 1
Challenge‚îÇSolution


Determine the number of 1s in the binary representation of a given integer
Contribute‚îÇContribute


Add a challenge
Contribute‚îÇContribute








Online Judges



Challenge
Static Notebooks




Find the longest substring with at most k distinct chars
Challenge‚îÇSolution


Find the highest product of three numbers
Challenge‚îÇSolution


Maximize stocks profit from 1 buy and 1 sell
Challenge‚îÇSolution


Move all zeroes in a list to the end
Challenge‚îÇSolution


Find the products of every other int
Challenge‚îÇSolution


Given a list of entries and exits, find the busiest period
Challenge‚îÇSolution


Determine an island's perimeter
Challenge‚îÇSolution


Format license keys
Challenge‚îÇSolution


Find the longest absolute file path
Challenge‚îÇSolution


Merge tuple ranges
Challenge‚îÇSolution


Assign cookies
Challenge‚îÇSolution


Determine if you can win in Nim
Challenge‚îÇSolution


Check if a magazine could have been used to create a ransom note
Challenge‚îÇSolution


Find the number of times a sentence can fit on a screen
Challenge‚îÇSolution


Utopian tree
Challenge‚îÇSolution


Maximizing xor
Challenge‚îÇSolution


Add a challenge
Contribute‚îÇContribute



Repo Structure
interactive-coding-challenges        # Repo
‚îú‚îÄ arrays_strings                    # Category of challenges
‚îÇ  ‚îú‚îÄ rotation                       # Challenge folder
‚îÇ  ‚îÇ  ‚îú‚îÄ rotation_challenge.ipynb    # Challenge notebook
‚îÇ  ‚îÇ  ‚îú‚îÄ rotation_solution.ipynb     # Solution notebook
‚îÇ  ‚îÇ  ‚îú‚îÄ test_rotation.py            # Unit test*
‚îÇ  ‚îú‚îÄ compress
‚îÇ  ‚îÇ  ‚îú‚îÄ compress_challenge.ipynb
‚îÇ  ‚îÇ  ‚îú‚îÄ compress_solution.ipynb
‚îÇ  ‚îÇ  ‚îú‚îÄ test_compress.py
‚îÇ  ‚îú‚îÄ ...
‚îú‚îÄ linked_lists
‚îÇ  ‚îú‚îÄ palindrome
‚îÇ  ‚îÇ  ‚îî‚îÄ ...
‚îÇ  ‚îú‚îÄ ...
‚îú‚îÄ ...

*The notebooks (.ipynb) read/write the associated unit test (.py) file.
Notebook Installation
Jupyter Notebook
Run:
pip install jupyter

For detailed instructions, scripts, and tools to more optimally set up your development environment, check out the dev-setup repo.
For more details on notebook installation, follow the directions here.
More information on IPython/Jupyter Notebooks can be found here.
Nose Tests
Install nose using setuptools/distribute:
easy_install nose

or
pip install nose

More information on Nose can be found here.
Running Challenges
Notebooks
Challenges are provided in the form of IPython/Jupyter Notebooks and have been tested with Python 2.7 and Python 3.x.
If you need to install IPython/Jupyter Notebook, see the Notebook Installation section.

This README contains links to nbviewer, which hosts static notebooks of the repo's contents
To interact with or to modify elements within the dynamic notebooks, refer to the instructions below

Run the notebook of challenges:
$ git clone https://github.com/donnemartin/interactive-coding-challenges.git
$ cd interactive-coding-challenges
$ jupyter notebook

This will launch your web browser with the list of challenge categories:

Navigate to the Challenge Notebook you wish to solve
Run the cells within the challenge notebook (Cell->Run All)

This will result in an expected unit test error


Solve the challenge and verify it passes the unit test
Check out the accompanying Solution Notebook for further discussion

To debug your solution with pdb, refer to the following ticket.
Note: If your solution is different from those listed in the Solution Notebook, consider submitting a pull request so others can benefit from your work.  Review the Contributing Guidelines for details.
Future Development
Challenges, solutions, and unit tests are presented in the form of IPython/Jupyter Notebooks.

Notebooks currently contain mostly Python solutions (tested on both Python 2.7 and Python 3.x), but can be extended to include 40+ supported languages
Repo will be continually updated with new solutions and challenges
Contributions are welcome!

Contributing
Contributions are welcome!
Review the Contributing Guidelines for details on how to:

Submit issues
Add solutions to existing challenges
Add new challenges

Credits
Resources

Cracking the Coding Interview | GitHub Solutions
Programming Interviews Exposed
The Algorithm Design Manual | Solutions
CareerCup
Quora
HackerRank
LeetCode

Images

Arrays and Strings: nltk.org
Linked Lists: wikipedia.org
Stacks: wikipedia.org
Queues: wikipedia.org
Sorting: wikipedia.org
Recursion and Dynamic Programming: wikipedia.org
Graphs and Trees: wikipedia.org
Mathematics and Probability: wikipedia.org
Bit Manipulation: wikipedia.org
Online Judges: topcoder.com

Contact Info
Feel free to contact me to discuss any issues, questions, or comments.
My contact info can be found on my GitHub page.
License
I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).
Copyright 2015 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

",GitHub - donnemartin/interactive-coding-challenges: 120+ interactive Python coding interview challenges (algorithms and data structures).  Includes Anki flashcards.
44,Python,"Tornado Web Server


Tornado is a Python web framework and
asynchronous networking library, originally developed at FriendFeed.  By using non-blocking network I/O, Tornado
can scale to tens of thousands of open connections, making it ideal for
long polling,
WebSockets, and other
applications that require a long-lived connection to each user.

Hello, world
Here is a simple ""Hello, world"" example web app for Tornado:
import tornado.ioloop
import tornado.web

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write(""Hello, world"")

def make_app():
    return tornado.web.Application([
        (r""/"", MainHandler),
    ])

if __name__ == ""__main__"":
    app = make_app()
    app.listen(8888)
    tornado.ioloop.IOLoop.current().start()
This example does not use any of Tornado's asynchronous features; for
that see this simple chat room.

Documentation
Documentation and links to additional resources are available at
https://www.tornadoweb.org
","GitHub - tornadoweb/tornado: Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed."
45,Python,"



















State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch

ü§ó Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides state-of-the-art general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet, CTRL...) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch.
Features

As easy to use as pytorch-transformers
As powerful and concise as Keras
High performance on NLU and NLG tasks
Low barrier to entry for educators and practitioners

State-of-the-art NLP for everyone

Deep learning researchers
Hands-on practitioners
AI/ML/NLP teachers and educators

Lower compute costs, smaller carbon footprint

Researchers can share trained models instead of always retraining
Practitioners can reduce compute time and production costs
10 architectures with over 30 pretrained models, some in more than 100 languages

Choose the right framework for every part of a model's lifetime

Train state-of-the-art models in 3 lines of code
Deep interoperability between TensorFlow 2.0 and PyTorch models
Move a single model between TF2.0/PyTorch frameworks at will
Seamlessly pick the right framework for training, evaluation, production




Section
Description




Installation
How to install the package


Model architectures
Architectures (with pretrained weights)


Online demo
Experimenting with this repo‚Äôs text generation capabilities


Quick tour: Usage
Tokenizers & models usage: Bert and GPT-2


Quick tour: TF 2.0 and PyTorch 
Train a TF 2.0 model in 10 lines of code, load it in PyTorch


Quick tour: Fine-tuning/usage scripts
Using provided scripts: GLUE, SQuAD and Text generation


Migrating from pytorch-transformers to transformers
Migrating your code from pytorch-transformers to transformers


Migrating from pytorch-pretrained-bert to pytorch-transformers
Migrating your code from pytorch-pretrained-bert to transformers


[Documentation](v2.2.0/v2.2.1) (v2.1.1) (v2.0.0) (v1.2.0) (v1.1.0) (v1.0.0) (master)
Full API documentation and more



Installation
This repo is tested on Python 2.7 and 3.5+ (examples are tested only on python 3.5+), PyTorch 1.0.0+ and TensorFlow 2.0.0-rc1
With pip
First you need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to TensorFlow installation page and/or PyTorch installation page regarding the specific install command for your platform.
When TensorFlow 2.0 and/or PyTorch has been installed, ü§ó Transformers can be installed using pip as follows:
pip install transformers
From source
Here also, you first need to install one of, or both, TensorFlow 2.0 and PyTorch.
Please refer to TensorFlow installation page and/or PyTorch installation page regarding the specific install command for your platform.
When TensorFlow 2.0 and/or PyTorch has been installed, you can install from source by cloning the repository and running:
pip install [--editable] .
Run the examples
Examples are included in the repository but are not shipped with the library.
Therefore, in order to run the latest versions of the examples you also need to install from source. To do so, create a new virtual environment and follow these steps:
git clone https://github.com/huggingface/transformers
cd transformers
pip install [--editable] .
Tests
A series of tests are included for the library and the example scripts. Library tests can be found in the tests folder and examples tests in the examples folder.
These tests can be run using unittest or pytest (install pytest if needed with pip install pytest).
Depending on which framework is installed (TensorFlow 2.0 and/or PyTorch), the irrelevant tests will be skipped. Ensure that both frameworks are installed if you want to execute all tests.
You can run the tests from the root of the cloned repository with the commands:
python -m unittest discover -s transformers/tests -p ""*test.py"" -t .
python -m unittest discover -s examples -p ""*test.py"" -t examples
or
python -m pytest -sv ./transformers/tests/
python -m pytest -sv ./examples/
By default, slow tests are skipped. Set the RUN_SLOW environment variable to yes to run them.
Do you want to run a Transformer model on a mobile device?
You should check out our swift-coreml-transformers repo.
It contains a set of tools to convert PyTorch or TensorFlow 2.0 trained Transformer models (currently contains GPT-2, DistilGPT-2, BERT, and DistilBERT) to CoreML models that run on iOS devices.
At some point in the future, you'll be able to seamlessly move from pre-training or fine-tuning models to productizing them in CoreML, or prototype a model or an app in CoreML then research its hyperparameters or architecture from TensorFlow 2.0 and/or PyTorch. Super exciting!
Model architectures
ü§ó Transformers currently provides 10 NLU/NLG architectures:

BERT (from Google) released with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
GPT (from OpenAI) released with the paper Improving Language Understanding by Generative Pre-Training by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.
GPT-2 (from OpenAI) released with the paper Language Models are Unsupervised Multitask Learners by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.
Transformer-XL (from Google/CMU) released with the paper Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.
XLNet (from Google/CMU) released with the paper ‚ÄãXLNet: Generalized Autoregressive Pretraining for Language Understanding by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.
XLM (from Facebook) released together with the paper Cross-lingual Language Model Pretraining by Guillaume Lample and Alexis Conneau.
RoBERTa (from Facebook), released together with the paper a Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
DistilBERT (from HuggingFace), released together with the paper DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into DistilGPT2, RoBERTa into DistilRoBERTa, Multilingual BERT into DistilmBERT and a German version of DistilBERT.
CTRL (from Salesforce) released with the paper CTRL: A Conditional Transformer Language Model for Controllable Generation by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.
CamemBERT (from Inria/Facebook/Sorbonne) released with the paper CamemBERT: a Tasty French Language Model by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su√°rez*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.
ALBERT (from Google Research and the Toyota Technological Institute at Chicago) released with the paper ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
Want to contribute a new model? We have added a detailed guide and templates to guide you in the process of adding a new model. You can find them in the templates folder of the repository. Be sure to check the contributing guidelines and contact the maintainers or open an issue to collect feedbacks before starting your PR.

These implementations have been tested on several datasets (see the example scripts) and should match the performances of the original implementations (e.g. ~93 F1 on SQuAD for BERT Whole-Word-Masking, ~88 F1 on RocStories for OpenAI GPT, ~18.3 perplexity on WikiText 103 for Transformer-XL, ~0.916 Peason R coefficient on STS-B for XLNet). You can find more details on the performances in the Examples section of the documentation.
Online demo
Write With Transformer, built by the Hugging Face team at transformer.huggingface.co, is the official demo of this repo‚Äôs text generation capabilities.
You can use it to experiment with completions generated by GPT2Model, TransfoXLModel, and XLNetModel.

‚Äúü¶Ñ Write with transformer is to writing what calculators are to calculus.‚Äù


Quick tour
Let's do a very quick overview of the model architectures in ü§ó Transformers. Detailed examples for each model architecture (Bert, GPT, GPT-2, Transformer-XL, XLNet and XLM) can be found in the full documentation.
import torch
from transformers import *

# Transformers has a unified API
# for 8 transformer architectures and 30 pretrained weights.
#          Model          | Tokenizer          | Pretrained weights shortcut
MODELS = [(BertModel,       BertTokenizer,       'bert-base-uncased'),
          (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),
          (GPT2Model,       GPT2Tokenizer,       'gpt2'),
          (CTRLModel,       CTRLTokenizer,       'ctrl'),
          (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),
          (XLNetModel,      XLNetTokenizer,      'xlnet-base-cased'),
          (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),
          (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),
          (RobertaModel,    RobertaTokenizer,    'roberta-base')]

# To use TensorFlow 2.0 versions of the models, simply prefix the class names with 'TF', e.g. `TFRobertaModel` is the TF 2.0 counterpart of the PyTorch model `RobertaModel`

# Let's encode some text in a sequence of hidden-states using each model:
for model_class, tokenizer_class, pretrained_weights in MODELS:
    # Load pretrained model/tokenizer
    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
    model = model_class.from_pretrained(pretrained_weights)

    # Encode text
    input_ids = torch.tensor([tokenizer.encode(""Here is some text to encode"", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.
    with torch.no_grad():
        last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples

# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.
BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,
                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]

# All the classes for an architecture can be initiated from pretrained weights for this architecture
# Note that additional weights added for fine-tuning are only initialized
# and need to be trained on the down-stream task
pretrained_weights = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(pretrained_weights)
for model_class in BERT_MODEL_CLASSES:
    # Load pretrained model/tokenizer
    model = model_class.from_pretrained(pretrained_weights)

    # Models can return full list of hidden-states & attentions weights at each layer
    model = model_class.from_pretrained(pretrained_weights,
                                        output_hidden_states=True,
                                        output_attentions=True)
    input_ids = torch.tensor([tokenizer.encode(""Let's see all hidden-states and attentions on this text"")])
    all_hidden_states, all_attentions = model(input_ids)[-2:]

    # Models are compatible with Torchscript
    model = model_class.from_pretrained(pretrained_weights, torchscript=True)
    traced_model = torch.jit.trace(model, (input_ids,))

    # Simple serialization for models and tokenizers
    model.save_pretrained('./directory/to/save/')  # save
    model = model_class.from_pretrained('./directory/to/save/')  # re-load
    tokenizer.save_pretrained('./directory/to/save/')  # save
    tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # re-load

    # SOTA examples for GLUE, SQUAD, text generation...
Quick tour TF 2.0 training and PyTorch interoperability
Let's do a quick example of how a TensorFlow 2.0 model can be trained in 12 lines of code with ü§ó Transformers and then loaded in PyTorch for fast inspection/tests.
import tensorflow as tf
import tensorflow_datasets
from transformers import *

# Load dataset, tokenizer, model from pretrained model/vocabulary
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')
data = tensorflow_datasets.load('glue/mrpc')

# Prepare dataset for GLUE as a tf.data.Dataset instance
train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')
valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')
train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)
valid_dataset = valid_dataset.batch(64)

# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule 
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

# Train and evaluate using tf.keras.Model.fit()
history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,
                    validation_data=valid_dataset, validation_steps=7)

# Load the TensorFlow model in PyTorch for inspection
model.save_pretrained('./save/')
pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)

# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task
sentence_0 = ""This research was consistent with his findings.""
sentence_1 = ""His findings were compatible with this research.""
sentence_2 = ""His findings were not compatible with this research.""
inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')
inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')

pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()
pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()

print(""sentence_1 is"", ""a paraphrase"" if pred_1 else ""not a paraphrase"", ""of sentence_0"")
print(""sentence_2 is"", ""a paraphrase"" if pred_2 else ""not a paraphrase"", ""of sentence_0"")
Quick tour of the fine-tuning/usage scripts
Important
Before running the fine-tuning scripts, please read the
instructions on how to
setup your environment to run the examples.
The library comprises several example scripts with SOTA performances for NLU and NLG tasks:

run_glue.py: an example fine-tuning Bert, XLNet and XLM on nine different GLUE tasks (sequence-level classification)
run_squad.py: an example fine-tuning Bert, XLNet and XLM on the question answering dataset SQuAD 2.0 (token-level classification)
run_generation.py: an example using GPT, GPT-2, CTRL, Transformer-XL and XLNet for conditional language generation
other model-specific examples (see the documentation).

Here are three quick usage examples for these scripts:
run_glue.py: Fine-tuning on GLUE tasks for sequence classification
The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine sentence- or sentence-pair language understanding tasks for evaluating and analyzing natural language understanding systems.
Before running anyone of these GLUE tasks you should download the
GLUE data by running
this script
and unpack it to some directory $GLUE_DIR.
You should also install the additional packages required by the examples:
pip install -r ./examples/requirements.txt
export GLUE_DIR=/path/to/glue
export TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_type bert \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --do_train \
    --do_eval \
    --do_lower_case \
    --data_dir $GLUE_DIR/$TASK_NAME \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/$TASK_NAME/
where task name can be one of CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, WNLI.
The dev set results will be present within the text file 'eval_results.txt' in the specified output_dir. In case of MNLI, since there are two separate dev sets, matched and mismatched, there will be a separate output folder called '/tmp/MNLI-MM/' in addition to '/tmp/MNLI/'.
Fine-tuning XLNet model on the STS-B regression task
This example code fine-tunes XLNet on the STS-B corpus using parallel training on a server with 4 V100 GPUs.
Parallel training is a simple way to use several GPUs (but is slower and less flexible than distributed training, see below).
export GLUE_DIR=/path/to/glue

python ./examples/run_glue.py \
    --model_type xlnet \
    --model_name_or_path xlnet-large-cased \
    --do_train  \
    --do_eval   \
    --task_name=sts-b     \
    --data_dir=${GLUE_DIR}/STS-B  \
    --output_dir=./proc_data/sts-b-110   \
    --max_seq_length=128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --gradient_accumulation_steps=1 \
    --max_steps=1200  \
    --model_name=xlnet-large-cased   \
    --overwrite_output_dir   \
    --overwrite_cache \
    --warmup_steps=120
On this machine we thus have a batch size of 32, please increase gradient_accumulation_steps to reach the same batch size if you have a smaller machine. These hyper-parameters should result in a Pearson correlation coefficient of +0.917 on the development set.
Fine-tuning Bert model on the MRPC classification task
This example code fine-tunes the Bert Whole Word Masking model on the Microsoft Research Paraphrase Corpus (MRPC) corpus using distributed training on 8 V100 GPUs to reach a F1 > 92.
python -m torch.distributed.launch --nproc_per_node 8 ./examples/run_glue.py   \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --task_name MRPC \
    --do_train   \
    --do_eval   \
    --do_lower_case   \
    --data_dir $GLUE_DIR/MRPC/   \
    --max_seq_length 128   \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5   \
    --num_train_epochs 3.0  \
    --output_dir /tmp/mrpc_output/ \
    --overwrite_output_dir   \
    --overwrite_cache \
Training with these hyper-parameters gave us the following results:
  acc = 0.8823529411764706
  acc_and_f1 = 0.901702786377709
  eval_loss = 0.3418912578906332
  f1 = 0.9210526315789473
  global_step = 174
  loss = 0.07231863956341798
run_squad.py: Fine-tuning on SQuAD for question-answering
This example code fine-tunes BERT on the SQuAD dataset using distributed training on 8 V100 GPUs and Bert Whole Word Masking uncased model to reach a F1 > 93 on SQuAD:
python -m torch.distributed.launch --nproc_per_node=8 ./examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file $SQUAD_DIR/train-v1.1.json \
    --predict_file $SQUAD_DIR/dev-v1.1.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../models/wwm_uncased_finetuned_squad/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \
Training with these hyper-parameters gave us the following results:
python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ../models/wwm_uncased_finetuned_squad/predictions.json
{""exact_match"": 86.91579943235573, ""f1"": 93.1532499015869}
This is the model provided as bert-large-uncased-whole-word-masking-finetuned-squad.
run_generation.py: Text generation with GPT, GPT-2, CTRL, Transformer-XL and XLNet
A conditional generation script is also included to generate text from a prompt.
The generation script includes the tricks proposed by Aman Rusia to get high-quality generation with memory models like Transformer-XL and XLNet (include a predefined text to make short inputs longer).
Here is how to run the script with the small version of OpenAI GPT-2 model:
python ./examples/run_generation.py \
    --model_type=gpt2 \
    --length=20 \
    --model_name_or_path=gpt2 \
and from the Salesforce CTRL model:
python ./examples/run_generation.py \
    --model_type=ctrl \
    --length=20 \
    --model_name_or_path=ctrl \
    --temperature=0 \
    --repetition_penalty=1.2 \
Migrating from pytorch-transformers to transformers
Here is a quick summary of what you should take care of when migrating from pytorch-transformers to transformers.
Positional order of some models' keywords inputs (attention_mask, token_type_ids...) changed
To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models keywords inputs (attention_mask, token_type_ids...) has been changed.
If you used to call the models with keyword names for keyword arguments, e.g. model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids), this should not cause any change.
If you used to call the models with positional inputs for keyword arguments, e.g. model(inputs_ids, attention_mask, token_type_ids), you may have to double check the exact order of input arguments.
Migrating from pytorch-pretrained-bert to transformers
Here is a quick summary of what you should take care of when migrating from pytorch-pretrained-bert to transformers.
Models always output tuples
The main breaking change when migrating from pytorch-pretrained-bert to transformers is that every model's forward method always outputs a tuple with various elements depending on the model and the configuration parameters.
The exact content of the tuples for each model is detailed in the models' docstrings and the documentation.
In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in pytorch-pretrained-bert.
Here is a pytorch-pretrained-bert to transformers conversion example for a BertForSequenceClassification classification model:
# Let's load our model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# If you used to have this line in pytorch-pretrained-bert:
loss = model(input_ids, labels=labels)

# Now just use this line in transformers to extract the loss from the output tuple:
outputs = model(input_ids, labels=labels)
loss = outputs[0]

# In transformers you can also have access to the logits:
loss, logits = outputs[:2]

# And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', output_attentions=True)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs
Using hidden states
By enabling the configuration option output_hidden_states, it was possible to retrieve the last hidden states of the encoder. In pytorch-transformers as well as transformers the return value has changed slightly: all_hidden_states now also includes the hidden state of the embeddings in addition to those of the encoding layers. This allows users to easily access the embeddings final state.
Serialization
Breaking change in the from_pretrained() method:


Models are now set in evaluation mode by default when instantiated with the from_pretrained() method. To train them, don't forget to set them back in training mode (model.train()) to activate the dropout modules.


The additional *input and **kwargs arguments supplied to the from_pretrained() method used to be directly passed to the underlying model's class __init__() method. They are now used to update the model configuration attribute instead, which can break derived model classes built based on the previous BertForSequenceClassification examples. We are working on a way to mitigate this breaking change in #866 by forwarding the the model's __init__() method (i) the provided positional arguments and (ii) the keyword arguments which do not match any configuration class attributes.


Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method save_pretrained(save_directory) if you were using any other serialization method before.
Here is an example:
### Let's load a model and tokenizer
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

### Do some stuff to our model and tokenizer
# Ex: add new tokens to the vocabulary and embeddings of our model
tokenizer.add_tokens(['[SPECIAL_TOKEN_1]', '[SPECIAL_TOKEN_2]'])
model.resize_token_embeddings(len(tokenizer))
# Train our model
train(model)

### Now let's save our model and tokenizer to a directory
model.save_pretrained('./my_saved_model_directory/')
tokenizer.save_pretrained('./my_saved_model_directory/')

### Reload the model and the tokenizer
model = BertForSequenceClassification.from_pretrained('./my_saved_model_directory/')
tokenizer = BertTokenizer.from_pretrained('./my_saved_model_directory/')
Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules
The two optimizers previously included, BertAdam and OpenAIAdam, have been replaced by a single AdamW optimizer which has a few differences:

it only implements weights decay correction,
schedules are now externals (see below),
gradient clipping is now also external (see below).

The new optimizer AdamW matches PyTorch Adam optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping.
The schedules are now standard PyTorch learning rate schedulers and not part of the optimizer anymore.
Here is a conversion examples from BertAdam with a linear warmup and decay schedule to AdamW and the same schedule:
# Parameters:
lr = 1e-3
max_grad_norm = 1.0
num_training_steps = 1000
num_warmup_steps = 100
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

### Previously BertAdam optimizer was instantiated like this:
optimizer = BertAdam(model.parameters(), lr=lr, schedule='warmup_linear', warmup=warmup_proportion, t_total=num_training_steps)
### and used like this:
for batch in train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

### In Transformers, optimizer and schedules are splitted and instantiated like this:
optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler
### and used like this:
for batch in train_data:
    model.train()
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()
Citation
We now have a paper you can cite for the ü§ó Transformers library:
@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

",GitHub - huggingface/transformers: ü§ó Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.
46,Python,"12306 Ë¥≠Á•®Â∞èÂä©Êâã
pythonÁâàÊú¨

 2.7.10 - 2.7.15
 3.6 - 3.7.4
 2.7.9

Â∑≤ÊúâÂäüËÉΩ

 Ëá™Âä®ÊâìÁ†Å
 Ëá™Âä®ÁôªÂΩï
 ÂáÜÁÇπÈ¢ÑÂîÆÂíåÊç°Êºè
 Êô∫ËÉΩÂÄôË°•
 ÈÇÆ‰ª∂ÈÄöÁü•
 serverÈÖ±ÈÄöÁü•

‰æùËµñÂ∫ì

È™åËØÅÁ†ÅÁõÆÂâçÂèØ‰ª•Êú¨Âú∞ËØÜÂà´ÔºåÈúÄË¶Å‰∏ãËΩΩÊ®°ÂûãÔºåÊîæ‰∫éÈ°πÁõÆÊ†πÁõÆÂΩïÔºåÂÖ®ÈÉ®‰ª£Á†ÅÊù•Ê∫ê‰∫éÊ≠§È°πÁõÆ ‰º†ÈÄÅÈó®ÔºåË°®Á§∫ÊÑüË∞¢
  PS: 
  1. Ê®°Âûã‰∏ãËΩΩÈìæÊé•:https://pan.baidu.com/s/1rS155VjweWVWIJogakechA  ÂØÜÁ†Å:bmlm
     Áæ§ÈáåÈù¢‰πüÂèØ‰ª•‰∏ãËΩΩ
  2. git‰ªìÂ∫ì‰∏ãËΩΩÔºöhttps://github.com/testerSunshine/12306model.git


Ëá™ÊâòÁÆ°‰∫ëÊâìÁ†ÅÊúçÂä°Âô®Êê≠Âª∫Ôºö12306_code_server

Â¶ÇÊûúÂ§ßÂÆ∂ÊúâÁ©∫Èó≤ÁöÑÊúçÂä°Âô®ÔºåÂèØÊê≠Âª∫‰πãÂêéÂÜçËøô‰∏™ issues ÈáåÈù¢Â°´ÂÖ•Ëá™Â∑±ÁöÑÊúçÂä°Âô®(ËØ∑Ê≥®ÊÑèÊúçÂä°Âô®ÂÆâÂÖ®ÔºÅ)


È°πÁõÆ‰æùËµñÂåÖÊü•Áúã requirements.txt
ÂÆâË£ÖÊñπÊ≥ïx:

rootÁî®Êà∑(ÈÅøÂÖçÂ§öpythonÁéØÂ¢É‰∫ßÁîüÈóÆÈ¢ò): pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt
ÈùûrootÁî®Êà∑ÔºàÈÅøÂÖçÂÆâË£ÖÂíåËøêË°åÊó∂‰ΩøÁî®‰∫Ü‰∏çÂêåÁéØÂ¢ÉÔºâ: pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt



È°πÁõÆ‰ΩøÁî®ËØ¥Êòé

ÊúçÂä°Âô®ÂêØÂä®:

‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂

ÂèØ‰ª•ÈÖçÁΩÆÈÇÆÁÆ±,ÈÖçÁΩÆÈÇÆÁÆ±ÁöÑÊ†ºÂºèÂú®ÈÖçÁΩÆÈáåÈù¢ÂèØ‰ª•ÁúãÂà∞ex
ÂèØ‰ª•ÈÖçÁΩÆserverÈÖ±ÊèêÈÜíÔºàÊé®ËçêÔºâÈÖçÁΩÆÊïôÁ®ã
ÈÖçÁΩÆÈÖçÁΩÆÊñá‰ª∂ÁöÑÊó∂ÂÄôÔºåÈúÄÊ≥®ÊÑèÁ©∫Ê†ºÂíåÈÅµÂæ™pythonËØ≠Ê≥ïÊ†ºÂºè


ËøêË°åÊ†πÁõÆÂΩïsudo python run.pyÔºåÂç≥ÂèØÂºÄÂßã


Â¶ÇÊûú‰Ω†ÁöÑÊúçÂä°Âô®ÂÆâË£Ö‰∫Üdocker‰∏édocker-compose, ÈÇ£‰πàÂ∞±ÂèØ‰ª•ÈÄöËøádocker-composeËøõË°åÂêØÂä®,docker.shËÑöÊú¨ÂØπÊ≠§ËøõË°å‰∫ÜÂ∞ÅË£ÖÔºåÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ãÂëΩ‰ª§ËøõË°åÂêØÂä®

1„ÄÅsudo ./docker.sh run #ÂàõÂª∫‰∏Ä‰∏™ÈïúÂÉèÂπ∂ÂêØÂä®ÂÆπÂô®ÔºåÂ¶ÇÊûúÈïúÂÉèÂ∑≤ÁªèÂàõÂª∫Ëøá‰∫Ü‰ºöÁõ¥Êé•ÂêØÂä®ÂÆπÂô®„ÄÇ
2„ÄÅsudo ./docker.sh restart #‰øÆÊîπÈÖçÁΩÆÊñá‰ª∂ÂêéÔºåÈÄöËøáÊ≠§ÂêçÂëΩ‰ª§ÂèØÈáçÊñ∞Âä†ËΩΩÂÆπÂô®ËøêË°å
3„ÄÅsudo ./docker.sh rm #Âà†Èô§ÂÆπÂô®
4„ÄÅsudo ./docker.sh drun #ÂêéÂè∞ËøêË°åÂÆπÂô®
5„ÄÅsudo ./docker.sh logs #Âú®ÂêéÂè∞ËøêË°åÊó∂ÔºåÈÄöËøáÊ≠§ÂëΩ‰ª§Êü•ÁúãËøêË°åÁöÑÂÜÖÂÆπ
Ê≥®: ÈíàÂØπÊ≤°ÊúâdockerÁéØÂ¢ÉÁöÑÂêåÂ≠¶Êèê‰æõ‰∫ÜdockerÂÆâË£ÖËÑöÊú¨(centos7)
- sudo ./docker_install_centos.sh
Ê≥®: Ëã•Âè™ÊúâdockerÊ≤°Êúâdocker-compose. ÂèØÈÄöËøápip install docker-composeËøõË°å‰∏ãËΩΩ



ÁõÆÂΩïÂØπÂ∫îËØ¥Êòé

agency - cdn‰ª£ÁêÜ
config - È°πÁõÆÈÖçÁΩÆ
verify - Ëá™Âä®ÊâìÁ†Å
init - È°πÁõÆ‰∏ªËøêË°åÁõÆÂΩï
inter - Êé•Âè£
myException - ÂºÇÂ∏∏
myUrllib  requestÁΩëÁªúËØ∑Ê±ÇÂ∫ì

ÊÄùË∑ØÂõæ



È°πÁõÆÂ£∞ÊòéÔºö

Êú¨ËΩØ‰ª∂Âè™‰æõÂ≠¶‰π†‰∫§ÊµÅ‰ΩøÁî®ÔºåÂãø‰Ωú‰∏∫ÂïÜ‰∏öÁî®ÈÄîÔºå‰∫§ÊµÅÁæ§Âè∑

1Áæ§Ôºö286271084(Â∑≤Êª°)
2Áæ§Ôºö649992274(Â∑≤Êª°)
3Áæ§Ôºö632501142(Â∑≤Êª°)
4Áæ§: 606340519(Â∑≤Êª°)
5Áæ§: 948526733(Â∑≤Êª°)
7Áæ§: 660689659(Â∑≤Êª°)
8Áæ§: 620629239(Â∑≤Êª°)
6Áæ§: 608792930(Êú™Êª°)
9Áæ§: 693035807(Êú™Êª°)


ËØ∑‰∏çË¶ÅÈáçÂ§çÂä†Áæ§Ôºå‰∏Ä‰∏™Áæ§Â∞±ÂèØ‰ª•‰∫ÜÔºåÊääÊú∫‰ºöÁïôÁªôÊõ¥Â§ö‰∫∫
ËøõÁæ§ÂÖàÁúãÂÖ¨ÂëäÔºÅÔºÅÔºÅËøõÁæ§ÂÖàÁúãÂÖ¨ÂëäÔºÅÔºÅÔºÅËøõÁæ§ÂÖàÁúãÂÖ¨ÂëäÔºÅÔºÅÔºÅ ÈáçË¶ÅÁöÑ‰∫ãÊÉÖËØ¥‰∏âÈÅç
ËÉΩ‰∏∫‰Ω†Êä¢Âà∞‰∏ÄÂº†ÂõûÂÆ∂ÁöÑÁ•®ÔºåÊòØÊàëÊúÄÂ§ßÁöÑÂøÉÊÑø

Êó•ÂøóÂàóÂ≠ê

ÊàêÂäülogÔºåÂ¶ÇÊûúÊòØË¥≠Á•®Â§±Ë¥•ÁöÑÔºåËØ∑Â∏¶‰∏äÂ§±Ë¥•ÁöÑlogÁªôÊàëÔºåÊàëÂ∞ΩÂäõÂ∏Æ‰Ω†Ë∞ÉÔºå‰πüÂèØÂä†Áæ§‰∏ÄËµ∑‰∫§ÊµÅÔºåÁ®ãÂ∫èÂè™ÊòØÂä†ÈÄü‰π∞Á•®ÁöÑËøáÁ®ãÔºåÂπ∂‰∏ç‰∏ÄÂÆöËÉΩ‰π∞Âà∞Á•®
Ê≠£Âú®Á¨¨355Ê¨°Êü•ËØ¢  ‰πòËΩ¶Êó•Êúü: 2018-02-12  ËΩ¶Ê¨°G4741,G2365,G1371,G1377,G1329 Êü•ËØ¢Êó†Á•®  ‰ª£ÁêÜËÆæÁΩÆ Êó†  ÊÄªËÄóÊó∂429ms
ËΩ¶Ê¨°: G4741 ÂßãÂèëËΩ¶Á´ô: ‰∏äÊµ∑ ÁªàÁÇπÁ´ô: ÈÇµÈò≥ ‰∫åÁ≠âÂ∫ß:Êúâ
Ê≠£Âú®Â∞ùËØïÊèê‰∫§ËÆ¢Á•®...
Â∞ùËØïÊèê‰∫§ËÆ¢Âçï...
Âá∫Á•®ÊàêÂäü
ÊéíÈòüÊàêÂäü, ÂΩìÂâç‰ΩôÁ•®ËøòÂâ©‰Ωô: 359 Âº†
Ê≠£Âú®‰ΩøÁî®Ëá™Âä®ËØÜÂà´È™åËØÅÁ†ÅÂäüËÉΩ
È™åËØÅÁ†ÅÈÄöËøá,Ê≠£Âú®Êèê‰∫§ËÆ¢Âçï
Êèê‰∫§ËÆ¢ÂçïÊàêÂäüÔºÅ
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -12 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -6 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -7 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -4 ms
ÊéíÈòüÁ≠âÂæÖÊó∂Èó¥È¢ÑËÆ°ËøòÂâ© -4 ms
ÊÅ≠ÂñúÊÇ®ËÆ¢Á•®ÊàêÂäüÔºåËÆ¢ÂçïÂè∑‰∏∫ÔºöEB52743573, ËØ∑Á´ãÂç≥ÊâìÂºÄÊµèËßàÂô®ÁôªÂΩï12306ÔºåËÆøÈóÆ‚ÄòÊú™ÂÆåÊàêËÆ¢Âçï‚ÄôÔºåÂú®30ÂàÜÈíüÂÜÖÂÆåÊàêÊîØ‰ªòÔºÅ



‰ΩøÁî®Â∏ÆÂä©(‰∏Ä‰∫õÂÆâË£ÖÈóÆÈ¢òÂíå‰ΩøÁî®ÂèçÈ¶àËæÉÂ§öÁöÑÈóÆÈ¢ò)Ôºö


ÊµãËØïÈÇÆÁÆ±ÊòØÂê¶ÂèØÁî® ÈÇÆÁÆ±ÈÖçÁΩÆÈóÆÈ¢òÁúãissues


Â≠¶ÁîüÁ•®issues Â≠¶ÁîüÁ•®‰øÆÊîπ


‰æùËµñÂÆâË£Ö‰∏çÂØπÁöÑÈóÆÈ¢òÔºàImportErrorÔºârequirements.txtÈóÆÈ¢ò


Ëã•Âø´Ë±ÜÂ≠êÁñëÈóÆ ÁÇπÊàë


IOError: „ÄêErrno 0„Äë Error ÈóÆÈ¢ò ÁÇπÊàë


ÊµãËØï‰∏ãÂçïÊé•Âè£ÊòØÂê¶ÂèØÁî®ÔºåÊúâ‰∏§‰∏™‰∏ãÂçïÊé•Âè£ÔºåÈöè‰æøÁî®Âì™‰∏™ÈÉΩok


Â¶ÇÊûú‰∏ãËΩΩÈ™åËØÅÁ†ÅËøáÊúüÊàñËÄÖ‰∏ãËΩΩÂ§±Ë¥•ÁöÑÈóÆÈ¢òÔºåÂ∫îËØ•ÊòØ12306Â∞ÅipÁöÑÁ≠ñÁï•ÔºåÂ§öÈáçËØïÂá†Ê¨°Ôºå12306Áé∞Âú®Â∞ÅÊúçÂä°Âô®(ÈòøÈáå‰∫ëÂíåËÖæËÆØ‰∫ë)ipÊØîËæÉ‰∏•ÈáçÔºåÂ∞ΩÈáè‰∏çË¶ÅÊîæÂú®ÊúçÂä°Âô®ÈáåÈù¢


ÁõÆÂâç12306ÂØπÊúçÂä°Âô®ipÊØîËæÉÊïèÊÑüÔºåÂ§ßÂÆ∂ËøòÊòØÂú®Ëá™Â∑±ÂÆ∂ÈáåÊåÇÁùÄÂêß


Ëá™Âä®Êõ¥Êç¢ipËΩØ‰ª∂ÁõÆÂâçÂ∑≤ÊîØÊåÅTPLINKÂíåÂ∞èÁ±≥Ë∑ØÁî±Âô®ÔºåÂè™ÈôêÂÆ∂Â∫≠ÁΩëÁªúÁÇπÊàëË∑≥ËΩ¨


ÊÑüË∞¢‰∏Ä‰∏ãÂ∞è‰ºô‰º¥ÂØπÊú¨È°πÁõÆÊèê‰æõÁöÑÂ∏ÆÂä©

@sun7127@126.com
@ Êâç
@MonsterTan
‰ª•ÂèäÊâÄÊúâ‰∏∫Ê≠§È°πÁõÆÊèê‰æõprÁöÑÂêåÂ≠¶

Êõ¥Êñ∞Êó•Âøó

Êõ¥Êñ∞Êó•Âøó

",GitHub - testerSunshine/12306: 12306Êô∫ËÉΩÂà∑Á•®ÔºåËÆ¢Á•®
47,Python,"



Apache MXNet (incubating) for Deep Learning



Master
Docs
License




             






Apache MXNet (incubating) is a deep learning framework designed for both efficiency and flexibility.
It allows you to mix symbolic and imperative programming
to maximize efficiency and productivity.
At its core, MXNet contains a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly.
A graph optimization layer on top of that makes symbolic execution fast and memory efficient.
MXNet is portable and lightweight, scaling effectively to multiple GPUs and multiple machines.
MXNet is more than a deep learning project. It is a collection of
blue prints and guidelines for building
deep learning systems, and interesting insights of DL systems for hackers.
Ask Questions

Please use discuss.mxnet.io for asking questions.
Please use mxnet/issues for reporting bugs.
Frequent Asked Questions

How to Contribute

Contribute to MXNet

What's New

Version 1.5.1 Release - MXNet 1.5.1 Patch Release.
Version 1.5.0 Release - MXNet 1.5.0 Release.
Version 1.4.1 Release - MXNet 1.4.1 Patch Release.
Version 1.4.0 Release - MXNet 1.4.0 Release.
Version 1.3.1 Release - MXNet 1.3.1 Patch Release.
Version 1.3.0 Release - MXNet 1.3.0 Release.
Version 1.2.0 Release - MXNet 1.2.0 Release.
Version 1.1.0 Release - MXNet 1.1.0 Release.
Version 1.0.0 Release - MXNet 1.0.0 Release.
Version 0.12.1 Release - MXNet 0.12.1 Patch Release.
Version 0.12.0 Release - MXNet 0.12.0 Release.
Version 0.11.0 Release - MXNet 0.11.0 Release.
Apache Incubator - We are now an Apache Incubator project.
Version 0.10.0 Release - MXNet 0.10.0 Release.
Version 0.9.3 Release - First 0.9 official release.
Version 0.9.1 Release (NNVM refactor) - NNVM branch is merged into master now. An official release will be made soon.
Version 0.8.0 Release
Updated Image Classification with new Pre-trained Models
Notebooks How to Use MXNet
MKLDNN for Faster CPU Performance
MXNet Memory Monger, Training Deeper Nets with Sublinear Memory Cost
Tutorial for NVidia GTC 2016
MXNet.js: Javascript Package for Deep Learning in Browser (without server)
Guide to Creating New Operators (Layers)
Go binding for inference
Amalgamation and Go Binding for Predictors - Outdated
Large Scale Image Classification

Contents

Website
Documentation
Blog
Code Examples
Installation
Features
Ecosystem

Features

Design notes providing useful insights that can re-used by other DL projects
Flexible configuration for arbitrary computation graph
Mix and match imperative and symbolic programming to maximize flexibility and efficiency
Lightweight, memory efficient and portable to smart devices
Scales up to multi GPUs and distributed setting with auto parallelism
Support for Python, Scala, C++, Java, Clojure, R, Go, Javascript, Perl, Matlab, and Julia
Cloud-friendly and directly compatible with AWS S3, AWS Deep Learning AMI, AWS SageMaker, HDFS, and Azure

License
Licensed under an Apache-2.0 license.
Reference Paper
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
Bing Xu, Chiyuan Zhang, and Zheng Zhang.
MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems.
In Neural Information Processing Systems, Workshop on Machine Learning Systems, 2015
History
MXNet emerged from a collaboration by the authors of cxxnet, minerva, and purine2. The project reflects what we have learned from the past projects. MXNet combines aspects of each of these projects to achieve flexibility, speed, and memory efficiency.
","GitHub - apache/incubator-mxnet: Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more"
48,Python,"Docker Compose

‚ùóÔ∏è The docker-compose project announces that as Python 2 reaches it's EOL, versions 1.25.x will be the last to support it. For more information, please refer to this issue.
Compose is a tool for defining and running multi-container Docker applications.
With Compose, you use a Compose file to configure your application's services.
Then, using a single command, you create and start all the services
from your configuration. To learn more about all the features of Compose
see the list of features.
Compose is great for development, testing, and staging environments, as well as
CI workflows. You can learn more about each case in
Common Use Cases.
Using Compose is basically a three-step process.

Define your app's environment with a Dockerfile so it can be
reproduced anywhere.
Define the services that make up your app in docker-compose.yml so
they can be run together in an isolated environment.
Lastly, run docker-compose up and Compose will start and run your entire app.

A docker-compose.yml looks like this:
version: '2'

services:
  web:
    build: .
    ports:
     - ""5000:5000""
    volumes:
     - .:/code
  redis:
    image: redis

For more information about the Compose file, see the
Compose file reference.
Compose has commands for managing the whole lifecycle of your application:

Start, stop and rebuild services
View the status of running services
Stream the log output of running services
Run a one-off command on a service

Installation and documentation

Full documentation is available on Docker's website.
Code repository for Compose is on GitHub.
If you find any problems please fill out an issue. Thank you!

Contributing

Want to help build Compose? Check out our contributing documentation.
Releasing
Releases are built by maintainers, following an outline of the release process.
",GitHub - docker/compose: Define and run multi-container applications with Docker
49,Python,"
Unified access to the best community driven cheat sheets repositories of the world.
Let's imagine for a moment that there is such a thing as an ideal cheat sheet.
What should it look like?
What features should it have?

Concise ‚Äî It should only contain the things you need, and nothing else.
Fast ‚Äî It should be possible to use it instantly.
Comprehensive ‚Äî It should contain answers for every possible question.
Universal ‚Äî It should be available everywhere, anytime, without any preparations.
Unobtrusive ‚Äî It should not distract you from your main task.
Tutoring ‚Äî It should help you to learn the subject.
Inconspicuous ‚Äî It should be possible to use it completely unnoticed.

Such a thing exists.

Features
cheat.sh

Has a simple curl/browser interface.
Covers 56 programming languages, several DBMSes, and more than 1000 most important UNIX/Linux commands.
Provides access to the best community driven cheat sheets repositories in the world, on par with StackOverflow.
Available everywhere, no installation needed.
Ultrafast, returns answers within 100 ms, as a rule.
Has a convenient command line client, cht.sh, that is very advantageous and helpful, though not mandatory.
Can be used directly from code editors, without opening a browser and not switching your mental context.
Supports a special stealth mode where it can be used fully invisibly without ever touching a key and and making sounds.




Contents

Features
Usage
Command line client, cht.sh

Installation
Client usage
Tab-completion
Stealth mode


Self-Hosting

Docker


Editors integration

Vim
Emacs
Visual Studio Code
Sublime
IntelliJ IDEA


Special pages
Search
Programming languages cheat sheets
Cheat sheets sources
How to contribute

How to edit a cheat sheet
How to add a cheat sheet
How to add a cheat sheet repository



Usage
To get a cheat sheet for a UNIX/Linux command from a command line, query the service using curl or any other HTTP/HTTPS client
specifying the name of the command in the query:
    curl cheat.sh/tar
    curl cht.sh/curl
    curl https://cheat.sh/rsync
    curl https://cht.sh/tr

As you can see, you can use both HTTPS and HTTP to access the service, and both the long (cheat.sh) and the short (cht.sh) service names.
Here tar, curl, rsync, and tr are names of the UNIX/Linux commands you want to get cheat sheets for.
If you don't know the name of the command you need, you can search for it using the ~KEYWORD notation.
For example, to see how you can make snapshots of a filesystem/volume/something else:
    curl cht.sh/~snapshot




The programming language cheat sheets are located in special namespaces dedicated to them.
    curl cht.sh/go/Pointers
    curl cht.sh/scala/Functions
    curl cht.sh/python/lambda

To get the list of available programming language cheat sheets, use the special query :list:
    curl cht.sh/go/:list

Almost each programming language has a special page named :learn
that describes the language basics (that's a direct mapping from the ""Learn X in Y"" project).
It could be a good starting point if you've just started learning a language.
If there is no cheat sheet for a programming language query (and it is almost always the case),
it is generated on the fly, based on available cheat sheets and answers on StackOverflow.
Of course, there is no guarantee that the returned cheat sheet will be a 100% hit, but it is almost always exactly what you are looking for.
Try these (and your own) queries to get the impression of that, what the answers look like:
    curl cht.sh/go/reverse+a+list
    curl cht.sh/python/random+list+elements
    curl cht.sh/js/parse+json
    curl cht.sh/lua/merge+tables
    curl cht.sh/clojure/variadic+function

If you don't like an answer for your queries, you can pick another one. For that, repeat the query with an additional parameter /1, /2 etc. appended:
    curl cht.sh/python/random+string
    curl cht.sh/python/random+string/1
    curl cht.sh/python/random+string/2

Cheat sheets are formatted as code of the queried programming language (at least we are trying our best to do so)
so they can be pasted into a program in this language directly. Text comments, if there are any, are formatted according to the language syntax.
    $ curl cht.sh/lua/table+keys
    -- lua: retrieve list of keys in a table

    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end

    --[[
       [ Note that you cannot guarantee any order in keyset. If you want the
       [ keys in sorted order, then sort keyset with table.sort(keyset).
       [ 
       [ [lhf] [so/q/12674345] [cc by-sa 3.0]
       ]]

If you don't need text comments in the answer, you can eliminate them
using a special option ?Q:
    $ curl cht.sh/lua/table+keys?Q
    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end
And if you don't need syntax highlighting, switch it off using ?T.
You can combine the options together:
    curl cht.sh/go/reverse+a+list?Q
    curl cht.sh/python/random+list+elements?Q
    curl cht.sh/js/parse+json?Q
    curl cht.sh/lua/merge+tables?QT
    curl cht.sh/clojure/variadic+function?QT

Full list of all options described below and in /:help.
Try your own queries. Follow these rules:

Try to be more specific (/python/append+file is better than /python/file and /python/append).
Ask practical question if possible (yet theoretical question are possible too).
Ask programming language questions only; specify the name of the programming language as the section name.
Separate words with + instead of spaces.
Do not use special characters, they are ignored anyway.
If you want to eliminate cheat sheets containing some word, add it to the query with +-: python/multiply+matrices+-numpy

Read more about the programming languages queries below.
Command line client, cht.sh
The cheat.sh service has its own command line client (cht.sh) that
has several useful features compared to querying the service directly with curl:

Special shell mode with a persistent queries context and readline support.
Queries history.
Clipboard integration.
Tab completion support for shells (bash, fish, zsh).
Stealth mode.

Installation
To install the client:
    mkdir -p ~/bin/
    curl https://cht.sh/:cht.sh > ~/bin/cht.sh
    chmod +x ~/bin/cht.sh

or to install it globally (for all users):
    curl https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh
    chmod +x /usr/local/bin/cht.sh

Note: The package ""rlwrap"" is a required dependency to run in shell mode. Install this using sudo apt install rlwrap
Client usage
Now, you can use cht.sh instead of curl, and write your queries in more natural way,
with spaces instead of +:
    $ cht.sh go reverse a list
    $ cht.sh python random list elements
    $ cht.sh js parse json

It is even more convenient to start the client in a special shell mode:
    $ cht.sh --shell
    cht.sh> go reverse a list

If all your queries are about the same language, you can change the context
and spare repeating the programming language name:
    $ cht.sh --shell
    cht.sh> cd go
    cht.sh/go> reverse a list

or even start the client in this context:
    $ cht.sh --shell go
    cht.sh/go> reverse a list
    ...
    cht.sh/go> join a list
    ...

If you want to change the context, you can do it with the cd command,
or if you want do a single query for some other language, just prepend it with /:
    $ cht.sh --shell go
    ...
    cht.sh/go> /python dictionary comprehension
    ...

If you want to copy the last answer into the clipboard, you can
use the c (copy) command, or C (ccopy, without comments).
    cht.sh/python> append file
    #  python - How do you append to a file?

    with open(""test.txt"", ""a"") as myfile:
        myfile.write(""appended text"")
    cht.sh/python> C
    copy: 2 lines copied to the selection

Type help for other internal cht.sh commands.
	cht.sh> help
	help    - show this help
	hush    - do not show the 'help' string at start anymore
	cd LANG - change the language context
	copy    - copy the last answer in the clipboard (aliases: yank, y, c)
	ccopy   - copy the last answer w/o comments (cut comments; aliases: cc, Y, C)
	exit    - exit the cheat shell (aliases: quit, ^D)
	id [ID] - set/show an unique session id (""reset"" to reset, ""remove"" to remove)
	stealth - stealth mode (automatic queries for selected text)
	update  - self update (only if the scriptfile is writeable)
	version - show current cht.sh version
	/:help  - service help
	QUERY   - space separated query staring (examples are below)
				  cht.sh> python zip list
				  cht.sh/python> zip list
				  cht.sh/go> /python zip list

The cht.sh client has its configuration file which is located at ~/.cht.sh/cht.sh.conf
(location of the file can be overriden by the environment variable CHTSH_CONF).
Use it to specify query options that you would use with each query.
For example, to switch syntax highlighting off create the file with the following
content:
CHTSH_QUERY_OPTIONS=""T""

Or if you want to use a special syntax highlighting theme:
CHTSH_QUERY_OPTIONS=""style=native""

(curl cht.sh/:styles-demo to see all supported styles).
Other cht.sh configuration parameters:
CHTSH_CURL_OPTIONS=""-A curl""        # curl options used for cht.sh queries
CHTSH_URL=https://cht.sh            # URL of the cheat.sh server

Tab completion
Bash Tab completion
To activate tab completion support for cht.sh, add the :bash_completion script to your ~/.bashrc:
    $ curl https://cheat.sh/:bash_completion > ~/.bash.d/cht.sh
    $ . ~/.bash.d/cht.sh
    $ # and add . ~/.bash.d/cht.sh to ~/.bashrc

ZSH Tab completion
To activate tab completion support for cht.sh, add the :zsh script to the fpath in your ~/.zshrc:
    $ curl https://cheat.sh/:zsh > ~/.zsh.d/_cht
    $ echo 'fpath=(~/.zsh.d/ $fpath)' >> ~/.zshrc
    $ # Open a new shell to load the plugin

Stealth mode
Being used fully unnoticed is one of the most important property of any cheat sheet.
cheat.sh can be used completely unnoticed too. The cheat.sh client, cht.sh, has
a special mode, called stealth mode. Using that, you don't even need to touch your
keyboard to open a cheat sheet.
In this mode, as soon as you select some text with the mouse (and thus adding it
into the selection buffer of X Window System or into the clipboard) it's used
as a query string for cheat.sh, and the correspondent cheat sheet is automatically shown.
Let's imagine, that you are having an online interview, where your interviewer asks you
some questions using a shared document (say Google Docs) and you are supposed
to write your coding answers there (it's possible too that you'll type in the questions
on your own, just to show to the interviewer that you've heard it right).
When using the stealth mode of cht.sh, the only thing you need to do in order to see
a cheat sheet for some question, is to select the question using the mouse.
If you don't want any text in the answers and the only thing you need is code,
use the Q option when starting the stealth mode.



You: Hi!                                            | $ cht.sh --shell python
She: Hi!                                            | cht.sh/python> stealth Q
She: Are you ready for a small interview?           | stealth: you are in the stealth mode; select any text
She: Just a couple of questions                     | stealth: selections longer than 5 words are ignored
She: We will talk about python                      | stealth: query arguments: ?Q
She: Let's start from something simple.             | stealth: use ^C to leave this mode
She: Do you know how to reverse a list in python?   |
You: Sure                                           |
You: (selecting ""reverse a list"")                   | stealth: reverse a list
                                                    | reverse_lst = lst[::-1]
You: lst[::-1]?                                     |
She: Good.                                          |
She: Do you know how to chain a list of lists?      |
You: (selecting ""chain a list of lists"")            | stealth: chain a list of lists
                                                    | import itertools
                                                    | a = [[""a"",""b""], [""c""]]
                                                    | print list(itertools.chain.from_iterable(a))
You: May I use external modules?                    |
She: What module do you want to use?                |
You: itertools                                      |
She: Yes, you may use it                            |
You: Ok, then:                                      |
You: itertools.chain.from_iterable(a)               |
She: Good. Let's try something harder.              |
She: What about quicksort implementation?           |
You: (selecting ""quicksort implementation"")         | stealth: quicksort implementation
You: Let me think about it.                         | (some big and clumsy lowlevel implementation shown)
You: Well...(starting typing it in)                 | def sort(array=[12,4,5,6,7,3,1,15]):
                                                    |     less = []
She: (seeing your ugly pascal style)                |     equal = []
She: Could you write it more concise?               |     greater = []
                                                    |     if len(array) > 1:
You: What do you mean?                              |         pivot = array[0]
                                                    |         for x in array:
She: I mean,                                        |             if x < pivot: less.append(x)
She: do you really need all these ifs and fors?     |             if x == pivot: equal.append(x)
She: Could you maybe just use filter instead?       |             if x > pivot: greater.append(x)
                                                    |         return sort(less)+equal+sort(greater)
You: quicksort with filter?                         |     else:
                                                    |         return array
She: Yes                                            |
You: (selecting ""quicksort with filter"")            | stealth: quicksort with filter
You: Ok, I will try.                                | return qsort(filter(lt, L[1:]))+[pivot] \
You: Something like this?                           |     +qsort(filter(ge, L[1:]))
You: qsort(filter(lt, L[1:]))+[pivot] \             |
       + qsort(filter(ge, L[1:]))                   |
                                                    |
She: Yes! Perfect! Exactly what I wanted to see!    |
                                                    |


Of course, this is just for fun, and you should never cheat in your coding interviews,
because you know what happens when you do.

Windows command line client
You can access cheat.sh from Windows command line too.
Use cheat.sh command line client for that: cht.exe.
It supports:

output colorization;
command line options;
its own configuration file.

You can also use scoop command-line installer for Windows to get it:
scoop install cht
Self-Hosting
Docker
Currently, the easiest way to get a self-hosted instance running is by using the docker-compose.yml file provided in the extra/docker folder.
This pulls down the latest image with baked in cheatsheets and starts the app and a Redis instance to back it, making the service available on port 8002 of the local host. This is currently an early implementation and should probably not be used for anything outside of internal/dev/personal use right now.
Editors integration
You can use cheat.sh directly from the editor
(Emacs, Sublime, Vim, and Visual Studio Code are currently supported;
not all features are supported by all plugins though; see below).
Instead of opening your browser, googling, browsing Stack Overflow
and eventually copying the code snippets you need into the clipboard
and later pasting them into the editor,
you can achieve the same instantly and without leaving the editor at all!
Here is what it looks like in Vim:


If you have a question while editing a program, you can just type
your question directly in the buffer and press <leader>KK. You will get
the answer to your question in pager. (with <leader>KB you'll get the answer
in a separate buffer).


If you like the answer, you can manually paste it from the buffer or
the pager, or if you are lazy you can use <leader>KP to paste it below/under
your question (or replace you question using <leader>KR). If you want the
answer without the comments, <leader>KC replays the last query
toggling them.


If you use some static analysis plugin such as syntastic (for Vim), you can use
its warning and error messages as cheat.sh queries: place the cursor on the problem line
and press <leader>KE: explanation for the warning will be opened in a new buffer.
Features supported by cheat.sh plugins for different editors:



Feature
Emacs
Sublime
Vim
VSCode
IDEA




Command queries
‚úì
‚úì
‚úì
‚úì
‚úì


Queries from buffer


‚úì
‚úì



Toggle comments


‚úì
‚úì
‚úì


Prev/next answer


‚úì
‚úì
‚úì


Multiple answers

‚úì


‚úì


Warnings as queries


‚úì




Queries history


‚úì
‚úì



Session id


‚úì




Configurable server
‚úì

‚úì
‚úì




Vim

cheat.sh-vim ‚Äî Vim support

Here is Vim configuration example:
"" some configuration above ...

let mapleader="" ""

call vundle#begin()
Bundle 'gmarik/vundle'
Bundle 'scrooloose/syntastic'
Bundle 'dbeniamine/cheat.sh-vim'
call vundle#end()

let g:syntastic_javascript_checkers = [ 'jshint' ]
let g:syntastic_ocaml_checkers = ['merlin']
let g:syntastic_python_checkers = ['pylint']
let g:syntastic_shell_checkers = ['shellcheck']

"" some configuration below ...
In this example, several Vim plugins are used:

gmarik/vundle ‚Äî Vim plugin manager
scrooloose/syntastic ‚Äî Syntax checking plugin
cheat.sh-vim ‚Äî Vim support

Syntastic shows warnings and errors (found by code analysis tools: jshint, merlin, pylint, shellcheckt etc.), and cheat.sh-vim` shows you explanations for the errors and warnings
and answers on programming languages queries written in the editor.
Watch a demo, where the most important features of the cheat.sh Vim plugin are shown (5 Min):



Or, if you want to scroll and/or pause, the same on YouTube:



Emacs

cheat-sh.el ‚Äî Emacs support (available also at cheat.sh/:emacs)
cheat.sh/:emacs-ivy ‚Äî Emacs support for ivy users


Visual Studio Code

vscode-snippet
Install it from VSCode Marketplace

Usage:

Hit ‚åò Command + ‚áß Shift + p
Run Snippet: Find.
Type your query and hit enter.


(GIF courtesy: Matthias Endler, @mre)
Sublime

cheat.sh-sublime-plugin

Usage:

Write your query string.
Select the query string.
Press Cmd + ‚áß Shift + B to replace the selected query string by the answer generated from cht.sh.


(GIF courtesy: Gaurav Kukreja, @gauravk-in)
IntelliJ IDEA

idea-cheatsh-plugin
Install from idea plugins marketplace

Usage:

Write query string
Select the query string
Press keyboard shortcut Alt + C , S to replace the selected query string by the answer


(GIF courtesy: Szymon Przebierowski, @szymonprz)
Special pages
There are several special pages that are not cheat sheets.
Their names start with colon and have special meaning.
Getting started:
    :help               description of all special pages and options
    :intro              cheat.sh introduction, covering the most important usage questions
    :list               list all cheat sheets (can be used in a subsection too: /go/:list)

Command line client cht.sh and shells support:
    :cht.sh             code of the cht.sh client
    :bash_completion    bash function for tab completion
    :bash               bash function and tab completion setup
    :fish               fish function and tab completion setup
    :zsh                zsh function and tab completion setup

Editors support:
    :vim                cheat.sh support for Vim
    :emacs              cheat.sh function for Emacs
    :emacs-ivy          cheat.sh function for Emacs (uses ivy)

Other pages:
    :post               how to post new cheat sheet
    :styles             list of color styles
    :styles-demo        show color styles usage examples

Search
To search for a keyword, use the query:
    /~keyword

In this case search is not recursive ‚Äî it is conducted only in a page of the specified level.
For example:
    /~snapshot          look for snapshot in the first level cheat sheets
    /scala/~currying     look for currying in scala cheat sheets

For a recursive search in all cheat sheets, use double slash:
    /~snapshot/r         look for snapshot in all cheat sheets

You can use special search options after the closing slash:
    /~shot/bi           case insensitive (i), word boundaries (b)

List of search options:
    i   case insensitive search
    b   word boundaries
    r   recursive search

Programming languages cheat sheets
Cheat sheets related to programming languages
are organized in namespaces (subdirectories), that are named according
to the programming language.
For each supported programming language
there are several special cheat sheets: its own sheet, hello, :list and :learn.
Say for lua it will look like:
    lua
    lua/hello
    lua/:list
    lua/:learn

Some languages has the one-liners-cheat sheet, 1line:
    perl/1line


hello describes how you can start with the language ‚Äî install it if needed, build and run its programs, and it shows the ""Hello world"" program written in the language;
:list shows all topics related to the language
:learn shows a learn-x-in-minutes language cheat sheet perfect for getting started with the language.
1line is a collection of one-liners in this language
weirdness is a collection of examples of weird things in this language


At the moment, cheat.sh covers the 58 following programming languages (alphabetically sorted):



Prefix
Language
Basics
One-liners
Weirdness
StackOverflow




arduino/
Arduino



‚úì


assembly/
Assembly



‚úì


awk/
AWK
‚úì


‚úì


bash/
Bash
‚úì


‚úì


basic/
BASIC



‚úì


bf/
Brainfuck
‚úì


‚úì


c/
C
‚úì


‚úì


chapel/
Chapel
‚úì


‚úì


clean/
Clean



‚úì


clojure/
Clojure
‚úì


‚úì


coffee/
CoffeeScript
‚úì


‚úì


cpp/
C++
‚úì


‚úì


csharp/
C#
‚úì


‚úì


d/
D
‚úì


‚úì


dart/
Dart
‚úì


‚úì


delphi/
Dephi



‚úì


dylan/
Dylan
‚úì


‚úì


eiffel/
Eiffel



‚úì


elixir/
Elixir
‚úì


‚úì


elisp/
ELisp
‚úì


‚úì


elm/
Elm
‚úì


‚úì


erlang/
Erlang
‚úì


‚úì


factor/
Factor
‚úì


‚úì


fortran/
Fortran
‚úì


‚úì


forth/
Forth
‚úì


‚úì


fsharp/
F#
‚úì


‚úì


go/
Go
‚úì


‚úì


groovy/
Groovy
‚úì


‚úì


haskell/
Haskell
‚úì


‚úì


java/
Java
‚úì


‚úì


js/
JavaScript
‚úì
‚úì
‚úì
‚úì


julia/
Julia
‚úì


‚úì


kotlin/
Kotlin
‚úì


‚úì


latex/
LaTeX
‚úì


‚úì


lisp/
Lisp
‚úì


‚úì


lua/
Lua
‚úì


‚úì


matlab/
MATLAB
‚úì


‚úì


nim/
Nim
‚úì


‚úì


ocaml/
OCaml
‚úì


‚úì


octave/
Octave
‚úì


‚úì


perl/
Perl
‚úì
‚úì

‚úì


perl6/
Perl 6
‚úì
‚úì

‚úì


php/
PHP
‚úì


‚úì


pike/
Pike



‚úì


python/
Python
‚úì
‚úì

‚úì


python3/
Python 3
‚úì


‚úì


r/
R
‚úì


‚úì


racket/
Racket
‚úì


‚úì


ruby/
Ruby
‚úì


‚úì


rust/
Rust
‚úì


‚úì


scala/
Scala
‚úì


‚úì


scheme/
Scheme
‚úì


‚úì


solidity/
Solidity
‚úì


‚úì


swift/
Swift
‚úì


‚úì


tcsh/
Tcsh
‚úì


‚úì


tcl/
Tcl
‚úì


‚úì


objective-c/
Objective-C
‚úì


‚úì


vb/
VisualBasic
‚úì


‚úì


vbnet/
VB.Net
‚úì


‚úì



And several other topics, that are though related to programming,
are not programming languages:



Prefix
Topic
Basics
StackOverflow




cmake/
CMake
‚úì
‚úì


django/
Django

‚úì


flask/
Flask

‚úì


git/
Git
‚úì
‚úì



Cheat sheets sources
Instead of creating yet another mediocre cheat sheet repository,
we are concentrating our efforts on creation of a unified
mechanism to access selected existing well developed and good maintained
cheat sheet repositories covering topics of our interest:
programming and operating systems usage.
cheat.sh uses selected community driven cheat sheet repositories
and information sources, maintained by thousands of users, developers and authors
all over the world
(in the Users column number of contributors/number of stars is shown):



Cheat sheets
Repository
Users
Creation Date




UNIX/Linux, programming
cheat.sheets
38/223
May 1, 2017


UNIX/Linux commands
tldr-pages/tldr
760/23158
Dec 8, 2013


UNIX/Linux commands
chrisallenlane/cheat
131/5240
Jul 28, 2013


Programming languages
adambard/learnxinyminutes-docs
1246/6748
Jun 23, 2013


Go
a8m/go-lang-cheat-sheet
31/4039
Feb 9, 2014


Perl
pkrumnis/perl1line.txt
5/190
Nov 4, 2011


Programming languages
StackOverflow
9M
Sep 15, 2008



Pie diagram reflecting cheat sheets sources distribution (by number of cheat sheets on cheat.sh originating from a repository):

How to contribute
How to edit a cheat sheet
If you want to edit a cheat.sh cheat sheet, you should edit it in the upstream repository.
You will find the name of the source repository in a browser when you open a cheat sheet.
There are two github buttons at the bottom of the page: the second one is the button
of the repository, which belongs the current cheat sheet.
You can edit the cheat sheet directly in your browser (you need a github account for it).
There is an edit button in the top right corner. If you click on it, an editor will be open.
There you will change the cheat sheet (under the hood: the upstrem repository is forked, your changes are
committed in the forked repository, a pull request to the upstream repository owner is sent).

How to add a cheat sheet
If you want to add a cheat sheet, you have one of the following
ways:

Add it to one of the external cheat sheets repositories; you should decide on your own what is the best repository for your cheat sheet;
Add it to the local cheat.sh repository (cheat.sheets) on github (fork, commit, pull request);
Post it on cheat.sh using curl or a web browser (cheat.sh/:post).

If you want to change an existing cheat sheet,
you have to find the original repository (when you open a cheat sheet in a browser,
you see the repository's github button in the bottom of the cheat sheet),
the cheat sheet is coming from, and change it there.
After some time the changes will be synchronized on cheat.sh.
How to add a cheat sheet repository
If you want to add a cheat sheet repository to cheat.sh, please open an issue:

Add a new repository

Please specify the name of the repository, and give its short description.
",GitHub - chubin/cheat.sh: the only cheat sheet you need
50,Python,"Algo VPN



Algo VPN is a set of Ansible scripts that simplify the setup of a personal Wireguard and IPSEC VPN. It uses the most secure defaults available, works with common cloud providers, and does not require client software on most devices. See our release announcement for more information.
Features

Supports only IKEv2 with strong crypto (AES-GCM, SHA2, and P-256) and WireGuard
Generates Apple profiles to auto-configure iOS and macOS devices
Includes a helper script to add and remove users
Blocks ads with a local DNS resolver (optional)
Sets up limited SSH users for tunneling traffic (optional)
Based on current versions of Ubuntu and strongSwan
Installs to DigitalOcean, Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, OpenStack, CloudStack, Hetzner Cloud, or your own Ubuntu server

Anti-features

Does not support legacy cipher suites or protocols like L2TP, IKEv1, or RSA
Does not install Tor, OpenVPN, or other risky servers
Does not depend on the security of TLS
Does not require client software on most platforms
Does not claim to provide anonymity or censorship avoidance
Does not claim to protect you from the FSB, MSS, DGSE, or FSM

Deploy the Algo Server
The easiest way to get an Algo server running is to run it on your local system and let it set up a new virtual machine in the cloud for you.


Setup an account on a cloud hosting provider. Algo supports DigitalOcean (most user friendly), Amazon Lightsail, Amazon EC2, Vultr, Microsoft Azure, Google Compute Engine, Scaleway, DreamCompute or other OpenStack-based cloud hosting, Exoscale or other CloudStack-based cloud hosting,  or Hetzner Cloud.


Get a copy of Algo. The Algo scripts will be installed on your local system. There are two ways to get a copy:


Download the ZIP file. Unzip the file to create a directory named algo-master containing the Algo scripts.


Run the command git clone https://github.com/trailofbits/algo.git to create a directory named algo containing the Algo scripts.




Install Algo's core dependencies. Algo requires that Python 3.6 or later and at least one supporting package are installed on your system.


macOS: Apple does not provide a suitable version of Python 3 with macOS. Here are two ways to obtain one:


Use the Homebrew package manager. After installing Homebrew install Python 3 by running brew install python3.


Download and install the latest stable Python 3.7 package (currently Python 3.8 will not work). Be sure to run the included Install Certificates command from Finder.


See Deploy from macOS for more detailed information on installing Python 3 on macOS.
Once Python 3 is installed on your Mac, from Terminal run:
python3 -m pip install --upgrade virtualenv


Linux: Recent releases of Ubuntu, Debian, and Fedora come with Python 3 already installed. Make sure your system is up-to-date and install the supporting package(s):

Ubuntu and Debian:

sudo apt install -y python3-virtualenv

Fedora:

sudo dnf install -y python3-virtualenv

Red Hat and CentOS 7 and later (for earlier versions see this documentation):

sudo yum -y install epel-release
sudo yum install -y python36-virtualenv


Windows: Use the Windows Subsystem for Linux (WSL) to create your own copy of Ubuntu running under Windows from which to install and run Algo. See the Windows documentation.




Install Algo's remaining dependencies. You'll need to run these commands from the Algo directory each time you download a new copy of Algo. In a Terminal window cd into the algo-master (ZIP file) or algo (git clone) directory and run:
python3 -m virtualenv --python=""$(command -v python3)"" .env &&
  source .env/bin/activate &&
  python3 -m pip install -U pip virtualenv &&
  python3 -m pip install -r requirements.txt
On Fedora add the option --system-site-packages to the first command above. On macOS install the C compiler if prompted.


List the users to create. Open the file config.cfg in your favorite text editor. Specify the users you wish to create in the users list. Create a unique user for each device you plan to connect to your VPN. If you want to be able to add or delete users later, you must select yes at the Do you want to retain the keys (PKI)? prompt during the deployment.


Start the deployment. Return to your terminal. In the Algo directory, run ./algo and follow the instructions. There are several optional features available. None are required for a fully functional VPN server. These optional features are described in greater detail in here.


That's it! You will get the message below when the server deployment process completes. Take note of the p12 (user certificate) password and the CA key in case you need them later, they will only be displayed this time.
You can now set up clients to connect to your VPN. Proceed to Configure the VPN Clients below.
    ""#                          Congratulations!                            #""
    ""#                     Your Algo server is running.                     #""
    ""#    Config files and certificates are in the ./configs/ directory.    #""
    ""#              Go to https://whoer.net/ after connecting               #""
    ""#        and ensure that all your traffic passes through the VPN.      #""
    ""#                     Local DNS resolver 172.16.0.1                    #""
    ""#        The p12 and SSH keys password for new users is XXXXXXXX       #""
    ""#        The CA key password is XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX       #""
    ""#      Shell access: ssh -i configs/algo.pem root@xxx.xxx.xx.xx        #""

Configure the VPN Clients
Certificates and configuration files that users will need are placed in the configs directory. Make sure to secure these files since many contain private keys. All files are saved under a subdirectory named with the IP address of your new Algo VPN server.
Apple Devices
WireGuard is used to provide VPN services on Apple devices. Algo generates a WireGuard configuration file, wireguard/<username>.conf, and a QR code, wireguard/<username>.png, for each user defined in config.cfg.
On iOS, install the WireGuard app from the iOS App Store. Then, use the WireGuard app to scan the QR code or AirDrop the configuration file to the device.
On macOS Mojave or later, install the WireGuard app from the Mac App Store. WireGuard will appear in the menu bar once you run the app. Click on the WireGuard icon, choose Import tunnel(s) from file..., then select the appropriate WireGuard configuration file.
On either iOS or macOS, you can enable ""Connect on Demand"" and/or exclude certain trusted Wi-Fi networks (such as your home or work) by editing the tunnel configuration in the WireGuard app. (Algo can't do this automatically for you.)
Installing WireGuard is a little more complicated on older version of macOS. See Using macOS as a Client with WireGuard.
If you prefer to use the built-in IPSEC VPN on Apple devices, or need ""Connect on Demand"" or excluded Wi-Fi networks automatically configured, then see Using Apple Devices as a Client with IPSEC.
Android Devices
WireGuard is used to provide VPN services on Android. Install the WireGuard VPN Client. Import the corresponding wireguard/<name>.conf file to your device, then setup a new connection with it. See the Android setup instructions for more detailed walkthrough.
Windows
WireGuard is used to provide VPN services on Windows. Algo generates a WireGuard configuration file, wireguard/<username>.conf, for each user defined in config.cfg.
Install the WireGuard VPN Client. Import the generated wireguard/<username>.conf file to your device, then setup a new connection with it.
Linux WireGuard Clients
WireGuard works great with Linux clients. See this page for an example of how to configure WireGuard on Ubuntu.
Linux strongSwan IPsec Clients (e.g., OpenWRT, Ubuntu Server, etc.)
Please see this page.
Other Devices
Depending on the platform, you may need one or multiple of the following files.

ipsec/manual/cacert.pem: CA Certificate
ipsec/manual/.p12: User Certificate and Private Key (in PKCS#12 format)
ipsec/manual/.conf: strongSwan client configuration
ipsec/manual/.secrets: strongSwan client configuration
ipsec/apple/.mobileconfig: Apple Profile
wireguard/.conf: WireGuard configuration profile
wireguard/.png: WireGuard configuration QR code

Setup an SSH Tunnel
If you turned on the optional SSH tunneling role, then local user accounts will be created for each user in config.cfg and SSH authorized_key files for them will be in the configs directory (user.ssh.pem). SSH user accounts do not have shell access, cannot authenticate with a password, and only have limited tunneling options (e.g., ssh -N is required). This ensures that SSH users have the least access required to setup a tunnel and can perform no other actions on the Algo server.
Use the example command below to start an SSH tunnel by replacing user and ip with your own. Once the tunnel is setup, you can configure a browser or other application to use 127.0.0.1:1080 as a SOCKS proxy to route traffic through the Algo server.
ssh -D 127.0.0.1:1080 -f -q -C -N user@ip -i configs/<server_ip>/ssh-tunnel/<user>.pem
SSH into Algo Server
Your Algo server is configured for key-only SSH access for administrative purposes. Open the Terminal app, cd into the algo-master directory where you originally downloaded Algo, and then use the command listed on the success message:
ssh -i configs/algo.pem user@ip
where user is either root or ubuntu as listed on the success message, and ip is the IP address of your Algo server. If you find yourself regularly logging into the server then it will be useful to load your Algo ssh key automatically. Add the following snippet to the bottom of ~/.bash_profile to add it to your shell environment permanently.
ssh-add ~/.ssh/algo > /dev/null 2>&1
Adding or Removing Users
If you chose to save the CA key during the deploy process, then Algo's own scripts can easily add and remove users from the VPN server.

Update the users list in your config.cfg
Open a terminal, cd to the algo directory, and activate the virtual environment with source .env/bin/activate
Run the command: ./algo update-users

After this process completes, the Algo VPN server will contain only the users listed in the config.cfg file.
Additional Documentation

Deployment instructions, cloud provider setup instructions, and further client setup instructions available here.
FAQ
Troubleshooting

If you read all the documentation and have further questions, join the chat on Gitter.
Endorsements

I've been ranting about the sorry state of VPN svcs for so long, probably about
time to give a proper talk on the subject. TL;DR: use Algo.

-- Kenn White

Before picking a VPN provider/app, make sure you do some research
https://research.csiro.au/ng/wp-content/uploads/sites/106/2016/08/paper-1.pdf ... ‚Äì or consider Algo

-- The Register

Algo is really easy and secure.

-- the grugq

I played around with Algo VPN, a set of scripts that let you set up a VPN in the cloud in very little time, even if you don‚Äôt know much about development. I‚Äôve got to say that I was quite impressed with Trail of Bits‚Äô approach.

-- Romain Dillet for TechCrunch

If you‚Äôre uncomfortable shelling out the cash to an anonymous, random VPN provider, this is the best solution.

-- Thorin Klosowski for Lifehacker
Support Algo VPN




All donations support continued development. Thanks!

We accept donations via PayPal, Patreon, and Flattr.
Use our referral code when you sign up to Digital Ocean for a $10 credit.
We also accept and appreciate contributions of new code and bugfixes via Github Pull Requests.

Algo is licensed and distributed under the AGPLv3. If you want to distribute a closed-source modification or service based on Algo, then please consider purchasing an exception . As with the methods above, this will help support continued development.
",GitHub - trailofbits/algo: Set up a personal VPN in the cloud
51,Python,"



Odoo
Odoo is a suite of web based open source business apps.
The main Odoo Apps include an Open Source CRM,
Website Builder,
eCommerce,
Warehouse Management,
Project Management,
Billing & Accounting,
Point of Sale,
Human Resources,
Marketing,
Manufacturing,
...
Odoo Apps can be used as stand-alone applications, but they also integrate seamlessly so you get
a full-featured Open Source ERP when you install several Apps.
Getting started with Odoo
For a standard installation please follow the Setup instructions
from the documentation.
To learn the software, we recommend the Odoo eLearning, or Scale-up, the business game. Developers can start with the developer tutorials
",GitHub - odoo/odoo: Odoo. Open Source Apps To Grow Your Business.
52,Python,"Django REST framework



Awesome web-browsable Web APIs.
Full documentation for the project is available at https://www.django-rest-framework.org/.

Funding
REST framework is a collaboratively funded project. If you use
REST framework commercially we strongly encourage you to invest in its
continued development by signing up for a paid plan.
The initial aim is to provide a single full-time position on REST framework.
Every single sign-up makes a significant impact towards making that possible.








Many thanks to all our wonderful sponsors, and in particular to our premium backers, Sentry, Stream, Rollbar, Cadre, Kloudless, ESG, Lights On Software, and Retool.

Overview
Django REST framework is a powerful and flexible toolkit for building Web APIs.
Some reasons you might want to use REST framework:

The Web browsable API is a huge usability win for your developers.
Authentication policies including optional packages for OAuth1a and OAuth2.
Serialization that supports both ORM and non-ORM data sources.
Customizable all the way down - just use regular function-based views if you don't need the more powerful features.
Extensive documentation, and great community support.

There is a live example API for testing purposes, available here.
Below: Screenshot from the browsable API


Requirements

Python (3.5, 3.6, 3.7)
Django (1.11, 2.0, 2.1, 2.2)

We highly recommend and only officially support the latest patch release of
each Python and Django series.
Installation
Install using pip...
pip install djangorestframework

Add 'rest_framework' to your INSTALLED_APPS setting.
INSTALLED_APPS = [
    ...
    'rest_framework',
]

Example
Let's take a look at a quick example of using REST framework to build a simple model-backed API for accessing users and groups.
Startup up a new project like so...
pip install django
pip install djangorestframework
django-admin startproject example .
./manage.py migrate
./manage.py createsuperuser

Now edit the example/urls.py module in your project:
from django.conf.urls import url, include
from django.contrib.auth.models import User
from rest_framework import serializers, viewsets, routers

# Serializers define the API representation.
class UserSerializer(serializers.HyperlinkedModelSerializer):
    class Meta:
        model = User
        fields = ['url', 'username', 'email', 'is_staff']


# ViewSets define the view behavior.
class UserViewSet(viewsets.ModelViewSet):
    queryset = User.objects.all()
    serializer_class = UserSerializer


# Routers provide a way of automatically determining the URL conf.
router = routers.DefaultRouter()
router.register(r'users', UserViewSet)


# Wire up our API using automatic URL routing.
# Additionally, we include login URLs for the browsable API.
urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework'))
]
We'd also like to configure a couple of settings for our API.
Add the following to your settings.py module:
INSTALLED_APPS = [
    ...  # Make sure to include the default installed apps here.
    'rest_framework',
]

REST_FRAMEWORK = {
    # Use Django's standard `django.contrib.auth` permissions,
    # or allow read-only access for unauthenticated users.
    'DEFAULT_PERMISSION_CLASSES': [
        'rest_framework.permissions.DjangoModelPermissionsOrAnonReadOnly'
    ]
}
That's it, we're done!
./manage.py runserver

You can now open the API in your browser at http://127.0.0.1:8000/, and view your new 'users' API. If you use the Login control in the top right corner you'll also be able to add, create and delete users from the system.
You can also interact with the API using command line tools such as curl. For example, to list the users endpoint:
$ curl -H 'Accept: application/json; indent=4' -u admin:password http://127.0.0.1:8000/users/
[
    {
        ""url"": ""http://127.0.0.1:8000/users/1/"",
        ""username"": ""admin"",
        ""email"": ""admin@example.com"",
        ""is_staff"": true,
    }
]

Or to create a new user:
$ curl -X POST -d username=new -d email=new@example.com -d is_staff=false -H 'Accept: application/json; indent=4' -u admin:password http://127.0.0.1:8000/users/
{
    ""url"": ""http://127.0.0.1:8000/users/2/"",
    ""username"": ""new"",
    ""email"": ""new@example.com"",
    ""is_staff"": false,
}

Documentation & Support
Full documentation for the project is available at https://www.django-rest-framework.org/.
For questions and support, use the REST framework discussion group, or #restframework on freenode IRC.
You may also want to follow the author on Twitter.
Security
Please see the security policy.
",GitHub - encode/django-rest-framework: Web APIs for Django. üé∏
53,Python,"Python Fire  
Python Fire is a library for automatically generating command line interfaces
(CLIs) from absolutely any Python object.

Python Fire is a simple way to create a CLI in Python. [1]
Python Fire is a helpful tool for developing and debugging Python code. [2]
Python Fire helps with exploring existing code or turning other people's code
into a CLI. [3]
Python Fire makes transitioning between Bash and Python easier. [4]
Python Fire makes using a Python REPL easier by setting up the REPL with the
modules and variables you'll need already imported and created. [5]

Installation
To install Python Fire with pip, run: pip install fire
To install Python Fire with conda, run: conda install fire -c conda-forge
To install Python Fire from source, first clone the repository and then run:
python setup.py install
Basic Usage
You can call Fire on any Python object:
functions, classes, modules, objects, dictionaries, lists, tuples, etc.
They all work!
Here's an example of calling Fire on a function.
import fire

def hello(name=""World""):
  return ""Hello %s!"" % name

if __name__ == '__main__':
  fire.Fire(hello)
Then, from the command line, you can run:
python hello.py  # Hello World!
python hello.py --name=David  # Hello David!
python hello.py --help  # Shows usage information.
Here's an example of calling Fire on a class.
import fire

class Calculator(object):
  """"""A simple calculator class.""""""

  def double(self, number):
    return 2 * number

if __name__ == '__main__':
  fire.Fire(Calculator)
Then, from the command line, you can run:
python calculator.py double 10  # 20
python calculator.py double --number=15  # 30
To learn how Fire behaves on functions, objects, dicts, lists, etc, and to learn
about Fire's other features, see the Using a Fire CLI page.
For additional examples, see The Python Fire Guide.
Why is it called Fire?
When you call Fire, it fires off (executes) your command.
Where can I learn more?
Please see The Python Fire Guide.
Reference



Setup
Command
Notes




install
pip install fire







Creating a CLI
Command
Notes




import
import fire



Call
fire.Fire()
Turns the current module into a Fire CLI.


Call
fire.Fire(component)
Turns component into a Fire CLI.






Using a CLI
Command
Notes




Help
command --help or command -- --help



REPL
command -- --interactive
Enters interactive mode.


Separator
command -- --separator=X
Sets the separator to X. The default separator is -.


Completion
command -- --completion [shell]
Generates a completion script for the CLI.


Trace
command -- --trace
Gets a Fire trace for the command.


Verbose
command -- --verbose




Note that these flags are separated from the Fire command by an isolated --.
License
Licensed under the
Apache 2.0 License.
Disclaimer
This is not an official Google product.
",GitHub - google/python-fire: Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python object.
54,Python,"My Python Examples
I do not consider myself a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me at craig@geekcomputers.co.uk.
These scripts contain important functions which help reduce human workload.
Code documentation is aligned correctly when the files are viewed in Notepad++.


batch_file_rename.py - This batch renames a group of files in a given directory, once you pass the current and the new extensions.


create_dir_if_not_there.py - Checks to see if a directory exists in the users home directory. If a directory does not exist, then one will be created.


Fast Youtube Downloader - Downloads YouTube videos quickly with parallel threads using aria2c.


Google Image Downloader - Query a given term and retrieve images from the Google Image database.


dir_test.py - Tests to see if the directory testdir exists, if not it will create the directory for you.


env_check.py - This script will check to see if all of the environment variables required are set.


blackjack.py - This script contains the Casino BlackJack-21 Game in Python.


fileinfo.py - Shows file information for a given file.


folder_size.py - Scans the current directory and all subdirectories and displays the size.


logs.py - This script will search for all *.log files in the given directory, zip them using the program you specify, and then date stamp them.


move_files_over_x_days.py - Moves all files over a specified age (in days) from the source directory to the destination directory.


nslookup_check.py - This simple script opens the file server_list.txt and then does an nslookup for each one to check the DNS entry.


osinfo.py - Displays some information about the OS on which you are running this script.


ping_servers.py - This script, depending on the arguments supplied, will ping the servers associated with that application group.


ping_subnet.py - After supplying the first 3 octets this file scans the final range for available addresses.


powerdown_startup.py - This file goes through the server list and pings the machine, if it is up it will load the putty session, if it is not it will notify you.


puttylogs.py -  This file zips up all the logs in the given directory.


script_count.py - This file scans the scripts directory and gives a count of the different types of scripts.


[get_youtube_view.py] - This is a simple python script used to get more views on your youtube videos. This script may also be used to repeat songs on Youtube.


script_listing.py - This file will list all the files in the given directory, and go through all the subdirectories as well.


testlines.py - This simple script opens a file and prints out 100 lines of whatever is the set for the line variable.


tweeter.py - Allows you to tweet text or a picture from the terminal.


serial_scanner.py contains a method called ListAvailablePorts which returns a list with the names of the serial ports that are in use in the computer. This method works only on Linux and Windows (can be extended for mac osx). If no port is found, an empty list is returned.


get_youtube_view.py - A simple python script to get more views for your YouTube videos. Useful for repeating songs on YouTube.


CountMillionCharacter.py And CountMillionCharacter2.0.py - Gets character count of a text file.


xkcd_downloader.py - Downloads the latest XKCD comic and places them in a new folder called ""comics"".


timymodule.py - A great alternative to Pythons 'timeit' module and easier to use.


calculator.py - Uses Python's eval() function to implement a calculator.


Google_News.py - Uses BeautifulSoup to provide Latest news headline along with news link.


cricket_live_score - Uses BeautifulSoup to provide live cricket score.


youtube.py - Takes a song name as input and fetches the YouTube URL of the best matching song and plays it.


site_health.py - Checks the health of a remote server


SimpleStopWatch.py - Simple Stop Watch implementation using Python's time module.


Changemac.py - This script change your MAC address , generate random MAC address or enter input as new MAC address in your linux(Successfully Tested in Ubuntu 18.04).


whatsapp-monitor.py - Uses Selenium to give online status about your contacts when your contacts become online in whatsapp you will get an update about it on terminal.


whatsapp-chat-analyzer.py - This is whatsapp group/individual chat analyzer .
This script is able to analyse all activity happened in whatsapp group and visualize all thing through matplotlib library(In Graph form).


JARVIS.py - Control windows programs with your voice.


",GitHub - geekcomputers/Python: My Python Examples
55,Python,"sqlmap 
     
sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester, and a broad range of switches including database fingerprinting, over data fetching from the database, accessing the underlying file system, and executing commands on the operating system via out-of-band connections.
The sqlmap project is currently searching for sponsor(s).
Screenshots

You can visit the collection of screenshots demonstrating some of the features on the wiki.
Installation
You can download the latest tarball by clicking here or latest zipball by clicking  here.
Preferably, you can download sqlmap by cloning the Git repository:
git clone --depth 1 https://github.com/sqlmapproject/sqlmap.git sqlmap-dev

sqlmap works out of the box with Python version 2.6, 2.7 and 3.x on any platform.
Usage
To get a list of basic options and switches use:
python sqlmap.py -h

To get a list of all options and switches use:
python sqlmap.py -hh

You can find a sample run here.
To get an overview of sqlmap capabilities, a list of supported features, and a description of all options and switches, along with examples, you are advised to consult the user's manual.
Links

Homepage: http://sqlmap.org
Download: .tar.gz or .zip
Commits RSS feed: https://github.com/sqlmapproject/sqlmap/commits/master.atom
Issue tracker: https://github.com/sqlmapproject/sqlmap/issues
User's manual: https://github.com/sqlmapproject/sqlmap/wiki
Frequently Asked Questions (FAQ): https://github.com/sqlmapproject/sqlmap/wiki/FAQ
Twitter: @sqlmap
Demos: http://www.youtube.com/user/inquisb/videos
Screenshots: https://github.com/sqlmapproject/sqlmap/wiki/Screenshots

Translations

Bulgarian
Chinese
Croatian
French
German
Greek
Indonesian
Italian
Japanese
Korean
Polish
Portuguese
Russian
Spanish
Turkish
Ukrainian

",GitHub - sqlmapproject/sqlmap: Automatic SQL injection and database takeover tool
56,Python,"





Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as seen in the videos at 3Blue1Brown.
Installation
Manim runs on Python 3.7. You can install it from PyPI via pip:
pip3 install manimlib
System requirements are cairo, ffmpeg, sox, latex (optional, if you want to use LaTeX).
You can now use it via the manim command. For example:
manim my_project.py MyScene
For more options, take a look at the Using manim sections further below.
Directly
If you want to hack on manimlib itself, clone this repository and in that directory execute:
# Install python requirements
python3 -m pip install -r requirements.txt

# Try it out
python3 ./manim.py example_scenes.py SquareToCircle -pl
Directly (Windows)


Install FFmpeg.


Install Cairo. For most users, pycairo‚Äë1.18.0‚Äëcp37‚Äëcp37m‚Äëwin32.whl will do fine.
pip3 install C:\path\to\wheel\pycairo‚Äë1.18.0‚Äëcp37‚Äëcp37m‚Äëwin32.whl


Install a LaTeX distribution. MiKTeX is recommended.


Install SoX.


Install the remaining Python packages. Make sure that pycairo==1.17.1 is changed to pycairo==1.18.0 in requirements.txt.
git clone https://github.com/3b1b/manim.git
cd manim
pip3 install -r requirements.txt
python3 manim.py example_scenes.py SquareToCircle -pl


Anaconda Install

Install sox and latex as above.
Create a conda environment using conda env create -f environment.yml
WINDOWS ONLY Install pyreadline via pip install pyreadline.

Using virtualenv and virtualenvwrapper
After installing virtualenv and virtualenvwrapper
git clone https://github.com/3b1b/manim.git
mkvirtualenv -a manim -r requirements.txt manim
python3 -m manim example_scenes.py SquareToCircle -pl
Using Docker
Since it's a bit tricky to get all the dependencies set up just right, there is a Dockerfile and Compose file provided in this repo as well as a premade image on Docker Hub. The Dockerfile contains instructions on how to build a manim image, while the Compose file contains instructions on how to run the image.
The prebuilt container image has manim repository included.
INPUT_PATH is where the container looks for scene files. You must set the INPUT_PATH
environment variable to the absolute path containing your scene file and the
OUTPUT_PATH environment variable to the directory where you want media to be written.

Install Docker
Install Docker Compose
Render an animation:

INPUT_PATH=/path/to/dir/containing/source/code \
OUTPUT_PATH=/path/to/output/ \
docker-compose run manim example_scenes.py SquareToCircle -l
The command needs to be run as root if your username is not in the docker group.
You can replace example.scenes.py with any relative path from your INPUT_PATH.

After running the output will say files ready at /tmp/output/, which refers to path inside the container. Your OUTPUT_PATH is bind mounted to this /tmp/output so any changes made by the container to /tmp/output will be mirrored on your OUTPUT_PATH. /media/ will be created in OUTPUT_PATH.
-p won't work as manim would look for video player in the container system, which it does not have.
The first time you execute the above command, Docker will pull the image from Docker Hub and cache it. Any subsequent runs until the image is evicted will use the cached image.
Note that the image doesn't have any development tools installed and can't preview animations. Its purpose is building and testing only.
Using manim
Try running the following:
python3 -m manim example_scenes.py SquareToCircle -pl
The -p flag in the command above is for previewing, meaning the video file will automatically open when it is done rendering. The -l flag is for a faster rendering at a lower quality.
Some other useful flags include:

-s to skip to the end and just show the final frame.
-n <number> to skip ahead to the n'th animation of a scene.
-f to show the file in finder (for OSX).

Set MEDIA_DIR environment variable to specify where the image and animation files will be written.
Look through the old_projects folder to see the code for previous 3b1b videos. Note, however, that developments are often made to the library without considering backwards compatibility with those old projects. To run an old project with a guarantee that it will work, you will have to go back to the commit which completed that project.
While developing a scene, the -sp flags are helpful to just see what things look like at the end without having to generate the full animation. It can also be helpful to use the -n flag to skip over some number of animations.
Documentation
Documentation is in progress at eulertour.com/learn/manim.
Walkthrough
Todd Zimmerman put together a tutorial on getting started with manim, which has been updated to run on Python 3.7.
Live Streaming
To live stream your animations, simply run manim with the --livestream option.
> python -m manim --livestream
Writing to media/videos/scene/scene/1080p30/LiveStreamTemp.mp4

Manim is now running in streaming mode. Stream animations by passing
them to manim.play(), e.g.
>>> c = Circle()
>>> manim.play(ShowCreation(c))

>>>
It is also possible to stream directly to Twitch. To do that simply pass
--livestream and --to-twitch to manim and specify the stream key with
--with-key. Then when you follow the above example the stream will directly
start on your Twitch channel (with no audio support).
Contributing
Is always welcome. In particular, there is a dire need for tests and documentation.
License
All files in the directories active_projects and old_projects, which by and large generate the visuals for 3b1b videos, are copyright 3Blue1Brown.
The general purpose animation code found in the remainder of the repository, on the other hand, is under the MIT license.
",GitHub - 3b1b/manim: Animation engine for explanatory math videos
57,Python,"This guide is a collection of techniques for improving the security and privacy of a modern Apple Macintosh computer (""MacBook"") running a recent version of macOS (formerly known as ""OS X"").
This guide is targeted to power users who wish to adopt enterprise-standard security, but is also suitable for novice users with an interest in improving their privacy and security on a Mac.
A system is only as secure as its administrator is capable of making it. There is no one single technology, software, nor technique to guarantee perfect computer security; a modern operating system and computer is very complex, and requires numerous incremental changes to meaningfully improve one's security and privacy posture.
This guide is provided on an 'as is' basis without any warranties of any kind. Only you are responsible if you break anything or get in any sort of trouble by following this guide.
To suggest an improvement, please send a pull request or open an issue.
This guide is also available in ÁÆÄ‰Ωì‰∏≠Êñá.

Basics
Preparing and installing macOS

Verifying installation integrity
Creating a bootable USB installer
Creating an install image

Manual way


Target disk mode
Creating a recovery partition
Virtualization


First boot
System activation
Admin and standard user accounts

Caveats
Setup


Full disk encryption
Firmware
Firewall

Application layer firewall
Third party firewalls
Kernel level packet filtering


Services
Spotlight Suggestions
Homebrew
DNS

Hosts file
dnscrypt
Dnsmasq

Test DNSSEC validation




Captive portal
Certificate authorities
OpenSSL
Curl
Web

Privoxy
Browser

Firefox
Chrome
Safari
Other Web browsers
Web browsers and privacy


Plugins


Tor
VPN
PGP/GPG
OTR
Viruses and malware
System Integrity Protection
Gatekeeper and XProtect
Metadata and artifacts
Passwords
Backup
Wi-Fi
SSH
Physical access
System monitoring

OpenBSM audit
DTrace
Execution
Network


Binary Whitelisting
Miscellaneous
Related software
Additional resources

Basics
Standard security best practices apply:


Create a threat model

What are you trying to protect and from whom? Is your adversary a three letter agency (if so, you may want to consider using OpenBSD instead); a nosy eavesdropper on the network; or a determined apt orchestrating a campaign against you?
Recognize threats and how to reduce attack surface against them.



Keep the system up to date

Patch the base operating system and all third party software.
macOS system updates can be completed using the App Store application, or the softwareupdate command-line utility - neither requires registering an Apple account. Updates can also be downloaded directly from Apple's support site.
Subscribe to announcement mailing lists like Apple security-announce.



Encrypt sensitive data at rest

In addition to full disk encryption, consider creating one or several encrypted partitions or volumes to store passwords, cryptographic keys, personal documents, etc. at rest.
This will mitigate damage in case of compromise and data exfiltration.



Assure data availability

Create regular backups of your data and be ready to format and re-install the operating system in case of compromise.
Always encrypt locally before copying backups to external media or the ""cloud"".
Verify backups work by testing them regularly, for example by accessing certain files or performing a hash based comparison.



Click carefully

Ultimately, the security of a system can be reduced to its administrator.
Care should be taken when installing new software. Always prefer free and open source software (which macOS is not).



Preparing and installing macOS
There are several ways to install macOS.
The simplest way is to boot into Recovery Mode by holding Command and R keys at boot. A system image can be downloaded and applied directly from Apple. However, this way exposes the serial number and other identifying information over the network in plaintext, which may not be desired for privacy reasons.

Packet capture of an unencrypted HTTP conversation during macOS recovery
An alternative way to install macOS is to first download macOS Mojave from the App Store or elsewhere, and create a custom installable system image.
Verifying installation integrity
The macOS installation application is code signed, which should be verified to make sure you received a legitimate copy, using the pkgutil --check-signature or codesign -dvv commands.
To verify the code signature and integrity of macOS application bundles:
$ pkgutil --check-signature /Applications/Install\ macOS\ Catalina.app
Package ""Install macOS Catalina"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Software Signing
       SHA1 fingerprint: 01 3E 27 87 74 8A 74 10 3D 62 D2 CD BF 77 A1 34 55 17 C4 82
       -----------------------------------------------------------------------------
    2. Apple Code Signing Certification Authority
       SHA1 fingerprint: 1D 01 00 78 A6 1F 4F A4 69 4A FF 4D B1 AC 26 6C E1 B4 59 46
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
Use the codesign command to examine an application's code signature:
$ codesign -dvv /Applications/Install\ macOS\ Catalina.app
Executable=/Applications/Install macOS Catalina.app/Contents/MacOS/InstallAssistant_springboard
Identifier=com.apple.InstallAssistant.Catalina
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20100 size=276 flags=0x2000(library-validation) hashes=3+3 location=embedded
Platform identifier=9
Signature size=4628
Authority=Software Signing
Authority=Apple Code Signing Certification Authority
Authority=Apple Root CA
Info.plist entries=33
TeamIdentifier=not set
Sealed Resources version=2 rules=13 files=234
Internal requirements count=1 size=84
Creating a bootable USB installer
Instead of booting from the network or using target disk mode, a bootable macOS installer can be made with the createinstallmedia utility included in Contents/Resources folder of the installer application bundle. See Create a bootable installer for macOS, or run the utility without arguments to see how it works.
To create a bootable USB installer, mount a USB drive, and erase and partition it, then use the createinstallmedia utility:
$ diskutil list
[Find disk matching correct size, usually the last disk, e.g. /dev/disk2]

$ diskutil unmountDisk /dev/disk2

$ diskutil partitionDisk /dev/disk2 1 JHFS+ Installer 100%

$ cd /Applications/Install\ macOS\ Catalina.app

$ sudo ./Contents/Resources/createinstallmedia --volume /Volumes/Installer --nointeraction
Erasing disk: 0%... 10%... 20%... 30%... 100%
Copying to disk: 0%... 10%... 20%... 30%... 40%... 50%... 60%... 70%... 80%... 90%... 100%
Making disk bootable...
Copying boot files...
Install media now available at ""/Volumes/Install macOS Catalina""
Creating an install image
Note Apple's AutoDMG installer does not appear to work across OS versions. If you want to build a 10.14 image, for example, the following steps must be performed on macOS 10.14!
To create a custom install image which can be restored to a Mac (using a USB-C cable and target disk mode, for example), use MagerValp/AutoDMG.
Manual way
Note The following instructions appear to work only on macOS versions before 10.13.
Find InstallESD.dmg which is inside the installation application. Locate it in Terminal or with Finder, right click on the application bundle, select Show Package Contents and navigate to Contents > SharedSupport to find the file InstallESD.dmg
Verify file integrity by comparing its SHA-256 hash with others found in InstallESD_Hashes.csv or notpeter/apple-installer-checksums.
To determine which macOS versions and builds originally shipped with or are available for a Mac, see HT204319.
$ shasum -a 256 InstallESD.dmg
Mount and install the operating system to a temporary image:
$ hdiutil attach -mountpoint /tmp/InstallESD ./InstallESD.dmg

$ hdiutil create -size 32g -type SPARSE -fs HFS+J -volname ""macOS"" -uid 0 -gid 80 -mode 1775 /tmp/macos.sparseimage

$ hdiutil attach -mountpoint /tmp/macos -owners on /tmp/macos.sparseimage

$ sudo installer -pkg /tmp/InstallESD/Packages/OSInstall.mpkg -tgt /tmp/macos -verbose
installer: OS Install started.
#############
[...]
The installation will take a while, so be patient. Use tail -F /var/log/install.log in another terminal to monitor progress and check for errors.
Once the installation is complete, detach, convert and verify the image:
$ hdiutil detach /tmp/macos
""disk4"" unmounted.
""disk4"" ejected.

$ hdiutil detach /tmp/InstallESD
""disk3"" unmounted.
""disk3"" ejected.

$ hdiutil convert -format UDZO /tmp/macos.sparseimage -o ~/sierra.dmg
Preparing imaging engine...
[...]

$ asr imagescan --source ~/sierra.dmg
The file sierra.dmg is now ready to be applied over Target Disk Mode, from a bootable USB installer, booting from the network or recovery mode. The image could be further customized to include provisioned users, installed applications, preferences, for example.
Target disk mode
To use Target Disk Mode, boot up the Mac you wish to image while holding the T key and connect it to another Mac using a USB-C, Thunderbolt or Firewire cable.
If you don't have another Mac, boot to a USB installer, with sierra.dmg and other required files copied to it, by holding the Option key at boot.
Use the command diskutil list to identify the disk of the connected Mac, usually /dev/disk2
Optionally, securely erase the disk with a single pass (if previously FileVault-encrypted, the disk must first be unlocked and mounted as /dev/disk3s2):
$ sudo diskutil secureErase freespace 1 /dev/disk3s2

Partition the disk to Journaled HFS+:
$ sudo diskutil unmountDisk /dev/disk2

$ sudo diskutil partitionDisk /dev/disk2 1 JHFS+ macOS 100%
Restore the image to the new volume, making sure /dev/disk2 is the disk being erased:
$ sudo asr restore --source ~/sierra.dmg --target /Volumes/macOS --erase --buffersize 4m
[...]
Erase contents of /dev/disk2s2 (/Volumes/macOS)? [ny]:y
[...]
The Disk Utility application may also be used to erase the connected disk and restore sierra.dmg to the newly created partition.
To transfer any files, copy them to a shared folder like /Users/Shared on the mounted disk image, e.g. cp Xcode_8.0.dmg /Volumes/macOS/Users/Shared

Finished restore install from USB recovery boot
Creating a recovery partition
Unless you have built the image with AutoDMG, or installed macOS to a second partition on the same Mac, you will need to create a recovery partition in order to use full disk encryption. You can do so using MagerValp/Create-Recovery-Partition-Installer or manually by following these steps:
Download RecoveryHDUpdate.dmg and verify its integrity:
$ shasum -a 256 RecoveryHDUpdate.dmg
f6a4f8ac25eaa6163aa33ac46d40f223f40e58ec0b6b9bf6ad96bdbfc771e12c  RecoveryHDUpdate.dmg
Attach and expand the installer, then run it - again ensuring /Volumes/macOS path is the newly created partition on the connected disk:
$ hdiutil attach RecoveryHDUpdate.dmg

$ pkgutil --expand /Volumes/Mac\ OS\ X\ Lion\ Recovery\ HD\ Update/RecoveryHDUpdate.pkg /tmp/recovery

$ hdiutil attach /tmp/recovery/RecoveryHDUpdate.pkg/RecoveryHDMeta.dmg

$ /tmp/recovery/RecoveryHDUpdate.pkg/Scripts/Tools/dmtest ensureRecoveryPartition /Volumes/macOS/ /Volumes/Recovery\ HD\ Update/BaseSystem.dmg 0 0 /Volumes/Recovery\ HD\ Update/BaseSystem.chunklist
[...]
Creating recovery partition: finished
Run diskutil list again to make sure Recovery HD now exists on /dev/disk2. Eject the disk with hdiutil unmount /Volumes/macOS and power down the target disk mode-booted Mac.
Virtualization
To install macOS as a virtual machine (VM) using VMware Fusion, follow the instructions above to create an image. You will not need to download and create a recovery partition manually.
For the Installation Method, select Install macOS from the recovery partition. Customize any memory or CPU requirements and complete setup. The guest VM should boot into Recovery Mode by default.
Note If the virtual machine does not boot due to a kernel panic, adjust the memory and process resource settings.
In Recovery Mode, select a language, then select Utilities > Terminal from the menu bar.
In the guest VM, type ifconfig | grep inet - you should see a private address like 172.16.34.129
On the host Mac, type ifconfig | grep inet - you should see a private gateway address like 172.16.34.1. From the host Mac, you should be able to ping 172.16.34.129 or the equivalent guest VM address.
From the host Mac, serve the installable image to the guest VM by editing /etc/apache2/httpd.conf and adding the following line to the top (using the gateway address assigned to the host Mac and port 80):
Listen 172.16.34.1:80

On the host Mac, link the image to the default Apache Web server directory:
$ sudo ln ~/sierra.dmg /Library/WebServer/Documents

From the host Mac, start Apache in the foreground:
$ sudo httpd -X

From the guest VM, install the disk image to the volume over the local network using asr:
-bash-3.2# asr restore --source http://172.16.34.1/sierra.dmg --target /Volumes/Macintosh\ HD/ --erase --buffersize 4m
	Validating target...done
	Validating source...done
	Erase contents of /dev/disk0s2 (/Volumes/Macintosh HD)? [ny]: y
	Retrieving scan information...done
	Validating sizes...done
	Restoring  ....10....20....30....40....50....60....70....80....90....100
	Verifying  ....10....20....30....40....50....60....70....80....90....100
	Remounting target volume...done
When it's finished, stop the Apache Web server on the host Mac by pressing Control C at the sudo httpd -X window and remove the image copy with sudo rm /Library/WebServer/Documents/sierra.dmg
In the guest VM, select Startup Disk from the menubar top-left, select the hard drive and restart. You may wish to disable the Network Adapter in VMware to configure the guest VM initially.
Take and Restore from saved guest VM snapshots before and after attempting risky browsing, for example, or use a guest VM to install and operate questionable software.
First boot
Note Before setting up macOS, consider disconnecting networking and configuring a firewall(s) first. However, late 2016 MacBooks with Touch Bar hardware require online OS activation (also see next section).
On first boot, hold Command Option P R keys to clear NVRAM.
When macOS first starts, you'll be greeted by Setup Assistant.
When creating the first account, use a strong password without a hint.
If you enter your real name at the account setup process, be aware that your computer's name and local hostname will comprise that name (e.g., John Appleseed's MacBook) and thus will appear on local networks and in various preference files.
Both should be verified and updated as needed in System Preferences > Sharing or with the following commands after installation:
$ sudo scutil --set ComputerName MacBook
$ sudo scutil --set LocalHostName MacBook

System activation
A few words on the privacy implications of activating ""Touch Bar"" MacBook devices from your friendly anonymous security researcher:

Apple increasingly seems (despite vague claims to the contrary) increasingly interested in merging or ""unifying"" the two OSes, and there are constantly rumors of fundamental changes to macOS that make it far more like iOS than the macOS of old. Apple's introduction of ARM-based coprocessors running iOS/sepOS, first with the T1 processor on the TouchBar MacBook Pros (run the TouchBar, implement NFC/ApplePay, add biometric login using sep, and verify firmware integrity) and the iMac Pro's T2 (implements/verifies embedded device firmware, implements secure boot, etc) seems to cement this concern and basically renders using macOS devices without sending metadata to Apple difficult to impossible.
iOS devices have always required ""activation"" on first boot and when the battery has gone dead which initializes sepOS to proceed with verified boot. First boot activation not only initializes sepOS as discussed below, but sends metadata to Apple (and carriers via Apple with cellular devices) to activate the baseband and SIM. In activation processes after first boot, just as with first boot, a long list of highly sensitive metadata are sent hashed (note hashing does not give you any privacy from Apple here since they link this exact metadata to payment information at purchase) to Apple so it can return the personalized response required for secure boot to complete. What is particularly worrying about this process is that it is a network-linked secure boot process where centralized external servers have the power to dictate what the device should boot. Equally there are significant privacy concerns with devices constantly sending metadata (both during activation and other Apple-linked/-hosted activities) and linking IP addresses very strongly with real identities based on purchase payment information and if a cellular device, metadata collected about SIM, etc unless such connections are blocked at the network level (which is only possible on self-managed infrastructure, i.e. not cellular) and doing this basically renders using the device impossible since simply installing an application requires sending device metadata to Apple.
That the activation verification mechanism is designed specifically to rely on unique device identifiers that are associated with payment information at purchase and actively associated on a continuing basis by Apple for every Apple-hosted service that the device interacts with (Apple ID-based services, softwareupdate, iMessage, FaceTime, etc.) the ability (and invitation) for Apple to silently send targeted malicious updates to devices matching specific unique ID criteria is a valid concern, and something that should not be dismissed as unlikely, especially given Apple's full compliance with recently implemented Chinese (and other authoritarian and ""non-authoritarian"" countries') national security laws.
iOS has from the start been designed with very little end-user control with no way for end-users to configure devices according to their wishes while maintaining security and relies heavily on new, closed source code. While macOS has for most of its history been designed on the surface in a similar fashion, power and enterprise users can (for the moment) still configure their devices relatively securely while maintaining basically zero network interaction with Apple and with the installation of third party software/kernel extensions, completely control the network stack and intercept filesystem events on a per-process basis. macOS, despite having a good deal of closed source code, was designed at a very different period in Apple's history and was designed more in line with open source standards, and designed to be configurable and controllable by enterprise/power users.
The introduction of these coprocessors to Mac devices, while increasing security in many ways, brings with it all the issues with iOS discussed above, and means that running mac devices securely with complete user control, and without forced network interaction with the Apple mothership in highly sensitive corporate and other environments problematic and risky. Given this author is unaware of the exact hardware configuration of the coprocessors, the following may be inaccurate. However, given the low-level nature of these coprocessors, it would not surprise the author if these coprocessors, if not already, will eventually have separate network access of their own, independent of the Intel CPU (indications suggest not currently the case for T1; unclear on T2), which leads to concerns similar to those that many have raised around Intel ME/AMT (and of course mac devices also have ME in the Intel CPU...). One could argue that these coprocessors increase security, and in many ways that is the case, but not the user's security against a malicious Apple.
The lack of configurability is the key issue. Apple could have introduced secure boot and firmware protection without making it require network access, without making verification linked to device-unique IDs and without introducing an enormous amount of potentially exploitable code to protect against a much smaller, but highly exploitable codebase, while running on a coprocessor with a highly privileged position on the board which gives immense power to an adversary with manufacturer compliance for targeted attacks.
This is an ongoing concern and in the worst case scenario could potentially represent the end of macs as independent, end-user controllable and relatively secure systems appropriate for sensitive environments with strict network and security policies.

From iOS, The Future Of macOS, Freedom, Security And Privacy In An Increasingly Hostile Global Environment.
Admin and standard user accounts
The first user account is always an admin account. Admin accounts are members of the admin group and have access to sudo, which allows them to usurp other accounts, in particular root, and gives them effective control over the system. Any program that the admin executes can potentially obtain the same access, making this a security risk.
Utilities like sudo have weaknesses that can be exploited by concurrently running programs and many panes in System Preferences are unlocked by default (pdf) (p. 61‚Äì62) for admin accounts.
It is considered a best practice by Apple and others (pdf) (p. 41‚Äì42) to use a separate standard account for day-to-day work and use the admin account for installations and system configuration.
It is not strictly required to ever log into the admin account via the macOS login screen. The system will prompt for authentication when required and Terminal can do the rest. To that end, Apple provides some recommendations for hiding the admin account and its home directory. This can be an elegant solution to avoid having a visible 'ghost' account. The admin account can also be removed from FileVault for additional hardening.
Caveats

Only administrators can install applications in /Applications (local directory). Finder and Installer will prompt a standard user with an authentication dialog. Many applications can be installed in ~/Applications instead (the directory can be created manually). As a rule of thumb: applications that do not require admin access ‚Äì or do not complain about not being installed in /Applications ‚Äì should be installed in the user directory, the rest in the local directory. Mac App Store applications are still installed in /Applications and require no additional authentication.
sudo is not available in shells of the standard user, which requires using su or login to enter a shell of the admin account. This can make some maneuvers trickier and requires some basic experience with command-line interfaces.
System Preferences and several system utilities (e.g. Wi-Fi Diagnostics) will require root privileges for full functionality. Many panels in System Preferences are locked and need to be unlocked separately by clicking on the lock icon. Some applications will simply prompt for authentication upon opening, others must be opened by an admin account directly to get access to all functions (e.g. Console).
There are third-party applications that will not work correctly because they assume that the user account is an admin. These programs may have to be executed by logging into the admin account, or by using the open utility.
See additional discussion in issue #167.

Setup
Accounts can be created and managed in System Preferences. On settled systems, it is generally easier to create a second admin account and then demote the first account. This avoids data migration. Newly installed systems can also just add a standard account.
Demoting an account can be done either from the the new admin account in System Preferences ‚Äì the other account must be logged out ‚Äì or by executing these commands (it may not be necessary to execute both, see issue #179):
$ sudo dscl . -delete /Groups/admin GroupMembership <username>
$ sudo dscl . -delete /Groups/admin GroupMembers <GeneratedUID>
To find the ‚ÄúGeneratedUID‚Äù of an account:
$ dscl . -read /Users/<username> GeneratedUID
See also this post for more information about how macOS determines group membership.
Full disk encryption
FileVault provides full disk (technically, full volume) encryption on macOS.
FileVault encryption protects data at rest and hardens (but not always prevents) someone with physical access from stealing data or tampering with your Mac.
With much of the cryptographic operations happening efficiently in hardware, the performance penalty for FileVault is not noticeable.
Like all cryptosystems, the security of FileVault greatly depends on the quality of the pseudo random number generator (PRNG).

The random device implements the Yarrow pseudo random number generator algorithm and maintains its entropy pool.  Additional entropy is fed to the generator regularly by the SecurityServer daemon from random jitter measurements of the kernel.

See man 4 random for more information.
Turning on FileVault in System Preferences after installing macOS, rather than creating an encrypted partition for the installation first, is more secure, because more PRNG entropy is available then.
Additionally, the PRNG can be manually seeded with entropy by writing to /dev/random before enabling FileVault. This can be done by simply using the Mac for a little while before activating FileVault.
It may also be possible to increase entropy with an external source, like OneRNG. See Entropy and Random Number Generators and Fun with encryption and randomness for more information.
Enable FileVault with sudo fdesetup enable or through System Preferences > Security & Privacy and reboot.
If you can remember the password, there's no reason to save the recovery key. However, all encrypted data will be lost forever if without either the password or recovery key.
To learn about how FileVault works, see the paper Infiltrate the Vault: Security Analysis and Decryption of Lion Full Disk Encryption (pdf) and related presentation (pdf). Also see IEEE Std 1619-2007: The XTS-AES Tweakable Block Cipher (pdf).
Optional Enforce system hibernation and evict FileVault keys from memory instead of traditional sleep to memory:
$ sudo pmset -a destroyfvkeyonstandby 1
$ sudo pmset -a hibernatemode 25

All computers have firmware of some type - EFI, BIOS - to help in the discovery of hardware components and ultimately to properly bootstrap the computer using the desired OS instance. In the case of Apple hardware and the use of EFI, Apple stores relevant information within EFI to aid in the functionality of macOS. For example, the FileVault key is stored in EFI to transparently come out of standby mode.


Organizations especially sensitive to a high-attack environment, or potentially exposed to full device access when the device is in standby mode, should mitigate this risk by destroying the FileVault key in firmware. Doing so doesn't destroy the use of FileVault, but simply requires the user to enter the password in order for the system to come out of standby mode.

If you choose to evict FileVault keys in standby mode, you should also modify your standby and power nap settings. Otherwise, your machine may wake while in standby mode and then power off due to the absence of the FileVault key. See issue #124 for more information. These settings can be changed with:
$ sudo pmset -a powernap 0
$ sudo pmset -a standby 0
$ sudo pmset -a standbydelay 0
$ sudo pmset -a autopoweroff 0
For more information, see Best Practices for
Deploying FileVault 2 (pdf) and paper Lest We Remember: Cold Boot Attacks on Encryption Keys (pdf)
Note APFS may make evicting FileVault keys redundant - see discussion and links in issue #283.
Firmware
Setting a firmware password prevents a Mac from starting up from any device other than the startup disk. It may also be set to be required on each boot. This may be useful for mitigating some attacks which require physical access to hardware.  See How to set a firmware password on your Mac for official documentation.
This feature can be helpful if your laptop is lost or stolen, protects against Direct Memory Access (DMA) attacks which can read your FileVault passwords and inject kernel modules such as pcileech, as the only way to reset the firmware password is through an Apple Store, or by using an SPI programmer, such as Bus Pirate or other flash IC programmer.

Start up pressing Command and R keys to boot to Recovery Mode mode.
When the Recovery window appears, choose Firmware Password Utility from the Utilities menu.
In the Firmware Utility window that appears, select Turn On Firmware Password.
Enter a new password, then enter the same password in the Verify field.
Select Set Password.
Select Quit Firmware Utility to close the Firmware Password Utility.
Select Restart or Shutdown from the Apple menu in the top-left corner.

The firmware password will activate at next boot. To validate the password, hold Alt during boot - you should be prompted to enter the password.
The firmware password can also be managed with the firmwarepasswd utility while booted into the OS. For example, to prompt for the firmware password when attempting to boot from a different volume:
$ sudo firmwarepasswd -setpasswd -setmode command
To verify the firmware password:
$ sudo firmwarepasswd -verify
Verifying Firmware Password
Enter password:
Correct
Note, a firmware password may be bypassed by a determined attacker or Apple, with physical access to the computer.

Using a Dediprog SF600 to dump and flash a 2013 MacBook SPI Flash chip to remove a firmware password, sans Apple
Newer Mac models (Mac Pro, iMac Pro, Macbook with TouchBar) with Apple T2 chips, which provide a secure enclave for encrypted keys, lessen the risk of EFI firmware attacks. See this blog post for more information.
See LongSoft/UEFITool, chipsec/chipsec and discussion in issue #213 for more information.
Firewall
There are several types of firewalls available for macOS which should be enabled.
Application layer firewall
Built-in, basic firewall which blocks incoming connections only. This firewall does not have the ability to monitor, nor block outgoing connections.
It can be controlled by the Firewall tab of Security & Privacy in System Preferences, or with the following commands.
Enable the firewall with logging and stealth mode:
$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on
Firewall is enabled. (State = 1)

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setloggingmode on
Turning on log mode

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setstealthmode on
Stealth mode enabled

Computer hackers scan networks so they can attempt to identify computers to attack. You can prevent your computer from responding to some of these scans by using stealth mode. When stealth mode is enabled, your computer does not respond to ICMP ping requests, and does not answer to connection attempts from a closed TCP or UDP port. This makes it more difficult for attackers to find your computer.

To prevent built-in software as well as code-signed, downloaded software from being whitelisted automatically:
$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsigned off
Disabled allow signed built-in applications automatically

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsignedapp off
Disabled allow signed downloaded applications automatically

Applications that are signed by a valid certificate authority are automatically added to the list of allowed apps, rather than prompting the user to authorize them. Apps included in macOS are signed by Apple and are allowed to receive incoming connections when this setting is enabled. For example, since iTunes is already signed by Apple, it is automatically allowed to receive incoming connections through the firewall.


If you run an unsigned app that is not listed in the firewall list, a dialog appears with options to Allow or Deny connections for the app. If you choose ""Allow"", macOS signs the application and automatically adds it to the firewall list. If you choose ""Deny"", macOS adds it to the list but denies incoming connections intended for this app.

After interacting with socketfilterfw, restart the process by sending a line hangup signal:
$ sudo pkill -HUP socketfilterfw
Third party firewalls
Programs such as Little Snitch, Hands Off, Radio Silence, LuLu and Security Growler provide a good balance of usability and security.
These programs are capable of monitoring and blocking incoming and outgoing network connections. However, they may require the use of a closed source kernel extension.
If the number of choices of allowing/blocking network connections is overwhelming, use Silent Mode with connections allowed, then periodically check the configuration to gain understanding of applications and what they are doing.
It is worth noting that these firewalls can be bypassed by programs running as root or through OS vulnerabilities (pdf), but they are still worth having - just don't expect absolute protection. However, some malware actually deletes itself and doesn't execute if Little Snitch, or other security software, is installed.
For more on how Little Snitch works, see the Network Kernel Extensions Programming Guide and Shut up snitch! ‚Äì reverse engineering and exploiting a critical Little Snitch vulnerability.
Kernel level packet filtering
A highly customizable, powerful, but also most complicated firewall exists in the kernel. It can be controlled with pfctl and various configuration files.
pf can also be controlled with a GUI application such as IceFloor or Murus.
There are many books and articles on the subject of pf firewall. Here's is just one example of blocking traffic by IP address.
Add the following into a file called pf.rules:
wifi = ""en0""
ether = ""en7""
set block-policy drop
set fingerprints ""/etc/pf.os""
set ruleset-optimization basic
set skip on lo0
scrub in all no-df
table <blocklist> persist
block in log
block in log quick from no-route to any
block log on $wifi from { <blocklist> } to any
block log on $wifi from any to { <blocklist> }
antispoof quick for { $wifi $ether }
pass out proto tcp from { $wifi $ether } to any keep state
pass out proto udp from { $wifi $ether } to any keep state
pass out proto icmp from $wifi to any keep state

Then use the following commands to manipulate the firewall:

sudo pfctl -e -f pf.rules to enable the firewall and load the configuration
sudo pfctl -d to disable the firewall
sudo pfctl -t blocklist -T add 1.2.3.4 to add an IP address to the blocklist
sudo pfctl -t blocklist -T show to view the blocklist
sudo ifconfig pflog0 create to create an interface for logging
sudo tcpdump -ni pflog0 to view filtered packets

Unless you're already familiar with packet filtering, spending too much time configuring pf is not recommended. It is also probably unnecessary if your Mac is behind a NAT on a secure home network.
It is possible to use the pf firewall to block network access to entire ranges of network addresses, for example to a whole organization:
Query Merit RADb for the list of networks in use by an autonomous system, like Facebook:
$ whois -h whois.radb.net '!gAS32934'
Copy and paste the list of networks returned into the blocklist command:
$ sudo pfctl -t blocklist -T add 31.13.24.0/21 31.13.64.0/24 157.240.0.0/16
Confirm the addresses were added:
$ sudo pfctl -t blocklist -T show
No ALTQ support in kernel
ALTQ related functions disabled
   31.13.24.0/21
   31.13.64.0/24
   157.240.0.0/16
Confirm network traffic is blocked to those addresses (note that DNS requests will still work):
$ dig a +short facebook.com
157.240.2.35

$ curl --connect-timeout 5 -I http://facebook.com/
*   Trying 157.240.2.35...
* TCP_NODELAY set
* Connection timed out after 5002 milliseconds
* Closing connection 0
curl: (28) Connection timed out after 5002 milliseconds

$ sudo tcpdump -tqni pflog0 'host 157.240.2.35'
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.162771 > 157.240.2.35.80: tcp 0
Outgoing TCP SYN packets are blocked, so a TCP connection is not established and thus a Web site is effectively blocked at the IP layer.
To use pf to audit ""phone home"" behavior of user and system-level processes, see fix-macosx/net-monitor. See drduh/config/scripts/pf-blocklist.sh for more inspiration.
Services
Note System Integrity Protection does not allow disabling system services on recent macOS versions. Either temporarily disable SIP or disable services from Recovery Mode.
See fix-macosx/yosemite-phone-home, l1k/osxparanoia and karek314/macOS-home-call-drop for further recommendations.
Services on macOS are managed by launchd. See launchd.info, as well as Apple's Daemons and Services Programming Guide and Technical Note TN2083
You can also run KnockKnock that shows more information about startup items.

Use launchctl list to view running user agents
Use sudo launchctl list to view running system daemons
Specify the service name to examine it, e.g. launchctl list com.apple.Maps.mapspushd
Use defaults read to examine job plists in /System/Library/LaunchDaemons and /System/Library/LaunchAgents
Use man and strings to find out more about what an agent/daemon does

For example, to learn what a system launch daemon or agent does, start with:
$ defaults read /System/Library/LaunchDaemons/com.apple.apsd.plist
Look at the Program or ProgramArguments section to see which binary is run, in this case apsd. To find more information about that, look at the man page with man apsd
For example, if you're not interested in Apple Push Notifications, disable the service:
$ sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.apsd.plist
Note Unloading services may break usability of some applications. Read the manual pages and use Google to make sure you understand what you're doing first.
Be careful about disabling any system daemons you don't understand, as it may render your system unbootable. If you break your Mac, use single user mode to fix it.
Use Console and Activity Monitor applications if you notice your Mac heating up, feeling sluggish, or generally misbehaving, as it may have resulted from your tinkering.
To view the status of services:
$ find /var/db/com.apple.xpc.launchd/ -type f -print -exec defaults read {} \; 2>/dev/null
Annotated lists of launch daemons and agents, the respective program executed, and the programs' hash sums are included in this repository.
(Optional) Run the read_launch_plists.py script and diff output to check for any discrepancies on your system, e.g.:
$ diff <(python read_launch_plists.py) <(cat 16A323_launchd.csv)
See also cirrusj.github.io/Yosemite-Stop-Launch for descriptions of services and Provisioning OS X and Disabling Unnecessary Services for another explanation.
Persistent login items may also exist in these directories:

/Library/LaunchAgents
/Library/LaunchDaemons
/Library/ScriptingAdditions
/Library/StartupItems
/System/Library/LaunchAgents
/System/Library/LaunchDaemons
/System/Library/ScriptingAdditions
/System/Library/StartupItems
~/Library/LaunchAgents
~/Library/Preferences/com.apple.loginitems.plist

See Mac OSX Startup (pdf) for more information.
Spotlight Suggestions
Disable Spotlight Suggestions in both the Spotlight preferences and Safari's Search preferences to avoid your search queries being sent to Apple.
Also disable Bing Web Searches in the Spotlight preferences to avoid your search queries being sent to Microsoft.
See fix-macosx.com for detailed instructions.

If you've upgraded to OS X 10.10 ""Yosemite"" and you're using the default settings, each time you start typing in Spotlight (to open an application or search for a file on your computer), your local search terms and location are sent to Apple and third parties (including Microsoft).

Note This Web site and instructions may no longer work on macOS Sierra - see issue 164.
For comparison to Windows 10, see https://fix10.isleaked.com/
Homebrew
Consider using Homebrew to make software installations easier and to update userland tools (see Apple's great GPL purge).
Note If you have not already installed Xcode or Command Line Tools, use xcode-select --install to download and install them, or check Apple's developer site.
Install Homebrew:
$ mkdir homebrew && curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew
Edit PATH in your shell or shell rc file to use ~/homebrew/bin and ~/homebrew/sbin. For example, echo 'PATH=$PATH:~/homebrew/sbin:~/homebrew/bin' >> .zshrc, then change your login shell to Z shell with chsh -s /bin/zsh, open a new Terminal window and run brew update.
Homebrew uses SSL/TLS to talk with GitHub and verifies integrity of downloaded packages, so it's fairly secure.
Remember to periodically run brew update and brew upgrade on trusted and secure networks to download and install software updates. To get information on a package before installation, run brew info <package> and check its recipe online.
According to Homebrew's Anonymous Aggregate User Behaviour Analytics, Homebrew gathers anonymous aggregate user behaviour analytics and reporting these to Google Analytics.
To opt out of Homebrew's analytics, you can set export HOMEBREW_NO_ANALYTICS=1 in your environment or shell rc file, or use brew analytics off.
You may also wish to enable additional security options, such as HOMEBREW_NO_INSECURE_REDIRECT=1 and HOMEBREW_CASK_OPTS=--require-sha.
DNS
Hosts file
Use the hosts file to block known malware, advertising or otherwise unwanted domains.
Edit the hosts file as root, for example with sudo vi /etc/hosts. The hosts file can also be managed with the GUI app 2ndalpha/gasmask.
To block a domain by A record, append any one of the following lines to /etc/hosts:
0 example.com
0.0.0.0 example.com
127.0.0.1 example.com

Note IPv6 uses the AAAA DNS record type, rather than A record type, so you may also want to block those connections by also including ::1 example.com entries, like shown here.
There are many lists of domains available online which you can paste in, just make sure each line starts with 0, 0.0.0.0, 127.0.0.1, and the line 127.0.0.1 localhost is included.
For hosts lists, see someonewhocares.org, l1k/osxparanoia/blob/master/hosts and StevenBlack/hosts.
Append a list of hosts with the tee command and confirm only non-routable addresses or comments were added:
$ curl https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts | sudo tee -a /etc/hosts

$ wc -l /etc/hosts
65580

$ egrep -ve ""^#|^255.255.255|^0.0.0.0|^127.0.0.1|^0 "" /etc/hosts | sort | uniq | sort
::1 localhost
fe80::1%lo0 localhost
[should not return any other IP addresses]
See man hosts and FreeBSD Configuration Files for more information.
See the dnsmasq section of this guide for more hosts blocking options.
dnscrypt
To encrypt outgoing DNS traffic, consider using jedisct1/dnscrypt-proxy. In combination with dnsmasq and DNSSEC, the integrity and authenticity of DNS traffic is greatly improved.
JayBrown/DNSCrypt-Menu and jedisct1/bitbar-dnscrypt-proxy-switcher provide a graphical user interface to dnscrypt.
Install dnscrypt from Homebrew and follow the instructions to configure and start dnscrypt-proxy:
$ brew install dnscrypt-proxy
If using in combination with Dnsmasq, find the file homebrew.mxcl.dnscrypt-proxy.plist by running
$ brew info dnscrypt-proxy
which will show a location like /usr/local/etc/dnscrypt-proxy.toml
Open it in a text editor, find the line starting with listen_addresses = and edit that line to use DNScrypt on a port other than 53, like 5355:
listen_addresses = ['127.0.0.1:5355', '[::1]:5355']

Start DNSCrypt:
$ sudo brew services restart dnscrypt-proxy
Make sure DNSCrypt is running:
$ sudo lsof +c 15 -Pni UDP:5355
COMMAND          PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
dnscrypt-proxy 15244 nobody    7u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   10u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355
dnscrypt-proxy 15244 nobody   12u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   14u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355

By default, dnscrypt-proxy runs on localhost (127.0.0.1), port 53,
and under the ""nobody"" user using the resolvers specified in https://raw.githubusercontent.com/DNSCrypt/dnscrypt-resolvers/master/v2/public-resolvers.md. If you would like to change these settings, you will have to edit the configuration file (e.g. listen_addresses, user_name, urls, etc.)

This can be accomplished by editing /usr/local/etc/dnscrypt-proxy.toml as described above.
You can run your own dnscrypt server (see also drduh/Debian-Privacy-Server-Guide#dnscrypt) from a trusted location or use one of many public servers instead.
Confirm outgoing DNS traffic is encrypted:
$ sudo tcpdump -qtni en0
IP 10.8.8.8.59636 > 107.181.168.52: UDP, length 512
IP 107.181.168.52 > 10.8.8.8.59636: UDP, length 368

$ dig +short -x 128.180.155.106.49321
d0wn-us-ns4
dnscrypt-proxy also has the capability to blacklist domains, including the use of wild-cards. See the Sample configuration file for dnscrypt-proxy for the options.
Note Applications and programs may resolve DNS using their own provided servers. If dnscrypt-proxy is used, it is possible to disable all other, non-dnscrypt DNS traffic with the following pf rules:
block drop quick on !lo0 proto udp from any to any port = 53
block drop quick on !lo0 proto tcp from any to any port = 53
See also What is a DNS leak, the mDNSResponder manual page and ipv6-test.com.
Dnsmasq
Among other features, dnsmasq is able to cache replies, prevent upstream queries for unqualified names, and block entire top-level domain names.
Use in combination with DNSCrypt to additionally encrypt outgoing DNS traffic.
If you don't wish to use DNSCrypt, you should at least use DNS not provided by your ISP. Two popular alternatives are Google DNS and OpenDNS.
(Optional) DNSSEC is a set of extensions to DNS which provide to DNS clients (resolvers) origin authentication of DNS data, authenticated denial of existence, and data integrity. All answers from DNSSEC protected zones are digitally signed. The signed records are authenticated via a chain of trust, starting with a set of verified public keys for the DNS root-zone. The current root-zone trust anchors may be downloaded from IANA website. There are a number of resources on DNSSEC, but probably the best one is dnssec.net website.
Install Dnsmasq (DNSSEC is optional):
$ brew install dnsmasq --with-dnssec
Download drduh/config/dnsmasq.conf:
$ curl -o homebrew/etc/dnsmasq.conf https://raw.githubusercontent.com/drduh/config/master/dnsmasq.conf

Edit the file and examine all the options. To block entire levels of domains, append drduh/config/domains or your own rules.
Install and start the program (sudo is required to bind to privileged port 53):
$ sudo brew services start dnsmasq
To set Dnsmasq as your local DNS server, open System Preferences > Network and select the active interface, then the DNS tab, select + and add 127.0.0.1, or use:
$ sudo networksetup -setdnsservers ""Wi-Fi"" 127.0.0.1
Make sure Dnsmasq is correctly configured:
$ scutil --dns | head
DNS configuration

resolver #1
  search domain[0] : whatever
  nameserver[0] : 127.0.0.1
  flags    : Request A records, Request AAAA records
  reach    : 0x00030002 (Reachable,Local Address,Directly Reachable Address)

$ networksetup -getdnsservers ""Wi-Fi""
127.0.0.1
Note Some VPN software overrides DNS settings on connect. See issue #24 for more information.
Test DNSSEC validation
Test DNSSEC validation succeeds for signed zones - the reply should have NOERROR status and contain ad flag:
$ dig +dnssec icann.org
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 47039
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
Test DNSSEC validation fails for zones that are signed improperly - the reply should have SERVFAIL status:
$ dig www.dnssec-failed.org
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 15190
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1
Captive portal
When macOS connects to new networks, it checks for Internet connectivity and may launch a Captive Portal assistant utility application.
An attacker could trigger the utility and direct a Mac to a site with malware without user interaction, so it's best to disable this feature and log in to captive portals using your regular Web browser by navigating to a non-secure HTTP page and accepting a redirect to the captive portal login interface (after disabling any custom proxy or DNS settings).
$ sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control.plist Active -bool false
Also see Apple's secret ""wispr"" request, How to disable the captive portal window in Mac OS Lion and An undocumented change to Captive Network Assistant settings in OS X 10.10 Yosemite.
Certificate authorities
macOS comes with over 200 root authority certificates installed from for-profit corporations like Apple, Verisign, Thawte, Digicert and government agencies from China, Japan, Netherlands, U.S., and more! These Certificate Authorities (CAs) are capable of issuing SSL/TLS certificates for any domain, code signing certificates, etc.
For more information, see Certification Authority Trust Tracker, Analysis of the HTTPS certificate ecosystem (pdf), and You Won‚Äôt Be Needing These Any More: On Removing Unused Certificates From Trust Stores (pdf).
Inspect system root certificates in Keychain Access, under the System Roots tab or by using the security command line tool and /System/Library/Keychains/SystemRootCertificates.keychain file.
Disable certificate authorities through Keychain Access by marking them as Never Trust and closing the window:

The risk of a man in the middle attack in which a coerced or compromised certificate authority trusted by your system issues a fake/rogue SSL certificate is quite low, but still possible.
OpenSSL
The version of OpenSSL in Sierra is 0.9.8zh which is not current. It doesn't support TLS 1.1 or newer, elliptic curve ciphers, and more.
Since Apple's official supported TLS library on macOS is Secure Transport, OpenSSL deprecated is considered deprecated (according to the Cryptographic Services Guide. Apple's version of OpenSSL may also have patches which may surprise you.
If you're going to use OpenSSL on your Mac, download and install a recent version of OpenSSL with brew install openssl. Note, linking brew to be used in favor of /usr/bin/openssl may interfere with built-in software. See issue #39.
Compare the TLS protocol and cipher between the homebrew version and the system version of OpenSSL:
$ ~/homebrew/bin/openssl version; echo | ~/homebrew/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 1.0.2j  26 Sep 2016
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-GCM-SHA256

$ /usr/bin/openssl version; echo | /usr/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 0.9.8zh 14 Jan 2016
SSL-Session:
    Protocol  : TLSv1
    Cipher    : AES128-SHA
See also Comparison of TLS implementations, How's My SSL and Qualys SSL Labs Tools.
Curl
The version of Curl which comes with macOS uses Secure Transport for SSL/TLS validation.
If you prefer to use OpenSSL, install with brew install curl --with-openssl and ensure it's the default with brew link --force curl
Download drduh/config/curlrc or see the man page:
$ curl -o ~/.curlrc https://raw.githubusercontent.com/drduh/config/master/curlrc
Web
Privoxy
Consider using Privoxy as a local proxy to filter Web browsing traffic.
Note macOS proxy settings are not universal; apps and services may not honor system proxy settings. Ensure the application you wish to proxy is correctly configured and manually verify connections don't leak. Additionally, it may be possible to configure the pf firewall to transparently proxy all traffic.
A signed installation package for privoxy can be downloaded from silvester.org.uk or Sourceforge. The signed package is more secure than the Homebrew version, and attracts full support from the Privoxy project.
Alternatively, install and start privoxy using Homebrew:
$ brew install privoxy

$ brew services start privoxy
By default, privoxy listens on localhost, TCP port 8118.
Set the system HTTP proxy for your active network interface 127.0.0.1 and 8118 (This can be done through System Preferences > Network > Advanced > Proxies):
$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 8118
(Optional) Set the system HTTPS proxy, which still allows for domain name filtering, with:
$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 8118
Confirm the proxy is set:
$ scutil --proxy
<dictionary> {
  ExceptionsList : <array> {
    0 : *.local
    1 : 169.254/16
  }
  FTPPassive : 1
  HTTPEnable : 1
  HTTPPort : 8118
  HTTPProxy : 127.0.0.1
}
Visit http://p.p/ in a browser, or with Curl:
$ ALL_PROXY=127.0.0.1:8118 curl -I http://p.p/
HTTP/1.1 200 OK
Content-Length: 2401
Content-Type: text/html
Cache-Control: no-cache
Privoxy already comes with many good rules, however you can also write your own.
Download drduh/config/privoxy/config and drduh/config/privoxy/user.action to get started:
$ curl -o homebrew/etc/privoxy/config https://raw.githubusercontent.com/drduh/config/master/privoxy/config

$ curl -o homebrew/etc/privoxy/user.action https://raw.githubusercontent.com/drduh/config/master/privoxy/user.action
Restart Privoxy: and verify it's blocking and redirecting traffic:
$ sudo brew services restart privoxy

$ ALL_PROXY=127.0.0.1:8118 curl ads.foo.com/ -IL
HTTP/1.1 403 Request blocked by Privoxy
Content-Type: image/gif
Content-Length: 64
Cache-Control: no-cache

$ ALL_PROXY=127.0.0.1:8118 curl imgur.com/ -IL
HTTP/1.1 302 Local Redirect from Privoxy
Location: https://imgur.com/
Content-Length: 0
Date: Sun, 09 Oct 2016 18:48:19 GMT

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
You can replace ad images with pictures of kittens, for example, by starting a local Web server and redirecting blocked requests to localhost.
Browser
The Web browser poses the largest security and privacy risk, as its fundamental job is to download and execute untrusted code from the Internet. This is an important statement. The unique use case of Web Browsers of operation in hostile environments, has forced them to adopt certain impressive security features. The cornerstone of Web Browser security is the Same Origin Policy (SOP). In a few words, SOP prevents a malicious script on one page from obtaining access to sensitive data on another web page through that page's Document Object Model (DOM). If SOP is compromised, the security of the whole Web Browser is compromised.
The best tip to ensure secure browsing regardless your choice of Web Browser is proper security hygiene. The majority of Web Browser exploits require social engineering attacks to achieve native code execution. Always be mindful of the links you click and be extra careful when websites ask you to download and install software. 99% percent of the time that software is malware.
Another important consideration about Web Browser security is Web Extensions. Web Extensions greatly increase the attack surface of the Web Browser. This is an issue that plagues Firefox and Chrome alike. Luckily, Web Extensions can only access specific browser APIs that are being governed by their manifest. That means we can quickly audit their behavior and remove them if they request access to information they shouldn't (why would an Ad blocker require camera access?). In the interest of security, it is best to limit your use of Web Extensions.
Mozilla Firefox, Google Chrome, Safari, and Tor Browser are covered in this guide. Each Web Browser offers certain benefits and drawbacks regarding their security and privacy. It is best to make an informed choice and not necessarily commit to only one.
Firefox
Mozilla Firefox is an excellent browser as well as being completely open source. Currently, Firefox is in a renaissance period. It replaces major parts of its infrastructure and code base under projects Quantum and Photon. Part of the Quantum project is to replace C++ code with Rust. Rust is a systems programming language with a focus on security and thread safety. It is expected that Rust adoption will greatly improve the overall security posture of Firefox.
Firefox offers a similar security model to Chrome: it has a bug bounty program, although it is not a lucrative as Chrome's. Firefox follows a six-week release cycle similar to Chrome. See discussion in issues #2 and #90 for more information about certain differences in Firefox and Chrome.
Firefox supports user-supplied configuration files. See drduh/config/user.js, pyllyukko/user.js and ghacksuserjs/ghacks-user.js for recommended preferences and hardening measures. Also see NoScript, an extension which allows whitelist-based, pre-emptive script blocking.
Firefox is focused on user privacy. It supports tracking protection in Private Browsing mode. The tracking protection can be enabled for the default account, although it may break the browsing experience on some websites. Another feature for added privacy unique to Firefox is Containers, similar to Chrome profiles.
Previous versions of Firefox used a Web Extension SDK that was quite invasive and offered immense freedom to developers. Sadly, that freedom also introduced a number of vulnerabilities in Firefox that greatly affected its users. You can find more information about vulnerabilities introduced by Firefox's legacy extensions in this paper (pdf). Currently, Firefox only supports Web Extensions through the Web Extension Api, which is very similar to Chrome's.
Submission of Web Extensions in Firefox is free. Web Extensions in Firefox most of the time are open source, although certain Web Extensions are proprietary.
Note Similar to Chrome and Safari, Firefox allows account sync across multiple devices. While stored login passwords are encrypted, Firefox does not require a password to reveal their plain text format. Firefox only displays as yes/no prompt. This is an important security issue. Keep that in mind if you sign in to your Firefox account from devices that do not belong to you and leave them unattended. The issue has been raised among the Firefox community and hopefully will be resolved in the coming versions.
See drduh/config/firefox.user.js for additional Firefox configuration options to improve security and privacy.
Chrome
Google Chrome is based on the open source Chromium project with certain proprietary components:

Automatic updates with GoogleSoftwareUpdateDaemon.
Usage tracking and crash reporting, which can be disabled through Chrome's settings.
Chrome Web Store.
Adobe Flash Plugin - supports a Pepper API version of Adobe Flash which gets updated automatically with Chrome.
Media Codec support - adds support for proprietary codecs.
Chrome PDF viewer.
Non-optional tracking. Google Chrome installer includes a randomly generated token. The token is sent to Google after the installation completes in order to measure the success rate. The RLZ identifier stores information ‚Äì in the form of encoded strings ‚Äì like the source of chrome download and installation week. It doesn‚Äôt include any personal information and it‚Äôs used to measure the effectiveness of a promotional campaign. Chrome downloaded from Google‚Äôs website doesn‚Äôt have the RLZ identifier. The source code to decode the strings is made open by Google.

Chrome offers account sync between multiple devices. Part of the sync data are stored website credentials. The login passwords are encrypted and in order to access them, a user's Google account password is required. You can use your Google account to sign to your Chrome customized settings from other devices while retaining your the security of your passwords.
Chrome's Web store for extensions requires a 5 dollar lifetime fee in order to submit extensions. The low cost allows the development of many quality Open Source Web Extensions that do not aim to monetize through usage.
Chrome has the largest share of global usage and is the preferred target platform for the majority of developers. Major technologies are based on Chrome's Open Source components, such as node.js which uses Chrome's V8 Engine and the Electron framework, which is based on Chromium and node.js. Chrome's vast user base makes it the most attractive target for threat actors and security researchers. Despite under constants attacks, Chrome has retained an impressive security track record over the years. This is not a small feat.
Chrome offers separate profiles, sandboxing, frequent updates (including Flash, although you should disable it - see below), and carries impressive credentials. In addition, Google offers a very lucrative bounty program for reporting vulnerabilities along with its own Project Zero. This means that a large number of highly talented and motivated people are constantly auditing Chrome's code base.
Create separate Chrome profiles to reduce XSS risk and compartmentalize cookies/identities. In each profile, either disable Javascript in Chrome settings and manually whitelist allowed origins - or use uBlock Origin to manage Javascript and/or disable third-party scripts/frames. Also install HTTPSEverywhere to upgrade insecure connections.
Change the default search engine from Google to reduce additional tracking.
Disable DNS prefetching (see also DNS Prefetching and Its Privacy Implications (pdf)).
Read Chromium Security and Chromium Privacy for more detailed, technical information.
Read Google's privacy policy and learn which Google services collect personal information. Google is open about the data it stores and how it used them. Users can opt out from many of those services and see what type of information Google has stored from their account settings.
Safari
Safari is the default Web browser of macOS. It is also the most optimized browser for reducing battery use. Safari, like Chrome, has both Open Source and proprietary components. Safari is based on the open source Web Engine WebKit, which is ubiquitous among the macOS ecosystem. WebKit is used by Apple apps such as Mail, iTunes, iBooks, and the App Store. Chrome's Blink engine is a fork of WebKit and both engines share a number of similarities.
Safari supports certain unique features that benefit user security and privacy. Content blockers enables the creation of content blocking rules without using Javascript. This rule based approach greatly improves memory user, security, and privacy. Safari 11 introduced an Intelligent Tracking Prevention system. This feature automatically removes tracking data stored in Safari after a period of non-interaction by the user from the tracker's website.
Similar to Chrome and Firefox, Safari offers an invite only bounty program for bug reporting to a select number of security researchers. The bounty program was announced during Apple's presentation at BlackHat 2016.
Web Extensions in Safari have an additional option to use native code in the Safari's sandbox environment, in addition to Web Extension APIs. Web Extensions in Safari are also distributed through Apple's App store. App store submission comes with the added benefit of Web Extension code being audited by Apple. On the other hand App store submission comes at a steep cost. Yearly developer subscription fee costs 100 USD (in contrast to Chrome's 5 dollar lifetime fee and Firefox's free submission). The high cost is prohibitive for the majority of Open Source developers. As a result, Safari has very few extensions to choose from. However, you should keep the high cost in mind when installing extensions. It is expected that most Web Extensions will have some way of monetizing usage in order to cover developer costs. And be extra careful when the Web Extension's source code is not Open Source. On a side note, some Safari extensions are Open Source and freely available. Be grateful to those developers.
Safari syncs user preferences and saved passwords with iCloud Keychain. In order to be viewed in plain text, a user must input the account password of the current device. This means that users can sync data across devices with added security.
Safari follows a slower release cycle than Chrome and Firefox (3-4 minor releases, 1 major release, per year). Newer features are slower to be adopted to the stable channel. Although security updates in Safari are handled independent of the stable release schedule and issued automatically through the App store. The Safari channel that follows a six-week release cycle (similar to as Chrome and Firefox) is called Safari Technology Preview and it is the recommended option instead of the stable channel of Safari.
An excellent open source ad blocker for Safari that fully leverages Content blockers is dgraham/Ka-Block. Ka-Block is focussed on user privacy. The only time the extension makes a network connection is when a new version of the extension is released. See also el1t/uBlock-Safari to disable hyperlink auditing beacons.
Other Web browsers
Many Chromium-derived browsers are not recommended. They are usually closed source, poorly maintained, have bugs, and make dubious claims to protect privacy. See The Private Life of Chromium Browsers.
Other miscellaneous browsers, such as Brave, are not evaluated in this guide, so are neither recommended nor actively discouraged from use.
Web browsers and privacy
All Web Browsers retain certain information about our browsing habits. That information is used for a number of reasons. One of them is to improve the overall performance of the Web Browser. Most Web Browsers offer prediction services to resolve typos or URL redirections, store analytics data of browsing patterns, crash reports and black listing of known malicious servers. Those options can be turned on and off from each Web browser's settings panel.
Since Web browsers execute untrusted code from the server, it is important to understand what type of information can be accessed. The Navigator interface gives access to information about the Web Browser's user agent. Those include information such as the operating system, Web sites' permissions, and the device's battery level. For more information about security conscious browsing and what type of information is being ""leaked"" by your browser, see HowTo: Privacy & Security Conscious Browsing, browserleaks.com and EFF Panopticlick.
To hinder third party trackers, it is recommended to disable third-party cookies in Web browser settings. A third party cookie is a cookie associated with a file requested by a different domain than the one the user is currently viewing. Most of the time third-party cookies are used to create browsing profiles by tracking a user's movement on the web. Disabling third-party cookies prevents HTTP responses and scripts from other domains from setting cookies. Moreover, cookies are removed from requests to domains that are not the document origin domain, so cookies are only sent to the current site that is being viewed.
Also be aware of WebRTC, which may reveal your local or public (if connected to VPN) IP address(es). In Firefox and Chrome/Chromium this can be disabled with extensions such as uBlock Origin and rentamob/WebRTC-Leak-Prevent. Disabling WebRTC in Safari is only possible with a system hack.
Plugins
Adobe Flash, Oracle Java, Adobe Reader, Microsoft Silverlight (Netflix now works with HTML5) and other plugins are security risks and should not be installed.
If they are necessary, only use them in a disposable virtual machine and subscribe to security announcements to make sure you're always patched.
See Hacking Team Flash Zero-Day, Java Trojan BackDoor.Flashback, Acrobat Reader: Security Vulnerabilities, and Angling for Silverlight Exploits for examples.
Tor
Tor is an anonymizing proxy which can be used for browsing the Web.
Download Tor Browser from Tor Project.
Do not attempt to configure other browsers or applications to use Tor as you may make a mistake which will compromise anonymity.
Download both the dmg and asc signature files, then verify the disk image has been signed by Tor developers:
$ cd ~/Downloads

$ file Tor*
TorBrowser-8.0.4-osx64_en-US.dmg:     bzip2 compressed data, block size = 900k
TorBrowser-8.0.4-osx64_en-US.dmg.asc: PGP signature Signature (old)

$ gpg Tor*asc
[...]
gpg: Can't check signature: No public key

$ gpg --recv 0x4E2C6E8793298290
gpg: key 0x4E2C6E8793298290: public key ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" imported
gpg: no ultimately trusted keys found
gpg: Total number processed: 1
gpg:               imported: 1

$ gpg --verify Tor*asc
gpg: assuming signed data in 'TorBrowser-8.0.4-osx64_en-US.dmg'
gpg: Signature made Mon Dec 10 07:16:22 2018 PST
gpg:                using RSA key 0xEB774491D9FF06E2
gpg: Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: EF6E 286D DA85 EA2A 4BA7  DE68 4E2C 6E87 9329 8290
     Subkey fingerprint: 1107 75B5 D101 FB36 BC6C  911B EB77 4491 D9FF 06E2
Make sure Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" appears in the output. The warning about the key not being certified is benign, as it has not yet been manually assigned trust.
See How to verify signatures for packages for more information.
To finish installing Tor Browser, open the disk image and drag the it into the Applications folder, or with:
$ hdiutil mount TorBrowser-8.0.4-osx64_en-US.dmg

$ cp -r /Volumes/Tor\ Browser/Tor\ Browser.app/ ~/Applications/

Verify the Tor application's code signature was made by with The Tor Project's Apple developer ID MADPSAYN6T, using the spctl -a -v and/or pkgutil --check-signature commands:
$ spctl -a -vv ~/Applications/Tor\ Browser.app
/Users/drduh/Applications/Tor Browser.app: accepted
source=Developer ID
origin=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)

$ pkgutil --check-signature ~/Applications/Tor\ Browser.app
Package ""Tor Browser.app"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
       SHA1 fingerprint: 95 80 54 F1 54 66 F3 9C C2 D8 27 7A 29 21 D9 61 11 93 B3 E8
       -----------------------------------------------------------------------------
    2. Developer ID Certification Authority
       SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
You can also use the codesign command to examine an application's code signature:
$ codesign -dvv ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox
Identifier=org.torproject.torbrowser
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20200 size=229 flags=0x0(none) hashes=4+3 location=embedded
Library validation warning=OS X SDK version before 10.9 does not support Library Validation
Signature size=4247
Authority=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
Authority=Developer ID Certification Authority
Authority=Apple Root CA
Signed Time=Dec 10, 2018 at 12:18:45 AM
Info.plist entries=24
TeamIdentifier=MADPSAYN6T
Sealed Resources version=2 rules=12 files=128
Internal requirements count=1 size=188
To view full certificate details for a signed application, extract them with codesign and decode it with openssl:
$ codesign -d --extract-certificates ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox

$ file codesign*
codesign0: data
codesign1: data
codesign2: data

$ openssl x509 -inform der -in codesign0 -subject -issuer -startdate -enddate -noout
subject= /UID=MADPSAYN6T/CN=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)/OU=MADPSAYN6T/O=The Tor Project, Inc/C=US
issuer= /CN=Developer ID Certification Authority/OU=Apple Certification Authority/O=Apple Inc./C=US
notBefore=Apr 12 22:40:13 2016 GMT
notAfter=Apr 13 22:40:13 2021 GMT

$ openssl x509 -inform der -in codesign0  -fingerprint -noout
SHA1 Fingerprint=95:80:54:F1:54:66:F3:9C:C2:D8:27:7A:29:21:D9:61:11:93:B3:E8

$ openssl x509 -inform der -in codesign0 -fingerprint -sha256 -noout
SHA256 Fingerprint=B5:0D:47:F0:3E:CB:42:B6:68:1C:6F:38:06:2B:C2:9F:41:FA:D6:54:F1:29:D3:E4:DD:9C:C7:49:35:FF:F5:D9
Tor traffic is encrypted to the exit node (i.e., cannot be read by a passive network eavesdropper), but Tor use can be identified - for example, TLS handshake ""hostnames"" will show up in plaintext:
$ sudo tcpdump -An ""tcp"" | grep ""www""
listening on pktap, link-type PKTAP (Apple DLT_PKTAP), capture size 262144 bytes
............."". ...www.odezz26nvv7jeqz1xghzs.com.........
.............#.!...www.bxbko3qi7vacgwyk4ggulh.com.........
.6....m.....>...:.........|../*	Z....W....X=..6...C../....................................0...0..0.......'....F./0..	*.H........0%1#0!..U....www.b6zazzahl3h3faf4x2.com0...160402000000Z..170317000000Z0'1%0#..U....www.tm3ddrghe22wgqna5u8g.net0..0..
See Tor Protocol Specification and Tor/TLSHistory for more information.
You may wish to additionally obfuscate Tor traffic using a pluggable transport, such as Yawning/obfs4proxy or SRI-CSL/stegotorus.
This can be done by setting up your own Tor relay or finding an existing private or public bridge to serve as an obfuscating entry node.
For extra security, use Tor inside a VirtualBox or VMware virtualized GNU/Linux or BSD machine.
Finally, remember the Tor network provides anonymity, which is not necessarily synonymous with privacy. The Tor network does not guarantee protection against a global observer capable of traffic analysis and correlation. See also Seeking Anonymity in an Internet Panopticon (pdf) and Traffic Correlation on Tor by Realistic Adversaries (pdf).
Also see Invisible Internet Project (I2P) and its Tor comparison.
VPN
Unencrypted network traffic is being actively monitored and possibly tampered with. Encrypted traffic still exposes connection metadata and could be used to infer behavior or specific actions.
It is a good idea to use a VPN with outgoing network traffic (not split tunnel) together with a trustworthy provider. drduh/Debian-Privacy-Server-Guide is one of many available guides for setting up a personal VPN server.
Don't just blindly sign up for a VPN service without understanding the full implications and how your traffic will be routed. If you don't understand how the VPN works or are not familiar with the software used, you are probably better off without it.
When choosing a VPN service or setting up your own, be sure to research the protocols, key exchange algorithms, authentication mechanisms, and type of encryption being used. Some protocols, such as PPTP, should be avoided in favor of OpenVPN, for example. Strong cryptographic algorithms like AES-256, RSA-4096, SHA-256 should be preferred.
Some clients may send traffic over the next available interface when VPN is interrupted or disconnected. See scy/8122924 for an example on how to allow traffic only over VPN.
Another set of scripts to lock down your system so it will only access the internet via a VPN can be found as part of the Voodoo Privacy project - sarfata/voodooprivacy and there is an updated guide to setting up an IPSec VPN on a virtual machine (hwdsl2/setup-ipsec-vpn) or a docker container (hwdsl2/docker-ipsec-vpn-server).
It may be worthwhile to consider the geographical location of the VPN provider. See further discussion in issue #114.
Also see this technical overview of the macOS built-in VPN L2TP/IPSec and IKEv2 client.
Further, it is possible to run the contemporary Linux-based Wireguard VPN either from a Linux VM or via a set of cross platform tools.
Other Open Source OpenVPN clients/GUI: Eddie, Pritunl are not evaluated in this guide, so are neither recommended nor actively discouraged from use.
PGP/GPG
PGP is a standard for encrypting email end to end. That means only the chosen recipients can decrypt a message, unlike regular email which is read and forever archived by providers.
GPG, or GNU Privacy Guard, is a GPL-licensed open source program compliant with the PGP standard.
GPG is used to verify signatures of software you download and install, as well as symmetrically or asymmetrically encrypt files and text.
Install from Homebrew with brew install gnupg.
If you prefer a graphical application, download and install GPG Suite.
Download drduh/config/gpg.conf to use recommended settings:
$ curl -o ~/.gnupg/gpg.conf https://raw.githubusercontent.com/drduh/config/master/gpg.conf
See drduh/YubiKey-Guide to securely generate and store GPG keys.
Read online guides and practice encrypting and decrypting email to yourself and your friends. Get them interested in this stuff!
OTR
OTR stands for off-the-record and is a cryptographic protocol for encrypting and authenticating conversations over instant messaging.
You can use OTR on top of any existing XMPP chat service, even Google Hangouts (which only encrypts conversations between users and the server using TLS).
The first time you start a conversation with someone new, you'll be asked to verify their public key fingerprint. Make sure to do this in person or by some other secure means (e.g. GPG encrypted mail).
A popular macOS GUI client for XMPP and other chat protocols is Adium.
Other XMPP clients include profanity and agl/xmpp-client. Another relatively new XMPP chat client is CoyIM, it's focused and security and has built-in support for OTR and Tor.
If you want to know how OTR works, read the paper Off-the-Record Communication, or, Why Not To Use PGP (pdf)
Viruses and malware
There is an ever-increasing amount of Mac malware in the wild. Macs aren't immune from viruses and malicious software!
Some malware comes bundled with both legitimate software, such as the Java bundling Ask Toolbar, and some with illegitimate software, such as Mac.BackDoor.iWorm bundled with pirated programs. Malwarebytes Anti-Malware for Mac is an excellent program for ridding oneself of ""garden-variety"" malware and other ""crapware"".
See Methods of malware persistence on Mac OS X (pdf) and Malware Persistence on OS X Yosemite to learn about how garden-variety malware functions.
You could periodically run a tool like Knock Knock to examine persistent applications (e.g. scripts, binaries). But by then, it is probably too late. Maybe applications such as Block Block and Ostiarius will help. See warnings and caveats in issue #90 first, however. An open-source alternative could be maclaunch.sh.
Anti-virus programs are a double-edged sword -- not so useful for advanced users and will likely increase attack surface against sophisticated threats; however possibly useful for catching ""garden variety"" malware on novice users' Macs. There is also the additional processing overhead to consider when using ""active"" scanning features.
See Sophail: Applied attacks against Antivirus (pdf), Analysis and Exploitation of an ESET Vulnerability, a trivial Avast RCE, Popular Security Software Came Under Relentless NSA and GCHQ Attacks, How Israel Caught Russian Hackers Scouring the World for U.S. Secrets and AVG: ""Web TuneUP"" extension multiple critical vulnerabilities.
Therefore, the best anti-virus is Common Sense 2019. See discussion in issue #44.
Local privilege escalation bugs are plenty on macOS, so always be careful when downloading and running untrusted programs or trusted programs from third party websites or downloaded over HTTP (example).
Subscribe to updates at The Safe Mac and Malwarebytes Blog for current Mac security news.
To scan an application with multiple AV products and examine its behavior, upload it to VirusTotal.
Also check out Hacking Team malware for macOS: root installation for MacOS, Support driver for Mac Agent and RCS Agent for Mac, which is a good example of advanced malware with capabilities to hide from userland (e.g., ps, ls), for example. For more, see A Brief Analysis of an RCS Implant Installer and reverse.put.as
System Integrity Protection
System Integrity Protection (SIP) is a security feature since OS X 10.11 ""El Capitan"". It is enabled by default, but can be disabled, which may be necessary to change some system settings, such as deleting root certificate authorities or unloading certain launch daemons. Keep this feature on, as it is by default.
From What's New in OS X 10.11:

A new security policy that applies to every running process, including privileged code and code that runs out of the sandbox. The policy extends additional protections to components on disk and at run-time, only allowing system binaries to be modified by the system installer and software updates. Code injection and runtime attachments to system binaries are no longer permitted.

Also see What is the ‚Äúrootless‚Äù feature in El Capitan, really?
Some MacBook hardware has shipped with SIP disabled. To verify SIP is enabled, use the command csrutil status, which should return: System Integrity Protection status: enabled. Otherwise, enable SIP through Recovery Mode.
Gatekeeper and XProtect
Gatekeeper and the quarantine system try to prevent unsigned or ""bad"" programs and files from running and opening.
XProtect prevents the execution of known bad files and outdated plugin versions, but does nothing to cleanup or stop existing malware.
Both offer trivial protection against common risks and are fine at default settings.
See also Mac Malware Guide : How does Mac OS X protect me? and Gatekeeper, XProtect and the Quarantine attribute.
Note Quarantine stores information about downloaded files in ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2, which may pose a privacy risk. To examine the file, simply use strings or the following command:
$ echo 'SELECT datetime(LSQuarantineTimeStamp + 978307200, ""unixepoch"") as LSQuarantineTimeStamp, ' \
  'LSQuarantineAgentName, LSQuarantineOriginURLString, LSQuarantineDataURLString from LSQuarantineEvent;' | \
  sqlite3 /Users/$USER/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
See here for more information.
To permanently disable this feature, clear the file and make it immutable:
$ :>~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2

$ sudo chflags schg ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
Metadata and artifacts
macOS attaches metadata (HFS+ extended attributes) to downloaded files, which can be viewed with the mdls and xattr commands:
$ ls -l@ ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
-rw-r--r--@ 1 drduh staff 63M Jan 1 12:00 TorBrowser-8.0.4-osx64_en-US.dmg
	com.apple.metadata:kMDItemWhereFroms	  46B
	com.apple.quarantine	  57B

$ mdls ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
kMDItemContentCreationDate         = 2019-01-01 00:00:00 +0000
kMDItemContentCreationDate_Ranking = 2019-01-01 00:00:00 +0000
kMDItemContentModificationDate     = 2019-01-01 00:00:00 +0000
kMDItemContentType                 = ""com.apple.disk-image-udif""
kMDItemContentTypeTree             = (
    ""public.archive"",
    ""public.item"",
    ""public.data"",
    ""public.disk-image"",
    ""com.apple.disk-image"",
    ""com.apple.disk-image-udif""
)
kMDItemDateAdded                   = 2019-01-01 00:00:00 +0000
kMDItemDateAdded_Ranking           = 2019-01-01 00:00:00 +0000
kMDItemDisplayName                 = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSContentChangeDate         = 2019-01-01 00:00:00 +0000
kMDItemFSCreationDate              = 2019-01-01 00:00:00 +0000
kMDItemFSCreatorCode               = """"
kMDItemFSFinderFlags               = 0
kMDItemFSHasCustomIcon             = (null)
kMDItemFSInvisible                 = 0
kMDItemFSIsExtensionHidden         = 0
kMDItemFSIsStationery              = (null)
kMDItemFSLabel                     = 0
kMDItemFSName                      = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSNodeCount                 = (null)
kMDItemFSOwnerGroupID              = 5000
kMDItemFSOwnerUserID               = 501
kMDItemFSSize                      = 65840402
kMDItemFSTypeCode                  = """"
kMDItemInterestingDate_Ranking     = 2019-01-01 00:00:00 +0000
kMDItemKind                        = ""Disk Image""
kMDItemWhereFroms                  = (
    ""https://dist.torproject.org/torbrowser/8.0.4/TorBrowser-8.0.4-osx64_en-US.dmg"",
    ""https://www.torproject.org/projects/torbrowser.html.en""
)

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
com.apple.metadata:kMDItemWhereFroms:
00000000 ¬†62 70 6C 69 73 74 30 30 A2 01 02 5F 10 4D 68 74 ¬†|bplist00..._.Mht|
00000010 ¬†74 70 73 3A 2F 2F 64 69 73 74 2E 74 6F 72 70 72 ¬†|tps://dist.torpr|
00000020 ¬†6F 6A 65 63 74 2E 6F 72 67 2F 74 6F 72 62 72 6F ¬†|oject.org/torbro|
[...]
com.apple.quarantine: 0081;58519ffa;Google Chrome.app;1F032CAB-F5A1-4D92-84EB-CBECA971B7BC
Metadata attributes can also be removed with the -d flag:
$ xattr -d com.apple.metadata:kMDItemWhereFroms ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -d com.apple.quarantine ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
[No output expected]
Other metadata and artifacts may be found in the directories including, but not limited to, ~/Library/Preferences/, ~/Library/Containers/<APP>/Data/Library/Preferences, /Library/Preferences, some of which is detailed below.
~/Library/Preferences/com.apple.sidebarlists.plist contains historical list of volumes attached. To clear it, use the command /usr/libexec/PlistBuddy -c ""delete :systemitems:VolumesList"" ~/Library/Preferences/com.apple.sidebarlists.plist
/Library/Preferences/com.apple.Bluetooth.plist contains Bluetooth metadata, including device history. If Bluetooth is not used, the metadata can be cleared with:
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist DeviceCache
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist IDSPairedDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANInterfaces
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist SCOAudioDevices
/var/spool/cups contains the CUPS printer job cache. To clear it, use the commands:
$ sudo rm -rfv /var/spool/cups/c0*
$ sudo rm -rfv /var/spool/cups/tmp/*
$ sudo rm -rfv /var/spool/cups/cache/job.cache*
To clear the list of iOS devices connected, use:
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist Devices
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist Devices
$ sudo rm -rfv /var/db/lockdown/*
Quicklook thumbnail data can be cleared using the qlmanage -r cache command, but this writes to the file resetreason in the Quicklook directories, and states that the Quicklook cache was manually cleared. Disable the thumbnail cache with qlmanage -r disablecache
It can also be manually cleared by getting the directory names with getconf DARWIN_USER_CACHE_DIR and sudo getconf DARWIN_USER_CACHE_DIR, then removing them:
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
Similarly, for the root user:
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
Also see 'quicklook' cache may leak encrypted data.
To clear Finder preferences:
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXDesktopVolumePositions
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXRecentFolders
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentMoveAndCopyDestinations
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentSearches
$ defaults delete ~/Library/Preferences/com.apple.finder.plist SGTRecentFileSearches
Additional diagnostic files may be found in the following directories - but caution should be taken before removing any, as it may break logging or cause other issues:
/var/db/CoreDuet/
/var/db/diagnostics/
/var/db/systemstats/
/var/db/uuidtext/
/var/log/DiagnosticMessages/

macOS stored preferred Wi-Fi data (including credentials) in NVRAM. To clear it, use the following commands:
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:current-network
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-networks
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-count
macOS may collect sensitive information about what you type, even if user dictionary and suggestions are off. To remove them, and prevent them from being created again, use the following commands:
$ rm -rfv ""~/Library/LanguageModeling/*"" ""~/Library/Spelling/*"" ""~/Library/Suggestions/*""
$ chmod -R 000 ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
$ chflags -R uchg ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
QuickLook application support metadata can be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Application Support/Quick Look/*""
$ chmod -R 000 ""~/Library/Application Support/Quick Look""
$ chflags -R uchg ""~/Library/Application Support/Quick Look""
Document revision metadata is stored in /.DocumentRevisions-V100 and can be cleared and locked with the following commands - caution should be taken as this may break some core Apple applications:
$ sudo rm -rfv /.DocumentRevisions-V100/*
$ sudo chmod -R 000 /.DocumentRevisions-V100
$ sudo chflags -R uchg /.DocumentRevisions-V100
Saved application state metadata may be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Saved Application State/*""
$ rm -rfv ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chmod -R 000 ""~/Library/Saved Application State/""
$ chmod -R 000 ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chflags -R uchg ""~/Library/Saved Application State/""
$ chflags -R uchg ""~/Library/Containers/<APPNAME>/Saved Application State""
Autosave metadata can be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ rm -rfv ""~/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Autosave Information""
The Siri analytics database, which is created even if the Siri launch agent disabled, can be cleared and locked with the following commands:
$ rm -rfv ~/Library/Assistant/SiriAnalytics.db
$ chmod -R 000 ~/Library/Assistant/SiriAnalytics.db
$ chflags -R uchg ~/Library/Assistant/SiriAnalytics.db
~/Library/Preferences/com.apple.iTunes.plist contains iTunes metadata. Recent iTunes search data may be cleared with the following command:
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist recentSearches
If you do not use Apple ID-linked services, the following keys may be cleared, too, using the following commands:
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist StoreUserInfo
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist WirelessBuddyID
All media played in QuickTime Player can be found in:
~/Library/Containers/com.apple.QuickTimePlayerX/Data/Library/Preferences/com.apple.QuickTimePlayerX.plist

Additional metadata may exist in the following files:
~/Library/Containers/com.apple.appstore/Data/Library/Preferences/com.apple.commerce.knownclients.plist
~/Library/Preferences/com.apple.commerce.plist
~/Library/Preferences/com.apple.QuickTimePlayerX.plist

Passwords
Generate strong passwords with several programs or directly from /dev/urandom:
$ openssl rand -base64 30
qb8ZWbUU2Ri3FOAPY/1wKSFAJwMXmpQM4mZU4YbO

$ gpg --gen-random -a 0 90 | fold -w 40
3e+kfHOvovHVXxZYPgu+OOWQ1g1ttbljr+kNGv7f
loD//RsjUXYGIjfPM/bT0itsoEstyGLVUsFns8wP
zYM8VRBga+TsnxWrS7lWKfH1uvVPowzkq9kXCdvJ

$ LANG=C tr -dc 'A-F0-9' < /dev/urandom | fold -w 40 | head -n 5
45D0371481EE5E5A5C1F68EA59E69F9CA52CB321
A30B37A00302643921F205621B145E7EAF520164
B6EF38A2DA1D0586D20105502AFFF0468EA5F16A
029D6EA9F76CD64D3356E342EA154BEFEBE23387
07F468F0569579A0A06471247CABC4F4C1386E24

$ tr -dc '[:alnum:]' < /dev/urandom | fold -w 40 | head -n5
zmj8S0iuxud8y8YHjzdg7Hefu6U1KAYBiLl3aE8v
nCNpuMkWohTjQHntTzbiLQJG5zLzEHWSWaYSwjtm
R2L6M909S3ih852IkJqQFMDawCiHcpPBxlllAPrt
aZOXKVUmxhzQwVSYb6nqAbGTVMFSJOLf094bFZAb
HfgwSNlkVBXwIPQST6E6x6vDNCCasMLSSOoTUfSK

$ tr -dc '[:lower:]' < /dev/urandom | fold -w 40 | head -n5
gfvkanntxutzwxficgvavbwdvttexdezdftvvtmn
lgrsuiugwkqbtbkyggcbpbqlynwbiyxzlabstqcf
ufctdlsbyonkowzpmotxiksnsbwdzkjrjsupoqvr
hjwibdjxtmuvqricljayzkgdfztcmapsgwsubggr
bjstlmvwjczakgeetkbmwbjnidbeaerhaonpkacg

$ tr -dc '[:upper:]' < /dev/urandom | fold -w 40 | head -n5
EUHZMAOBOLNFXUNNDSTLJTPDCPVQBPUEQOLRZUQZ
HVNVKBEPAAYMXRCGVCNEZLFHNUYMRYPTWPWOOZVM
TAHEUPQJTSYQVJVYSKLURESMKWEZONXLUDHWQODB
PRDITWMAXXZLTRXEEOGOSGAWUXYDGDRJYRHUWICM
VHERIQBLBPHSIUZSGYZRDHTNAPUGJMRODIKBWZRJ

$ tr -dc '[:graph:]' < /dev/urandom | fold -w 40 | head -n5
n\T2|zUz:\C,@z9!#p3!B/[t6m:B94}q&t(^)Ol~
J%MMDbAgGdP}zrSQO!3mrP3$w!.[Ng_xx-_[C<3g
^)6V&*<2""ZOgU.mBd]iInvFKiT<dq~y\O[cdDK`V
+RE]UYPIf3:StX`y#w,.iG~g""urD)'FnDIFI_q^)
6?HRillpgvvFDBAr4[:H{^oAL<`Em7$roF=2w;1~
You can also generate passwords, even memorable ones, using Keychain Access password assistant, or a command line equivalent like anders/pwgen.
Keychains are encrypted with a PBKDF2 derived key and are a pretty safe place to store credentials. See also Breaking into the OS X keychain. Also be aware that Keychain does not encrypt the names corresponding to password entries.
Alternatively, you can manage an encrypted passwords file yourself with GnuPG (see drduh/Purse and drduh/pwd.sh for example).
In addition to passwords, ensure eligible online accounts, such as GitHub, Google accounts, banking, have two factor authentication enabled.
Look to Yubikey for a two factor and private key (e.g., ssh, gpg) hardware token. See drduh/YubiKey-Guide and trmm.net/Yubikey. One of two Yubikey's slots can also be programmed to emit a long, static password (which can be used in combination with a short, memorized password, for example).
In Addition to Login and other PAMs, you can use Yubikey to secure your login and sudo, here is a pdf guide from Yubico. Yubikey are a bit pricey, there is cheaper alternative, but not as capable, U2F Zero. Here is a great guide to set it up
Backup
Always encrypt files locally before backing them up to external media or online services.
One way is to use a symmetric cipher with GPG and a password of your choosing. Files can also be encrypted to a public key with GPG, with the private key stored on YubiKey.
To compress and encrypt a directory:
$ tar zcvf - ~/Downloads | gpg -c > ~/Desktop/backup-$(date +%F-%H%M).tar.gz.gpg
tar: Removing leading '/' from member names
a Users/drduh/Downloads
a Users/drduh/Downloads/.DS_Store
a Users/drduh/Downloads/.localized
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
To decrypt and decompress the directory:
$ gpg -o ~/Desktop/decrypted-backup.tar.gz -d ~/Desktop/backup-2015-01-01-0000.tar.gz.gpg
gpg: AES256 encrypted data
gpg: encrypted with 1 passphrase

$ tar zxvf ~/Desktop/decrypted-backup.tar.gz
tar: Removing leading '/' from member names
x Users/drduh/._Downloads
x Users/drduh/Downloads/
x Users/drduh/Downloads/._.DS_Store
x Users/drduh/Downloads/.DS_Store
x Users/drduh/Downloads/.localized
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
You can also create and use encrypted volumes using Disk Utility or hdiutil:
$ hdiutil create ~/Desktop/encrypted.dmg -encryption -size 50M -volname ""secretStuff"" -fs JHFS+
Enter a new password to secure ""encrypted.dmg"":
Re-enter new password:
....................................
Created: /Users/drduh/Desktop/encrypted.img

$ hdiutil mount ~/Desktop/encrypted.dmg
Enter password to access ""encrypted.dmg"":
[...]
/Volumes/secretStuff

$ cp -v ~/Documents/passwords.txt /Volumes/secretStuff
[...]

$ hdiutil eject /Volumes/secretStuff
""disk4"" unmounted.
""disk4"" ejected.
See also the following applications and services: Tresorit, SpiderOak, Arq, Espionage, and restic.
Wi-Fi
macOS remembers access points it has connected to. Like all wireless devices, the Mac will broadcast all access point names it remembers (e.g., MyHomeNetwork) each time it looks for a network, such as when waking from sleep.
This is a privacy risk, so remove networks from the list in System Preferences > Network > Advanced when they are no longer needed.
Also see Signals from the Crowd: Uncovering Social Relationships through Smartphone Probes (pdf) and Wi-Fi told me everything about you (pdf).
Saved Wi-Fi information (SSID, last connection, etc.) can be found in:
/Library/Preferences/SystemConfiguration/com.apple.airport.preferences.plist

You may want to spoof the MAC address of the network card before connecting to new and untrusted wireless networks to mitigate passive fingerprinting:
$ sudo ifconfig en0 ether $(openssl rand -hex 6 | sed 's%\(..\)%\1:%g; s%.$%%')
It is also good to know that macOS will store Wi-Fi SSIDs and passwords in NVRAM, because Recovery Mode needs access to restore from the Internet. Be sure to either clear NVRAM or de-authenticate your Mac from your Apple account, which will clear the NVRAM, before passing a Mac along. (Resetting the SMC will clear some of the NVRAM, but not all.)
Note MAC addresses will reset to hardware defaults on each boot.
Also see feross/SpoofMAC.
Finally, WEP protection on wireless networks is not secure and you should favor connecting to WPA2 protected networks only to mitigate the risk of passive eavesdroppers.
SSH
For outgoing SSH connections, use hardware or password-protected keys, set up remote hosts and consider hashing them for added privacy. See drduh/config/ssh_config for recommended client options.
You can also use ssh to create an encrypted tunnel to send traffic through, similar to a VPN.
For example, to use Privoxy running on a remote host port 8118:
$ ssh -C -L 5555:127.0.0.1:8118 you@remote-host.tld

$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 5555

$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 5555
Or to use an ssh connection as a SOCKS proxy:
$ ssh -NCD 3000 you@remote-host.tld
By default, macOS does not have sshd or Remote Login enabled.
To enable sshd and allow incoming ssh connections:
$ sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist
Or use the System Preferences > Sharing menu.
If enabling sshd, be sure to disable password authentication and consider further hardening your configuration. See drduh/config/sshd_config for recommended options.
Confirm whether sshd is running:
$ sudo lsof -Pni TCP:22
Physical access
Keep your Mac physically secure at all times. Don't leave it unattended in hotels and such.
A skilled attacker with unsupervised physical access to your computer can infect the boot ROM to install a keylogger and steal your password - see Thunderstrike for an example.
A helpful tool is usbkill, which is an anti-forensic kill-switch that waits for a change on your USB ports and then immediately shuts down your computer.
Consider purchasing a privacy filter for your screen to thwart shoulder surfers.
Superglues or epoxy resins can also be used to disable physical access to computer ports. Nail polish and tamper-evidence seals can be applied to components to detect tampering.
System monitoring
OpenBSM audit
macOS has a powerful OpenBSM (Basic Security Module) auditing capability. You can use it to monitor process execution, network activity, and much more.
To tail audit logs, use the praudit utility:
$ sudo praudit -l /dev/auditpipe
header,201,11,execve(2),0,Thu Sep  1 12:00:00 2015, + 195 msec,exec arg,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,attribute,100755,root,wheel,16777220,986535,0,subject,drduh,root,wheel,root,wheel,412,100005,50511731,0.0.0.0,return,success,0,trailer,201,
header,88,11,connect(2),0,Thu Sep  1 12:00:00 2015, + 238 msec,argument,1,0x5,fd,socket-inet,2,443,173.194.74.104,subject,drduh,root,wheel,root,wheel,326,100005,50331650,0.0.0.0,return,failure : Operation now in progress,4354967105,trailer,88
header,111,11,OpenSSH login,0,Thu Sep  1 12:00:00 2015, + 16 msec,subject_ex,drduh,drduh,staff,drduh,staff,404,404,49271,::1,text,successful login drduh,return,success,0,trailer,111,
See the manual pages for audit, praudit, audit_control and other files in /etc/security
Note although man audit says the -s flag will synchronize the audit configuration, it appears necessary to reboot for changes to take effect.
See articles on ilostmynotes.blogspot.com and derflounder.wordpress.com for more information.
DTrace
Note System Integrity Protection interferes with DTrace, so it is not possible to use it in recent macOS versions without disabling SIP.

iosnoop monitors disk I/O
opensnoop monitors file opens
execsnoop monitors execution of processes
errinfo monitors failed system calls
dtruss monitors all system calls

See man -k dtrace for more information.
Execution
ps -ef lists information about all running processes.
You can also view processes with Activity Monitor.
launchctl list and sudo launchctl list list loaded and running user and system launch daemons and agents.
Network
List open network files:
$ sudo lsof -Pni
List contents of various network-related data structures:
$ sudo netstat -atln
Wireshark can be used from the command line with tshark.
Monitor DNS queries and replies:
$ tshark -Y ""dns.flags.response == 1"" -Tfields \
  -e frame.time_delta \
  -e dns.qry.name \
  -e dns.a \
  -Eseparator=,
Monitor HTTP requests and responses:
$ tshark -Y ""http.request or http.response"" -Tfields \
  -e ip.dst \
  -e http.request.full_uri \
  -e http.request.method \
  -e http.response.code \
  -e http.response.phrase \
  -Eseparator=/s
Monitor x509 (SSL/TLS) certificates:
$ tshark -Y ""ssl.handshake.certificate"" -Tfields \
  -e ip.src \
  -e x509sat.uTF8String \
  -e x509sat.printableString \
  -e x509sat.universalString \
  -e x509sat.IA5String \
  -e x509sat.teletexString \
  -Eseparator=/s -Equote=d
Also see the simple networking monitoring application BonzaiThePenguin/Loading.
Binary Whitelisting
google/santa is a security software developed for Google's corporate Macintosh fleet and open sourced.

Santa is a binary whitelisting/blacklisting system for macOS. It consists of a kernel extension that monitors for executions, a userland daemon that makes execution decisions based on the contents of a SQLite database, a GUI agent that notifies the user in case of a block decision and a command-line utility for managing the system and synchronizing the database with a server.

Santa uses the Kernel Authorization API to monitor and allow/disallow binaries from executing in the kernel. Binaries can be white- or black-listed by unique hash or signing developer certificate. Santa can be used to only allow trusted code execution, or to blacklist known malware from executing on a Mac, similar to Bit9 software for Windows.
Note Santa does not currently have a graphical user interface for managing rules. The following instructions are for advanced users only!
To install Santa, visit the Releases page and download the latest disk image, the mount it and install the contained package:
$ hdiutil mount ~/Downloads/santa-0.9.20.dmg

$ sudo installer -pkg /Volumes/santa-0.9.20/santa-0.9.20.pkg -tgt /
By default, Santa installs in ""Monitor"" mode (meaning, nothing gets blocked, only logged) and comes with two rules: one for Apple binaries and another for Santa software itself.
Verify Santa is running and its kernel module is loaded:
$ santactl status
>>> Daemon Info
  Mode                   | Monitor
  File Logging           | No
  Watchdog CPU Events    | 0  (Peak: 0.00%)
  Watchdog RAM Events    | 0  (Peak: 0.00MB)
>>> Kernel Info
  Kernel cache count     | 0
>>> Database Info
  Binary Rules           | 0
  Certificate Rules      | 2
  Events Pending Upload  | 0

$ ps -ef | grep ""[s]anta""
    0   786     1   0 10:01AM ??         0:00.39 /Library/Extensions/santa-driver.kext/Contents/MacOS/santad --syslog

$ kextstat | grep santa
  119    0 0xffffff7f822ff000 0x6000     0x6000     com.google.santa-driver (0.9.14) 693D8E4D-3161-30E0-B83D-66A273CAE026 <5 4 3 1>
Create a blacklist rule to prevent iTunes from executing:
$ sudo santactl rule --blacklist --path /Applications/iTunes.app/
Added rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Try to launch iTunes - it will be blocked.
$ open /Applications/iTunes.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/iTunes.app.

To remove the rule:
$ sudo santactl rule --remove --path /Applications/iTunes.app/
Removed rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Open iTunes:
$ open /Applications/iTunes.app/
[iTunes will open successfully]
Create a new, example C program:
$ cat <<EOF > foo.c
> #include <stdio.h>
> main() { printf(""Hello World\n‚Äù); }
> EOF
Compile the program with GCC (requires installation of Xcode or command-line tools):
$ gcc -o foo foo.c

$ file foo
foo: Mach-O 64-bit executable x86_64

$ codesign -d foo
foo: code object is not signed at all
Run it:
$ ./foo
Hello World
Toggle Santa into ""Lockdown"" mode, which only allows whitelisted binaries to run:
$ sudo defaults write /var/db/santa/config.plist ClientMode -int 2

Try to run the unsigned binary:
$ ./foo
bash: ./foo: Operation not permitted

Santa

The following application has been blocked from executing
because its trustworthiness cannot be determined.

Path:       /Users/demouser/foo
Identifier: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Parent:     bash (701)
To whitelist a specific binary, determine its SHA-256 sum:
$ santactl fileinfo /Users/demouser/foo
Path                 : /Users/demouser/foo
SHA-256              : 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
SHA-1                : 4506f3a8c0a5abe4cacb98e6267549a4d8734d82
Type                 : Executable (x86-64)
Code-signed          : No
Rule                 : Blacklisted (Unknown)
Add a whitelist rule:
$ sudo santactl rule --whitelist --sha256 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Added rule for SHA-256: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed.
Run it:
$ ./foo
Hello World
It's allowed and works!
Applications can also be whitelisted by developer certificate (so that new binary versions will not need to be manually whitelisted on each update). For example, download and run Google Chrome - it will be blocked by Santa in ""Lockdown"" mode:
$ curl -sO https://dl.google.com/chrome/mac/stable/GGRO/googlechrome.dmg

$ hdiutil mount googlechrome.dmg

$ cp -r /Volumes/Google\ Chrome/Google\ Chrome.app /Applications/

$ open /Applications/Google\ Chrome.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/Google Chrome.app.
Whitelist the application by its developer certificate (first item in the Signing Chain):
$ santactl fileinfo /Applications/Google\ Chrome.app/
Path                 : /Applications/Google Chrome.app/Contents/MacOS/Google Chrome
SHA-256              : 0eb08224d427fb1d87d2276d911bbb6c4326ec9f74448a4d9a3cfce0c3413810
SHA-1                : 9213cbc7dfaaf7580f3936a915faa56d40479f6a
Bundle Name          : Google Chrome
Bundle Version       : 2883.87
Bundle Version Str   : 55.0.2883.87
Type                 : Executable (x86-64)
Code-signed          : Yes
Rule                 : Blacklisted (Unknown)
Signing Chain:
     1. SHA-256             : 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
        SHA-1               : 85cee8254216185620ddc8851c7a9fc4dfe120ef
        Common Name         : Developer ID Application: Google Inc.
        Organization        : Google Inc.
        Organizational Unit : EQHXZ8M8AV
        Valid From          : 2012/04/26 07:10:10 -0700
        Valid Until         : 2017/04/27 07:10:10 -0700

     2. SHA-256             : 7afc9d01a62f03a2de9637936d4afe68090d2de18d03f29c88cfb0b1ba63587f
        SHA-1               : 3b166c3b7dc4b751c9fe2afab9135641e388e186
        Common Name         : Developer ID Certification Authority
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2012/02/01 14:12:15 -0800
        Valid Until         : 2027/02/01 14:12:15 -0800

     3. SHA-256             : b0b1730ecbc7ff4505142c49f1295e6eda6bcaed7e2c68c5be91b5a11001f024
        SHA-1               : 611e5b662c593a08ff58d14ae22452d198df6c60
        Common Name         : Apple Root CA
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2006/04/25 14:40:36 -0700
        Valid Until         : 2035/02/09 13:40:36 -0800
In this case, 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153 is the SHA-256 of Google‚Äôs Apple developer certificate (team ID EQHXZ8M8AV). To whitelist it:
$ sudo santactl rule --whitelist --certificate --sha256 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
Added rule for SHA-256: 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153.
Google Chrome should now launch, and subsequent updates to the application will continue to work as long as the code signing certificate doesn‚Äôt change or expire.
To disable ""Lockdown"" mode:
$ sudo defaults delete /var/db/santa/config.plist ClientMode
See /var/log/santa.log to monitor ALLOW and DENY execution decisions.
A log and configuration server for Santa is available in Zentral, an open source event monitoring solution and TLS server for osquery and Santa.
Zentral will support Santa in both MONITORING and LOCKDOWN operation mode. Clients need to be enrolled with a TLS connection to sync Santa Rules, all Santa events from endpoints are aggregated and logged back in Zentral. Santa events can trigger actions and notifications from within the Zentral Framework.
Note Python, Bash and other interpreters are whitelisted (since they are signed by Apple's developer certificate), so Santa will not be able to block such scripts from executing. Thus, a potential non-binary program which disables Santa is a weakness (not vulnerability, since it is so by design) to take note of.
Miscellaneous
Disable Diagnostics & Usage Data.
If you want to play music or watch videos, use VLC media player which is free and open source.
If you want to use torrents, use Transmission which is free and open source (note: like all software, even open source projects, malware may still find its way in). You may also wish to use a block list to avoid peering with known bad hosts - see Which is the best blocklist for Transmission and johntyree/3331662.
Manage default file handlers with duti, which can be installed with brew install duti. One reason to manage extensions is to prevent auto-mounting of remote filesystems in Finder (see Protecting Yourself From Sparklegate). Here are several recommended file handlers to manage:
$ duti -s com.apple.Safari afp

$ duti -s com.apple.Safari ftp

$ duti -s com.apple.Safari nfs

$ duti -s com.apple.Safari smb

$ duti -s com.apple.TextEdit public.unix-executable
Monitor system logs with the Console application or syslog -w or /usr/bin/log stream commands.
In systems prior to macOS Sierra (10.12), enable the tty_tickets flag in /etc/sudoers to restrict the sudo session to the Terminal window/tab that started it. To do so, use sudo visudo and add the line Defaults    tty_tickets.
Set your screen to lock as soon as the screensaver starts:
$ defaults write com.apple.screensaver askForPassword -int 1

$ defaults write com.apple.screensaver askForPasswordDelay -int 0
Expose hidden files and Library folder in Finder:
$ defaults write com.apple.finder AppleShowAllFiles -bool true

$ chflags nohidden ~/Library
Show all filename extensions (so that ""Evil.jpg.app"" cannot masquerade easily).
$ defaults write NSGlobalDomain AppleShowAllExtensions -bool true
Don't default to saving documents to iCloud:
$ defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false
Enable Secure Keyboard Entry in Terminal (unless you use YubiKey or applications such as TextExpander).
Disable crash reporter (the dialog which appears after an application crashes and prompts to report the problem to Apple):
$ defaults write com.apple.CrashReporter DialogType none
Disable Bonjour multicast advertisements:
$ sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES
Disable Handoff and Bluetooth features, if they aren't necessary.
Consider sandboxing your applications. See fG! Sandbox Guide (pdf) and s7ephen/OSX-Sandbox--Seatbelt--Profiles.
Did you know Apple has not shipped a computer with TPM since 2006?
macOS comes with this line in /etc/sudoers:
Defaults env_keep += ""HOME MAIL""

Which stops sudo from changing the HOME variable when you elevate privileges. This means it will execute as root the bash dotfiles in the non-root user's home directory when you run ""sudo bash"". It is advisable to comment this line out to avoid a potentially easy way for malware or a local attacker to escalate privileges to root.
If you want to retain the convenience of the root user having a non-root user's home directory, you can append an export line to /var/root/.bashrc, e.g.:
export HOME=/Users/blah
Set a custom umask:
$ sudo launchctl config user umask 077
Reboot, create a file in Finder and verify its permissions (macOS default allows 'group/other' read access):
$ ls -ld umask*
drwx------  2 kevin  staff       64 Dec  4 12:27 umask_testing_dir
-rw-------@ 1 kevin  staff  2026566 Dec  4 12:28 umask_testing_file
Related software

CISOfy/lynis - Cross-platform security auditing tool and assists with compliance testing and system hardening.
Dylib Hijack Scanner - Scan for applications that are either susceptible to dylib hijacking or have been hijacked.
F-Secure XFENCE (formerly Little Flocker) - ""Little Snitch for files""; prevents applications from accessing files.
Lockdown - Audits and remediates security configuration settings.
Zentral - A log and configuration server for santa and osquery. Run audit and probes on inventory, events, logfiles, combine with point-in-time alerting. A full Framework and Django web server build on top of the elastic stack (formerly known as ELK stack).
facebook/osquery - Can be used to retrieve low level system information.  Users can write SQL queries to retrieve system information.
google/grr - Incident response framework focused on remote live forensics.
jipegit/OSXAuditor - Analyzes artifacts on a running system, such as quarantined files, Safari, Chrome and Firefox history, downloads, HTML5 databases and localstore, social media and email accounts, and Wi-Fi access point names.
kristovatlas/osx-config-check - Checks your OSX machine against various hardened configuration settings.
libyal/libfvde - Library to access FileVault Drive Encryption (FVDE) (or FileVault2) encrypted volumes.
stronghold - Securely and easily configure your Mac from the terminal. Inspired by this guide.
yelp/osxcollector - Forensic evidence collection & analysis toolkit for OS X.
The Eclectic Light Company - Downloads - A collection of useful diagnostics and control applications and utilities for macOS.

Additional resources

Apple Open Source
Auditing and Exploiting Apple IPC
CIS Benchmarks
Demystifying the DMG File Format
Demystifying the i-Device NVMe NAND (New storage used by Apple)
Developing Mac OSX kernel rootkits
DoD Security Technical Implementation Guides for Mac OS
EFF Surveillance Self-Defense Guide
Extracting FileVault 2 Keys with Volatility
Fuzzing the macOS WindowServer for Exploitable Vulnerabilities
Hacker News discussion 2
Hacker News discussion
Harden the World: Mac OSX 10.11 El Capitan
Hidden backdoor API to root privileges in Apple OS X
How to Switch to the Mac
How to make macOS Spotlight fuck the fuck off and do your bidding
IOKit kernel code execution exploit
IPv6 Hardening Guide for OS X
Mac Developer Library: Secure Coding Guide
Mac Forensics: Mac OS X and the HFS+ File System (pdf)
Mac OS X Forensics - Technical Report (pdf)
Mac OS X and iOS Internals: To the Apple's Core by Jonathan Levin
MacAdmins on Slack
MacOS Hardening Guide - Appendix of *OS Internals: Volume III - Security & Insecurity Internals (pdf)
Managing Macs at Google Scale (LISA '13)
OS X 10.10 Yosemite: The Ars Technica Review
OS X Core Technologies Overview White Paper (pdf)
OS X Hardening: Securing a Large Global Mac Fleet (LISA '13)
OSX.Pirrit Mac Adware Part III: The DaVinci Code
Over The Air - Vol. 2, Pt. 1: Exploiting The Wi-Fi Stack on Apple Devices
Patrick Wardle's Objective-See blog
Remote code execution, git, and OS X
Reverse Engineering Mac OS X blog
Reverse Engineering Resources
Security Configuration For Mac OS X Version 10.6 Snow Leopard (pdf)
The EFI boot process
The Great DOM Fuzz-off of 2017
The Intel Mac boot process
The macOS Phishing Easy Button: AppleScript Dangers
There's a lot of vulnerable OS X applications out there (Sparkle Framework RCE)
Userland Persistence on Mac OS X
iCloud security and privacy overview
iSeeYou: Disabling the MacBook Webcam Indicator LED

",GitHub - drduh/macOS-Security-and-Privacy-Guide: Guide to securing and improving privacy on macOS
58,Python,"
spaCy: Industrial-strength NLP
spaCy is a library for advanced Natural Language Processing in Python and
Cython. It's built on the very latest research, and was designed from day one to
be used in real products. spaCy comes with
pretrained statistical models and word vectors, and
currently supports tokenization for 50+ languages. It features
state-of-the-art speed, convolutional neural network models for tagging,
parsing and named entity recognition and easy deep learning integration.
It's commercial open-source software, released under the MIT license.
üí´ Version 2.2 out now!
Check out the release notes here.











üìñ Documentation



Documentation





spaCy 101
New to spaCy? Here's everything you need to know!


Usage Guides
How to use spaCy and its features.


New in v2.2
New features, backwards incompatibilities and migration guide.


API Reference
The detailed reference for spaCy's API.


Models
Download statistical language models for spaCy.


Universe
Libraries, extensions, demos, books and courses.


Changelog
Changes and version history.


Contribute
How to contribute to the spaCy project and code base.



üí¨ Where to ask questions
The spaCy project is maintained by @honnibal and
@ines, along with core contributors
@svlandeg and
@adrianeboyd. Please understand that we won't
be able to provide individual support via email. We also believe that help is
much more valuable if it's shared publicly, so that more people can benefit from
it.



Type
Platforms




üö® Bug Reports
GitHub Issue Tracker


üéÅ Feature Requests
GitHub Issue Tracker


üë©‚Äçüíª Usage Questions
Stack Overflow ¬∑ Gitter Chat ¬∑ Reddit User Group


üóØ General Discussion
Gitter Chat ¬∑ Reddit User Group



Features

Non-destructive tokenization
Named entity recognition
Support for 50+ languages
pretrained statistical models and word vectors
State-of-the-art speed
Easy deep learning integration
Part-of-speech tagging
Labelled dependency parsing
Syntax-driven sentence segmentation
Built in visualizers for syntax and NER
Convenient string-to-hash mapping
Export to numpy data arrays
Efficient binary serialization
Easy model packaging and deployment
Robust, rigorously evaluated accuracy

üìñ For more details, see the
facts, figures and benchmarks.
Install spaCy
For detailed installation instructions, see the
documentation.

Operating system: macOS / OS X ¬∑ Linux ¬∑ Windows (Cygwin, MinGW, Visual
Studio)
Python version: Python 2.7, 3.5+ (only 64 bit)
Package managers: pip ¬∑ conda (via conda-forge)

pip
Using pip, spaCy releases are available as source packages and binary wheels (as
of v2.0.13).
pip install spacy
To install additional data tables for lemmatization in spaCy v2.2+ you can
run pip install spacy[lookups] or install
spacy-lookups-data
separately. The lookups package is needed to create blank models with
lemmatization data, and to lemmatize in languages that don't yet come with
pretrained models and aren't powered by third-party libraries.
When using pip it is generally recommended to install packages in a virtual
environment to avoid modifying system state:
python -m venv .env
source .env/bin/activate
pip install spacy
conda
Thanks to our great community, we've finally re-added conda support. You can now
install spaCy via conda-forge:
conda install -c conda-forge spacy
For the feedstock including the build recipe and configuration, check out
this repository. Improvements
and pull requests to the recipe and setup are always appreciated.
Updating spaCy
Some updates to spaCy may require downloading new statistical models. If you're
running spaCy v2.0 or higher, you can use the validate command to check if
your installed models are compatible and if not, print details on how to update
them:
pip install -U spacy
python -m spacy validate
If you've trained your own models, keep in mind that your training and runtime
inputs must match. After updating spaCy, we recommend retraining your models
with the new version.
üìñ For details on upgrading from spaCy 1.x to spaCy 2.x, see the
migration guide.
Download models
As of v1.7.0, models for spaCy can be installed as Python packages. This
means that they're a component of your application, just like any other module.
Models can be installed using spaCy's download command, or manually by
pointing pip to a path or URL.



Documentation





Available Models
Detailed model descriptions, accuracy figures and benchmarks.


Models Documentation
Detailed usage instructions.



# download best-matching version of specific model for your spaCy installation
python -m spacy download en_core_web_sm

# pip install .tar.gz archive from path or URL
pip install /Users/you/en_core_web_sm-2.2.0.tar.gz
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz
Loading and using models
To load a model, use spacy.load() with the model name, a shortcut link or a
path to the model data directory.
import spacy
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is a sentence."")
You can also import a model directly via its full name and then call its
load() method with no arguments.
import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()
doc = nlp(""This is a sentence."")
üìñ For more info and examples, check out the
models documentation.
Compile from source
The other way to install spaCy is to clone its
GitHub repository and build it from
source. That is the common way if you want to make changes to the code base.
You'll need to make sure that you have a development environment consisting of a
Python distribution including header files, a compiler,
pip,
virtualenv and
git installed. The compiler part is the trickiest. How to
do that depends on your system. See notes on Ubuntu, OS X and Windows for
details.
# make sure you are using the latest pip
python -m pip install -U pip
git clone https://github.com/explosion/spaCy
cd spaCy

python -m venv .env
source .env/bin/activate
export PYTHONPATH=`pwd`
pip install -r requirements.txt
python setup.py build_ext --inplace
Compared to regular install via pip, requirements.txt
additionally installs developer dependencies such as Cython. For more details
and instructions, see the documentation on
compiling spaCy from source and the
quickstart widget to get the right
commands for your platform and Python version.
Ubuntu
Install system-level dependencies via apt-get:
sudo apt-get install build-essential python-dev git
macOS / OS X
Install a recent version of XCode,
including the so-called ""Command Line Tools"". macOS and OS X ship with Python
and git preinstalled.
Windows
Install a version of the
Visual C++ Build Tools
or Visual Studio Express that
matches the version that was used to compile your Python interpreter. For
official distributions these are VS 2008 (Python 2.7), VS 2010 (Python 3.4) and
VS 2015 (Python 3.5).
Run tests
spaCy comes with an extensive test suite. In order to run the
tests, you'll usually want to clone the repository and build spaCy from source.
This will also install the required development dependencies and test utilities
defined in the requirements.txt.
Alternatively, you can find out where spaCy is installed and run pytest on
that directory. Don't forget to also install the test utilities via spaCy's
requirements.txt:
python -c ""import os; import spacy; print(os.path.dirname(spacy.__file__))""
pip install -r path/to/requirements.txt
python -m pytest <spacy-directory>
See the documentation for more details and
examples.
",GitHub - explosion/spaCy: üí´ Industrial-strength Natural Language Processing (NLP) with Python and Cython
59,Python,"ÊúÄËøëÈúÄË¶Å‰ªéÊñáÊú¨‰∏≠ÊäΩÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÔºåÁî®Âà∞‰∫ÜÂæàÂ§ögithub‰∏äÁöÑÂåÖÔºåÈÅÇÊï¥ÁêÜ‰∫Ü‰∏Ä‰∏ãÔºåÂêéÁª≠‰ºö‰∏çÊñ≠Êõ¥Êñ∞„ÄÇ
ÂæàÂ§öÂåÖÈùûÂ∏∏ÊúâË∂£ÔºåÂÄºÂæóÊî∂ËóèÔºåÊª°Ë∂≥Â§ßÂÆ∂ÁöÑÊî∂ÈõÜÁôñÔºÅ
Â¶ÇÊûúËßâÂæóÊúâÁî®ÔºåËØ∑ÂàÜ‰∫´Âπ∂starÔºåË∞¢Ë∞¢ÔºÅ
Ê∂âÂèäÂÜÖÂÆπÂåÖÊã¨Ôºö‰∏≠Ëã±ÊñáÊïèÊÑüËØç„ÄÅËØ≠Ë®ÄÊ£ÄÊµã„ÄÅ‰∏≠Â§ñÊâãÊú∫/ÁîµËØùÂΩíÂ±ûÂú∞/ËøêËê•ÂïÜÊü•ËØ¢„ÄÅÂêçÂ≠óÊé®Êñ≠ÊÄßÂà´„ÄÅÊâãÊú∫Âè∑ÊäΩÂèñ„ÄÅË∫´‰ªΩËØÅÊäΩÂèñ„ÄÅÈÇÆÁÆ±ÊäΩÂèñ„ÄÅ‰∏≠Êó•Êñá‰∫∫ÂêçÂ∫ì„ÄÅ‰∏≠ÊñáÁº©ÂÜôÂ∫ì„ÄÅÊãÜÂ≠óËØçÂÖ∏„ÄÅËØçÊ±áÊÉÖÊÑüÂÄº„ÄÅÂÅúÁî®ËØç„ÄÅÂèçÂä®ËØçË°®„ÄÅÊö¥ÊÅêËØçË°®„ÄÅÁπÅÁÆÄ‰ΩìËΩ¨Êç¢„ÄÅËã±ÊñáÊ®°Êãü‰∏≠ÊñáÂèëÈü≥„ÄÅÊ±™Â≥∞Ê≠åËØçÁîüÊàêÂô®„ÄÅËÅå‰∏öÂêçÁß∞ËØçÂ∫ì„ÄÅÂêå‰πâËØçÂ∫ì„ÄÅÂèç‰πâËØçÂ∫ì„ÄÅÂê¶ÂÆöËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ÂìÅÁâåËØçÂ∫ì„ÄÅÊ±ΩËΩ¶Èõ∂‰ª∂ËØçÂ∫ì„ÄÅËøûÁª≠Ëã±ÊñáÂàáÂâ≤„ÄÅÂêÑÁßç‰∏≠ÊñáËØçÂêëÈáè„ÄÅÂÖ¨Âè∏ÂêçÂ≠óÂ§ßÂÖ®„ÄÅÂè§ËØóËØçÂ∫ì„ÄÅITËØçÂ∫ì„ÄÅË¥¢ÁªèËØçÂ∫ì„ÄÅÊàêËØ≠ËØçÂ∫ì„ÄÅÂú∞ÂêçËØçÂ∫ì„ÄÅÂéÜÂè≤Âêç‰∫∫ËØçÂ∫ì„ÄÅËØóËØçËØçÂ∫ì„ÄÅÂåªÂ≠¶ËØçÂ∫ì„ÄÅÈ•ÆÈ£üËØçÂ∫ì„ÄÅÊ≥ïÂæãËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ËØçÂ∫ì„ÄÅÂä®Áâ©ËØçÂ∫ì„ÄÅ‰∏≠ÊñáËÅäÂ§©ËØ≠Êñô„ÄÅ‰∏≠ÊñáË∞£Ë®ÄÊï∞ÊçÆ„ÄÅÁôæÂ∫¶‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜ„ÄÅÂè•Â≠êÁõ∏‰ººÂ∫¶ÂåπÈÖçÁÆóÊ≥ïÈõÜÂêà„ÄÅbertËµÑÊ∫ê„ÄÅÊñáÊú¨ÁîüÊàê&ÊëòË¶ÅÁõ∏ÂÖ≥Â∑•ÂÖ∑„ÄÅcocoNLP‰ø°ÊÅØÊäΩÂèñÂ∑•ÂÖ∑„ÄÅÂõΩÂÜÖÁîµËØùÂè∑Á†ÅÊ≠£ÂàôÂåπÈÖç„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶XLORE:‰∏≠Ëã±ÊñáË∑®ËØ≠Ë®ÄÁôæÁßëÁü•ËØÜÂõæË∞±„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁ≥ªÂàóÊä•Âëä„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê„ÄÅNLUÂ§™Èöæ‰∫ÜÁ≥ªÂàó„ÄÅËá™Âä®ÂØπËÅîÊï∞ÊçÆÂèäÊú∫Âô®‰∫∫„ÄÅÁî®Êà∑ÂêçÈªëÂêçÂçïÂàóË°®„ÄÅÁΩ™ÂêçÊ≥ïÂä°ÂêçËØçÂèäÂàÜÁ±ªÊ®°Âûã„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ËØ≠Êñô„ÄÅcs224nÊ∑±Â∫¶Â≠¶‰π†Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã„ÄÅ‰∏≠ÊñáÊâãÂÜôÊ±âÂ≠óËØÜÂà´„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ ËØ≠Êñô/Êï∞ÊçÆÈõÜ„ÄÅÂèòÈáèÂëΩÂêçÁ•ûÂô®„ÄÅÂàÜËØçËØ≠ÊñôÂ∫ì+‰ª£Á†Å„ÄÅ‰ªªÂä°ÂûãÂØπËØùËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÅASR ËØ≠Èü≥Êï∞ÊçÆÈõÜ + Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´Á≥ªÁªü„ÄÅÁ¨ëÂ£∞Ê£ÄÊµãÂô®„ÄÅMicrosoftÂ§öËØ≠Ë®ÄÊï∞Â≠ó/Âçï‰Ωç/Â¶ÇÊó•ÊúüÊó∂Èó¥ËØÜÂà´ÂåÖ„ÄÅ‰∏≠ÂçéÊñ∞ÂçéÂ≠óÂÖ∏Êï∞ÊçÆÂ∫ìÂèäapi(ÂåÖÊã¨Â∏∏Áî®Ê≠áÂêéËØ≠„ÄÅÊàêËØ≠„ÄÅËØçËØ≠ÂíåÊ±âÂ≠ó)„ÄÅÊñáÊ°£ÂõæË∞±Ëá™Âä®ÁîüÊàê„ÄÅSpaCy ‰∏≠ÊñáÊ®°Âûã„ÄÅCommon VoiceËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜÊñ∞Áâà„ÄÅÁ•ûÁªèÁΩëÁªúÂÖ≥Á≥ªÊäΩÂèñ„ÄÅÂü∫‰∫ébertÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥ÈîÆËØç(Keyphrase)ÊäΩÂèñÂåÖpke„ÄÅÂü∫‰∫éÂåªÁñóÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂü∫‰∫é‰æùÂ≠òÂè•Ê≥ï‰∏éËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÁöÑ‰∫ã‰ª∂‰∏âÂÖÉÁªÑÊäΩÂèñ„ÄÅ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê4‰∏áÂè•È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÅcnocrÔºöÁî®Êù•ÂÅö‰∏≠ÊñáOCRÁöÑPython3ÂåÖ„ÄÅ‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÁü•ËØÜÂõæË∞±È°πÁõÆ„ÄÅ‰∏≠ÊñánlpÁ´ûËµõÈ°πÁõÆÂèä‰ª£Á†ÅÊ±áÊÄª„ÄÅ‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ÊçÆ„ÄÅspeech-aligner: ‰ªé‚Äú‰∫∫Â£∞ËØ≠Èü≥‚ÄùÂèäÂÖ∂‚ÄúËØ≠Ë®ÄÊñáÊú¨‚Äù‰∫ßÁîüÈü≥Á¥†Á∫ßÂà´Êó∂Èó¥ÂØπÈΩêÊ†áÊ≥®ÁöÑÂ∑•ÂÖ∑„ÄÅAmpliGraph: Áü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†(Python)Â∫ìÔºöÁü•ËØÜÂõæË∞±Ê¶ÇÂøµÈìæÊé•È¢ÑÊµã„ÄÅScattertext ÊñáÊú¨ÂèØËßÜÂåñ(python)„ÄÅËØ≠Ë®Ä/Áü•ËØÜË°®Á§∫Â∑•ÂÖ∑ÔºöBERT & ERNIE„ÄÅ‰∏≠ÊñáÂØπÊØîËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜNLPÁöÑÂå∫Âà´ÁªºËø∞„ÄÅSynonyms‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖ„ÄÅHarvestTextÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñáÊú¨ÊåñÊéòÂ∑•ÂÖ∑ÔºàÊñ∞ËØçÂèëÁé∞-ÊÉÖÊÑüÂàÜÊûê-ÂÆû‰ΩìÈìæÊé•Á≠âÔºâ„ÄÅword2wordÔºö(Python)Êñπ‰æøÊòìÁî®ÁöÑÂ§öËØ≠Ë®ÄËØç-ËØçÂØπÈõÜÔºö62ÁßçËØ≠Ë®Ä/3,564‰∏™Â§öËØ≠Ë®ÄÂØπ„ÄÅËØ≠Èü≥ËØÜÂà´ËØ≠ÊñôÁîüÊàêÂ∑•ÂÖ∑Ôºö‰ªéÂÖ∑ÊúâÈü≥È¢ë/Â≠óÂπïÁöÑÂú®Á∫øËßÜÈ¢ëÂàõÂª∫Ëá™Âä®ËØ≠Èü≥ËØÜÂà´(ASR)ËØ≠ÊñôÂ∫ì„ÄÅÊûÑÂª∫ÂåªÁñóÂÆû‰ΩìËØÜÂà´ÁöÑÊ®°ÂûãÔºàÂåÖÂê´ËØçÂÖ∏ÂíåËØ≠ÊñôÊ†áÊ≥®Ôºâ„ÄÅÂçïÊñáÊ°£ÈùûÁõëÁù£ÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñ„ÄÅKashgari‰∏≠‰ΩøÁî®gpt-2ËØ≠Ë®ÄÊ®°Âûã„ÄÅÂºÄÊ∫êÁöÑÈáëËûçÊäïËµÑÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑„ÄÅÊñáÊú¨Ëá™Âä®ÊëòË¶ÅÂ∫ìTextTeaser: ‰ªÖÊîØÊåÅËã±Êñá„ÄÅ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ„ÄÅ‰∏Ä‰∫õÂÖ≥‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂü∫Êú¨Ê®°Âûã„ÄÅÂü∫‰∫é14WÊ≠åÊõ≤Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∞ùËØï--ÂäüËÉΩÂåÖÊã¨Ê≠åËØçÊé•ÈæôandÂ∑≤Áü•Ê≠åËØçÊâæÊ≠åÊõ≤‰ª•ÂèäÊ≠åÊõ≤Ê≠åÊâãÊ≠åËØç‰∏âËßíÂÖ≥Á≥ªÁöÑÈóÆÁ≠î„ÄÅÂü∫‰∫éSiamese bilstmÊ®°ÂûãÁöÑÁõ∏‰ººÂè•Â≠êÂà§ÂÆöÊ®°ÂûãÂπ∂Êèê‰æõËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÊµãËØïÊï∞ÊçÆÈõÜ„ÄÅÁî®TransformerÁºñËß£Á†ÅÊ®°ÂûãÂÆûÁé∞ÁöÑÊ†πÊçÆHacker NewsÊñáÁ´†Ê†áÈ¢òËá™Âä®ÁîüÊàêËØÑËÆ∫„ÄÅÁî®BERTËøõË°åÂ∫èÂàóÊ†áËÆ∞ÂíåÊñáÊú¨ÂàÜÁ±ªÁöÑÊ®°Êùø‰ª£Á†Å„ÄÅLitBankÔºöNLPÊï∞ÊçÆÈõÜ‚Äî‚ÄîÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆó‰∫∫ÊñáÂ≠¶Áßë‰ªªÂä°ÁöÑ100ÈÉ®Â∏¶Ê†áËÆ∞Ëã±ÊñáÂ∞èËØ¥ËØ≠Êñô„ÄÅÁôæÂ∫¶ÂºÄÊ∫êÁöÑÂü∫ÂáÜ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü„ÄÅËôöÂÅáÊñ∞ÈóªÊï∞ÊçÆÈõÜ„ÄÅFacebook: LAMAËØ≠Ë®ÄÊ®°ÂûãÂàÜÊûêÔºåÊèê‰æõTransformer-XL/BERT/ELMo/GPTÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªü‰∏ÄËÆøÈóÆÊé•Âè£„ÄÅCommonsenseQAÔºöÈù¢ÂêëÂ∏∏ËØÜÁöÑËã±ÊñáQAÊåëÊàò„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ËµÑÊñô„ÄÅÊï∞ÊçÆÂèäÂ∑•ÂÖ∑„ÄÅÂêÑÂ§ßÂÖ¨Âè∏ÂÜÖÈÉ®ÈáåÂ§ßÁâõÂàÜ‰∫´ÁöÑÊäÄÊúØÊñáÊ°£ PDF ÊàñËÄÖ PPT„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêSQLËØ≠Âè•ÔºàËã±ÊñáÔºâ„ÄÅ‰∏≠ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫ÔºàEDAÔºâÂ∑•ÂÖ∑„ÄÅËã±ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫Â∑•ÂÖ∑ „ÄÅÂü∫‰∫éÂåªËçØÁü•ËØÜÂõæË∞±ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅ‰∫¨‰∏úÂïÜÂìÅÁü•ËØÜÂõæË∞±„ÄÅÂü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆ„ÄÅÂü∫‰∫éËøúÁõëÁù£ÁöÑ‰∏≠ÊñáÂÖ≥Á≥ªÊäΩÂèñ„ÄÅËØ≠Èü≥ÊÉÖÊÑüÂàÜÊûê„ÄÅ‰∏≠ÊñáULMFiT-ÊÉÖÊÑüÂàÜÊûê-ÊñáÊú¨ÂàÜÁ±ª-ËØ≠ÊñôÂèäÊ®°Âûã„ÄÅ‰∏Ä‰∏™ÊãçÁÖßÂÅöÈ¢òÁ®ãÂ∫è„ÄÅ‰∏ñÁïåÂêÑÂõΩÂ§ßËßÑÊ®°‰∫∫ÂêçÂ∫ì„ÄÅ‰∏Ä‰∏™Âà©Áî®ÊúâË∂£‰∏≠ÊñáËØ≠ÊñôÂ∫ì qingyun ËÆ≠ÁªÉÂá∫Êù•ÁöÑ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫seqGAN„ÄÅÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥®„ÄÅÊïôËÇ≤Ë°å‰∏öÊñ∞ÈóªËØ≠ÊñôÂ∫ìÂåÖÂê´Ëá™Âä®ÊñáÊëòÂäüËÉΩ„ÄÅÂºÄÊîæ‰∫ÜÂØπËØùÊú∫Âô®‰∫∫-Áü•ËØÜÂõæË∞±-ËØ≠‰πâÁêÜËß£-Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèäÊï∞ÊçÆ„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºöÂü∫‰∫éÁôæÂ∫¶ÁôæÁßë‰∏≠ÊñáÈ°µÈù¢-ÊäΩÂèñ‰∏âÂÖÉÁªÑ‰ø°ÊÅØ-ÊûÑÂª∫‰∏≠ÊñáÁü•ËØÜÂõæË∞±„ÄÅmasr: ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´-Êèê‰æõÈ¢ÑËÆ≠ÁªÉÊ®°Âûã-È´òËØÜÂà´Áéá„ÄÅPythonÈü≥È¢ëÊï∞ÊçÆÂ¢ûÂπøÂ∫ì„ÄÅ‰∏≠ÊñáÂÖ®ËØçË¶ÜÁõñBERTÂèä‰∏§‰ªΩÈòÖËØªÁêÜËß£Êï∞ÊçÆ„ÄÅConvLabÔºöÂºÄÊ∫êÂ§öÂüüÁ´ØÂà∞Á´ØÂØπËØùÁ≥ªÁªüÂπ≥Âè∞„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊï∞ÊçÆÈõÜ„ÄÅÂü∫‰∫éÊúÄÊñ∞ÁâàÊú¨rasaÊê≠Âª∫ÁöÑÂØπËØùÁ≥ªÁªü„ÄÅÂü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñ„ÄÅ‰∏Ä‰∏™Â∞èÂûãÁöÑËØÅÂà∏Áü•ËØÜÂõæË∞±/Áü•ËØÜÂ∫ì„ÄÅÂ§çÁõòÊâÄÊúâNLPÊØîËµõÁöÑTOPÊñπÊ°à„ÄÅOpenCLaPÔºöÂ§öÈ¢ÜÂüüÂºÄÊ∫ê‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã‰ªìÂ∫ì„ÄÅUERÔºöÂü∫‰∫é‰∏çÂêåËØ≠Êñô+ÁºñÁ†ÅÂô®+ÁõÆÊ†á‰ªªÂä°ÁöÑ‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ªìÂ∫ì„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂêëÈáèÂêàÈõÜ„ÄÅÂü∫‰∫éÈáëËûç-Âè∏Ê≥ïÈ¢ÜÂüü(ÂÖºÊúâÈó≤ËÅäÊÄßË¥®)ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅg2pCÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ±âËØ≠ËØªÈü≥Ëá™Âä®Ê†áËÆ∞Ê®°Âùó„ÄÅZincbase Áü•ËØÜÂõæË∞±ÊûÑÂª∫Â∑•ÂÖ∑ÂåÖ„ÄÅËØóÊ≠åË¥®ÈáèËØÑ‰ª∑/ÁªÜÁ≤íÂ∫¶ÊÉÖÊÑüËØóÊ≠åËØ≠ÊñôÂ∫ì„ÄÅÂø´ÈÄüËΩ¨Âåñ„Äå‰∏≠ÊñáÊï∞Â≠ó„ÄçÂíå„ÄåÈòøÊãâ‰ºØÊï∞Â≠ó„Äç„ÄÅÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îËØ≠ÊñôÂ∫ì„ÄÅÂü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÅjieba_fast Âä†ÈÄüÁâàÁöÑjieba„ÄÅÊ≠£ÂàôË°®ËææÂºèÊïôÁ®ã„ÄÅ‰∏≠ÊñáÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜ„ÄÅÂü∫‰∫éBERTÁ≠âÊúÄÊñ∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊäΩÂèñÂºèÊëòË¶ÅÊèêÂèñ„ÄÅPythonÂà©Áî®Ê∑±Â∫¶Â≠¶‰π†ËøõË°åÊñáÊú¨ÊëòË¶ÅÁöÑÁªºÂêàÊåáÂçó„ÄÅÁü•ËØÜÂõæË∞±Ê∑±Â∫¶Â≠¶‰π†Áõ∏ÂÖ≥ËµÑÊñôÊï¥ÁêÜ„ÄÅÁª¥Âü∫Â§ßËßÑÊ®°Âπ≥Ë°åÊñáÊú¨ËØ≠Êñô„ÄÅStanfordNLP 0.2.0ÔºöÁ∫ØPythonÁâàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂåÖ„ÄÅNeuralNLP-NeuralClassifierÔºöËÖæËÆØÂºÄÊ∫êÊ∑±Â∫¶Â≠¶‰π†ÊñáÊú¨ÂàÜÁ±ªÂ∑•ÂÖ∑„ÄÅÁ´ØÂà∞Á´ØÁöÑÂ∞ÅÈó≠ÂüüÂØπËØùÁ≥ªÁªü„ÄÅ‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºöNeuroNER vs. BertNER„ÄÅÊñ∞Èóª‰∫ã‰ª∂Á∫øÁ¥¢ÊäΩÂèñ„ÄÅ2019Âπ¥ÁôæÂ∫¶ÁöÑ‰∏âÂÖÉÁªÑÊäΩÂèñÊØîËµõÔºö‚ÄúÁßëÂ≠¶Á©∫Èó¥Èòü‚ÄùÊ∫êÁ†Å„ÄÅÂü∫‰∫é‰æùÂ≠òÂè•Ê≥ïÁöÑÂºÄÊîæÂüüÊñáÊú¨Áü•ËØÜ‰∏âÂÖÉÁªÑÊäΩÂèñÂíåÁü•ËØÜÂ∫ìÊûÑÂª∫„ÄÅ‰∏≠ÊñáÁöÑGPT2ËÆ≠ÁªÉ‰ª£Á†Å„ÄÅML-NLP - Êú∫Âô®Â≠¶‰π†(Machine Learning)NLPÈù¢ËØï‰∏≠Â∏∏ËÄÉÂà∞ÁöÑÁü•ËØÜÁÇπÂíå‰ª£Á†ÅÂÆûÁé∞„ÄÅnlp4han:‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ(Êñ≠Âè•/ÂàÜËØç/ËØçÊÄßÊ†áÊ≥®/ÁªÑÂùó/Âè•Ê≥ïÂàÜÊûê/ËØ≠‰πâÂàÜÊûê/NER/NÂÖÉËØ≠Ê≥ï/HMM/‰ª£ËØçÊ∂àËß£/ÊÉÖÊÑüÂàÜÊûê/ÊãºÂÜôÊ£ÄÊü•„ÄÅXLMÔºöFacebookÁöÑË∑®ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã„ÄÅÁî®Âü∫‰∫éBERTÁöÑÂæÆË∞ÉÂíåÁâπÂæÅÊèêÂèñÊñπÊ≥ïÊù•ËøõË°åÁü•ËØÜÂõæË∞±ÁôæÂ∫¶ÁôæÁßë‰∫∫Áâ©ËØçÊù°Â±ûÊÄßÊäΩÂèñ„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁõ∏ÂÖ≥ÁöÑÂºÄÊîæ‰ªªÂä°-Êï∞ÊçÆÈõÜ-ÂΩìÂâçÊúÄ‰Ω≥ÁªìÊûú„ÄÅCoupletAI - Âü∫‰∫éCNN+Bi-LSTM+Attention ÁöÑËá™Âä®ÂØπÂØπËÅîÁ≥ªÁªü„ÄÅÊäΩË±°Áü•ËØÜÂõæË∞±„ÄÅMiningZhiDaoQACorpus - 580‰∏áÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îÊï∞ÊçÆÊåñÊéòÈ°πÁõÆ„ÄÇ
1. textfilter: ‰∏≠Ëã±ÊñáÊïèÊÑüËØçËøáÊª§ observerss/textfilter
 >>> f = DFAFilter()
 >>> f.add(""sexy"")
 >>> f.filter(""hello sexy baby"")
 hello **** baby

ÊïèÊÑüËØçÂåÖÊã¨ÊîøÊ≤ª„ÄÅËÑèËØùÁ≠âËØùÈ¢òËØçÊ±á„ÄÇÂÖ∂ÂéüÁêÜ‰∏ªË¶ÅÊòØÂü∫‰∫éËØçÂÖ∏ÁöÑÊü•ÊâæÔºàÈ°πÁõÆ‰∏≠ÁöÑkeywordÊñá‰ª∂ÔºâÔºåÂÜÖÂÆπÂæàÂä≤ÁàÜ„ÄÇ„ÄÇ„ÄÇ
2. langidÔºö97ÁßçËØ≠Ë®ÄÊ£ÄÊµã https://github.com/saffsd/langid.py

pip install langid

>>> import langid
>>> langid.classify(""This is a test"")
('en', -54.41310358047485)

3. langdetectÔºöÂè¶‰∏Ä‰∏™ËØ≠Ë®ÄÊ£ÄÊµãhttps://code.google.com/archive/p/language-detection/

pip install langdetect

from langdetect import detect
from langdetect import detect_langs

s1 = ""Êú¨ÁØáÂçöÂÆ¢‰∏ªË¶Å‰ªãÁªç‰∏§Ê¨æËØ≠Ë®ÄÊé¢ÊµãÂ∑•ÂÖ∑ÔºåÁî®‰∫éÂå∫ÂàÜÊñáÊú¨Âà∞Â∫ïÊòØ‰ªÄ‰πàËØ≠Ë®ÄÔºå""
s2 = 'We are pleased to introduce today a new technology'
print(detect(s1))
print(detect(s2))
print(detect_langs(s3))    # detect_langs()ËæìÂá∫Êé¢ÊµãÂá∫ÁöÑÊâÄÊúâËØ≠Ë®ÄÁ±ªÂûãÂèäÂÖ∂ÊâÄÂç†ÁöÑÊØî‰æã

ËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ãÔºö Ê≥®ÔºöËØ≠Ë®ÄÁ±ªÂûã‰∏ªË¶ÅÂèÇËÄÉÁöÑÊòØISO 639-1ËØ≠Ë®ÄÁºñÁ†ÅÊ†áÂáÜÔºåËØ¶ËßÅISO 639-1ÁôæÂ∫¶ÁôæÁßë
Ë∑ü‰∏ä‰∏Ä‰∏™ËØ≠Ë®ÄÊ£ÄÊµãÊØîËæÉÔºåÂáÜÁ°ÆÁéá‰ΩéÔºåÊïàÁéáÈ´ò„ÄÇ
4. phone ‰∏≠ÂõΩÊâãÊú∫ÂΩíÂ±ûÂú∞Êü•ËØ¢Ôºö ls0f/phone

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

from phone import Phone
p  = Phone()
p.find(18100065143)
#return {'phone': '18100065143', 'province': '‰∏äÊµ∑', 'city': '‰∏äÊµ∑', 'zip_code': '200000', 'area_code': '021', 'phone_type': 'Áîµ‰ø°'}

ÊîØÊåÅÂè∑ÊÆµ: 13*,15*,18*,14[5,7],17[0,6,7,8]
ËÆ∞ÂΩïÊù°Êï∞: 360569 (updated:2017Âπ¥4Êúà)
‰ΩúËÄÖÊèê‰æõ‰∫ÜÊï∞ÊçÆphone.dat Êñπ‰æøÈùûpythonÁî®Êà∑LoadÊï∞ÊçÆ„ÄÇ
5. phoneÂõΩÈôÖÊâãÊú∫„ÄÅÁîµËØùÂΩíÂ±ûÂú∞Êü•ËØ¢ÔºöAfterShip/phone

npm install phone

import phone from 'phone';
phone('+852 6569-8900'); // return ['+85265698900', 'HKG']
phone('(817) 569-8900'); // return ['+18175698900, 'USA']

6. ngender Ê†πÊçÆÂêçÂ≠óÂà§Êñ≠ÊÄßÂà´Ôºöobserverss/ngender Âü∫‰∫éÊú¥Á¥†Ë¥ùÂè∂ÊñØËÆ°ÁÆóÁöÑÊ¶ÇÁéá

pip install ngender

>>> import ngender
>>> ngender.guess('ËµµÊú¨Â±±')
('male', 0.9836229687547046)
>>> ngender.guess('ÂÆã‰∏π‰∏π')
('female', 0.9759486128949907)

7. ÊäΩÂèñemailÁöÑÊ≠£ÂàôË°®ËææÂºè

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

email_pattern = '^[*#\u4e00-\u9fa5 a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*\.[a-zA-Z0-9]{2,6}$'
emails = re.findall(email_pattern, text, flags=0)

8. ÊäΩÂèñphone_numberÁöÑÊ≠£ÂàôË°®ËææÂºè

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

cellphone_pattern = '^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\d{8}$'
phoneNumbers = re.findall(cellphone_pattern, text, flags=0)

9. ÊäΩÂèñË∫´‰ªΩËØÅÂè∑ÁöÑÊ≠£ÂàôË°®ËææÂºè
IDCards_pattern = r'^([1-9]\d{5}[12]\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\d{3}[0-9xX])$'
IDs = re.findall(IDCards_pattern, text, flags=0)

10.  ‰∫∫ÂêçËØ≠ÊñôÂ∫ìÔºö wainshine/Chinese-Names-Corpus

‰∫∫ÂêçÊäΩÂèñÂäüËÉΩ python package cocoNLPÔºåÊ¨¢ËøéËØïÁî®

‰∏≠ÊñáÔºàÁé∞‰ª£„ÄÅÂè§‰ª£ÔºâÂêçÂ≠ó„ÄÅÊó•ÊñáÂêçÂ≠ó„ÄÅ‰∏≠ÊñáÁöÑÂßìÂíåÂêç„ÄÅÁß∞ÂëºÔºàÂ§ßÂß®Â¶à„ÄÅÂ∞èÂß®Â¶àÁ≠âÔºâ„ÄÅËã±Êñá->‰∏≠ÊñáÂêçÂ≠óÔºàÊùéÁ∫¶Áø∞Ôºâ„ÄÅÊàêËØ≠ËØçÂÖ∏

ÔºàÂèØÁî®‰∫é‰∏≠ÊñáÂàÜËØç„ÄÅÂßìÂêçËØÜÂà´Ôºâ
11. ‰∏≠ÊñáÁº©ÂÜôÂ∫ìÔºögithub
ÂÖ®ÂõΩ‰∫∫Â§ß: ÂÖ®ÂõΩ/n ‰∫∫Ê∞ë/n ‰ª£Ë°®Â§ß‰ºö/n
‰∏≠ÂõΩ: ‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ/ns
Â•≥ÁΩëËµõ: Â•≥Â≠ê/n ÁΩëÁêÉ/n ÊØîËµõ/vn

12. Ê±âËØ≠ÊãÜÂ≠óËØçÂÖ∏Ôºökfcd/chaizi
Êº¢Â≠ó	ÊãÜÊ≥ï (‰∏Ä)	ÊãÜÊ≥ï (‰∫å)	ÊãÜÊ≥ï (‰∏â)
ÊãÜ	Êâã Êñ•	Êâå Êñ•	Êâç Êñ•

13. ËØçÊ±áÊÉÖÊÑüÂÄºÔºörainarch/SentiBridge
Â±±Ê≥âÊ∞¥	ÂÖÖÊ≤õ	0.400704566541	0.370067395878
ËßÜÈáé	        ÂÆΩÂπø	0.305762728932	0.325320747491
Â§ßÂ≥°Ë∞∑	ÊÉäÈô©	0.312137906517	0.378594957281

14. ‰∏≠ÊñáËØçÂ∫ì„ÄÅÂÅúÁî®ËØç„ÄÅÊïèÊÑüËØç dongxiexidian/Chinese
Ê≠§packageÁöÑÊïèÊÑüËØçÂ∫ìÂàÜÁ±ªÊõ¥ÁªÜÔºö
ÂèçÂä®ËØçÂ∫ìÔºå ÊïèÊÑüËØçÂ∫ìË°®ÁªüËÆ°Ôºå Êö¥ÊÅêËØçÂ∫ìÔºå Ê∞ëÁîüËØçÂ∫ìÔºå Ëâ≤ÊÉÖËØçÂ∫ì
15. Ê±âÂ≠óËΩ¨ÊãºÈü≥Ôºömozillazg/python-pinyin
ÊñáÊú¨Á∫†Èîô‰ºöÁî®Âà∞
16. ‰∏≠ÊñáÁπÅÁÆÄ‰Ωì‰∫íËΩ¨Ôºöskydark/nstools
17. Ëã±ÊñáÊ®°Êãü‰∏≠ÊñáÂèëÈü≥ÂºïÊìé funny chinese text to speech engineeÔºötinyfool/ChineseWithEnglish
say wo i ni
#ËØ¥ÔºöÊàëÁà±‰Ω†

Áõ∏ÂΩì‰∫éÁî®Ëã±ÊñáÈü≥Ê†áÔºåÊ®°Êãü‰∏≠ÊñáÂèëÈü≥„ÄÇ
18. Ê±™Â≥∞Ê≠åËØçÁîüÊàêÂô®Ôºöphunterlau/wangfeng-rnn
ÊàëÂú®ËøôÈáå‰∏≠ÁöÑÂ§úÈáå
Â∞±ÂÉè‰∏ÄÂú∫ÊòØ‰∏ÄÁßçÁîüÂëΩÁöÑÊÑèÊó™
Â∞±ÂÉèÊàëÁöÑÁîüÊ¥ªÂèòÂæóÂú®Êàë‰∏ÄÊ†∑
ÂèØÊàë‰ª¨ËøôÊòØ‰∏Ä‰∏™Áü•ÈÅì
ÊàëÂè™ÊòØ‰∏ÄÂ§©‰Ω†‰ºöÊÄéÂêó

19. Âêå‰πâËØçÂ∫ì„ÄÅÂèç‰πâËØçÂ∫ì„ÄÅÂê¶ÂÆöËØçÂ∫ìÔºöguotong1988/chinese_dictionary
20. Êó†Á©∫Ê†ºËã±Êñá‰∏≤ÂàÜÂâ≤„ÄÅÊäΩÂèñÂçïËØçÔºöwordinja
>>> import wordninja
>>> wordninja.split('derekanderson')
['derek', 'anderson']
>>> wordninja.split('imateapot')
['im', 'a', 'teapot']

21. IPÂú∞ÂùÄÊ≠£ÂàôË°®ËææÂºèÔºö
(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)

22. ËÖæËÆØQQÂè∑Ê≠£ÂàôË°®ËææÂºèÔºö
[1-9]([0-9]{5,11})

23. ÂõΩÂÜÖÂõ∫ËØùÂè∑Á†ÅÊ≠£ÂàôË°®ËææÂºèÔºö
[0-9-()ÔºàÔºâ]{7,18}

24. Áî®Êà∑ÂêçÊ≠£ÂàôË°®ËææÂºèÔºö
[A-Za-z0-9_\-\u4e00-\u9fa5]+

25. Ê±ΩËΩ¶ÂìÅÁâå„ÄÅÊ±ΩËΩ¶Èõ∂‰ª∂Áõ∏ÂÖ≥ËØçÊ±áÔºö
ËßÅÊú¨repoÁöÑdataÊñá‰ª∂ [data](https://github.com/fighting41love/funNLP/tree/master/data)

26. Êó∂Èó¥ÊäΩÂèñÔºö

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

Âú®2016Âπ¥6Êúà7Êó•9:44ÊâßË°åÊ∏¨Ë©¶ÔºåÁªìÊûúÂ¶Ç‰∏ã

HiÔºåall„ÄÇ‰∏ãÂë®‰∏Ä‰∏ãÂçà‰∏âÁÇπÂºÄ‰ºö

>> 2016-06-13 15:00:00-false

Âë®‰∏ÄÂºÄ‰ºö

>> 2016-06-13 00:00:00-true

‰∏ã‰∏ãÂë®‰∏ÄÂºÄ‰ºö

>> 2016-06-20 00:00:00-true

java version
python version
27. ÂêÑÁßç‰∏≠ÊñáËØçÂêëÈáèÔºö github repo
‰∏≠ÊñáËØçÂêëÈáèÂ§ßÂÖ®
28. ÂÖ¨Âè∏ÂêçÂ≠óÂ§ßÂÖ®Ôºö github repo
29. Âè§ËØóËØçÂ∫ìÔºö github repo Êõ¥ÂÖ®ÁöÑÂè§ËØóËØçÂ∫ì
30. THUÊï¥ÁêÜÁöÑËØçÂ∫ìÔºö link
Â∑≤Êï¥ÁêÜÂà∞Êú¨repoÁöÑdataÊñá‰ª∂Â§π‰∏≠.
ITËØçÂ∫ì„ÄÅË¥¢ÁªèËØçÂ∫ì„ÄÅÊàêËØ≠ËØçÂ∫ì„ÄÅÂú∞ÂêçËØçÂ∫ì„ÄÅÂéÜÂè≤Âêç‰∫∫ËØçÂ∫ì„ÄÅËØóËØçËØçÂ∫ì„ÄÅÂåªÂ≠¶ËØçÂ∫ì„ÄÅÈ•ÆÈ£üËØçÂ∫ì„ÄÅÊ≥ïÂæãËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ËØçÂ∫ì„ÄÅÂä®Áâ©ËØçÂ∫ì

31. ‰∏≠ÊñáËÅäÂ§©ËØ≠Êñô link
ËØ•Â∫ìÊêúÈõÜ‰∫ÜÂåÖÂê´:Ë±ÜÁì£Â§öËΩÆ, PTTÂÖ´Âç¶ËØ≠Êñô, Èùí‰∫ëËØ≠Êñô, ÁîµËßÜÂâßÂØπÁôΩËØ≠Êñô, Ë¥¥ÂêßËÆ∫ÂùõÂõûÂ∏ñËØ≠Êñô,ÂæÆÂçöËØ≠Êñô,Â∞èÈªÑÈ∏°ËØ≠Êñô

32. ‰∏≠ÊñáË∞£Ë®ÄÊï∞ÊçÆ: github
ËØ•Êï∞ÊçÆÊñá‰ª∂‰∏≠ÔºåÊØè‰∏ÄË°å‰∏∫‰∏ÄÊù°jsonÊ†ºÂºèÁöÑË∞£Ë®ÄÊï∞ÊçÆÔºåÂ≠óÊÆµÈáä‰πâÂ¶Ç‰∏ãÔºö

rumorCode: ËØ•Êù°Ë∞£Ë®ÄÁöÑÂîØ‰∏ÄÁºñÁ†ÅÔºåÂèØ‰ª•ÈÄöËøáËØ•ÁºñÁ†ÅÁõ¥Êé•ËÆøÈóÆËØ•Ë∞£Ë®Ä‰∏æÊä•È°µÈù¢„ÄÇ
title: ËØ•Êù°Ë∞£Ë®ÄË¢´‰∏æÊä•ÁöÑÊ†áÈ¢òÂÜÖÂÆπ
informerName: ‰∏æÊä•ËÄÖÂæÆÂçöÂêçÁß∞
informerUrl: ‰∏æÊä•ËÄÖÂæÆÂçöÈìæÊé•
rumormongerName: ÂèëÂ∏ÉË∞£Ë®ÄËÄÖÁöÑÂæÆÂçöÂêçÁß∞
rumormongerUr: ÂèëÂ∏ÉË∞£Ë®ÄËÄÖÁöÑÂæÆÂçöÈìæÊé•
rumorText: Ë∞£Ë®ÄÂÜÖÂÆπ
visitTimes: ËØ•Ë∞£Ë®ÄË¢´ËÆøÈóÆÊ¨°Êï∞
result: ËØ•Ë∞£Ë®ÄÂÆ°Êü•ÁªìÊûú
publishTime: ËØ•Ë∞£Ë®ÄË¢´‰∏æÊä•Êó∂Èó¥

33. ÊÉÖÊÑüÊ≥¢Âä®ÂàÜÊûêÔºögithub
ËØçÂ∫ìÂ∑≤Êï¥ÁêÜÂà∞Êú¨repoÁöÑdataÊñá‰ª∂Â§π‰∏≠.
Êú¨repoÈ°πÁõÆÊòØ‰∏Ä‰∏™ÈÄöËøá‰∏é‰∫∫ÂØπËØùËé∑ÂæóÂÖ∂ÊÉÖÊÑüÂÄºÊ≥¢Âä®ÂõæË∞±, ÂÜÖÁî®ËØçÂ∫ìÂú®dataÊñá‰ª∂Â§π‰∏≠.

34. ÁôæÂ∫¶‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºöÈìæÊé• ÊèêÂèñÁ†Å: 2dva
35. Âè•Â≠ê„ÄÅQAÁõ∏‰ººÂ∫¶ÂåπÈÖç:MatchZoo github
ÊñáÊú¨Áõ∏‰ººÂ∫¶ÂåπÈÖçÁÆóÊ≥ïÁöÑÈõÜÂêàÔºåÂåÖÂê´Â§ö‰∏™Ê∑±Â∫¶Â≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÂÄºÂæóÂ∞ùËØï„ÄÇ
36. bertËµÑÊ∫êÔºö

bertËÆ∫Êñá‰∏≠ÊñáÁøªËØë: link



bertÂéü‰ΩúËÄÖÁöÑslides: link
ÊèêÂèñÁ†Å: iarj


ÊñáÊú¨ÂàÜÁ±ªÂÆûË∑µ: github


bert tutorialÊñáÊú¨ÂàÜÁ±ªÊïôÁ®ã: github


bert pytorchÂÆûÁé∞:  github


bertÁî®‰∫é‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´ tensorflowÁâàÊú¨: github


BERTÁîüÊàêÂè•ÂêëÈáèÔºåBERTÂÅöÊñáÊú¨ÂàÜÁ±ª„ÄÅÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆógithub


bert Âü∫‰∫é keras ÁöÑÂ∞ÅË£ÖÂàÜÁ±ªÊ†áÊ≥®Ê°ÜÊû∂ KashgariÔºåÂá†ÂàÜÈíüÂç≥ÂèØÊê≠Âª∫‰∏Ä‰∏™ÂàÜÁ±ªÊàñËÄÖÂ∫èÂàóÊ†áÊ≥®Ê®°Âûã: github


bert„ÄÅELMOÁöÑÂõæËß£Ôºö github


BERT: Pre-trained models and downstream applications: github


37. Texar - Toolkit for Text Generation and Beyond: github
Âü∫‰∫éTensorflowÁöÑÂºÄÊ∫êÂ∑•ÂÖ∑ÂåÖÔºåÊó®Âú®ÊîØÊåÅÂπøÊ≥õÁöÑÊú∫Âô®Â≠¶‰π†ÔºåÁâπÂà´ÊòØÊñáÊú¨ÁîüÊàê‰ªªÂä°ÔºåÂ¶ÇÊú∫Âô®ÁøªËØë„ÄÅÂØπËØù„ÄÅÊëòË¶Å„ÄÅÂÜÖÂÆπÂ§ÑÁΩÆ„ÄÅËØ≠Ë®ÄÂª∫Ê®°Á≠â
38. ‰∏≠Êñá‰∫ã‰ª∂ÊäΩÂèñÔºö github
‰∏≠ÊñáÂ§çÂêà‰∫ã‰ª∂ÊäΩÂèñÔºåÂåÖÊã¨Êù°‰ª∂‰∫ã‰ª∂„ÄÅÂõ†Êûú‰∫ã‰ª∂„ÄÅÈ°∫Êâø‰∫ã‰ª∂„ÄÅÂèçËΩ¨‰∫ã‰ª∂Á≠â‰∫ã‰ª∂ÊäΩÂèñÔºåÂπ∂ÂΩ¢Êàê‰∫ãÁêÜÂõæË∞±„ÄÇ
39. cocoNLP: github
‰∫∫Âêç„ÄÅÂú∞ÂùÄ„ÄÅÈÇÆÁÆ±„ÄÅÊâãÊú∫Âè∑„ÄÅÊâãÊú∫ÂΩíÂ±ûÂú∞ Á≠â‰ø°ÊÅØÁöÑÊäΩÂèñÔºårakeÁü≠ËØ≠ÊäΩÂèñÁÆóÊ≥ï„ÄÇ

pip install cocoNLP

>>> from cocoNLP.extractor import extractor

>>> ex = extractor()

>>> text = 'ÊÄ•ÂØªÁâπÊúóÊôÆÔºåÁî∑Â≠©Ôºå‰∫é2018Âπ¥11Êúà27Âè∑11Êó∂Âú®ÈôïË•øÁúÅÂÆâÂ∫∑Â∏ÇÊ±âÊª®Âå∫Ëµ∞Â§±„ÄÇ‰∏¢Â§±ÂèëÂûãÁü≠ÂèëÔºå...Â¶ÇÊúâÁ∫øÁ¥¢ÔºåËØ∑ËøÖÈÄü‰∏éË≠¶ÊñπËÅîÁ≥ªÔºö18100065143Ôºå132-6156-2938Ôºåbaizhantang@sina.com.cn Âíåyangyangfuture at gmail dot com'

# ÊäΩÂèñÈÇÆÁÆ±
>>> emails = ex.extract_email(text)
>>> print(emails)

['baizhantang@sina.com.cn', 'yangyangfuture@gmail.com.cn']
# ÊäΩÂèñÊâãÊú∫Âè∑
>>> cellphones = ex.extract_cellphone(text,nation='CHN')
>>> print(cellphones)

['18100065143', '13261562938']
# ÊäΩÂèñÊâãÊú∫ÂΩíÂ±ûÂú∞„ÄÅËøêËê•ÂïÜ
>>> cell_locs = [ex.extract_cellphone_location(cell,'CHN') for cell in cellphones]
>>> print(cell_locs)

cellphone_location [{'phone': '18100065143', 'province': '‰∏äÊµ∑', 'city': '‰∏äÊµ∑', 'zip_code': '200000', 'area_code': '021', 'phone_type': 'Áîµ‰ø°'}]
# ÊäΩÂèñÂú∞ÂùÄ‰ø°ÊÅØ
>>> locations = ex.extract_locations(text)
>>> print(locations)
['ÈôïË•øÁúÅÂÆâÂ∫∑Â∏ÇÊ±âÊª®Âå∫', 'ÂÆâÂ∫∑Â∏ÇÊ±âÊª®Âå∫', 'Ê±âÊª®Âå∫']
# ÊäΩÂèñÊó∂Èó¥ÁÇπ
>>> times = ex.extract_time(text)
>>> print(times)
time {""type"": ""timestamp"", ""timestamp"": ""2018-11-27 11:00:00""}
# ÊäΩÂèñ‰∫∫Âêç
>>> name = ex.extract_name(text)
>>> print(name)
ÁâπÊúóÊôÆ


40. ÂõΩÂÜÖÁîµËØùÂè∑Á†ÅÊ≠£ÂàôÂåπÈÖçÔºà‰∏âÂ§ßËøêËê•ÂïÜ+ËôöÊãüÁ≠âÔºâ: github
41. Ê∏ÖÂçéÂ§ßÂ≠¶XLORE:‰∏≠Ëã±ÊñáË∑®ËØ≠Ë®ÄÁôæÁßëÁü•ËØÜÂõæË∞±: link
‰∏äËø∞ÈìæÊé•‰∏≠ÂåÖÂê´‰∫ÜÊâÄÊúâÂÆû‰ΩìÂèäÂÖ≥Á≥ªÁöÑTTLÊñá‰ª∂ÔºåÊõ¥Â§öÊï∞ÊçÆÂ∞ÜÂú®ËøëÊúüÂèëÂ∏É„ÄÇ
Ê¶ÇÂøµÔºåÂÆû‰æãÔºåÂ±ûÊÄßÂíå‰∏ä‰∏ã‰ΩçÂÖ≥Á≥ªÊï∞ÁõÆ




ÁôæÂ∫¶
‰∏≠ÊñáÁª¥Âü∫
Ëã±ÊñáÁª¥Âü∫
ÊÄªÊï∞




Ê¶ÇÂøµÊï∞Èáè
32,009
150,241
326,518
508,768


ÂÆû‰æãÊï∞Èáè
1,629,591
640,622
1,235,178
3,505,391


Â±ûÊÄßÊï∞Èáè
157,370
45,190
26,723
229.283


InstanceOf
7,584,931
1,449,925
3,032,515
12,067,371


SubClassOf
2,784
191,577
555,538
749,899



Ë∑®ËØ≠Ë®ÄËøûÊé•ÔºàÊ¶ÇÂøµ/ÂÆû‰æãÔºâ




ÁôæÂ∫¶
‰∏≠ÊñáÁª¥Âü∫
Ëã±ÊñáÁª¥Âü∫




ÁôæÂ∫¶
-
10,216/336,890
4,846/303,108


‰∏≠ÊñáÁª¥Âü∫
10,216/336,890
-
28,921/454,579


Ëã±ÊñáÁª¥Âü∫
4,846/303,108
28,921/454,579
-



42. Ê∏ÖÂçéÂ§ßÂ≠¶‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁ≥ªÂàóÊä•ÂëäÔºö link
ÊØèÂπ¥‰ºöÂá∫AIÈ¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÊä•ÂëäÔºåÂÜÖÂÆπÂåÖÂê´

Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ link
Áü•ËØÜÂõæË∞± link
Êï∞ÊçÆÊåñÊéò link
Ëá™Âä®È©æÈ©∂ link
Êú∫Âô®ÁøªËØë link
Âå∫ÂùóÈìæ link
Êú∫Âô®‰∫∫ link
ËÆ°ÁÆóÊú∫ÂõæÂΩ¢Â≠¶ link
3DÊâìÂç∞ link
‰∫∫ËÑ∏ËØÜÂà´ link
‰∫∫Â∑•Êô∫ËÉΩËäØÁâá link
Á≠âÁ≠â

43.Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÊñπÈù¢:
Ehud ReiterÊïôÊéàÁöÑÂçöÂÆ¢  ÂåóÂ§ß‰∏áÂ∞èÂÜõÊïôÊéàÂº∫ÂäõÊé®ËçêÔºåËØ•ÂçöÂÆ¢ÂØπNLGÊäÄÊúØ„ÄÅËØÑ‰ª∑‰∏éÂ∫îÁî®ËøõË°å‰∫ÜÊ∑±ÂÖ•ÁöÑÊé¢ËÆ®‰∏éÂèçÊÄù„ÄÇ
ÊñáÊú¨ÁîüÊàêÁõ∏ÂÖ≥ËµÑÊ∫êÂ§ßÂàóË°®
Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÔºöËÆ©Êú∫Âô®ÊéåÊè°Ëá™Âä®Âàõ‰ΩúÁöÑÊú¨È¢Ü - ÂºÄÊîæÂüüÂØπËØùÁîüÊàêÂèäÂú®ÂæÆËΩØÂ∞èÂÜ∞‰∏≠ÁöÑÂÆûË∑µ
ÊñáÊú¨ÁîüÊàêÊéßÂà∂
44.:
jiebaÂíåhanlpÂ∞±‰∏çÂøÖ‰ªãÁªç‰∫ÜÂêß„ÄÇ
45.NLPÂ§™Èöæ‰∫ÜÁ≥ªÂàó: github

Êù•Âà∞Êù®ËøáÊõæÁªèÁîüÊ¥ªËøáÁöÑÂú∞ÊñπÔºåÂ∞èÈæôÂ•≥Âä®ÊÉÖÂú∞ËØ¥Ôºö‚ÄúÊàë‰πüÊÉ≥ËøáËøáËøáÂÑøËøáËøáÁöÑÁîüÊ¥ª„ÄÇ‚Äù ‚Äã‚Äã‚Äã
Êù•Âà∞ÂÑøÂ≠êÁ≠âÊ†°ËΩ¶ÁöÑÂú∞ÊñπÔºåÈÇìË∂ÖÂØπÂ≠ô‰ø™ËØ¥Ôºö‚ÄúÊàë‰πüÊÉ≥Á≠âÁ≠âÁ≠âÁ≠âÁ≠âËøáÁöÑÈÇ£ËæÜËΩ¶„ÄÇ‚Äù
ËµµÊïèËØ¥ÔºöÊàë‰πüÊÉ≥ÊéßÂøåÂøåÂ∑±‰∏çÊÉ≥Êó†Âøå„ÄÇ
‰Ω†‰πüÊÉ≥ÁäØËåÉËåÉËåÉÁéÆÁê™ÁäØËøáÁöÑÈîôÂêó
ÂØπÂèôÊâìÂáªÊòØ‰∏ÄÊ¨°ÊÄßË°å‰∏∫Ôºü

46.Ëá™Âä®ÂØπËÅîÊï∞ÊçÆÂèäÊú∫Âô®‰∫∫:
70‰∏áÂØπËÅîÊï∞ÊçÆ link
‰ª£Á†Å link



‰∏äËÅî
‰∏ãËÅî




ÊÆ∑Âã§ÊÄïË¥ü‰∏âÊò•ÊÑè
ÊΩáÊ¥íÈöæ‰π¶‰∏ÄÂ≠óÊÑÅ


Â¶ÇÊ≠§Ê∏ÖÁßã‰ΩïÂêùÈÖí
ËøôËà¨ÊòéÊúà‰∏çÈ°ªÈí±



47.Áî®Êà∑ÂêçÈªëÂêçÂçïÂàóË°®Ôºö github
ÂåÖÂê´‰∫ÜÁî®Êà∑ÂêçÁ¶ÅÁî®ÂàóË°®ÔºåÊØîÂ¶Ç: link
administrator
administration
autoconfig
autodiscover
broadcasthost
domain
editor
guest
host
hostmaster
info
keybase.txt
localdomain
localhost
master
mail
mail0
mail1

48.ÁΩ™ÂêçÊ≥ïÂä°ÂêçËØçÂèäÂàÜÁ±ªÊ®°Âûã: github
ÂåÖÂê´856È°πÁΩ™ÂêçÁü•ËØÜÂõæË∞±, Âü∫‰∫é280‰∏áÁΩ™ÂêçËÆ≠ÁªÉÂ∫ìÁöÑÁΩ™ÂêçÈ¢ÑÊµã,Âü∫‰∫é20WÊ≥ïÂä°ÈóÆÁ≠îÂØπÁöÑ13Á±ªÈóÆÈ¢òÂàÜÁ±ª‰∏éÊ≥ïÂæãËµÑËÆØÈóÆÁ≠îÂäüËÉΩ

49.ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ËØ≠Êñô: github
3GËØ≠ÊñôÔºåÂåÖÂê´ÈÉ®ÂàÜÁΩëÁªúÊäìÂèñÁöÑÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÁöÑÊñáÁ´†ÔºåÂ∑≤ÁªèÂéªÈô§HTMLÔºåÂè™ÂåÖÂê´‰∫ÜÁ∫ØÊñáÊú¨„ÄÇÊØèË°å‰∏ÄÁØáÔºåÊòØJSONÊ†ºÂºèÔºånameÊòØÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÂêçÂ≠óÔºåaccountÊòØÂæÆ‰ø°ÂÖ¨‰ºóÂè∑IDÔºåtitleÊòØÈ¢òÁõÆÔºåcontentÊòØÊ≠£Êñá
50.cs224nÊ∑±Â∫¶Â≠¶‰π†Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ãÔºölink

ËØæÁ®ã‰∏≠Ê®°ÂûãÁöÑpytorchÂÆûÁé∞ link
Èù¢ÂêëÊ∑±Â∫¶Â≠¶‰π†Á†îÁ©∂‰∫∫ÂëòÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÆû‰æãÊïôÁ®ã link

51.‰∏≠ÊñáÊâãÂÜôÊ±âÂ≠óËØÜÂà´Ôºögithub
52.‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ ËØ≠Êñô/Êï∞ÊçÆÈõÜÔºögithub
Á´ûÂìÅÔºöTHUOCLÔºàTHU Open Chinese LexiconÔºâ‰∏≠ÊñáËØçÂ∫ì
53.ÂèòÈáèÂëΩÂêçÁ•ûÂô®Ôºögithub link
54.ÂàÜËØçËØ≠ÊñôÂ∫ì+‰ª£Á†ÅÔºöÁôæÂ∫¶ÁΩëÁõòÈìæÊé•

ÊèêÂèñÁ†Å: pea6
kerasÂÆûÁé∞ÁöÑÂü∫‰∫éBi-LSTM + CRFÁöÑ‰∏≠ÊñáÂàÜËØç+ËØçÊÄßÊ†áÊ≥®
Âü∫‰∫éUniversal Transformer + CRF ÁöÑ‰∏≠ÊñáÂàÜËØçÂíåËØçÊÄßÊ†áÊ≥®
Âø´ÈÄüÁ•ûÁªèÁΩëÁªúÂàÜËØçÂåÖ java version

55. NLPÊñ∞‰π¶Êé®Ëçê„ÄäNatural Language Processing„Äãby Jacob EisensteinÔºö link
56. ‰ªªÂä°ÂûãÂØπËØùËã±ÊñáÊï∞ÊçÆÈõÜÔºö github
„ÄêÊúÄÂÖ®‰ªªÂä°ÂûãÂØπËØùÊï∞ÊçÆÈõÜ„Äë‰∏ªË¶Å‰ªãÁªç‰∫Ü‰∏Ä‰ªΩ‰ªªÂä°ÂûãÂØπËØùÊï∞ÊçÆÈõÜÂ§ßÂÖ®ÔºåËøô‰ªΩÊï∞ÊçÆÈõÜÂ§ßÂÖ®Ê∂µÁõñ‰∫ÜÂà∞ÁõÆÂâçÂú®‰ªªÂä°ÂûãÂØπËØùÈ¢ÜÂüüÁöÑÊâÄÊúâÂ∏∏Áî®Êï∞ÊçÆÈõÜÁöÑ‰∏ªË¶Å‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÂ∏ÆÂä©Á†îÁ©∂ËÄÖÊõ¥Â•ΩÁöÑÊääÊè°È¢ÜÂüüËøõÂ±ïÁöÑËÑâÁªúÔºåÊàë‰ª¨‰ª•LeaderboardÁöÑÂΩ¢ÂºèÁªôÂá∫‰∫ÜÂá†‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑState-of-the-artÂÆûÈ™åÁªìÊûú„ÄÇ
57. ASR ËØ≠Èü≥Êï∞ÊçÆÈõÜ + Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´Á≥ªÁªüÔºö github


Data Sets Êï∞ÊçÆÈõÜ


Ê∏ÖÂçéÂ§ßÂ≠¶THCHS30‰∏≠ÊñáËØ≠Èü≥Êï∞ÊçÆÈõÜ
data_thchs30.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè
test-noise.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè
resource.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè


Free ST Chinese Mandarin Corpus
ST-CMDS-20170001_1-OS.tar.gz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè


AIShell-1 ÂºÄÊ∫êÁâàÊï∞ÊçÆÈõÜ
data_aishell.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè


Ê≥®ÔºöÊï∞ÊçÆÈõÜËß£ÂéãÊñπÊ≥ï
$ tar xzf data_aishell.tgz
$ cd data_aishell/wav
$ for tar in *.tar.gz;  do tar xvf $tar; done



Primewords Chinese Corpus Set 1
primewords_md_2018_set1.tar.gz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè




58. Á¨ëÂ£∞Ê£ÄÊµãÂô®Ôºö github
59. MicrosoftÂ§öËØ≠Ë®ÄÊï∞Â≠ó/Âçï‰Ωç/Â¶ÇÊó•ÊúüÊó∂Èó¥ËØÜÂà´ÂåÖÔºö [github](https://github.com/Microsoft/Recognizers-Text
60. chinese-xinhua ‰∏≠ÂçéÊñ∞ÂçéÂ≠óÂÖ∏Êï∞ÊçÆÂ∫ìÂèäapiÔºåÂåÖÊã¨Â∏∏Áî®Ê≠áÂêéËØ≠„ÄÅÊàêËØ≠„ÄÅËØçËØ≠ÂíåÊ±âÂ≠ó github
61. ÊñáÊ°£ÂõæË∞±Ëá™Âä®ÁîüÊàê github

TextGrapher - Text Content Grapher based on keyinfo extraction by NLP method„ÄÇËæìÂÖ•‰∏ÄÁØáÊñáÊ°£ÔºåÂ∞ÜÊñáÊ°£ËøõË°åÂÖ≥ÈîÆ‰ø°ÊÅØÊèêÂèñÔºåËøõË°åÁªìÊûÑÂåñÔºåÂπ∂ÊúÄÁªàÁªÑÁªáÊàêÂõæË∞±ÁªÑÁªáÂΩ¢ÂºèÔºåÂΩ¢ÊàêÂØπÊñáÁ´†ËØ≠‰πâ‰ø°ÊÅØÁöÑÂõæË∞±ÂåñÂ±ïÁ§∫

62. SpaCy ‰∏≠ÊñáÊ®°Âûã github

ÂåÖÂê´Parser, NER, ËØ≠Ê≥ïÊ†ëÁ≠âÂäüËÉΩ„ÄÇÊúâ‰∏Ä‰∫õËã±Êñápackage‰ΩøÁî®spacyÁöÑËã±ÊñáÊ®°ÂûãÁöÑÔºåÂ¶ÇÊûúË¶ÅÈÄÇÈÖç‰∏≠ÊñáÔºåÂèØËÉΩÈúÄË¶Å‰ΩøÁî®spacy‰∏≠ÊñáÊ®°Âûã„ÄÇ

63. Common VoiceËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜÊñ∞Áâà link

ÂåÖÊã¨Êù•Ëá™42,000ÂêçË¥°ÁåÆËÄÖË∂ÖËøá1,400Â∞èÊó∂ÁöÑËØ≠Èü≥Ê†∑Êú¨ÔºåÊ∂µgithub

64. Á•ûÁªèÁΩëÁªúÂÖ≥Á≥ªÊäΩÂèñ pytorch github

ÊöÇ‰∏çÊîØÊåÅ‰∏≠Êñá

65. Âü∫‰∫ébertÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´ pytorch github

ÊöÇ‰∏çÊîØÊåÅ‰∏≠Êñá

66. ÂÖ≥ÈîÆËØç(Keyphrase)ÊäΩÂèñÂåÖ pke github
pke: an open source python-based keyphrase extraction toolkit

ÊöÇ‰∏çÊîØÊåÅ‰∏≠ÊñáÔºåÊàë‰∫éËøëÊúüÂØπÂÖ∂ËøõË°å‰øÆÊîπÔºå‰ΩøÂÖ∂ÈÄÇÈÖç‰∏≠Êñá„ÄÇ
ËØ∑ÂÖ≥Ê≥®ÊàëÁöÑgithubÂä®ÊÄÅÔºåË∞¢Ë∞¢ÔºÅ

67. Âü∫‰∫éÂåªÁñóÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü github

ËØ•repoÂèÇËÄÉ‰∫Ügithub

68. Âü∫‰∫é‰æùÂ≠òÂè•Ê≥ï‰∏éËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÁöÑ‰∫ã‰ª∂‰∏âÂÖÉÁªÑÊäΩÂèñ github
69. ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê4‰∏áÂè•È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆ by ËãèÂ∑ûÂ§ßÂ≠¶Ê±âËØ≠‰æùÂ≠òÊ†ëÂ∫ìÔºàSUCDTÔºâ
Homepage
Êï∞ÊçÆ‰∏ãËΩΩËØ¶ËßÅhomepageÂ∫ïÈÉ®ÔºåÈúÄË¶ÅÁ≠æÁΩ≤ÂçèËÆÆÔºåÈúÄË¶ÅÈÇÆ‰ª∂Êé•Êî∂Ëß£ÂéãÂØÜÁ†Å„ÄÇ
70. cnocrÔºöÁî®Êù•ÂÅö‰∏≠ÊñáOCRÁöÑPython3ÂåÖÔºåËá™Â∏¶‰∫ÜËÆ≠ÁªÉÂ•ΩÁöÑËØÜÂà´Ê®°Âûã github
71. ‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÁü•ËØÜÂõæË∞±È°πÁõÆ github

‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÂõæË∞±ÊûÑÂª∫
Âü∫‰∫éÁü•ËØÜÂ∫ìÁöÑÊï∞ÊçÆÂõûÊ†á
Âü∫‰∫éËøúÁ®ãÁõëÁù£‰∏ébootstrappingÊñπÊ≥ïÁöÑ‰∫∫Áâ©ÂÖ≥Á≥ªÊäΩÂèñ
Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÁü•ËØÜÈóÆÁ≠îÁ≠âÂ∫îÁî®

72. ‰∏≠ÊñánlpÁ´ûËµõÈ°πÁõÆÂèä‰ª£Á†ÅÊ±áÊÄª github

ÊñáÊú¨ÁîüÊàê„ÄÅÊñáÊú¨ÊëòË¶ÅÔºöByte Cup 2018 ÂõΩÈôÖÊú∫Âô®Â≠¶‰π†Á´ûËµõ
Áü•ËØÜÂõæË∞±ÔºöÁëûÈáëÂåªÈô¢MMC‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©ÊûÑÂª∫Áü•ËØÜÂõæË∞±Â§ßËµõ
ËßÜÈ¢ëËØÜÂà´ ÈóÆÁ≠îÔºö2018‰πãÊ±üÊùØÂÖ®ÁêÉ‰∫∫Â∑•Êô∫ËÉΩÂ§ßËµõ‚Ä®ÔºöËßÜÈ¢ëËØÜÂà´&ÈóÆÁ≠î

73. ‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ÊçÆ github

ÁÆÄ/ÁπÅ‰ΩìÊ±âÂ≠óÁ¨îÈ°∫
Áü¢ÈáèÁ¨îÁîª

74. speech-aligner: ‰ªé‚Äú‰∫∫Â£∞ËØ≠Èü≥‚ÄùÂèäÂÖ∂‚ÄúËØ≠Ë®ÄÊñáÊú¨‚ÄùÔºå‰∫ßÁîüÈü≥Á¥†Á∫ßÂà´Êó∂Èó¥ÂØπÈΩêÊ†áÊ≥®ÁöÑÂ∑•ÂÖ∑ github
75. AmpliGraph: Áü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†(Python)Â∫ìÔºöÁü•ËØÜÂõæË∞±Ê¶ÇÂøµÈìæÊé•È¢ÑÊµã github

ÂüÉÊ£ÆÂì≤Âá∫ÂìÅÔºåÁõÆÂâçÂ∞ö‰∏çÊîØÊåÅ‰∏≠Êñá

76. Scattertext ÊñáÊú¨ÂèØËßÜÂåñ(python) github

ÂæàÂ•ΩÁî®ÁöÑÂ∑•ÂÖ∑ÂåÖÔºåÁÆÄÂçï‰øÆÊîπÂêéÂèØÊîØÊåÅ‰∏≠Êñá
ËÉΩÂê¶ÂàÜÊûêÂá∫Êüê‰∏™Á±ªÂà´ÁöÑÊñáÊú¨‰∏éÂÖ∂‰ªñÊñáÊú¨ÁöÑÁî®ËØçÂ∑ÆÂºÇ

77. ËØ≠Ë®Ä/Áü•ËØÜË°®Á§∫Â∑•ÂÖ∑ÔºöBERT & ERNIE github

ÁôæÂ∫¶Âá∫ÂìÅÔºåERNIE‰πüÂè∑Áß∞Âú®Â§öÈ°πnlp‰ªªÂä°‰∏≠ÂáªË¥•‰∫Übert

78. ‰∏≠ÊñáÂØπÊØîËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜNLPÁöÑÂå∫Âà´ÁªºËø∞ link
79. Synonyms‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖ github

Synonyms ‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖÔºåÂèØ‰ª•Áî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ÁöÑÂæàÂ§ö‰ªªÂä°ÔºöÊñáÊú¨ÂØπÈΩêÔºåÊé®ËçêÁÆóÊ≥ïÔºåÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåËØ≠‰πâÂÅèÁßªÔºåÂÖ≥ÈîÆÂ≠óÊèêÂèñÔºåÊ¶ÇÂøµÊèêÂèñÔºåËá™Âä®ÊëòË¶ÅÔºåÊêúÁ¥¢ÂºïÊìéÁ≠â

80. HarvestTextÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñáÊú¨ÊåñÊéòÂ∑•ÂÖ∑ÔºàÊñ∞ËØçÂèëÁé∞-ÊÉÖÊÑüÂàÜÊûê-ÂÆû‰ΩìÈìæÊé•Á≠âÔºâ github
81. word2wordÔºö(Python)Êñπ‰æøÊòìÁî®ÁöÑÂ§öËØ≠Ë®ÄËØç-ËØçÂØπÈõÜÔºö62ÁßçËØ≠Ë®Ä/3,564‰∏™Â§öËØ≠Ë®ÄÂØπ github
82. ËØ≠Èü≥ËØÜÂà´ËØ≠ÊñôÁîüÊàêÂ∑•ÂÖ∑Ôºö‰ªéÂÖ∑ÊúâÈü≥È¢ë/Â≠óÂπïÁöÑÂú®Á∫øËßÜÈ¢ëÂàõÂª∫Ëá™Âä®ËØ≠Èü≥ËØÜÂà´(ASR)ËØ≠ÊñôÂ∫ì github
83. ASRËØ≠Èü≥Â§ßËæûÂÖ∏/ËØçÂÖ∏Ôºö github
84. ÊûÑÂª∫ÂåªÁñóÂÆû‰ΩìËØÜÂà´ÁöÑÊ®°ÂûãÔºåÂåÖÂê´ËØçÂÖ∏ÂíåËØ≠ÊñôÊ†áÊ≥®ÔºåÂü∫‰∫épython: github
85. ÂçïÊñáÊ°£ÈùûÁõëÁù£ÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñÔºö github
86. Kashgari‰∏≠‰ΩøÁî®gpt-2ËØ≠Ë®ÄÊ®°Âûã github
87.  ÂºÄÊ∫êÁöÑÈáëËûçÊäïËµÑÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑ github
88. ÊñáÊú¨Ëá™Âä®ÊëòË¶ÅÂ∫ìTextTeaser: ‰ªÖÊîØÊåÅËã±Êñá github
89. ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ github
90. ‰∏Ä‰∫õÂÖ≥‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂü∫Êú¨Ê®°Âûã github
91. Âü∫‰∫é14WÊ≠åÊõ≤Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∞ùËØïÔºåÂäüËÉΩÂåÖÊã¨Ê≠åËØçÊé•ÈæôÔºåÂ∑≤Áü•Ê≠åËØçÊâæÊ≠åÊõ≤‰ª•ÂèäÊ≠åÊõ≤Ê≠åÊâãÊ≠åËØç‰∏âËßíÂÖ≥Á≥ªÁöÑÈóÆÁ≠î github
92. Âü∫‰∫éSiamese bilstmÊ®°ÂûãÁöÑÁõ∏‰ººÂè•Â≠êÂà§ÂÆöÊ®°Âûã,Êèê‰æõËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÊµãËØïÊï∞ÊçÆÈõÜ github

Êèê‰æõ‰∫Ü10‰∏á‰∏™ËÆ≠ÁªÉÊ†∑Êú¨

93. Áî®TransformerÁºñËß£Á†ÅÊ®°ÂûãÂÆûÁé∞ÁöÑÊ†πÊçÆHacker NewsÊñáÁ´†Ê†áÈ¢òËá™Âä®ÁîüÊàêËØÑËÆ∫ github
94. Áî®BERTËøõË°åÂ∫èÂàóÊ†áËÆ∞ÂíåÊñáÊú¨ÂàÜÁ±ªÁöÑÊ®°Êùø‰ª£Á†Å github
95. LitBankÔºöNLPÊï∞ÊçÆÈõÜ‚Äî‚ÄîÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆó‰∫∫ÊñáÂ≠¶Áßë‰ªªÂä°ÁöÑ100ÈÉ®Â∏¶Ê†áËÆ∞Ëã±ÊñáÂ∞èËØ¥ËØ≠Êñô github
96. ÁôæÂ∫¶ÂºÄÊ∫êÁöÑÂü∫ÂáÜ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü github
97. ËôöÂÅáÊñ∞ÈóªÊï∞ÊçÆÈõÜ fake news corpus github
98. Facebook: LAMAËØ≠Ë®ÄÊ®°ÂûãÂàÜÊûêÔºåÊèê‰æõTransformer-XL/BERT/ELMo/GPTÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªü‰∏ÄËÆøÈóÆÊé•Âè£ github
99. CommonsenseQAÔºöÈù¢ÂêëÂ∏∏ËØÜÁöÑËã±ÊñáQAÊåëÊàò link
100. ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ËµÑÊñô„ÄÅÊï∞ÊçÆÂèäÂ∑•ÂÖ∑ github
101. ÂêÑÂ§ßÂÖ¨Âè∏ÂÜÖÈÉ®ÈáåÂ§ßÁâõÂàÜ‰∫´ÁöÑÊäÄÊúØÊñáÊ°£ PDF ÊàñËÄÖ PPT github
102. Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêSQLËØ≠Âè•ÔºàËã±ÊñáÔºâ github
103. ‰∏≠ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫ÔºàEDAÔºâÂ∑•ÂÖ∑ github

 Ëã±ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫Â∑•ÂÖ∑ github

104. Âü∫‰∫éÂåªËçØÁü•ËØÜÂõæË∞±ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü github
105. ‰∫¨‰∏úÂïÜÂìÅÁü•ËØÜÂõæË∞± github

Âü∫‰∫é‰∫¨‰∏úÁΩëÁ´ôÁöÑ1300ÁßçÂïÜÂìÅ‰∏ä‰∏ãÁ∫ßÊ¶ÇÂøµÔºåÁ∫¶10‰∏áÂïÜÂìÅÂìÅÁâåÔºåÁ∫¶65‰∏áÂìÅÁâåÈîÄÂîÆÂÖ≥Á≥ªÔºåÂïÜÂìÅÊèèËø∞Áª¥Â∫¶Á≠âÁü•ËØÜÂ∫ìÔºåÂü∫‰∫éËØ•Áü•ËØÜÂ∫ìÂèØ‰ª•ÊîØÊåÅÂïÜÂìÅÂ±ûÊÄßÂ∫ìÊûÑÂª∫ÔºåÂïÜÂìÅÈîÄÂîÆÈóÆÁ≠îÔºåÂìÅÁâåÁâ©ÂìÅÁîü‰∫ßÁ≠âÁü•ËØÜÊü•ËØ¢ÊúçÂä°Ôºå‰πüÂèØÁî®‰∫éÊÉÖÊÑüÂàÜÊûêÁ≠â‰∏ãÊ∏∏Â∫îÁî®Ôºé

106. Âü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆ github

Âü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆÔºåÂåÖÊã¨È£ûË°åÂô®„ÄÅÂ§™Á©∫Ë£ÖÂ§áÁ≠â8Â§ßÁ±ªÔºå100‰ΩôÂ∞èÁ±ªÔºåÂÖ±ËÆ°5800È°πÁöÑÂÜõ‰∫ãÊ≠¶Âô®Áü•ËØÜÂ∫ìÔºåËØ•È°πÁõÆ‰∏ç‰ΩøÁî®ÂõæÊï∞ÊçÆÂ∫ìËøõË°åÂ≠òÂÇ®ÔºåÈÄöËøájiebaËøõË°åÈóÆÂè•Ëß£ÊûêÔºåÈóÆÂè•ÂÆû‰ΩìÈ°πËØÜÂà´ÔºåÂü∫‰∫éÊü•ËØ¢Ê®°ÊùøÂÆåÊàêÂ§öÁ±ªÈóÆÈ¢òÁöÑÊü•ËØ¢Ôºå‰∏ªË¶ÅÊòØÊèê‰æõ‰∏ÄÁßçÂ∑•‰∏öÁïåÁöÑÈóÆÁ≠îÊÄùÊÉ≥demo„ÄÇ

107. Âü∫‰∫éËøúÁõëÁù£ÁöÑ‰∏≠ÊñáÂÖ≥Á≥ªÊäΩÂèñ github
108. ËØ≠Èü≥ÊÉÖÊÑüÂàÜÊûê github
109. ‰∏≠ÊñáULMFiT ÊÉÖÊÑüÂàÜÊûê ÊñáÊú¨ÂàÜÁ±ª ËØ≠ÊñôÂèäÊ®°Âûã github
110. ‰∏Ä‰∏™ÊãçÁÖßÂÅöÈ¢òÁ®ãÂ∫è„ÄÇËæìÂÖ•‰∏ÄÂº†ÂåÖÂê´Êï∞Â≠¶ËÆ°ÁÆóÈ¢òÁöÑÂõæÁâáÔºåËæìÂá∫ËØÜÂà´Âá∫ÁöÑÊï∞Â≠¶ËÆ°ÁÆóÂºè‰ª•ÂèäËÆ°ÁÆóÁªìÊûú github
111. ‰∏ñÁïåÂêÑÂõΩÂ§ßËßÑÊ®°‰∫∫ÂêçÂ∫ì github
112. ‰∏Ä‰∏™Âà©Áî®ÊúâË∂£‰∏≠ÊñáËØ≠ÊñôÂ∫ì qingyun ËÆ≠ÁªÉÂá∫Êù•ÁöÑ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫ github

‰ΩøÁî®‰∫ÜÈùí‰∫ëËØ≠Êñô10‰∏áËØ≠ÊñôÔºåÊú¨repo‰∏≠‰πüÊúâËØ•ËØ≠ÊñôÁöÑÈìæÊé•

113. ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫Ôºå Ê†πÊçÆËá™Â∑±ÁöÑËØ≠ÊñôËÆ≠ÁªÉÂá∫Ëá™Â∑±ÊÉ≥Ë¶ÅÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂèØ‰ª•Áî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅÂú®Á∫øÈóÆÁ≠î„ÄÅÊô∫ËÉΩËÅäÂ§©Á≠âÂú∫ÊôØ github

Ê†πÊçÆËá™Â∑±ÁöÑËØ≠ÊñôËÆ≠ÁªÉÂá∫Ëá™Â∑±ÊÉ≥Ë¶ÅÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂèØ‰ª•Áî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅÂú®Á∫øÈóÆÁ≠î„ÄÅÊô∫ËÉΩËÅäÂ§©Á≠âÂú∫ÊôØ„ÄÇÂä†ÂÖ•seqGANÁâàÊú¨„ÄÇ
repo‰∏≠Êèê‰æõ‰∫Ü‰∏Ä‰ªΩË¥®Èáè‰∏çÂ§™È´òÁöÑËØ≠Êñô

114. ÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥® github

ÂõΩÂÆ∂ÁªüËÆ°Â±Ä‰∏≠ÁöÑÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥®ÔºåÈ´òÂæ∑Âú∞ÂõæÁöÑÂùêÊ†áÂíåË°åÊîøÂå∫ÂüüËæπÁïåËåÉÂõ¥ÔºåÂú®ÊµèËßàÂô®ÈáåÈù¢ËøêË°åjs‰ª£Á†ÅÈááÈõÜÁöÑ2019Âπ¥ÂèëÂ∏ÉÁöÑÊúÄÊñ∞Êï∞ÊçÆÔºåÂê´ÈááÈõÜÊ∫êÁ†ÅÔºåÊèê‰æõcsvÊ†ºÂºèÊï∞ÊçÆÔºåÊîØÊåÅcsvËΩ¨ÊàêÁúÅÂ∏ÇÂå∫Â§öÁ∫ßËÅîÂä®js‰ª£Á†Å
ÂùêÊ†á„ÄÅËæπÁïåËåÉÂõ¥„ÄÅÂêçÁß∞„ÄÅÊãºÈü≥„ÄÅË°åÊîøÂå∫Á≠âÂ§öÁ∫ßÂú∞ÂùÄ

115. ÊïôËÇ≤Ë°å‰∏öÊñ∞Èóª Ëá™Âä®ÊñáÊëò ËØ≠ÊñôÂ∫ì github
116. ÂºÄÊîæ‰∫ÜÂØπËØùÊú∫Âô®‰∫∫„ÄÅÁü•ËØÜÂõæË∞±„ÄÅËØ≠‰πâÁêÜËß£„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèäÊï∞ÊçÆ github

Âè¶‰∏Ä‰∏™qaÂØπÁöÑÊú∫Âô®‰∫∫ Amodel-for-Retrivalchatbot - ÂÆ¢ÊúçÊú∫Âô®‰∫∫ÔºåChinese Retreival chatbotÔºà‰∏≠ÊñáÊ£ÄÁ¥¢ÂºèÊú∫Âô®‰∫∫Ôºâ

117. ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºöÂü∫‰∫éÁôæÂ∫¶ÁôæÁßë‰∏≠ÊñáÈ°µÈù¢ÔºåÊäΩÂèñ‰∏âÂÖÉÁªÑ‰ø°ÊÅØÔºåÊûÑÂª∫‰∏≠ÊñáÁü•ËØÜÂõæË∞± github
118. masr: ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´ÔºåÊèê‰æõÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÈ´òËØÜÂà´Áéá github
119. PythonÈü≥È¢ëÊï∞ÊçÆÂ¢ûÂπøÂ∫ì github
120. ‰∏≠ÊñáÂÖ®ËØçË¶ÜÁõñBERTÂèä‰∏§‰ªΩÈòÖËØªÁêÜËß£Êï∞ÊçÆ github

DRCDÊï∞ÊçÆÈõÜÁî±‰∏≠ÂõΩÂè∞ÊπæÂè∞ËææÁ†îÁ©∂Èô¢ÂèëÂ∏ÉÔºåÂÖ∂ÂΩ¢Âºè‰∏éSQuADÁõ∏ÂêåÔºåÊòØÂü∫‰∫éÁπÅ‰Ωì‰∏≠ÊñáÁöÑÊäΩÂèñÂºèÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜ„ÄÇ
CMRC 2018Êï∞ÊçÆÈõÜÊòØÂìàÂ∑•Â§ßËÆØÈ£ûËÅîÂêàÂÆûÈ™åÂÆ§ÂèëÂ∏ÉÁöÑ‰∏≠ÊñáÊú∫Âô®ÈòÖËØªÁêÜËß£Êï∞ÊçÆ„ÄÇÊ†πÊçÆÁªôÂÆöÈóÆÈ¢òÔºåÁ≥ªÁªüÈúÄË¶Å‰ªéÁØáÁ´†‰∏≠ÊäΩÂèñÂá∫ÁâáÊÆµ‰Ωú‰∏∫Á≠îÊ°àÔºåÂΩ¢Âºè‰∏éSQuADÁõ∏Âêå„ÄÇ

121. ConvLabÔºöÂºÄÊ∫êÂ§öÂüüÁ´ØÂà∞Á´ØÂØπËØùÁ≥ªÁªüÂπ≥Âè∞ github
122. ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊï∞ÊçÆÈõÜ github
123. Âü∫‰∫éÊúÄÊñ∞ÁâàÊú¨rasaÊê≠Âª∫ÁöÑÂØπËØùÁ≥ªÁªü github
124. Âü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñ github

Entity and Relation Extraction Based on TensorFlow and BERT. Âü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñÔºå2019ËØ≠Ë®Ä‰∏éÊô∫ËÉΩÊäÄÊúØÁ´ûËµõ‰ø°ÊÅØÊäΩÂèñ‰ªªÂä°Ëß£ÂÜ≥ÊñπÊ°à„ÄÇSchema based Knowledge Extraction, SKE 2019

125. ‰∏Ä‰∏™Â∞èÂûãÁöÑËØÅÂà∏Áü•ËØÜÂõæË∞±/Áü•ËØÜÂ∫ì github
126. Â§çÁõòÊâÄÊúâNLPÊØîËµõÁöÑTOPÊñπÊ°à github
127. OpenCLaPÔºöÂ§öÈ¢ÜÂüüÂºÄÊ∫ê‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã‰ªìÂ∫ì github
ÂåÖÂê´Â¶Ç‰∏ãËØ≠Ë®ÄÊ®°ÂûãÂèäÁôæÂ∫¶ÁôæÁßëÊï∞ÊçÆ

Ê∞ë‰∫ãÊñá‰π¶BERT	bert-base	ÂÖ®ÈÉ®Ê∞ë‰∫ãÊñá‰π¶	2654‰∏áÁØáÊñá‰π¶	22554ËØç	370MB
Âàë‰∫ãÊñá‰π¶BERT	bert-base	ÂÖ®ÈÉ®Âàë‰∫ãÊñá‰π¶	663‰∏áÁØáÊñá‰π¶	22554ËØç	370MB
ÁôæÂ∫¶ÁôæÁßëBERT	bert-base	ÁôæÂ∫¶ÁôæÁßë	903‰∏áÁØáËØçÊù°	22166ËØç	367MB

128. UERÔºöÂü∫‰∫é‰∏çÂêåËØ≠Êñô„ÄÅÁºñÁ†ÅÂô®„ÄÅÁõÆÊ†á‰ªªÂä°ÁöÑ‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ªìÂ∫ìÔºàÂåÖÊã¨BERT„ÄÅGPT„ÄÅELMOÁ≠âÔºâ github

Âü∫‰∫éPyTorchÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÊ°ÜÊû∂ÔºåÊîØÊåÅÂØπÁºñÁ†ÅÂô®ÔºåÁõÆÊ†á‰ªªÂä°Á≠âËøõË°å‰ªªÊÑèÁöÑÁªÑÂêàÔºå‰ªéËÄåÂ§çÁé∞Â∑≤ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÊàñÂú®Â∑≤ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏äËøõ‰∏ÄÊ≠•ÊîπËøõ„ÄÇÂü∫‰∫éUERËÆ≠ÁªÉ‰∫Ü‰∏çÂêåÊÄßË¥®ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºà‰∏çÂêåËØ≠Êñô„ÄÅÁºñÁ†ÅÂô®„ÄÅÁõÆÊ†á‰ªªÂä°ÔºâÔºåÊûÑÊàê‰∫Ü‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ªìÂ∫ìÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÂú∫ÊôØ„ÄÇ

129. ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂêëÈáèÂêàÈõÜ github

ÂåÖÊã¨Â≠óÂêëÈáè,ÊãºÈü≥ÂêëÈáè,ËØçÂêëÈáè,ËØçÊÄßÂêëÈáè,‰æùÂ≠òÂÖ≥Á≥ªÂêëÈáè.ÂÖ±5ÁßçÁ±ªÂûãÁöÑÂêëÈáè

130. Âü∫‰∫éÈáëËûç-Âè∏Ê≥ïÈ¢ÜÂüü(ÂÖºÊúâÈó≤ËÅäÊÄßË¥®)ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ github

ÂÖ∂‰∏≠ÁöÑ‰∏ªË¶ÅÊ®°ÂùóÊúâ‰ø°ÊÅØÊäΩÂèñ„ÄÅNLU„ÄÅNLG„ÄÅÁü•ËØÜÂõæË∞±Á≠âÔºåÂπ∂‰∏îÂà©Áî®DjangoÊï¥Âêà‰∫ÜÂâçÁ´ØÂ±ïÁ§∫,ÁõÆÂâçÂ∑≤ÁªèÂ∞ÅË£Ö‰∫ÜnlpÂíåkgÁöÑrestfulÊé•Âè£

131. g2pCÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ±âËØ≠ËØªÈü≥Ëá™Âä®Ê†áËÆ∞Ê®°Âùó github
132. Zincbase Áü•ËØÜÂõæË∞±ÊûÑÂª∫Â∑•ÂÖ∑ÂåÖ github
133. ËØóÊ≠åË¥®ÈáèËØÑ‰ª∑/ÁªÜÁ≤íÂ∫¶ÊÉÖÊÑüËØóÊ≠åËØ≠ÊñôÂ∫ì github
134. Âø´ÈÄüËΩ¨Âåñ„Äå‰∏≠ÊñáÊï∞Â≠ó„ÄçÂíå„ÄåÈòøÊãâ‰ºØÊï∞Â≠ó„Äç github

‰∏≠Êñá„ÄÅÈòøÊãâ‰ºØÊï∞Â≠ó‰∫íËΩ¨
‰∏≠Êñá‰∏éÈòøÊãâ‰ºØÊï∞Â≠óÊ∑∑ÂêàÁöÑÊÉÖÂÜµÔºåÂú®ÂºÄÂèë‰∏≠

135. ÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îËØ≠ÊñôÂ∫ì github

Ë∂ÖËøá580‰∏áÁöÑÈóÆÈ¢òÔºå938‰∏áÁöÑÁ≠îÊ°àÔºå5800‰∏™ÂàÜÁ±ªÊ†áÁ≠æ„ÄÇÂü∫‰∫éËØ•ÈóÆÁ≠îËØ≠ÊñôÂ∫ìÔºåÂèØÊîØÊåÅÂ§öÁßçÂ∫îÁî®ÔºåÂ¶ÇÈó≤ËÅäÈóÆÁ≠îÔºåÈÄªËæëÊåñÊéò

136. Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü github

BERTÂÅöÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÂíåÂè•Â≠êÁõ∏‰ººÂ∫¶ÔºåÂàÜ‰∏∫onlineÂíåoutlineÊ®°Âºè

137. jieba_fast Âä†ÈÄüÁâàÁöÑjieba github

‰ΩøÁî®cpythonÈáçÂÜô‰∫ÜjiebaÂàÜËØçÂ∫ì‰∏≠ËÆ°ÁÆóDAGÂíåHMM‰∏≠ÁöÑvitrebiÂáΩÊï∞ÔºåÈÄüÂ∫¶ÂæóÂà∞Â§ßÂπÖÊèêÂçá

138. Ê≠£ÂàôË°®ËææÂºèÊïôÁ®ã github
139. ‰∏≠ÊñáÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜ github
140. Âü∫‰∫éBERTÁ≠âÊúÄÊñ∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊäΩÂèñÂºèÊëòË¶ÅÊèêÂèñ github
141. PythonÂà©Áî®Ê∑±Â∫¶Â≠¶‰π†ËøõË°åÊñáÊú¨ÊëòË¶ÅÁöÑÁªºÂêàÊåáÂçó link
142. Áü•ËØÜÂõæË∞±Ê∑±Â∫¶Â≠¶‰π†Áõ∏ÂÖ≥ËµÑÊñôÊï¥ÁêÜ github

Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÁü•ËØÜÂõæË∞±„ÄÅÂØπËØùÁ≥ªÁªü„ÄÇÂåÖÊã¨Áü•ËØÜËé∑Âèñ„ÄÅÁü•ËØÜÂ∫ìÊûÑÂª∫„ÄÅÁü•ËØÜÂ∫ìÂ∫îÁî®‰∏âÂ§ßÊäÄÊúØÁ†îÁ©∂‰∏éÂ∫îÁî®

143. Áª¥Âü∫Â§ßËßÑÊ®°Âπ≥Ë°åÊñáÊú¨ËØ≠Êñô github

85ÁßçËØ≠Ë®Ä„ÄÅ1620ÁßçËØ≠Ë®ÄÂØπ„ÄÅ135MÂØπÁÖßÂè•

144. StanfordNLP 0.2.0ÔºöÁ∫ØPythonÁâàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂåÖ link
145. NeuralNLP-NeuralClassifierÔºöËÖæËÆØÂºÄÊ∫êÊ∑±Â∫¶Â≠¶‰π†ÊñáÊú¨ÂàÜÁ±ªÂ∑•ÂÖ∑ github
146. Á´ØÂà∞Á´ØÁöÑÂ∞ÅÈó≠ÂüüÂØπËØùÁ≥ªÁªü github
147. ‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºöNeuroNER vs. BertNER github
148. Êñ∞Èóª‰∫ã‰ª∂Á∫øÁ¥¢ÊäΩÂèñ github

An exploration for Eventline (important news Rank organized by pulic time)ÔºåÈíàÂØπÊüê‰∏Ä‰∫ã‰ª∂ËØùÈ¢ò‰∏ãÁöÑÊñ∞ÈóªÊä•ÈÅìÈõÜÂêàÔºåÈÄöËøá‰ΩøÁî®docrankÁÆóÊ≥ïÔºåÂØπÊñ∞ÈóªÊä•ÈÅìËøõË°åÈáçË¶ÅÊÄßËØÜÂà´ÔºåÂπ∂ÈÄöËøáÊñ∞ÈóªÊä•ÈÅìÊó∂Èó¥ÊåëÈÄâÂá∫Êó∂Èó¥Á∫ø‰∏äÈáçË¶ÅÊñ∞Èóª

149. 2019Âπ¥ÁôæÂ∫¶ÁöÑ‰∏âÂÖÉÁªÑÊäΩÂèñÊØîËµõÔºå‚ÄúÁßëÂ≠¶Á©∫Èó¥Èòü‚ÄùÊ∫êÁ†Å(Á¨¨7Âêç) github
150. Âü∫‰∫é‰æùÂ≠òÂè•Ê≥ïÁöÑÂºÄÊîæÂüüÊñáÊú¨Áü•ËØÜ‰∏âÂÖÉÁªÑÊäΩÂèñÂíåÁü•ËØÜÂ∫ìÊûÑÂª∫ github
151. ‰∏≠ÊñáÁöÑGPT2ËÆ≠ÁªÉ‰ª£Á†Å github
152. ML-NLP - Êú∫Âô®Â≠¶‰π†(Machine Learning)„ÄÅNLPÈù¢ËØï‰∏≠Â∏∏ËÄÉÂà∞ÁöÑÁü•ËØÜÁÇπÂíå‰ª£Á†ÅÂÆûÁé∞ github
153. nlp4han:‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ(Êñ≠Âè•/ÂàÜËØç/ËØçÊÄßÊ†áÊ≥®/ÁªÑÂùó/Âè•Ê≥ïÂàÜÊûê/ËØ≠‰πâÂàÜÊûê/NER/NÂÖÉËØ≠Ê≥ï/HMM/‰ª£ËØçÊ∂àËß£/ÊÉÖÊÑüÂàÜÊûê/ÊãºÂÜôÊ£ÄÊü• github
154. XLMÔºöFacebookÁöÑË∑®ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã github
155. Áî®Âü∫‰∫éBERTÁöÑÂæÆË∞ÉÂíåÁâπÂæÅÊèêÂèñÊñπÊ≥ïÊù•ËøõË°åÁü•ËØÜÂõæË∞±ÁôæÂ∫¶ÁôæÁßë‰∫∫Áâ©ËØçÊù°Â±ûÊÄßÊäΩÂèñ github
156. ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁõ∏ÂÖ≥ÁöÑÂºÄÊîæ‰ªªÂä°ÔºåÊï∞ÊçÆÈõÜ, ‰ª•ÂèäÂΩìÂâçÊúÄ‰Ω≥ÁªìÊûú github
157. CoupletAI - Âü∫‰∫éCNN+Bi-LSTM+Attention ÁöÑËá™Âä®ÂØπÂØπËÅîÁ≥ªÁªü github
158. ÊäΩË±°Áü•ËØÜÂõæË∞±ÔºåÁõÆÂâçËßÑÊ®°50‰∏áÔºåÊîØÊåÅÂêçËØçÊÄßÂÆû‰Ωì„ÄÅÁä∂ÊÄÅÊÄßÊèèËø∞„ÄÅ‰∫ã‰ª∂ÊÄßÂä®‰ΩúËøõË°åÊäΩË±° github
159. MiningZhiDaoQACorpus - 580‰∏áÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îÊï∞ÊçÆÊåñÊéòÈ°πÁõÆ github
160. brat rapid annotation tool: Â∫èÂàóÊ†áÊ≥®Â∑•ÂÖ∑ link
161. Â§ßËßÑÊ®°‰∏≠ÊñáÁü•ËØÜÂõæË∞±Êï∞ÊçÆÔºöÔºö1.4‰∫øÂÆû‰Ωì github
162. Êï∞ÊçÆÂ¢ûÂº∫Âú®Êú∫Âô®ÁøªËØëÂèäÂÖ∂‰ªñnlp‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÂèäÊïàÊûú link
163. allennlpÈòÖËØªÁêÜËß£:ÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÂíåÊ®°Âûã github
164. PDFË°®Ê†ºÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑ github
165. GraphbrainÔºöAIÂºÄÊ∫êËΩØ‰ª∂Â∫ìÂíåÁßëÁ†îÂ∑•ÂÖ∑ÔºåÁõÆÁöÑÊòØ‰øÉËøõËá™Âä®ÊÑè‰πâÊèêÂèñÂíåÊñáÊú¨ÁêÜËß£‰ª•ÂèäÁü•ËØÜÁöÑÊé¢Á¥¢ÂíåÊé®Êñ≠ github
166. ÁÆÄÂéÜËá™Âä®Á≠õÈÄâÁ≥ªÁªü github
167. Âü∫‰∫éÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÁöÑÁÆÄÂéÜËá™Âä®ÊëòË¶Å github
168. ‰∏≠ÊñáËØ≠Ë®ÄÁêÜËß£ÊµãËØÑÂü∫ÂáÜÔºåÂåÖÊã¨‰ª£Ë°®ÊÄßÁöÑÊï∞ÊçÆÈõÜ&Âü∫ÂáÜÊ®°Âûã&ËØ≠ÊñôÂ∫ì&ÊéíË°åÊ¶ú github
169. Ê†ëÊ¥û OCR ÊñáÂ≠óËØÜÂà´ github

‰∏Ä‰∏™c++ OCR github


170. ‰ªéÂåÖÂê´Ë°®Ê†ºÁöÑÊâ´ÊèèÂõæÁâá‰∏≠ËØÜÂà´Ë°®Ê†ºÂíåÊñáÂ≠ó github
171. ËØ≠Â£∞ËøÅÁßª github
172. PythonÂè£ËØ≠Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ(Ëã±Êñá) github
173. similarityÔºöÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÂ∑•ÂÖ∑ÂåÖÔºåjavaÁºñÂÜô github

Áî®‰∫éËØçËØ≠„ÄÅÁü≠ËØ≠„ÄÅÂè•Â≠ê„ÄÅËØçÊ≥ïÂàÜÊûê„ÄÅÊÉÖÊÑüÂàÜÊûê„ÄÅËØ≠‰πâÂàÜÊûêÁ≠âÁõ∏ÂÖ≥ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆó

174. Êµ∑Èáè‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉALBERTÊ®°Âûã github
175. Transformers 2.0 github

ÊîØÊåÅTensorFlow 2.0 Âíå PyTorch ÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) 8ÁßçÊû∂ÊûÑ/33ÁßçÈ¢ÑËÆ≠ÁªÉÊ®°Âûã/102ÁßçËØ≠Ë®Ä

176. Âü∫‰∫éÂ§ßËßÑÊ®°Èü≥È¢ëÊï∞ÊçÆÈõÜAudiosetÁöÑÈü≥È¢ëÂ¢ûÂº∫ github
177. PoplarÔºöÁΩëÈ°µÁâàËá™ÁÑ∂ËØ≠Ë®ÄÊ†áÊ≥®Â∑•ÂÖ∑ github
178. ÂõæÁâáÊñáÂ≠óÂéªÈô§ÔºåÂèØÁî®‰∫éÊº´ÁîªÁøªËØë github
179. 186ÁßçËØ≠Ë®ÄÁöÑÊï∞Â≠óÂè´Ê≥ïÂ∫ì github
180. AmazonÂèëÂ∏ÉÂü∫‰∫éÁü•ËØÜÁöÑ‰∫∫-‰∫∫ÂºÄÊîæÈ¢ÜÂüüÂØπËØùÊï∞ÊçÆÈõÜ github
181. ‰∏≠ÊñáÊñáÊú¨Á∫†ÈîôÊ®°Âùó‰ª£Á†Å github
182. ÁπÅÁÆÄ‰ΩìËΩ¨Êç¢ github
183. PythonÂÆûÁé∞ÁöÑÂ§öÁßçÊñáÊú¨ÂèØËØªÊÄßËØÑ‰ª∑ÊåáÊ†á github
184. Á±ª‰ºº‰∫é‰∫∫Âêç/Âú∞Âêç/ÁªÑÁªáÊú∫ÊûÑÂêçÁöÑÂëΩÂêç‰ΩìËØÜÂà´Êï∞ÊçÆÈõÜ github
185. ‰∏úÂçóÂ§ßÂ≠¶„ÄäÁü•ËØÜÂõæË∞±„ÄãÁ†îÁ©∂ÁîüËØæÁ®ã(ËµÑÊñô) github
186. Ëã±ÊñáÊãºÂÜôÊ£ÄÊü•Â∫ì github
from spellchecker import SpellChecker

spell = SpellChecker()

# find those words that may be misspelled
misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])

for word in misspelled:
    # Get the one `most likely` answer
    print(spell.correction(word))

    # Get a list of `likely` options
    print(spell.candidates(word))

187. wwsearchÊòØ‰ºÅ‰∏öÂæÆ‰ø°ÂêéÂè∞Ëá™Á†îÁöÑÂÖ®ÊñáÊ£ÄÁ¥¢ÂºïÊìé github
188. CHAMELEONÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞ÈóªÊé®ËçêÁ≥ªÁªüÂÖÉÊû∂ÊûÑ github
189. 8ÁØáËÆ∫ÊñáÊ¢≥ÁêÜBERTÁõ∏ÂÖ≥Ê®°ÂûãËøõÂ±ï‰∏éÂèçÊÄù github
190. DocSearchÔºöÂÖçË¥πÊñáÊ°£ÊêúÁ¥¢ÂºïÊìé github
191. LIDAÔºöËΩªÈáè‰∫§‰∫íÂºèÂØπËØùÊ†áÊ≥®Â∑•ÂÖ∑ github
192. aili - the fastest in-memory index in the East ‰∏úÂçäÁêÉÊúÄÂø´Âπ∂ÂèëÁ¥¢Âºï github
","GitHub - fighting41love/funNLP: ‰∏≠Ëã±ÊñáÊïèÊÑüËØç„ÄÅËØ≠Ë®ÄÊ£ÄÊµã„ÄÅ‰∏≠Â§ñÊâãÊú∫/ÁîµËØùÂΩíÂ±ûÂú∞/ËøêËê•ÂïÜÊü•ËØ¢„ÄÅÂêçÂ≠óÊé®Êñ≠ÊÄßÂà´„ÄÅÊâãÊú∫Âè∑ÊäΩÂèñ„ÄÅË∫´‰ªΩËØÅÊäΩÂèñ„ÄÅÈÇÆÁÆ±ÊäΩÂèñ„ÄÅ‰∏≠Êó•Êñá‰∫∫ÂêçÂ∫ì„ÄÅ‰∏≠ÊñáÁº©ÂÜôÂ∫ì„ÄÅÊãÜÂ≠óËØçÂÖ∏„ÄÅËØçÊ±áÊÉÖÊÑüÂÄº„ÄÅÂÅúÁî®ËØç„ÄÅÂèçÂä®ËØçË°®„ÄÅÊö¥ÊÅêËØçË°®„ÄÅÁπÅÁÆÄ‰ΩìËΩ¨Êç¢„ÄÅËã±ÊñáÊ®°Êãü‰∏≠ÊñáÂèëÈü≥„ÄÅÊ±™Â≥∞Ê≠åËØçÁîüÊàêÂô®„ÄÅËÅå‰∏öÂêçÁß∞ËØçÂ∫ì„ÄÅÂêå‰πâËØçÂ∫ì„ÄÅÂèç‰πâËØçÂ∫ì„ÄÅÂê¶ÂÆöËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ÂìÅÁâåËØçÂ∫ì„ÄÅÊ±ΩËΩ¶Èõ∂‰ª∂ËØçÂ∫ì„ÄÅËøûÁª≠Ëã±ÊñáÂàáÂâ≤„ÄÅÂêÑÁßç‰∏≠ÊñáËØçÂêëÈáè„ÄÅÂÖ¨Âè∏ÂêçÂ≠óÂ§ßÂÖ®„ÄÅÂè§ËØóËØçÂ∫ì„ÄÅITËØçÂ∫ì„ÄÅË¥¢ÁªèËØçÂ∫ì„ÄÅÊàêËØ≠ËØçÂ∫ì„ÄÅÂú∞ÂêçËØçÂ∫ì„ÄÅÂéÜÂè≤Âêç‰∫∫ËØçÂ∫ì„ÄÅËØóËØçËØçÂ∫ì„ÄÅÂåªÂ≠¶ËØçÂ∫ì„ÄÅÈ•ÆÈ£üËØçÂ∫ì„ÄÅÊ≥ïÂæãËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ËØçÂ∫ì„ÄÅÂä®Áâ©ËØçÂ∫ì„ÄÅ‰∏≠ÊñáËÅäÂ§©ËØ≠Êñô„ÄÅ‰∏≠ÊñáË∞£Ë®ÄÊï∞ÊçÆ„ÄÅÁôæÂ∫¶‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜ„ÄÅÂè•Â≠êÁõ∏‰ººÂ∫¶ÂåπÈÖçÁÆóÊ≥ïÈõÜÂêà„ÄÅbertËµÑÊ∫ê„ÄÅÊñáÊú¨ÁîüÊàê&ÊëòË¶ÅÁõ∏ÂÖ≥Â∑•ÂÖ∑„ÄÅcocoNLP‰ø°ÊÅØÊäΩÂèñÂ∑•ÂÖ∑„ÄÅÂõΩÂÜÖÁîµËØùÂè∑Á†ÅÊ≠£ÂàôÂåπÈÖç„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶XLORE:‰∏≠Ëã±ÊñáË∑®ËØ≠Ë®ÄÁôæÁßëÁü•ËØÜÂõæË∞±„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁ≥ªÂàóÊä•Âëä„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê„ÄÅNLUÂ§™Èöæ‰∫ÜÁ≥ªÂàó„ÄÅËá™Âä®ÂØπËÅîÊï∞ÊçÆÂèäÊú∫Âô®‰∫∫„ÄÅÁî®Êà∑ÂêçÈªëÂêçÂçïÂàóË°®„ÄÅÁΩ™ÂêçÊ≥ïÂä°ÂêçËØçÂèäÂàÜÁ±ªÊ®°Âûã„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ËØ≠Êñô„ÄÅcs224nÊ∑±Â∫¶Â≠¶‰π†Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã„ÄÅ‰∏≠ÊñáÊâãÂÜôÊ±âÂ≠óËØÜÂà´„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ ËØ≠Êñô/Êï∞ÊçÆÈõÜ„ÄÅÂèòÈáèÂëΩÂêçÁ•ûÂô®„ÄÅÂàÜËØçËØ≠ÊñôÂ∫ì+‰ª£Á†Å„ÄÅ‰ªªÂä°ÂûãÂØπËØùËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÅASR ËØ≠Èü≥Êï∞ÊçÆÈõÜ + Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´Á≥ªÁªü„ÄÅÁ¨ëÂ£∞Ê£ÄÊµãÂô®„ÄÅMicrosoftÂ§öËØ≠Ë®ÄÊï∞Â≠ó/Âçï‰Ωç/Â¶ÇÊó•ÊúüÊó∂Èó¥ËØÜÂà´ÂåÖ„ÄÅ‰∏≠ÂçéÊñ∞ÂçéÂ≠óÂÖ∏Êï∞ÊçÆÂ∫ìÂèäapi(ÂåÖÊã¨Â∏∏Áî®Ê≠áÂêéËØ≠„ÄÅÊàêËØ≠„ÄÅËØçËØ≠ÂíåÊ±âÂ≠ó)„ÄÅÊñáÊ°£ÂõæË∞±Ëá™Âä®ÁîüÊàê„ÄÅSpaCy ‰∏≠ÊñáÊ®°Âûã„ÄÅCommon VoiceËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜÊñ∞Áâà„ÄÅÁ•ûÁªèÁΩëÁªúÂÖ≥Á≥ªÊäΩÂèñ„ÄÅÂü∫‰∫ébertÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥ÈîÆËØç(Keyphrase)ÊäΩÂèñÂåÖpke„ÄÅÂü∫‰∫éÂåªÁñóÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂü∫‰∫é‰æùÂ≠òÂè•Ê≥ï‰∏éËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÁöÑ‰∫ã‰ª∂‰∏âÂÖÉÁªÑÊäΩÂèñ„ÄÅ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê4‰∏áÂè•È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÅcnocrÔºöÁî®Êù•ÂÅö‰∏≠ÊñáOCRÁöÑPython3ÂåÖ„ÄÅ‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÁü•ËØÜÂõæË∞±È°πÁõÆ„ÄÅ‰∏≠ÊñánlpÁ´ûËµõÈ°πÁõÆÂèä‰ª£Á†ÅÊ±áÊÄª„ÄÅ‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ÊçÆ„ÄÅspeech-aligner: ‰ªé‚Äú‰∫∫Â£∞ËØ≠Èü≥‚ÄùÂèäÂÖ∂‚ÄúËØ≠Ë®ÄÊñáÊú¨‚Äù‰∫ßÁîüÈü≥Á¥†Á∫ßÂà´Êó∂Èó¥ÂØπÈΩêÊ†áÊ≥®ÁöÑÂ∑•ÂÖ∑„ÄÅAmpliGraph: Áü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†(Python)Â∫ìÔºöÁü•ËØÜÂõæË∞±Ê¶ÇÂøµÈìæÊé•È¢ÑÊµã„ÄÅScattertext ÊñáÊú¨ÂèØËßÜÂåñ(python)„ÄÅËØ≠Ë®Ä/Áü•ËØÜË°®Á§∫Â∑•ÂÖ∑ÔºöBERT & ERNIE„ÄÅ‰∏≠ÊñáÂØπÊØîËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜNLPÁöÑÂå∫Âà´ÁªºËø∞„ÄÅSynonyms‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖ„ÄÅHarvestTextÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñáÊú¨ÊåñÊéòÂ∑•ÂÖ∑ÔºàÊñ∞ËØçÂèëÁé∞-ÊÉÖÊÑüÂàÜÊûê-ÂÆû‰ΩìÈìæÊé•Á≠âÔºâ„ÄÅword2wordÔºö(Python)Êñπ‰æøÊòìÁî®ÁöÑÂ§öËØ≠Ë®ÄËØç-ËØçÂØπÈõÜÔºö62ÁßçËØ≠Ë®Ä/3,564‰∏™Â§öËØ≠Ë®ÄÂØπ„ÄÅËØ≠Èü≥ËØÜÂà´ËØ≠ÊñôÁîüÊàêÂ∑•ÂÖ∑Ôºö‰ªéÂÖ∑ÊúâÈü≥È¢ë/Â≠óÂπïÁöÑÂú®Á∫øËßÜÈ¢ëÂàõÂª∫Ëá™Âä®ËØ≠Èü≥ËØÜÂà´(ASR)ËØ≠ÊñôÂ∫ì„ÄÅÊûÑÂª∫ÂåªÁñóÂÆû‰ΩìËØÜÂà´ÁöÑÊ®°ÂûãÔºàÂåÖÂê´ËØçÂÖ∏ÂíåËØ≠ÊñôÊ†áÊ≥®Ôºâ„ÄÅÂçïÊñáÊ°£ÈùûÁõëÁù£ÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñ„ÄÅKashgari‰∏≠‰ΩøÁî®gpt-2ËØ≠Ë®ÄÊ®°Âûã„ÄÅÂºÄÊ∫êÁöÑÈáëËûçÊäïËµÑÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑„ÄÅÊñáÊú¨Ëá™Âä®ÊëòË¶ÅÂ∫ìTextTeaser: ‰ªÖÊîØÊåÅËã±Êñá„ÄÅ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ„ÄÅ‰∏Ä‰∫õÂÖ≥‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂü∫Êú¨Ê®°Âûã„ÄÅÂü∫‰∫é14WÊ≠åÊõ≤Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∞ùËØï--ÂäüËÉΩÂåÖÊã¨Ê≠åËØçÊé•ÈæôandÂ∑≤Áü•Ê≠åËØçÊâæÊ≠åÊõ≤‰ª•ÂèäÊ≠åÊõ≤Ê≠åÊâãÊ≠åËØç‰∏âËßíÂÖ≥Á≥ªÁöÑÈóÆÁ≠î„ÄÅÂü∫‰∫éSiamese bilstmÊ®°ÂûãÁöÑÁõ∏‰ººÂè•Â≠êÂà§ÂÆöÊ®°ÂûãÂπ∂Êèê‰æõËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÊµãËØïÊï∞ÊçÆÈõÜ„ÄÅÁî®TransformerÁºñËß£Á†ÅÊ®°ÂûãÂÆûÁé∞ÁöÑÊ†πÊçÆHacker NewsÊñáÁ´†Ê†áÈ¢òËá™Âä®ÁîüÊàêËØÑËÆ∫„ÄÅÁî®BERTËøõË°åÂ∫èÂàóÊ†áËÆ∞ÂíåÊñáÊú¨ÂàÜÁ±ªÁöÑÊ®°Êùø‰ª£Á†Å„ÄÅLitBankÔºöNLPÊï∞ÊçÆÈõÜ‚Äî‚ÄîÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆó‰∫∫ÊñáÂ≠¶Áßë‰ªªÂä°ÁöÑ100ÈÉ®Â∏¶Ê†áËÆ∞Ëã±ÊñáÂ∞èËØ¥ËØ≠Êñô„ÄÅÁôæÂ∫¶ÂºÄÊ∫êÁöÑÂü∫ÂáÜ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü„ÄÅËôöÂÅáÊñ∞ÈóªÊï∞ÊçÆÈõÜ„ÄÅFacebook: LAMAËØ≠Ë®ÄÊ®°ÂûãÂàÜÊûêÔºåÊèê‰æõTransformer-XL/BERT/ELMo/GPTÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªü‰∏ÄËÆøÈóÆÊé•Âè£„ÄÅCommonsenseQAÔºöÈù¢ÂêëÂ∏∏ËØÜÁöÑËã±ÊñáQAÊåëÊàò„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ËµÑÊñô„ÄÅÊï∞ÊçÆÂèäÂ∑•ÂÖ∑„ÄÅÂêÑÂ§ßÂÖ¨Âè∏ÂÜÖÈÉ®ÈáåÂ§ßÁâõÂàÜ‰∫´ÁöÑÊäÄÊúØÊñáÊ°£ PDF ÊàñËÄÖ PPT„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêSQLËØ≠Âè•ÔºàËã±ÊñáÔºâ„ÄÅ‰∏≠ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫ÔºàEDAÔºâÂ∑•ÂÖ∑„ÄÅËã±ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫Â∑•ÂÖ∑ „ÄÅÂü∫‰∫éÂåªËçØÁü•ËØÜÂõæË∞±ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅ‰∫¨‰∏úÂïÜÂìÅÁü•ËØÜÂõæË∞±„ÄÅÂü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆ„ÄÅÂü∫‰∫éËøúÁõëÁù£ÁöÑ‰∏≠ÊñáÂÖ≥Á≥ªÊäΩÂèñ„ÄÅËØ≠Èü≥ÊÉÖÊÑüÂàÜÊûê„ÄÅ‰∏≠ÊñáULMFiT-ÊÉÖÊÑüÂàÜÊûê-ÊñáÊú¨ÂàÜÁ±ª-ËØ≠ÊñôÂèäÊ®°Âûã„ÄÅ‰∏Ä‰∏™ÊãçÁÖßÂÅöÈ¢òÁ®ãÂ∫è„ÄÅ‰∏ñÁïåÂêÑÂõΩÂ§ßËßÑÊ®°‰∫∫ÂêçÂ∫ì„ÄÅ‰∏Ä‰∏™Âà©Áî®ÊúâË∂£‰∏≠ÊñáËØ≠ÊñôÂ∫ì qingyun ËÆ≠ÁªÉÂá∫Êù•ÁöÑ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫seqGAN„ÄÅÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥®„ÄÅÊïôËÇ≤Ë°å‰∏öÊñ∞ÈóªËØ≠ÊñôÂ∫ìÂåÖÂê´Ëá™Âä®ÊñáÊëòÂäüËÉΩ„ÄÅÂºÄÊîæ‰∫ÜÂØπËØùÊú∫Âô®‰∫∫-Áü•ËØÜÂõæË∞±-ËØ≠‰πâÁêÜËß£-Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèäÊï∞ÊçÆ„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºöÂü∫‰∫éÁôæÂ∫¶ÁôæÁßë‰∏≠ÊñáÈ°µÈù¢-ÊäΩÂèñ‰∏âÂÖÉÁªÑ‰ø°ÊÅØ-ÊûÑÂª∫‰∏≠ÊñáÁü•ËØÜÂõæË∞±„ÄÅmasr: ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´-Êèê‰æõÈ¢ÑËÆ≠ÁªÉÊ®°Âûã-È´òËØÜÂà´Áéá„ÄÅPythonÈü≥È¢ëÊï∞ÊçÆÂ¢ûÂπøÂ∫ì„ÄÅ‰∏≠ÊñáÂÖ®ËØçË¶ÜÁõñBERTÂèä‰∏§‰ªΩÈòÖËØªÁêÜËß£Êï∞ÊçÆ„ÄÅConvLabÔºöÂºÄÊ∫êÂ§öÂüüÁ´ØÂà∞Á´ØÂØπËØùÁ≥ªÁªüÂπ≥Âè∞„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊï∞ÊçÆÈõÜ„ÄÅÂü∫‰∫éÊúÄÊñ∞ÁâàÊú¨rasaÊê≠Âª∫ÁöÑÂØπËØùÁ≥ªÁªü„ÄÅÂü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñ„ÄÅ‰∏Ä‰∏™Â∞èÂûãÁöÑËØÅÂà∏Áü•ËØÜÂõæË∞±/Áü•ËØÜÂ∫ì„ÄÅÂ§çÁõòÊâÄÊúâNLPÊØîËµõÁöÑTOPÊñπÊ°à„ÄÅOpenCLaPÔºöÂ§öÈ¢ÜÂüüÂºÄÊ∫ê‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã‰ªìÂ∫ì„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂêëÈáèÂêàÈõÜ„ÄÅÂü∫‰∫éÈáëËûç-Âè∏Ê≥ïÈ¢ÜÂüü(ÂÖºÊúâÈó≤ËÅäÊÄßË¥®)ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅg2pCÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ±âËØ≠ËØªÈü≥Ëá™Âä®Ê†áËÆ∞Ê®°Âùó„ÄÅZincbase Áü•ËØÜÂõæË∞±ÊûÑÂª∫Â∑•ÂÖ∑ÂåÖ„ÄÅËØóÊ≠åË¥®ÈáèËØÑ‰ª∑/ÁªÜÁ≤íÂ∫¶ÊÉÖÊÑüËØóÊ≠åËØ≠ÊñôÂ∫ì„ÄÇ"
60,Python,"





Manim is an animation engine for explanatory math videos. It's used to create precise animations programmatically, as seen in the videos at 3Blue1Brown.
Installation
Manim runs on Python 3.7. You can install it from PyPI via pip:
pip3 install manimlib
System requirements are cairo, ffmpeg, sox, latex (optional, if you want to use LaTeX).
You can now use it via the manim command. For example:
manim my_project.py MyScene
For more options, take a look at the Using manim sections further below.
Directly
If you want to hack on manimlib itself, clone this repository and in that directory execute:
# Install python requirements
python3 -m pip install -r requirements.txt

# Try it out
python3 ./manim.py example_scenes.py SquareToCircle -pl
Directly (Windows)


Install FFmpeg.


Install Cairo. For most users, pycairo‚Äë1.18.0‚Äëcp37‚Äëcp37m‚Äëwin32.whl will do fine.
pip3 install C:\path\to\wheel\pycairo‚Äë1.18.0‚Äëcp37‚Äëcp37m‚Äëwin32.whl


Install a LaTeX distribution. MiKTeX is recommended.


Install SoX.


Install the remaining Python packages. Make sure that pycairo==1.17.1 is changed to pycairo==1.18.0 in requirements.txt.
git clone https://github.com/3b1b/manim.git
cd manim
pip3 install -r requirements.txt
python3 manim.py example_scenes.py SquareToCircle -pl


Anaconda Install

Install sox and latex as above.
Create a conda environment using conda env create -f environment.yml
WINDOWS ONLY Install pyreadline via pip install pyreadline.

Using virtualenv and virtualenvwrapper
After installing virtualenv and virtualenvwrapper
git clone https://github.com/3b1b/manim.git
mkvirtualenv -a manim -r requirements.txt manim
python3 -m manim example_scenes.py SquareToCircle -pl
Using Docker
Since it's a bit tricky to get all the dependencies set up just right, there is a Dockerfile and Compose file provided in this repo as well as a premade image on Docker Hub. The Dockerfile contains instructions on how to build a manim image, while the Compose file contains instructions on how to run the image.
The prebuilt container image has manim repository included.
INPUT_PATH is where the container looks for scene files. You must set the INPUT_PATH
environment variable to the absolute path containing your scene file and the
OUTPUT_PATH environment variable to the directory where you want media to be written.

Install Docker
Install Docker Compose
Render an animation:

INPUT_PATH=/path/to/dir/containing/source/code \
OUTPUT_PATH=/path/to/output/ \
docker-compose run manim example_scenes.py SquareToCircle -l
The command needs to be run as root if your username is not in the docker group.
You can replace example.scenes.py with any relative path from your INPUT_PATH.

After running the output will say files ready at /tmp/output/, which refers to path inside the container. Your OUTPUT_PATH is bind mounted to this /tmp/output so any changes made by the container to /tmp/output will be mirrored on your OUTPUT_PATH. /media/ will be created in OUTPUT_PATH.
-p won't work as manim would look for video player in the container system, which it does not have.
The first time you execute the above command, Docker will pull the image from Docker Hub and cache it. Any subsequent runs until the image is evicted will use the cached image.
Note that the image doesn't have any development tools installed and can't preview animations. Its purpose is building and testing only.
Using manim
Try running the following:
python3 -m manim example_scenes.py SquareToCircle -pl
The -p flag in the command above is for previewing, meaning the video file will automatically open when it is done rendering. The -l flag is for a faster rendering at a lower quality.
Some other useful flags include:

-s to skip to the end and just show the final frame.
-n <number> to skip ahead to the n'th animation of a scene.
-f to show the file in finder (for OSX).

Set MEDIA_DIR environment variable to specify where the image and animation files will be written.
Look through the old_projects folder to see the code for previous 3b1b videos. Note, however, that developments are often made to the library without considering backwards compatibility with those old projects. To run an old project with a guarantee that it will work, you will have to go back to the commit which completed that project.
While developing a scene, the -sp flags are helpful to just see what things look like at the end without having to generate the full animation. It can also be helpful to use the -n flag to skip over some number of animations.
Documentation
Documentation is in progress at eulertour.com/learn/manim.
Walkthrough
Todd Zimmerman put together a tutorial on getting started with manim, which has been updated to run on Python 3.7.
Live Streaming
To live stream your animations, simply run manim with the --livestream option.
> python -m manim --livestream
Writing to media/videos/scene/scene/1080p30/LiveStreamTemp.mp4

Manim is now running in streaming mode. Stream animations by passing
them to manim.play(), e.g.
>>> c = Circle()
>>> manim.play(ShowCreation(c))

>>>
It is also possible to stream directly to Twitch. To do that simply pass
--livestream and --to-twitch to manim and specify the stream key with
--with-key. Then when you follow the above example the stream will directly
start on your Twitch channel (with no audio support).
Contributing
Is always welcome. In particular, there is a dire need for tests and documentation.
License
All files in the directories active_projects and old_projects, which by and large generate the visuals for 3b1b videos, are copyright 3Blue1Brown.
The general purpose animation code found in the remainder of the repository, on the other hand, is under the MIT license.
",GitHub - 3b1b/manim: Animation engine for explanatory math videos
61,Python,"This guide is a collection of techniques for improving the security and privacy of a modern Apple Macintosh computer (""MacBook"") running a recent version of macOS (formerly known as ""OS X"").
This guide is targeted to power users who wish to adopt enterprise-standard security, but is also suitable for novice users with an interest in improving their privacy and security on a Mac.
A system is only as secure as its administrator is capable of making it. There is no one single technology, software, nor technique to guarantee perfect computer security; a modern operating system and computer is very complex, and requires numerous incremental changes to meaningfully improve one's security and privacy posture.
This guide is provided on an 'as is' basis without any warranties of any kind. Only you are responsible if you break anything or get in any sort of trouble by following this guide.
To suggest an improvement, please send a pull request or open an issue.
This guide is also available in ÁÆÄ‰Ωì‰∏≠Êñá.

Basics
Preparing and installing macOS

Verifying installation integrity
Creating a bootable USB installer
Creating an install image

Manual way


Target disk mode
Creating a recovery partition
Virtualization


First boot
System activation
Admin and standard user accounts

Caveats
Setup


Full disk encryption
Firmware
Firewall

Application layer firewall
Third party firewalls
Kernel level packet filtering


Services
Spotlight Suggestions
Homebrew
DNS

Hosts file
dnscrypt
Dnsmasq

Test DNSSEC validation




Captive portal
Certificate authorities
OpenSSL
Curl
Web

Privoxy
Browser

Firefox
Chrome
Safari
Other Web browsers
Web browsers and privacy


Plugins


Tor
VPN
PGP/GPG
OTR
Viruses and malware
System Integrity Protection
Gatekeeper and XProtect
Metadata and artifacts
Passwords
Backup
Wi-Fi
SSH
Physical access
System monitoring

OpenBSM audit
DTrace
Execution
Network


Binary Whitelisting
Miscellaneous
Related software
Additional resources

Basics
Standard security best practices apply:


Create a threat model

What are you trying to protect and from whom? Is your adversary a three letter agency (if so, you may want to consider using OpenBSD instead); a nosy eavesdropper on the network; or a determined apt orchestrating a campaign against you?
Recognize threats and how to reduce attack surface against them.



Keep the system up to date

Patch the base operating system and all third party software.
macOS system updates can be completed using the App Store application, or the softwareupdate command-line utility - neither requires registering an Apple account. Updates can also be downloaded directly from Apple's support site.
Subscribe to announcement mailing lists like Apple security-announce.



Encrypt sensitive data at rest

In addition to full disk encryption, consider creating one or several encrypted partitions or volumes to store passwords, cryptographic keys, personal documents, etc. at rest.
This will mitigate damage in case of compromise and data exfiltration.



Assure data availability

Create regular backups of your data and be ready to format and re-install the operating system in case of compromise.
Always encrypt locally before copying backups to external media or the ""cloud"".
Verify backups work by testing them regularly, for example by accessing certain files or performing a hash based comparison.



Click carefully

Ultimately, the security of a system can be reduced to its administrator.
Care should be taken when installing new software. Always prefer free and open source software (which macOS is not).



Preparing and installing macOS
There are several ways to install macOS.
The simplest way is to boot into Recovery Mode by holding Command and R keys at boot. A system image can be downloaded and applied directly from Apple. However, this way exposes the serial number and other identifying information over the network in plaintext, which may not be desired for privacy reasons.

Packet capture of an unencrypted HTTP conversation during macOS recovery
An alternative way to install macOS is to first download macOS Mojave from the App Store or elsewhere, and create a custom installable system image.
Verifying installation integrity
The macOS installation application is code signed, which should be verified to make sure you received a legitimate copy, using the pkgutil --check-signature or codesign -dvv commands.
To verify the code signature and integrity of macOS application bundles:
$ pkgutil --check-signature /Applications/Install\ macOS\ Catalina.app
Package ""Install macOS Catalina"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Software Signing
       SHA1 fingerprint: 01 3E 27 87 74 8A 74 10 3D 62 D2 CD BF 77 A1 34 55 17 C4 82
       -----------------------------------------------------------------------------
    2. Apple Code Signing Certification Authority
       SHA1 fingerprint: 1D 01 00 78 A6 1F 4F A4 69 4A FF 4D B1 AC 26 6C E1 B4 59 46
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
Use the codesign command to examine an application's code signature:
$ codesign -dvv /Applications/Install\ macOS\ Catalina.app
Executable=/Applications/Install macOS Catalina.app/Contents/MacOS/InstallAssistant_springboard
Identifier=com.apple.InstallAssistant.Catalina
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20100 size=276 flags=0x2000(library-validation) hashes=3+3 location=embedded
Platform identifier=9
Signature size=4628
Authority=Software Signing
Authority=Apple Code Signing Certification Authority
Authority=Apple Root CA
Info.plist entries=33
TeamIdentifier=not set
Sealed Resources version=2 rules=13 files=234
Internal requirements count=1 size=84
Creating a bootable USB installer
Instead of booting from the network or using target disk mode, a bootable macOS installer can be made with the createinstallmedia utility included in Contents/Resources folder of the installer application bundle. See Create a bootable installer for macOS, or run the utility without arguments to see how it works.
To create a bootable USB installer, mount a USB drive, and erase and partition it, then use the createinstallmedia utility:
$ diskutil list
[Find disk matching correct size, usually the last disk, e.g. /dev/disk2]

$ diskutil unmountDisk /dev/disk2

$ diskutil partitionDisk /dev/disk2 1 JHFS+ Installer 100%

$ cd /Applications/Install\ macOS\ Catalina.app

$ sudo ./Contents/Resources/createinstallmedia --volume /Volumes/Installer --nointeraction
Erasing disk: 0%... 10%... 20%... 30%... 100%
Copying to disk: 0%... 10%... 20%... 30%... 40%... 50%... 60%... 70%... 80%... 90%... 100%
Making disk bootable...
Copying boot files...
Install media now available at ""/Volumes/Install macOS Catalina""
Creating an install image
Note Apple's AutoDMG installer does not appear to work across OS versions. If you want to build a 10.14 image, for example, the following steps must be performed on macOS 10.14!
To create a custom install image which can be restored to a Mac (using a USB-C cable and target disk mode, for example), use MagerValp/AutoDMG.
Manual way
Note The following instructions appear to work only on macOS versions before 10.13.
Find InstallESD.dmg which is inside the installation application. Locate it in Terminal or with Finder, right click on the application bundle, select Show Package Contents and navigate to Contents > SharedSupport to find the file InstallESD.dmg
Verify file integrity by comparing its SHA-256 hash with others found in InstallESD_Hashes.csv or notpeter/apple-installer-checksums.
To determine which macOS versions and builds originally shipped with or are available for a Mac, see HT204319.
$ shasum -a 256 InstallESD.dmg
Mount and install the operating system to a temporary image:
$ hdiutil attach -mountpoint /tmp/InstallESD ./InstallESD.dmg

$ hdiutil create -size 32g -type SPARSE -fs HFS+J -volname ""macOS"" -uid 0 -gid 80 -mode 1775 /tmp/macos.sparseimage

$ hdiutil attach -mountpoint /tmp/macos -owners on /tmp/macos.sparseimage

$ sudo installer -pkg /tmp/InstallESD/Packages/OSInstall.mpkg -tgt /tmp/macos -verbose
installer: OS Install started.
#############
[...]
The installation will take a while, so be patient. Use tail -F /var/log/install.log in another terminal to monitor progress and check for errors.
Once the installation is complete, detach, convert and verify the image:
$ hdiutil detach /tmp/macos
""disk4"" unmounted.
""disk4"" ejected.

$ hdiutil detach /tmp/InstallESD
""disk3"" unmounted.
""disk3"" ejected.

$ hdiutil convert -format UDZO /tmp/macos.sparseimage -o ~/sierra.dmg
Preparing imaging engine...
[...]

$ asr imagescan --source ~/sierra.dmg
The file sierra.dmg is now ready to be applied over Target Disk Mode, from a bootable USB installer, booting from the network or recovery mode. The image could be further customized to include provisioned users, installed applications, preferences, for example.
Target disk mode
To use Target Disk Mode, boot up the Mac you wish to image while holding the T key and connect it to another Mac using a USB-C, Thunderbolt or Firewire cable.
If you don't have another Mac, boot to a USB installer, with sierra.dmg and other required files copied to it, by holding the Option key at boot.
Use the command diskutil list to identify the disk of the connected Mac, usually /dev/disk2
Optionally, securely erase the disk with a single pass (if previously FileVault-encrypted, the disk must first be unlocked and mounted as /dev/disk3s2):
$ sudo diskutil secureErase freespace 1 /dev/disk3s2

Partition the disk to Journaled HFS+:
$ sudo diskutil unmountDisk /dev/disk2

$ sudo diskutil partitionDisk /dev/disk2 1 JHFS+ macOS 100%
Restore the image to the new volume, making sure /dev/disk2 is the disk being erased:
$ sudo asr restore --source ~/sierra.dmg --target /Volumes/macOS --erase --buffersize 4m
[...]
Erase contents of /dev/disk2s2 (/Volumes/macOS)? [ny]:y
[...]
The Disk Utility application may also be used to erase the connected disk and restore sierra.dmg to the newly created partition.
To transfer any files, copy them to a shared folder like /Users/Shared on the mounted disk image, e.g. cp Xcode_8.0.dmg /Volumes/macOS/Users/Shared

Finished restore install from USB recovery boot
Creating a recovery partition
Unless you have built the image with AutoDMG, or installed macOS to a second partition on the same Mac, you will need to create a recovery partition in order to use full disk encryption. You can do so using MagerValp/Create-Recovery-Partition-Installer or manually by following these steps:
Download RecoveryHDUpdate.dmg and verify its integrity:
$ shasum -a 256 RecoveryHDUpdate.dmg
f6a4f8ac25eaa6163aa33ac46d40f223f40e58ec0b6b9bf6ad96bdbfc771e12c  RecoveryHDUpdate.dmg
Attach and expand the installer, then run it - again ensuring /Volumes/macOS path is the newly created partition on the connected disk:
$ hdiutil attach RecoveryHDUpdate.dmg

$ pkgutil --expand /Volumes/Mac\ OS\ X\ Lion\ Recovery\ HD\ Update/RecoveryHDUpdate.pkg /tmp/recovery

$ hdiutil attach /tmp/recovery/RecoveryHDUpdate.pkg/RecoveryHDMeta.dmg

$ /tmp/recovery/RecoveryHDUpdate.pkg/Scripts/Tools/dmtest ensureRecoveryPartition /Volumes/macOS/ /Volumes/Recovery\ HD\ Update/BaseSystem.dmg 0 0 /Volumes/Recovery\ HD\ Update/BaseSystem.chunklist
[...]
Creating recovery partition: finished
Run diskutil list again to make sure Recovery HD now exists on /dev/disk2. Eject the disk with hdiutil unmount /Volumes/macOS and power down the target disk mode-booted Mac.
Virtualization
To install macOS as a virtual machine (VM) using VMware Fusion, follow the instructions above to create an image. You will not need to download and create a recovery partition manually.
For the Installation Method, select Install macOS from the recovery partition. Customize any memory or CPU requirements and complete setup. The guest VM should boot into Recovery Mode by default.
Note If the virtual machine does not boot due to a kernel panic, adjust the memory and process resource settings.
In Recovery Mode, select a language, then select Utilities > Terminal from the menu bar.
In the guest VM, type ifconfig | grep inet - you should see a private address like 172.16.34.129
On the host Mac, type ifconfig | grep inet - you should see a private gateway address like 172.16.34.1. From the host Mac, you should be able to ping 172.16.34.129 or the equivalent guest VM address.
From the host Mac, serve the installable image to the guest VM by editing /etc/apache2/httpd.conf and adding the following line to the top (using the gateway address assigned to the host Mac and port 80):
Listen 172.16.34.1:80

On the host Mac, link the image to the default Apache Web server directory:
$ sudo ln ~/sierra.dmg /Library/WebServer/Documents

From the host Mac, start Apache in the foreground:
$ sudo httpd -X

From the guest VM, install the disk image to the volume over the local network using asr:
-bash-3.2# asr restore --source http://172.16.34.1/sierra.dmg --target /Volumes/Macintosh\ HD/ --erase --buffersize 4m
	Validating target...done
	Validating source...done
	Erase contents of /dev/disk0s2 (/Volumes/Macintosh HD)? [ny]: y
	Retrieving scan information...done
	Validating sizes...done
	Restoring  ....10....20....30....40....50....60....70....80....90....100
	Verifying  ....10....20....30....40....50....60....70....80....90....100
	Remounting target volume...done
When it's finished, stop the Apache Web server on the host Mac by pressing Control C at the sudo httpd -X window and remove the image copy with sudo rm /Library/WebServer/Documents/sierra.dmg
In the guest VM, select Startup Disk from the menubar top-left, select the hard drive and restart. You may wish to disable the Network Adapter in VMware to configure the guest VM initially.
Take and Restore from saved guest VM snapshots before and after attempting risky browsing, for example, or use a guest VM to install and operate questionable software.
First boot
Note Before setting up macOS, consider disconnecting networking and configuring a firewall(s) first. However, late 2016 MacBooks with Touch Bar hardware require online OS activation (also see next section).
On first boot, hold Command Option P R keys to clear NVRAM.
When macOS first starts, you'll be greeted by Setup Assistant.
When creating the first account, use a strong password without a hint.
If you enter your real name at the account setup process, be aware that your computer's name and local hostname will comprise that name (e.g., John Appleseed's MacBook) and thus will appear on local networks and in various preference files.
Both should be verified and updated as needed in System Preferences > Sharing or with the following commands after installation:
$ sudo scutil --set ComputerName MacBook
$ sudo scutil --set LocalHostName MacBook

System activation
A few words on the privacy implications of activating ""Touch Bar"" MacBook devices from your friendly anonymous security researcher:

Apple increasingly seems (despite vague claims to the contrary) increasingly interested in merging or ""unifying"" the two OSes, and there are constantly rumors of fundamental changes to macOS that make it far more like iOS than the macOS of old. Apple's introduction of ARM-based coprocessors running iOS/sepOS, first with the T1 processor on the TouchBar MacBook Pros (run the TouchBar, implement NFC/ApplePay, add biometric login using sep, and verify firmware integrity) and the iMac Pro's T2 (implements/verifies embedded device firmware, implements secure boot, etc) seems to cement this concern and basically renders using macOS devices without sending metadata to Apple difficult to impossible.
iOS devices have always required ""activation"" on first boot and when the battery has gone dead which initializes sepOS to proceed with verified boot. First boot activation not only initializes sepOS as discussed below, but sends metadata to Apple (and carriers via Apple with cellular devices) to activate the baseband and SIM. In activation processes after first boot, just as with first boot, a long list of highly sensitive metadata are sent hashed (note hashing does not give you any privacy from Apple here since they link this exact metadata to payment information at purchase) to Apple so it can return the personalized response required for secure boot to complete. What is particularly worrying about this process is that it is a network-linked secure boot process where centralized external servers have the power to dictate what the device should boot. Equally there are significant privacy concerns with devices constantly sending metadata (both during activation and other Apple-linked/-hosted activities) and linking IP addresses very strongly with real identities based on purchase payment information and if a cellular device, metadata collected about SIM, etc unless such connections are blocked at the network level (which is only possible on self-managed infrastructure, i.e. not cellular) and doing this basically renders using the device impossible since simply installing an application requires sending device metadata to Apple.
That the activation verification mechanism is designed specifically to rely on unique device identifiers that are associated with payment information at purchase and actively associated on a continuing basis by Apple for every Apple-hosted service that the device interacts with (Apple ID-based services, softwareupdate, iMessage, FaceTime, etc.) the ability (and invitation) for Apple to silently send targeted malicious updates to devices matching specific unique ID criteria is a valid concern, and something that should not be dismissed as unlikely, especially given Apple's full compliance with recently implemented Chinese (and other authoritarian and ""non-authoritarian"" countries') national security laws.
iOS has from the start been designed with very little end-user control with no way for end-users to configure devices according to their wishes while maintaining security and relies heavily on new, closed source code. While macOS has for most of its history been designed on the surface in a similar fashion, power and enterprise users can (for the moment) still configure their devices relatively securely while maintaining basically zero network interaction with Apple and with the installation of third party software/kernel extensions, completely control the network stack and intercept filesystem events on a per-process basis. macOS, despite having a good deal of closed source code, was designed at a very different period in Apple's history and was designed more in line with open source standards, and designed to be configurable and controllable by enterprise/power users.
The introduction of these coprocessors to Mac devices, while increasing security in many ways, brings with it all the issues with iOS discussed above, and means that running mac devices securely with complete user control, and without forced network interaction with the Apple mothership in highly sensitive corporate and other environments problematic and risky. Given this author is unaware of the exact hardware configuration of the coprocessors, the following may be inaccurate. However, given the low-level nature of these coprocessors, it would not surprise the author if these coprocessors, if not already, will eventually have separate network access of their own, independent of the Intel CPU (indications suggest not currently the case for T1; unclear on T2), which leads to concerns similar to those that many have raised around Intel ME/AMT (and of course mac devices also have ME in the Intel CPU...). One could argue that these coprocessors increase security, and in many ways that is the case, but not the user's security against a malicious Apple.
The lack of configurability is the key issue. Apple could have introduced secure boot and firmware protection without making it require network access, without making verification linked to device-unique IDs and without introducing an enormous amount of potentially exploitable code to protect against a much smaller, but highly exploitable codebase, while running on a coprocessor with a highly privileged position on the board which gives immense power to an adversary with manufacturer compliance for targeted attacks.
This is an ongoing concern and in the worst case scenario could potentially represent the end of macs as independent, end-user controllable and relatively secure systems appropriate for sensitive environments with strict network and security policies.

From iOS, The Future Of macOS, Freedom, Security And Privacy In An Increasingly Hostile Global Environment.
Admin and standard user accounts
The first user account is always an admin account. Admin accounts are members of the admin group and have access to sudo, which allows them to usurp other accounts, in particular root, and gives them effective control over the system. Any program that the admin executes can potentially obtain the same access, making this a security risk.
Utilities like sudo have weaknesses that can be exploited by concurrently running programs and many panes in System Preferences are unlocked by default (pdf) (p. 61‚Äì62) for admin accounts.
It is considered a best practice by Apple and others (pdf) (p. 41‚Äì42) to use a separate standard account for day-to-day work and use the admin account for installations and system configuration.
It is not strictly required to ever log into the admin account via the macOS login screen. The system will prompt for authentication when required and Terminal can do the rest. To that end, Apple provides some recommendations for hiding the admin account and its home directory. This can be an elegant solution to avoid having a visible 'ghost' account. The admin account can also be removed from FileVault for additional hardening.
Caveats

Only administrators can install applications in /Applications (local directory). Finder and Installer will prompt a standard user with an authentication dialog. Many applications can be installed in ~/Applications instead (the directory can be created manually). As a rule of thumb: applications that do not require admin access ‚Äì or do not complain about not being installed in /Applications ‚Äì should be installed in the user directory, the rest in the local directory. Mac App Store applications are still installed in /Applications and require no additional authentication.
sudo is not available in shells of the standard user, which requires using su or login to enter a shell of the admin account. This can make some maneuvers trickier and requires some basic experience with command-line interfaces.
System Preferences and several system utilities (e.g. Wi-Fi Diagnostics) will require root privileges for full functionality. Many panels in System Preferences are locked and need to be unlocked separately by clicking on the lock icon. Some applications will simply prompt for authentication upon opening, others must be opened by an admin account directly to get access to all functions (e.g. Console).
There are third-party applications that will not work correctly because they assume that the user account is an admin. These programs may have to be executed by logging into the admin account, or by using the open utility.
See additional discussion in issue #167.

Setup
Accounts can be created and managed in System Preferences. On settled systems, it is generally easier to create a second admin account and then demote the first account. This avoids data migration. Newly installed systems can also just add a standard account.
Demoting an account can be done either from the the new admin account in System Preferences ‚Äì the other account must be logged out ‚Äì or by executing these commands (it may not be necessary to execute both, see issue #179):
$ sudo dscl . -delete /Groups/admin GroupMembership <username>
$ sudo dscl . -delete /Groups/admin GroupMembers <GeneratedUID>
To find the ‚ÄúGeneratedUID‚Äù of an account:
$ dscl . -read /Users/<username> GeneratedUID
See also this post for more information about how macOS determines group membership.
Full disk encryption
FileVault provides full disk (technically, full volume) encryption on macOS.
FileVault encryption protects data at rest and hardens (but not always prevents) someone with physical access from stealing data or tampering with your Mac.
With much of the cryptographic operations happening efficiently in hardware, the performance penalty for FileVault is not noticeable.
Like all cryptosystems, the security of FileVault greatly depends on the quality of the pseudo random number generator (PRNG).

The random device implements the Yarrow pseudo random number generator algorithm and maintains its entropy pool.  Additional entropy is fed to the generator regularly by the SecurityServer daemon from random jitter measurements of the kernel.

See man 4 random for more information.
Turning on FileVault in System Preferences after installing macOS, rather than creating an encrypted partition for the installation first, is more secure, because more PRNG entropy is available then.
Additionally, the PRNG can be manually seeded with entropy by writing to /dev/random before enabling FileVault. This can be done by simply using the Mac for a little while before activating FileVault.
It may also be possible to increase entropy with an external source, like OneRNG. See Entropy and Random Number Generators and Fun with encryption and randomness for more information.
Enable FileVault with sudo fdesetup enable or through System Preferences > Security & Privacy and reboot.
If you can remember the password, there's no reason to save the recovery key. However, all encrypted data will be lost forever if without either the password or recovery key.
To learn about how FileVault works, see the paper Infiltrate the Vault: Security Analysis and Decryption of Lion Full Disk Encryption (pdf) and related presentation (pdf). Also see IEEE Std 1619-2007: The XTS-AES Tweakable Block Cipher (pdf).
Optional Enforce system hibernation and evict FileVault keys from memory instead of traditional sleep to memory:
$ sudo pmset -a destroyfvkeyonstandby 1
$ sudo pmset -a hibernatemode 25

All computers have firmware of some type - EFI, BIOS - to help in the discovery of hardware components and ultimately to properly bootstrap the computer using the desired OS instance. In the case of Apple hardware and the use of EFI, Apple stores relevant information within EFI to aid in the functionality of macOS. For example, the FileVault key is stored in EFI to transparently come out of standby mode.


Organizations especially sensitive to a high-attack environment, or potentially exposed to full device access when the device is in standby mode, should mitigate this risk by destroying the FileVault key in firmware. Doing so doesn't destroy the use of FileVault, but simply requires the user to enter the password in order for the system to come out of standby mode.

If you choose to evict FileVault keys in standby mode, you should also modify your standby and power nap settings. Otherwise, your machine may wake while in standby mode and then power off due to the absence of the FileVault key. See issue #124 for more information. These settings can be changed with:
$ sudo pmset -a powernap 0
$ sudo pmset -a standby 0
$ sudo pmset -a standbydelay 0
$ sudo pmset -a autopoweroff 0
For more information, see Best Practices for
Deploying FileVault 2 (pdf) and paper Lest We Remember: Cold Boot Attacks on Encryption Keys (pdf)
Note APFS may make evicting FileVault keys redundant - see discussion and links in issue #283.
Firmware
Setting a firmware password prevents a Mac from starting up from any device other than the startup disk. It may also be set to be required on each boot. This may be useful for mitigating some attacks which require physical access to hardware.  See How to set a firmware password on your Mac for official documentation.
This feature can be helpful if your laptop is lost or stolen, protects against Direct Memory Access (DMA) attacks which can read your FileVault passwords and inject kernel modules such as pcileech, as the only way to reset the firmware password is through an Apple Store, or by using an SPI programmer, such as Bus Pirate or other flash IC programmer.

Start up pressing Command and R keys to boot to Recovery Mode mode.
When the Recovery window appears, choose Firmware Password Utility from the Utilities menu.
In the Firmware Utility window that appears, select Turn On Firmware Password.
Enter a new password, then enter the same password in the Verify field.
Select Set Password.
Select Quit Firmware Utility to close the Firmware Password Utility.
Select Restart or Shutdown from the Apple menu in the top-left corner.

The firmware password will activate at next boot. To validate the password, hold Alt during boot - you should be prompted to enter the password.
The firmware password can also be managed with the firmwarepasswd utility while booted into the OS. For example, to prompt for the firmware password when attempting to boot from a different volume:
$ sudo firmwarepasswd -setpasswd -setmode command
To verify the firmware password:
$ sudo firmwarepasswd -verify
Verifying Firmware Password
Enter password:
Correct
Note, a firmware password may be bypassed by a determined attacker or Apple, with physical access to the computer.

Using a Dediprog SF600 to dump and flash a 2013 MacBook SPI Flash chip to remove a firmware password, sans Apple
Newer Mac models (Mac Pro, iMac Pro, Macbook with TouchBar) with Apple T2 chips, which provide a secure enclave for encrypted keys, lessen the risk of EFI firmware attacks. See this blog post for more information.
See LongSoft/UEFITool, chipsec/chipsec and discussion in issue #213 for more information.
Firewall
There are several types of firewalls available for macOS which should be enabled.
Application layer firewall
Built-in, basic firewall which blocks incoming connections only. This firewall does not have the ability to monitor, nor block outgoing connections.
It can be controlled by the Firewall tab of Security & Privacy in System Preferences, or with the following commands.
Enable the firewall with logging and stealth mode:
$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on
Firewall is enabled. (State = 1)

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setloggingmode on
Turning on log mode

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setstealthmode on
Stealth mode enabled

Computer hackers scan networks so they can attempt to identify computers to attack. You can prevent your computer from responding to some of these scans by using stealth mode. When stealth mode is enabled, your computer does not respond to ICMP ping requests, and does not answer to connection attempts from a closed TCP or UDP port. This makes it more difficult for attackers to find your computer.

To prevent built-in software as well as code-signed, downloaded software from being whitelisted automatically:
$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsigned off
Disabled allow signed built-in applications automatically

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsignedapp off
Disabled allow signed downloaded applications automatically

Applications that are signed by a valid certificate authority are automatically added to the list of allowed apps, rather than prompting the user to authorize them. Apps included in macOS are signed by Apple and are allowed to receive incoming connections when this setting is enabled. For example, since iTunes is already signed by Apple, it is automatically allowed to receive incoming connections through the firewall.


If you run an unsigned app that is not listed in the firewall list, a dialog appears with options to Allow or Deny connections for the app. If you choose ""Allow"", macOS signs the application and automatically adds it to the firewall list. If you choose ""Deny"", macOS adds it to the list but denies incoming connections intended for this app.

After interacting with socketfilterfw, restart the process by sending a line hangup signal:
$ sudo pkill -HUP socketfilterfw
Third party firewalls
Programs such as Little Snitch, Hands Off, Radio Silence, LuLu and Security Growler provide a good balance of usability and security.
These programs are capable of monitoring and blocking incoming and outgoing network connections. However, they may require the use of a closed source kernel extension.
If the number of choices of allowing/blocking network connections is overwhelming, use Silent Mode with connections allowed, then periodically check the configuration to gain understanding of applications and what they are doing.
It is worth noting that these firewalls can be bypassed by programs running as root or through OS vulnerabilities (pdf), but they are still worth having - just don't expect absolute protection. However, some malware actually deletes itself and doesn't execute if Little Snitch, or other security software, is installed.
For more on how Little Snitch works, see the Network Kernel Extensions Programming Guide and Shut up snitch! ‚Äì reverse engineering and exploiting a critical Little Snitch vulnerability.
Kernel level packet filtering
A highly customizable, powerful, but also most complicated firewall exists in the kernel. It can be controlled with pfctl and various configuration files.
pf can also be controlled with a GUI application such as IceFloor or Murus.
There are many books and articles on the subject of pf firewall. Here's is just one example of blocking traffic by IP address.
Add the following into a file called pf.rules:
wifi = ""en0""
ether = ""en7""
set block-policy drop
set fingerprints ""/etc/pf.os""
set ruleset-optimization basic
set skip on lo0
scrub in all no-df
table <blocklist> persist
block in log
block in log quick from no-route to any
block log on $wifi from { <blocklist> } to any
block log on $wifi from any to { <blocklist> }
antispoof quick for { $wifi $ether }
pass out proto tcp from { $wifi $ether } to any keep state
pass out proto udp from { $wifi $ether } to any keep state
pass out proto icmp from $wifi to any keep state

Then use the following commands to manipulate the firewall:

sudo pfctl -e -f pf.rules to enable the firewall and load the configuration
sudo pfctl -d to disable the firewall
sudo pfctl -t blocklist -T add 1.2.3.4 to add an IP address to the blocklist
sudo pfctl -t blocklist -T show to view the blocklist
sudo ifconfig pflog0 create to create an interface for logging
sudo tcpdump -ni pflog0 to view filtered packets

Unless you're already familiar with packet filtering, spending too much time configuring pf is not recommended. It is also probably unnecessary if your Mac is behind a NAT on a secure home network.
It is possible to use the pf firewall to block network access to entire ranges of network addresses, for example to a whole organization:
Query Merit RADb for the list of networks in use by an autonomous system, like Facebook:
$ whois -h whois.radb.net '!gAS32934'
Copy and paste the list of networks returned into the blocklist command:
$ sudo pfctl -t blocklist -T add 31.13.24.0/21 31.13.64.0/24 157.240.0.0/16
Confirm the addresses were added:
$ sudo pfctl -t blocklist -T show
No ALTQ support in kernel
ALTQ related functions disabled
   31.13.24.0/21
   31.13.64.0/24
   157.240.0.0/16
Confirm network traffic is blocked to those addresses (note that DNS requests will still work):
$ dig a +short facebook.com
157.240.2.35

$ curl --connect-timeout 5 -I http://facebook.com/
*   Trying 157.240.2.35...
* TCP_NODELAY set
* Connection timed out after 5002 milliseconds
* Closing connection 0
curl: (28) Connection timed out after 5002 milliseconds

$ sudo tcpdump -tqni pflog0 'host 157.240.2.35'
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.162771 > 157.240.2.35.80: tcp 0
Outgoing TCP SYN packets are blocked, so a TCP connection is not established and thus a Web site is effectively blocked at the IP layer.
To use pf to audit ""phone home"" behavior of user and system-level processes, see fix-macosx/net-monitor. See drduh/config/scripts/pf-blocklist.sh for more inspiration.
Services
Note System Integrity Protection does not allow disabling system services on recent macOS versions. Either temporarily disable SIP or disable services from Recovery Mode.
See fix-macosx/yosemite-phone-home, l1k/osxparanoia and karek314/macOS-home-call-drop for further recommendations.
Services on macOS are managed by launchd. See launchd.info, as well as Apple's Daemons and Services Programming Guide and Technical Note TN2083
You can also run KnockKnock that shows more information about startup items.

Use launchctl list to view running user agents
Use sudo launchctl list to view running system daemons
Specify the service name to examine it, e.g. launchctl list com.apple.Maps.mapspushd
Use defaults read to examine job plists in /System/Library/LaunchDaemons and /System/Library/LaunchAgents
Use man and strings to find out more about what an agent/daemon does

For example, to learn what a system launch daemon or agent does, start with:
$ defaults read /System/Library/LaunchDaemons/com.apple.apsd.plist
Look at the Program or ProgramArguments section to see which binary is run, in this case apsd. To find more information about that, look at the man page with man apsd
For example, if you're not interested in Apple Push Notifications, disable the service:
$ sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.apsd.plist
Note Unloading services may break usability of some applications. Read the manual pages and use Google to make sure you understand what you're doing first.
Be careful about disabling any system daemons you don't understand, as it may render your system unbootable. If you break your Mac, use single user mode to fix it.
Use Console and Activity Monitor applications if you notice your Mac heating up, feeling sluggish, or generally misbehaving, as it may have resulted from your tinkering.
To view the status of services:
$ find /var/db/com.apple.xpc.launchd/ -type f -print -exec defaults read {} \; 2>/dev/null
Annotated lists of launch daemons and agents, the respective program executed, and the programs' hash sums are included in this repository.
(Optional) Run the read_launch_plists.py script and diff output to check for any discrepancies on your system, e.g.:
$ diff <(python read_launch_plists.py) <(cat 16A323_launchd.csv)
See also cirrusj.github.io/Yosemite-Stop-Launch for descriptions of services and Provisioning OS X and Disabling Unnecessary Services for another explanation.
Persistent login items may also exist in these directories:

/Library/LaunchAgents
/Library/LaunchDaemons
/Library/ScriptingAdditions
/Library/StartupItems
/System/Library/LaunchAgents
/System/Library/LaunchDaemons
/System/Library/ScriptingAdditions
/System/Library/StartupItems
~/Library/LaunchAgents
~/Library/Preferences/com.apple.loginitems.plist

See Mac OSX Startup (pdf) for more information.
Spotlight Suggestions
Disable Spotlight Suggestions in both the Spotlight preferences and Safari's Search preferences to avoid your search queries being sent to Apple.
Also disable Bing Web Searches in the Spotlight preferences to avoid your search queries being sent to Microsoft.
See fix-macosx.com for detailed instructions.

If you've upgraded to OS X 10.10 ""Yosemite"" and you're using the default settings, each time you start typing in Spotlight (to open an application or search for a file on your computer), your local search terms and location are sent to Apple and third parties (including Microsoft).

Note This Web site and instructions may no longer work on macOS Sierra - see issue 164.
For comparison to Windows 10, see https://fix10.isleaked.com/
Homebrew
Consider using Homebrew to make software installations easier and to update userland tools (see Apple's great GPL purge).
Note If you have not already installed Xcode or Command Line Tools, use xcode-select --install to download and install them, or check Apple's developer site.
Install Homebrew:
$ mkdir homebrew && curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew
Edit PATH in your shell or shell rc file to use ~/homebrew/bin and ~/homebrew/sbin. For example, echo 'PATH=$PATH:~/homebrew/sbin:~/homebrew/bin' >> .zshrc, then change your login shell to Z shell with chsh -s /bin/zsh, open a new Terminal window and run brew update.
Homebrew uses SSL/TLS to talk with GitHub and verifies integrity of downloaded packages, so it's fairly secure.
Remember to periodically run brew update and brew upgrade on trusted and secure networks to download and install software updates. To get information on a package before installation, run brew info <package> and check its recipe online.
According to Homebrew's Anonymous Aggregate User Behaviour Analytics, Homebrew gathers anonymous aggregate user behaviour analytics and reporting these to Google Analytics.
To opt out of Homebrew's analytics, you can set export HOMEBREW_NO_ANALYTICS=1 in your environment or shell rc file, or use brew analytics off.
You may also wish to enable additional security options, such as HOMEBREW_NO_INSECURE_REDIRECT=1 and HOMEBREW_CASK_OPTS=--require-sha.
DNS
Hosts file
Use the hosts file to block known malware, advertising or otherwise unwanted domains.
Edit the hosts file as root, for example with sudo vi /etc/hosts. The hosts file can also be managed with the GUI app 2ndalpha/gasmask.
To block a domain by A record, append any one of the following lines to /etc/hosts:
0 example.com
0.0.0.0 example.com
127.0.0.1 example.com

Note IPv6 uses the AAAA DNS record type, rather than A record type, so you may also want to block those connections by also including ::1 example.com entries, like shown here.
There are many lists of domains available online which you can paste in, just make sure each line starts with 0, 0.0.0.0, 127.0.0.1, and the line 127.0.0.1 localhost is included.
For hosts lists, see someonewhocares.org, l1k/osxparanoia/blob/master/hosts and StevenBlack/hosts.
Append a list of hosts with the tee command and confirm only non-routable addresses or comments were added:
$ curl https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts | sudo tee -a /etc/hosts

$ wc -l /etc/hosts
65580

$ egrep -ve ""^#|^255.255.255|^0.0.0.0|^127.0.0.1|^0 "" /etc/hosts | sort | uniq | sort
::1 localhost
fe80::1%lo0 localhost
[should not return any other IP addresses]
See man hosts and FreeBSD Configuration Files for more information.
See the dnsmasq section of this guide for more hosts blocking options.
dnscrypt
To encrypt outgoing DNS traffic, consider using jedisct1/dnscrypt-proxy. In combination with dnsmasq and DNSSEC, the integrity and authenticity of DNS traffic is greatly improved.
JayBrown/DNSCrypt-Menu and jedisct1/bitbar-dnscrypt-proxy-switcher provide a graphical user interface to dnscrypt.
Install dnscrypt from Homebrew and follow the instructions to configure and start dnscrypt-proxy:
$ brew install dnscrypt-proxy
If using in combination with Dnsmasq, find the file homebrew.mxcl.dnscrypt-proxy.plist by running
$ brew info dnscrypt-proxy
which will show a location like /usr/local/etc/dnscrypt-proxy.toml
Open it in a text editor, find the line starting with listen_addresses = and edit that line to use DNScrypt on a port other than 53, like 5355:
listen_addresses = ['127.0.0.1:5355', '[::1]:5355']

Start DNSCrypt:
$ sudo brew services restart dnscrypt-proxy
Make sure DNSCrypt is running:
$ sudo lsof +c 15 -Pni UDP:5355
COMMAND          PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
dnscrypt-proxy 15244 nobody    7u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   10u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355
dnscrypt-proxy 15244 nobody   12u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   14u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355

By default, dnscrypt-proxy runs on localhost (127.0.0.1), port 53,
and under the ""nobody"" user using the resolvers specified in https://raw.githubusercontent.com/DNSCrypt/dnscrypt-resolvers/master/v2/public-resolvers.md. If you would like to change these settings, you will have to edit the configuration file (e.g. listen_addresses, user_name, urls, etc.)

This can be accomplished by editing /usr/local/etc/dnscrypt-proxy.toml as described above.
You can run your own dnscrypt server (see also drduh/Debian-Privacy-Server-Guide#dnscrypt) from a trusted location or use one of many public servers instead.
Confirm outgoing DNS traffic is encrypted:
$ sudo tcpdump -qtni en0
IP 10.8.8.8.59636 > 107.181.168.52: UDP, length 512
IP 107.181.168.52 > 10.8.8.8.59636: UDP, length 368

$ dig +short -x 128.180.155.106.49321
d0wn-us-ns4
dnscrypt-proxy also has the capability to blacklist domains, including the use of wild-cards. See the Sample configuration file for dnscrypt-proxy for the options.
Note Applications and programs may resolve DNS using their own provided servers. If dnscrypt-proxy is used, it is possible to disable all other, non-dnscrypt DNS traffic with the following pf rules:
block drop quick on !lo0 proto udp from any to any port = 53
block drop quick on !lo0 proto tcp from any to any port = 53
See also What is a DNS leak, the mDNSResponder manual page and ipv6-test.com.
Dnsmasq
Among other features, dnsmasq is able to cache replies, prevent upstream queries for unqualified names, and block entire top-level domain names.
Use in combination with DNSCrypt to additionally encrypt outgoing DNS traffic.
If you don't wish to use DNSCrypt, you should at least use DNS not provided by your ISP. Two popular alternatives are Google DNS and OpenDNS.
(Optional) DNSSEC is a set of extensions to DNS which provide to DNS clients (resolvers) origin authentication of DNS data, authenticated denial of existence, and data integrity. All answers from DNSSEC protected zones are digitally signed. The signed records are authenticated via a chain of trust, starting with a set of verified public keys for the DNS root-zone. The current root-zone trust anchors may be downloaded from IANA website. There are a number of resources on DNSSEC, but probably the best one is dnssec.net website.
Install Dnsmasq (DNSSEC is optional):
$ brew install dnsmasq --with-dnssec
Download drduh/config/dnsmasq.conf:
$ curl -o homebrew/etc/dnsmasq.conf https://raw.githubusercontent.com/drduh/config/master/dnsmasq.conf

Edit the file and examine all the options. To block entire levels of domains, append drduh/config/domains or your own rules.
Install and start the program (sudo is required to bind to privileged port 53):
$ sudo brew services start dnsmasq
To set Dnsmasq as your local DNS server, open System Preferences > Network and select the active interface, then the DNS tab, select + and add 127.0.0.1, or use:
$ sudo networksetup -setdnsservers ""Wi-Fi"" 127.0.0.1
Make sure Dnsmasq is correctly configured:
$ scutil --dns | head
DNS configuration

resolver #1
  search domain[0] : whatever
  nameserver[0] : 127.0.0.1
  flags    : Request A records, Request AAAA records
  reach    : 0x00030002 (Reachable,Local Address,Directly Reachable Address)

$ networksetup -getdnsservers ""Wi-Fi""
127.0.0.1
Note Some VPN software overrides DNS settings on connect. See issue #24 for more information.
Test DNSSEC validation
Test DNSSEC validation succeeds for signed zones - the reply should have NOERROR status and contain ad flag:
$ dig +dnssec icann.org
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 47039
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
Test DNSSEC validation fails for zones that are signed improperly - the reply should have SERVFAIL status:
$ dig www.dnssec-failed.org
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 15190
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1
Captive portal
When macOS connects to new networks, it checks for Internet connectivity and may launch a Captive Portal assistant utility application.
An attacker could trigger the utility and direct a Mac to a site with malware without user interaction, so it's best to disable this feature and log in to captive portals using your regular Web browser by navigating to a non-secure HTTP page and accepting a redirect to the captive portal login interface (after disabling any custom proxy or DNS settings).
$ sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control.plist Active -bool false
Also see Apple's secret ""wispr"" request, How to disable the captive portal window in Mac OS Lion and An undocumented change to Captive Network Assistant settings in OS X 10.10 Yosemite.
Certificate authorities
macOS comes with over 200 root authority certificates installed from for-profit corporations like Apple, Verisign, Thawte, Digicert and government agencies from China, Japan, Netherlands, U.S., and more! These Certificate Authorities (CAs) are capable of issuing SSL/TLS certificates for any domain, code signing certificates, etc.
For more information, see Certification Authority Trust Tracker, Analysis of the HTTPS certificate ecosystem (pdf), and You Won‚Äôt Be Needing These Any More: On Removing Unused Certificates From Trust Stores (pdf).
Inspect system root certificates in Keychain Access, under the System Roots tab or by using the security command line tool and /System/Library/Keychains/SystemRootCertificates.keychain file.
Disable certificate authorities through Keychain Access by marking them as Never Trust and closing the window:

The risk of a man in the middle attack in which a coerced or compromised certificate authority trusted by your system issues a fake/rogue SSL certificate is quite low, but still possible.
OpenSSL
The version of OpenSSL in Sierra is 0.9.8zh which is not current. It doesn't support TLS 1.1 or newer, elliptic curve ciphers, and more.
Since Apple's official supported TLS library on macOS is Secure Transport, OpenSSL deprecated is considered deprecated (according to the Cryptographic Services Guide. Apple's version of OpenSSL may also have patches which may surprise you.
If you're going to use OpenSSL on your Mac, download and install a recent version of OpenSSL with brew install openssl. Note, linking brew to be used in favor of /usr/bin/openssl may interfere with built-in software. See issue #39.
Compare the TLS protocol and cipher between the homebrew version and the system version of OpenSSL:
$ ~/homebrew/bin/openssl version; echo | ~/homebrew/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 1.0.2j  26 Sep 2016
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-GCM-SHA256

$ /usr/bin/openssl version; echo | /usr/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 0.9.8zh 14 Jan 2016
SSL-Session:
    Protocol  : TLSv1
    Cipher    : AES128-SHA
See also Comparison of TLS implementations, How's My SSL and Qualys SSL Labs Tools.
Curl
The version of Curl which comes with macOS uses Secure Transport for SSL/TLS validation.
If you prefer to use OpenSSL, install with brew install curl --with-openssl and ensure it's the default with brew link --force curl
Download drduh/config/curlrc or see the man page:
$ curl -o ~/.curlrc https://raw.githubusercontent.com/drduh/config/master/curlrc
Web
Privoxy
Consider using Privoxy as a local proxy to filter Web browsing traffic.
Note macOS proxy settings are not universal; apps and services may not honor system proxy settings. Ensure the application you wish to proxy is correctly configured and manually verify connections don't leak. Additionally, it may be possible to configure the pf firewall to transparently proxy all traffic.
A signed installation package for privoxy can be downloaded from silvester.org.uk or Sourceforge. The signed package is more secure than the Homebrew version, and attracts full support from the Privoxy project.
Alternatively, install and start privoxy using Homebrew:
$ brew install privoxy

$ brew services start privoxy
By default, privoxy listens on localhost, TCP port 8118.
Set the system HTTP proxy for your active network interface 127.0.0.1 and 8118 (This can be done through System Preferences > Network > Advanced > Proxies):
$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 8118
(Optional) Set the system HTTPS proxy, which still allows for domain name filtering, with:
$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 8118
Confirm the proxy is set:
$ scutil --proxy
<dictionary> {
  ExceptionsList : <array> {
    0 : *.local
    1 : 169.254/16
  }
  FTPPassive : 1
  HTTPEnable : 1
  HTTPPort : 8118
  HTTPProxy : 127.0.0.1
}
Visit http://p.p/ in a browser, or with Curl:
$ ALL_PROXY=127.0.0.1:8118 curl -I http://p.p/
HTTP/1.1 200 OK
Content-Length: 2401
Content-Type: text/html
Cache-Control: no-cache
Privoxy already comes with many good rules, however you can also write your own.
Download drduh/config/privoxy/config and drduh/config/privoxy/user.action to get started:
$ curl -o homebrew/etc/privoxy/config https://raw.githubusercontent.com/drduh/config/master/privoxy/config

$ curl -o homebrew/etc/privoxy/user.action https://raw.githubusercontent.com/drduh/config/master/privoxy/user.action
Restart Privoxy: and verify it's blocking and redirecting traffic:
$ sudo brew services restart privoxy

$ ALL_PROXY=127.0.0.1:8118 curl ads.foo.com/ -IL
HTTP/1.1 403 Request blocked by Privoxy
Content-Type: image/gif
Content-Length: 64
Cache-Control: no-cache

$ ALL_PROXY=127.0.0.1:8118 curl imgur.com/ -IL
HTTP/1.1 302 Local Redirect from Privoxy
Location: https://imgur.com/
Content-Length: 0
Date: Sun, 09 Oct 2016 18:48:19 GMT

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
You can replace ad images with pictures of kittens, for example, by starting a local Web server and redirecting blocked requests to localhost.
Browser
The Web browser poses the largest security and privacy risk, as its fundamental job is to download and execute untrusted code from the Internet. This is an important statement. The unique use case of Web Browsers of operation in hostile environments, has forced them to adopt certain impressive security features. The cornerstone of Web Browser security is the Same Origin Policy (SOP). In a few words, SOP prevents a malicious script on one page from obtaining access to sensitive data on another web page through that page's Document Object Model (DOM). If SOP is compromised, the security of the whole Web Browser is compromised.
The best tip to ensure secure browsing regardless your choice of Web Browser is proper security hygiene. The majority of Web Browser exploits require social engineering attacks to achieve native code execution. Always be mindful of the links you click and be extra careful when websites ask you to download and install software. 99% percent of the time that software is malware.
Another important consideration about Web Browser security is Web Extensions. Web Extensions greatly increase the attack surface of the Web Browser. This is an issue that plagues Firefox and Chrome alike. Luckily, Web Extensions can only access specific browser APIs that are being governed by their manifest. That means we can quickly audit their behavior and remove them if they request access to information they shouldn't (why would an Ad blocker require camera access?). In the interest of security, it is best to limit your use of Web Extensions.
Mozilla Firefox, Google Chrome, Safari, and Tor Browser are covered in this guide. Each Web Browser offers certain benefits and drawbacks regarding their security and privacy. It is best to make an informed choice and not necessarily commit to only one.
Firefox
Mozilla Firefox is an excellent browser as well as being completely open source. Currently, Firefox is in a renaissance period. It replaces major parts of its infrastructure and code base under projects Quantum and Photon. Part of the Quantum project is to replace C++ code with Rust. Rust is a systems programming language with a focus on security and thread safety. It is expected that Rust adoption will greatly improve the overall security posture of Firefox.
Firefox offers a similar security model to Chrome: it has a bug bounty program, although it is not a lucrative as Chrome's. Firefox follows a six-week release cycle similar to Chrome. See discussion in issues #2 and #90 for more information about certain differences in Firefox and Chrome.
Firefox supports user-supplied configuration files. See drduh/config/user.js, pyllyukko/user.js and ghacksuserjs/ghacks-user.js for recommended preferences and hardening measures. Also see NoScript, an extension which allows whitelist-based, pre-emptive script blocking.
Firefox is focused on user privacy. It supports tracking protection in Private Browsing mode. The tracking protection can be enabled for the default account, although it may break the browsing experience on some websites. Another feature for added privacy unique to Firefox is Containers, similar to Chrome profiles.
Previous versions of Firefox used a Web Extension SDK that was quite invasive and offered immense freedom to developers. Sadly, that freedom also introduced a number of vulnerabilities in Firefox that greatly affected its users. You can find more information about vulnerabilities introduced by Firefox's legacy extensions in this paper (pdf). Currently, Firefox only supports Web Extensions through the Web Extension Api, which is very similar to Chrome's.
Submission of Web Extensions in Firefox is free. Web Extensions in Firefox most of the time are open source, although certain Web Extensions are proprietary.
Note Similar to Chrome and Safari, Firefox allows account sync across multiple devices. While stored login passwords are encrypted, Firefox does not require a password to reveal their plain text format. Firefox only displays as yes/no prompt. This is an important security issue. Keep that in mind if you sign in to your Firefox account from devices that do not belong to you and leave them unattended. The issue has been raised among the Firefox community and hopefully will be resolved in the coming versions.
See drduh/config/firefox.user.js for additional Firefox configuration options to improve security and privacy.
Chrome
Google Chrome is based on the open source Chromium project with certain proprietary components:

Automatic updates with GoogleSoftwareUpdateDaemon.
Usage tracking and crash reporting, which can be disabled through Chrome's settings.
Chrome Web Store.
Adobe Flash Plugin - supports a Pepper API version of Adobe Flash which gets updated automatically with Chrome.
Media Codec support - adds support for proprietary codecs.
Chrome PDF viewer.
Non-optional tracking. Google Chrome installer includes a randomly generated token. The token is sent to Google after the installation completes in order to measure the success rate. The RLZ identifier stores information ‚Äì in the form of encoded strings ‚Äì like the source of chrome download and installation week. It doesn‚Äôt include any personal information and it‚Äôs used to measure the effectiveness of a promotional campaign. Chrome downloaded from Google‚Äôs website doesn‚Äôt have the RLZ identifier. The source code to decode the strings is made open by Google.

Chrome offers account sync between multiple devices. Part of the sync data are stored website credentials. The login passwords are encrypted and in order to access them, a user's Google account password is required. You can use your Google account to sign to your Chrome customized settings from other devices while retaining your the security of your passwords.
Chrome's Web store for extensions requires a 5 dollar lifetime fee in order to submit extensions. The low cost allows the development of many quality Open Source Web Extensions that do not aim to monetize through usage.
Chrome has the largest share of global usage and is the preferred target platform for the majority of developers. Major technologies are based on Chrome's Open Source components, such as node.js which uses Chrome's V8 Engine and the Electron framework, which is based on Chromium and node.js. Chrome's vast user base makes it the most attractive target for threat actors and security researchers. Despite under constants attacks, Chrome has retained an impressive security track record over the years. This is not a small feat.
Chrome offers separate profiles, sandboxing, frequent updates (including Flash, although you should disable it - see below), and carries impressive credentials. In addition, Google offers a very lucrative bounty program for reporting vulnerabilities along with its own Project Zero. This means that a large number of highly talented and motivated people are constantly auditing Chrome's code base.
Create separate Chrome profiles to reduce XSS risk and compartmentalize cookies/identities. In each profile, either disable Javascript in Chrome settings and manually whitelist allowed origins - or use uBlock Origin to manage Javascript and/or disable third-party scripts/frames. Also install HTTPSEverywhere to upgrade insecure connections.
Change the default search engine from Google to reduce additional tracking.
Disable DNS prefetching (see also DNS Prefetching and Its Privacy Implications (pdf)).
Read Chromium Security and Chromium Privacy for more detailed, technical information.
Read Google's privacy policy and learn which Google services collect personal information. Google is open about the data it stores and how it used them. Users can opt out from many of those services and see what type of information Google has stored from their account settings.
Safari
Safari is the default Web browser of macOS. It is also the most optimized browser for reducing battery use. Safari, like Chrome, has both Open Source and proprietary components. Safari is based on the open source Web Engine WebKit, which is ubiquitous among the macOS ecosystem. WebKit is used by Apple apps such as Mail, iTunes, iBooks, and the App Store. Chrome's Blink engine is a fork of WebKit and both engines share a number of similarities.
Safari supports certain unique features that benefit user security and privacy. Content blockers enables the creation of content blocking rules without using Javascript. This rule based approach greatly improves memory user, security, and privacy. Safari 11 introduced an Intelligent Tracking Prevention system. This feature automatically removes tracking data stored in Safari after a period of non-interaction by the user from the tracker's website.
Similar to Chrome and Firefox, Safari offers an invite only bounty program for bug reporting to a select number of security researchers. The bounty program was announced during Apple's presentation at BlackHat 2016.
Web Extensions in Safari have an additional option to use native code in the Safari's sandbox environment, in addition to Web Extension APIs. Web Extensions in Safari are also distributed through Apple's App store. App store submission comes with the added benefit of Web Extension code being audited by Apple. On the other hand App store submission comes at a steep cost. Yearly developer subscription fee costs 100 USD (in contrast to Chrome's 5 dollar lifetime fee and Firefox's free submission). The high cost is prohibitive for the majority of Open Source developers. As a result, Safari has very few extensions to choose from. However, you should keep the high cost in mind when installing extensions. It is expected that most Web Extensions will have some way of monetizing usage in order to cover developer costs. And be extra careful when the Web Extension's source code is not Open Source. On a side note, some Safari extensions are Open Source and freely available. Be grateful to those developers.
Safari syncs user preferences and saved passwords with iCloud Keychain. In order to be viewed in plain text, a user must input the account password of the current device. This means that users can sync data across devices with added security.
Safari follows a slower release cycle than Chrome and Firefox (3-4 minor releases, 1 major release, per year). Newer features are slower to be adopted to the stable channel. Although security updates in Safari are handled independent of the stable release schedule and issued automatically through the App store. The Safari channel that follows a six-week release cycle (similar to as Chrome and Firefox) is called Safari Technology Preview and it is the recommended option instead of the stable channel of Safari.
An excellent open source ad blocker for Safari that fully leverages Content blockers is dgraham/Ka-Block. Ka-Block is focussed on user privacy. The only time the extension makes a network connection is when a new version of the extension is released. See also el1t/uBlock-Safari to disable hyperlink auditing beacons.
Other Web browsers
Many Chromium-derived browsers are not recommended. They are usually closed source, poorly maintained, have bugs, and make dubious claims to protect privacy. See The Private Life of Chromium Browsers.
Other miscellaneous browsers, such as Brave, are not evaluated in this guide, so are neither recommended nor actively discouraged from use.
Web browsers and privacy
All Web Browsers retain certain information about our browsing habits. That information is used for a number of reasons. One of them is to improve the overall performance of the Web Browser. Most Web Browsers offer prediction services to resolve typos or URL redirections, store analytics data of browsing patterns, crash reports and black listing of known malicious servers. Those options can be turned on and off from each Web browser's settings panel.
Since Web browsers execute untrusted code from the server, it is important to understand what type of information can be accessed. The Navigator interface gives access to information about the Web Browser's user agent. Those include information such as the operating system, Web sites' permissions, and the device's battery level. For more information about security conscious browsing and what type of information is being ""leaked"" by your browser, see HowTo: Privacy & Security Conscious Browsing, browserleaks.com and EFF Panopticlick.
To hinder third party trackers, it is recommended to disable third-party cookies in Web browser settings. A third party cookie is a cookie associated with a file requested by a different domain than the one the user is currently viewing. Most of the time third-party cookies are used to create browsing profiles by tracking a user's movement on the web. Disabling third-party cookies prevents HTTP responses and scripts from other domains from setting cookies. Moreover, cookies are removed from requests to domains that are not the document origin domain, so cookies are only sent to the current site that is being viewed.
Also be aware of WebRTC, which may reveal your local or public (if connected to VPN) IP address(es). In Firefox and Chrome/Chromium this can be disabled with extensions such as uBlock Origin and rentamob/WebRTC-Leak-Prevent. Disabling WebRTC in Safari is only possible with a system hack.
Plugins
Adobe Flash, Oracle Java, Adobe Reader, Microsoft Silverlight (Netflix now works with HTML5) and other plugins are security risks and should not be installed.
If they are necessary, only use them in a disposable virtual machine and subscribe to security announcements to make sure you're always patched.
See Hacking Team Flash Zero-Day, Java Trojan BackDoor.Flashback, Acrobat Reader: Security Vulnerabilities, and Angling for Silverlight Exploits for examples.
Tor
Tor is an anonymizing proxy which can be used for browsing the Web.
Download Tor Browser from Tor Project.
Do not attempt to configure other browsers or applications to use Tor as you may make a mistake which will compromise anonymity.
Download both the dmg and asc signature files, then verify the disk image has been signed by Tor developers:
$ cd ~/Downloads

$ file Tor*
TorBrowser-8.0.4-osx64_en-US.dmg:     bzip2 compressed data, block size = 900k
TorBrowser-8.0.4-osx64_en-US.dmg.asc: PGP signature Signature (old)

$ gpg Tor*asc
[...]
gpg: Can't check signature: No public key

$ gpg --recv 0x4E2C6E8793298290
gpg: key 0x4E2C6E8793298290: public key ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" imported
gpg: no ultimately trusted keys found
gpg: Total number processed: 1
gpg:               imported: 1

$ gpg --verify Tor*asc
gpg: assuming signed data in 'TorBrowser-8.0.4-osx64_en-US.dmg'
gpg: Signature made Mon Dec 10 07:16:22 2018 PST
gpg:                using RSA key 0xEB774491D9FF06E2
gpg: Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: EF6E 286D DA85 EA2A 4BA7  DE68 4E2C 6E87 9329 8290
     Subkey fingerprint: 1107 75B5 D101 FB36 BC6C  911B EB77 4491 D9FF 06E2
Make sure Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" appears in the output. The warning about the key not being certified is benign, as it has not yet been manually assigned trust.
See How to verify signatures for packages for more information.
To finish installing Tor Browser, open the disk image and drag the it into the Applications folder, or with:
$ hdiutil mount TorBrowser-8.0.4-osx64_en-US.dmg

$ cp -r /Volumes/Tor\ Browser/Tor\ Browser.app/ ~/Applications/

Verify the Tor application's code signature was made by with The Tor Project's Apple developer ID MADPSAYN6T, using the spctl -a -v and/or pkgutil --check-signature commands:
$ spctl -a -vv ~/Applications/Tor\ Browser.app
/Users/drduh/Applications/Tor Browser.app: accepted
source=Developer ID
origin=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)

$ pkgutil --check-signature ~/Applications/Tor\ Browser.app
Package ""Tor Browser.app"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
       SHA1 fingerprint: 95 80 54 F1 54 66 F3 9C C2 D8 27 7A 29 21 D9 61 11 93 B3 E8
       -----------------------------------------------------------------------------
    2. Developer ID Certification Authority
       SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
You can also use the codesign command to examine an application's code signature:
$ codesign -dvv ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox
Identifier=org.torproject.torbrowser
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20200 size=229 flags=0x0(none) hashes=4+3 location=embedded
Library validation warning=OS X SDK version before 10.9 does not support Library Validation
Signature size=4247
Authority=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
Authority=Developer ID Certification Authority
Authority=Apple Root CA
Signed Time=Dec 10, 2018 at 12:18:45 AM
Info.plist entries=24
TeamIdentifier=MADPSAYN6T
Sealed Resources version=2 rules=12 files=128
Internal requirements count=1 size=188
To view full certificate details for a signed application, extract them with codesign and decode it with openssl:
$ codesign -d --extract-certificates ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox

$ file codesign*
codesign0: data
codesign1: data
codesign2: data

$ openssl x509 -inform der -in codesign0 -subject -issuer -startdate -enddate -noout
subject= /UID=MADPSAYN6T/CN=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)/OU=MADPSAYN6T/O=The Tor Project, Inc/C=US
issuer= /CN=Developer ID Certification Authority/OU=Apple Certification Authority/O=Apple Inc./C=US
notBefore=Apr 12 22:40:13 2016 GMT
notAfter=Apr 13 22:40:13 2021 GMT

$ openssl x509 -inform der -in codesign0  -fingerprint -noout
SHA1 Fingerprint=95:80:54:F1:54:66:F3:9C:C2:D8:27:7A:29:21:D9:61:11:93:B3:E8

$ openssl x509 -inform der -in codesign0 -fingerprint -sha256 -noout
SHA256 Fingerprint=B5:0D:47:F0:3E:CB:42:B6:68:1C:6F:38:06:2B:C2:9F:41:FA:D6:54:F1:29:D3:E4:DD:9C:C7:49:35:FF:F5:D9
Tor traffic is encrypted to the exit node (i.e., cannot be read by a passive network eavesdropper), but Tor use can be identified - for example, TLS handshake ""hostnames"" will show up in plaintext:
$ sudo tcpdump -An ""tcp"" | grep ""www""
listening on pktap, link-type PKTAP (Apple DLT_PKTAP), capture size 262144 bytes
............."". ...www.odezz26nvv7jeqz1xghzs.com.........
.............#.!...www.bxbko3qi7vacgwyk4ggulh.com.........
.6....m.....>...:.........|../*	Z....W....X=..6...C../....................................0...0..0.......'....F./0..	*.H........0%1#0!..U....www.b6zazzahl3h3faf4x2.com0...160402000000Z..170317000000Z0'1%0#..U....www.tm3ddrghe22wgqna5u8g.net0..0..
See Tor Protocol Specification and Tor/TLSHistory for more information.
You may wish to additionally obfuscate Tor traffic using a pluggable transport, such as Yawning/obfs4proxy or SRI-CSL/stegotorus.
This can be done by setting up your own Tor relay or finding an existing private or public bridge to serve as an obfuscating entry node.
For extra security, use Tor inside a VirtualBox or VMware virtualized GNU/Linux or BSD machine.
Finally, remember the Tor network provides anonymity, which is not necessarily synonymous with privacy. The Tor network does not guarantee protection against a global observer capable of traffic analysis and correlation. See also Seeking Anonymity in an Internet Panopticon (pdf) and Traffic Correlation on Tor by Realistic Adversaries (pdf).
Also see Invisible Internet Project (I2P) and its Tor comparison.
VPN
Unencrypted network traffic is being actively monitored and possibly tampered with. Encrypted traffic still exposes connection metadata and could be used to infer behavior or specific actions.
It is a good idea to use a VPN with outgoing network traffic (not split tunnel) together with a trustworthy provider. drduh/Debian-Privacy-Server-Guide is one of many available guides for setting up a personal VPN server.
Don't just blindly sign up for a VPN service without understanding the full implications and how your traffic will be routed. If you don't understand how the VPN works or are not familiar with the software used, you are probably better off without it.
When choosing a VPN service or setting up your own, be sure to research the protocols, key exchange algorithms, authentication mechanisms, and type of encryption being used. Some protocols, such as PPTP, should be avoided in favor of OpenVPN, for example. Strong cryptographic algorithms like AES-256, RSA-4096, SHA-256 should be preferred.
Some clients may send traffic over the next available interface when VPN is interrupted or disconnected. See scy/8122924 for an example on how to allow traffic only over VPN.
Another set of scripts to lock down your system so it will only access the internet via a VPN can be found as part of the Voodoo Privacy project - sarfata/voodooprivacy and there is an updated guide to setting up an IPSec VPN on a virtual machine (hwdsl2/setup-ipsec-vpn) or a docker container (hwdsl2/docker-ipsec-vpn-server).
It may be worthwhile to consider the geographical location of the VPN provider. See further discussion in issue #114.
Also see this technical overview of the macOS built-in VPN L2TP/IPSec and IKEv2 client.
Further, it is possible to run the contemporary Linux-based Wireguard VPN either from a Linux VM or via a set of cross platform tools.
Other Open Source OpenVPN clients/GUI: Eddie, Pritunl are not evaluated in this guide, so are neither recommended nor actively discouraged from use.
PGP/GPG
PGP is a standard for encrypting email end to end. That means only the chosen recipients can decrypt a message, unlike regular email which is read and forever archived by providers.
GPG, or GNU Privacy Guard, is a GPL-licensed open source program compliant with the PGP standard.
GPG is used to verify signatures of software you download and install, as well as symmetrically or asymmetrically encrypt files and text.
Install from Homebrew with brew install gnupg.
If you prefer a graphical application, download and install GPG Suite.
Download drduh/config/gpg.conf to use recommended settings:
$ curl -o ~/.gnupg/gpg.conf https://raw.githubusercontent.com/drduh/config/master/gpg.conf
See drduh/YubiKey-Guide to securely generate and store GPG keys.
Read online guides and practice encrypting and decrypting email to yourself and your friends. Get them interested in this stuff!
OTR
OTR stands for off-the-record and is a cryptographic protocol for encrypting and authenticating conversations over instant messaging.
You can use OTR on top of any existing XMPP chat service, even Google Hangouts (which only encrypts conversations between users and the server using TLS).
The first time you start a conversation with someone new, you'll be asked to verify their public key fingerprint. Make sure to do this in person or by some other secure means (e.g. GPG encrypted mail).
A popular macOS GUI client for XMPP and other chat protocols is Adium.
Other XMPP clients include profanity and agl/xmpp-client. Another relatively new XMPP chat client is CoyIM, it's focused and security and has built-in support for OTR and Tor.
If you want to know how OTR works, read the paper Off-the-Record Communication, or, Why Not To Use PGP (pdf)
Viruses and malware
There is an ever-increasing amount of Mac malware in the wild. Macs aren't immune from viruses and malicious software!
Some malware comes bundled with both legitimate software, such as the Java bundling Ask Toolbar, and some with illegitimate software, such as Mac.BackDoor.iWorm bundled with pirated programs. Malwarebytes Anti-Malware for Mac is an excellent program for ridding oneself of ""garden-variety"" malware and other ""crapware"".
See Methods of malware persistence on Mac OS X (pdf) and Malware Persistence on OS X Yosemite to learn about how garden-variety malware functions.
You could periodically run a tool like Knock Knock to examine persistent applications (e.g. scripts, binaries). But by then, it is probably too late. Maybe applications such as Block Block and Ostiarius will help. See warnings and caveats in issue #90 first, however. An open-source alternative could be maclaunch.sh.
Anti-virus programs are a double-edged sword -- not so useful for advanced users and will likely increase attack surface against sophisticated threats; however possibly useful for catching ""garden variety"" malware on novice users' Macs. There is also the additional processing overhead to consider when using ""active"" scanning features.
See Sophail: Applied attacks against Antivirus (pdf), Analysis and Exploitation of an ESET Vulnerability, a trivial Avast RCE, Popular Security Software Came Under Relentless NSA and GCHQ Attacks, How Israel Caught Russian Hackers Scouring the World for U.S. Secrets and AVG: ""Web TuneUP"" extension multiple critical vulnerabilities.
Therefore, the best anti-virus is Common Sense 2019. See discussion in issue #44.
Local privilege escalation bugs are plenty on macOS, so always be careful when downloading and running untrusted programs or trusted programs from third party websites or downloaded over HTTP (example).
Subscribe to updates at The Safe Mac and Malwarebytes Blog for current Mac security news.
To scan an application with multiple AV products and examine its behavior, upload it to VirusTotal.
Also check out Hacking Team malware for macOS: root installation for MacOS, Support driver for Mac Agent and RCS Agent for Mac, which is a good example of advanced malware with capabilities to hide from userland (e.g., ps, ls), for example. For more, see A Brief Analysis of an RCS Implant Installer and reverse.put.as
System Integrity Protection
System Integrity Protection (SIP) is a security feature since OS X 10.11 ""El Capitan"". It is enabled by default, but can be disabled, which may be necessary to change some system settings, such as deleting root certificate authorities or unloading certain launch daemons. Keep this feature on, as it is by default.
From What's New in OS X 10.11:

A new security policy that applies to every running process, including privileged code and code that runs out of the sandbox. The policy extends additional protections to components on disk and at run-time, only allowing system binaries to be modified by the system installer and software updates. Code injection and runtime attachments to system binaries are no longer permitted.

Also see What is the ‚Äúrootless‚Äù feature in El Capitan, really?
Some MacBook hardware has shipped with SIP disabled. To verify SIP is enabled, use the command csrutil status, which should return: System Integrity Protection status: enabled. Otherwise, enable SIP through Recovery Mode.
Gatekeeper and XProtect
Gatekeeper and the quarantine system try to prevent unsigned or ""bad"" programs and files from running and opening.
XProtect prevents the execution of known bad files and outdated plugin versions, but does nothing to cleanup or stop existing malware.
Both offer trivial protection against common risks and are fine at default settings.
See also Mac Malware Guide : How does Mac OS X protect me? and Gatekeeper, XProtect and the Quarantine attribute.
Note Quarantine stores information about downloaded files in ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2, which may pose a privacy risk. To examine the file, simply use strings or the following command:
$ echo 'SELECT datetime(LSQuarantineTimeStamp + 978307200, ""unixepoch"") as LSQuarantineTimeStamp, ' \
  'LSQuarantineAgentName, LSQuarantineOriginURLString, LSQuarantineDataURLString from LSQuarantineEvent;' | \
  sqlite3 /Users/$USER/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
See here for more information.
To permanently disable this feature, clear the file and make it immutable:
$ :>~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2

$ sudo chflags schg ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
Metadata and artifacts
macOS attaches metadata (HFS+ extended attributes) to downloaded files, which can be viewed with the mdls and xattr commands:
$ ls -l@ ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
-rw-r--r--@ 1 drduh staff 63M Jan 1 12:00 TorBrowser-8.0.4-osx64_en-US.dmg
	com.apple.metadata:kMDItemWhereFroms	  46B
	com.apple.quarantine	  57B

$ mdls ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
kMDItemContentCreationDate         = 2019-01-01 00:00:00 +0000
kMDItemContentCreationDate_Ranking = 2019-01-01 00:00:00 +0000
kMDItemContentModificationDate     = 2019-01-01 00:00:00 +0000
kMDItemContentType                 = ""com.apple.disk-image-udif""
kMDItemContentTypeTree             = (
    ""public.archive"",
    ""public.item"",
    ""public.data"",
    ""public.disk-image"",
    ""com.apple.disk-image"",
    ""com.apple.disk-image-udif""
)
kMDItemDateAdded                   = 2019-01-01 00:00:00 +0000
kMDItemDateAdded_Ranking           = 2019-01-01 00:00:00 +0000
kMDItemDisplayName                 = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSContentChangeDate         = 2019-01-01 00:00:00 +0000
kMDItemFSCreationDate              = 2019-01-01 00:00:00 +0000
kMDItemFSCreatorCode               = """"
kMDItemFSFinderFlags               = 0
kMDItemFSHasCustomIcon             = (null)
kMDItemFSInvisible                 = 0
kMDItemFSIsExtensionHidden         = 0
kMDItemFSIsStationery              = (null)
kMDItemFSLabel                     = 0
kMDItemFSName                      = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSNodeCount                 = (null)
kMDItemFSOwnerGroupID              = 5000
kMDItemFSOwnerUserID               = 501
kMDItemFSSize                      = 65840402
kMDItemFSTypeCode                  = """"
kMDItemInterestingDate_Ranking     = 2019-01-01 00:00:00 +0000
kMDItemKind                        = ""Disk Image""
kMDItemWhereFroms                  = (
    ""https://dist.torproject.org/torbrowser/8.0.4/TorBrowser-8.0.4-osx64_en-US.dmg"",
    ""https://www.torproject.org/projects/torbrowser.html.en""
)

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
com.apple.metadata:kMDItemWhereFroms:
00000000 ¬†62 70 6C 69 73 74 30 30 A2 01 02 5F 10 4D 68 74 ¬†|bplist00..._.Mht|
00000010 ¬†74 70 73 3A 2F 2F 64 69 73 74 2E 74 6F 72 70 72 ¬†|tps://dist.torpr|
00000020 ¬†6F 6A 65 63 74 2E 6F 72 67 2F 74 6F 72 62 72 6F ¬†|oject.org/torbro|
[...]
com.apple.quarantine: 0081;58519ffa;Google Chrome.app;1F032CAB-F5A1-4D92-84EB-CBECA971B7BC
Metadata attributes can also be removed with the -d flag:
$ xattr -d com.apple.metadata:kMDItemWhereFroms ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -d com.apple.quarantine ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
[No output expected]
Other metadata and artifacts may be found in the directories including, but not limited to, ~/Library/Preferences/, ~/Library/Containers/<APP>/Data/Library/Preferences, /Library/Preferences, some of which is detailed below.
~/Library/Preferences/com.apple.sidebarlists.plist contains historical list of volumes attached. To clear it, use the command /usr/libexec/PlistBuddy -c ""delete :systemitems:VolumesList"" ~/Library/Preferences/com.apple.sidebarlists.plist
/Library/Preferences/com.apple.Bluetooth.plist contains Bluetooth metadata, including device history. If Bluetooth is not used, the metadata can be cleared with:
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist DeviceCache
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist IDSPairedDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANInterfaces
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist SCOAudioDevices
/var/spool/cups contains the CUPS printer job cache. To clear it, use the commands:
$ sudo rm -rfv /var/spool/cups/c0*
$ sudo rm -rfv /var/spool/cups/tmp/*
$ sudo rm -rfv /var/spool/cups/cache/job.cache*
To clear the list of iOS devices connected, use:
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist Devices
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist Devices
$ sudo rm -rfv /var/db/lockdown/*
Quicklook thumbnail data can be cleared using the qlmanage -r cache command, but this writes to the file resetreason in the Quicklook directories, and states that the Quicklook cache was manually cleared. Disable the thumbnail cache with qlmanage -r disablecache
It can also be manually cleared by getting the directory names with getconf DARWIN_USER_CACHE_DIR and sudo getconf DARWIN_USER_CACHE_DIR, then removing them:
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
Similarly, for the root user:
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
Also see 'quicklook' cache may leak encrypted data.
To clear Finder preferences:
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXDesktopVolumePositions
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXRecentFolders
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentMoveAndCopyDestinations
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentSearches
$ defaults delete ~/Library/Preferences/com.apple.finder.plist SGTRecentFileSearches
Additional diagnostic files may be found in the following directories - but caution should be taken before removing any, as it may break logging or cause other issues:
/var/db/CoreDuet/
/var/db/diagnostics/
/var/db/systemstats/
/var/db/uuidtext/
/var/log/DiagnosticMessages/

macOS stored preferred Wi-Fi data (including credentials) in NVRAM. To clear it, use the following commands:
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:current-network
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-networks
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-count
macOS may collect sensitive information about what you type, even if user dictionary and suggestions are off. To remove them, and prevent them from being created again, use the following commands:
$ rm -rfv ""~/Library/LanguageModeling/*"" ""~/Library/Spelling/*"" ""~/Library/Suggestions/*""
$ chmod -R 000 ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
$ chflags -R uchg ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
QuickLook application support metadata can be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Application Support/Quick Look/*""
$ chmod -R 000 ""~/Library/Application Support/Quick Look""
$ chflags -R uchg ""~/Library/Application Support/Quick Look""
Document revision metadata is stored in /.DocumentRevisions-V100 and can be cleared and locked with the following commands - caution should be taken as this may break some core Apple applications:
$ sudo rm -rfv /.DocumentRevisions-V100/*
$ sudo chmod -R 000 /.DocumentRevisions-V100
$ sudo chflags -R uchg /.DocumentRevisions-V100
Saved application state metadata may be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Saved Application State/*""
$ rm -rfv ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chmod -R 000 ""~/Library/Saved Application State/""
$ chmod -R 000 ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chflags -R uchg ""~/Library/Saved Application State/""
$ chflags -R uchg ""~/Library/Containers/<APPNAME>/Saved Application State""
Autosave metadata can be cleared and locked with the following commands:
$ rm -rfv ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ rm -rfv ""~/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Autosave Information""
The Siri analytics database, which is created even if the Siri launch agent disabled, can be cleared and locked with the following commands:
$ rm -rfv ~/Library/Assistant/SiriAnalytics.db
$ chmod -R 000 ~/Library/Assistant/SiriAnalytics.db
$ chflags -R uchg ~/Library/Assistant/SiriAnalytics.db
~/Library/Preferences/com.apple.iTunes.plist contains iTunes metadata. Recent iTunes search data may be cleared with the following command:
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist recentSearches
If you do not use Apple ID-linked services, the following keys may be cleared, too, using the following commands:
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist StoreUserInfo
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist WirelessBuddyID
All media played in QuickTime Player can be found in:
~/Library/Containers/com.apple.QuickTimePlayerX/Data/Library/Preferences/com.apple.QuickTimePlayerX.plist

Additional metadata may exist in the following files:
~/Library/Containers/com.apple.appstore/Data/Library/Preferences/com.apple.commerce.knownclients.plist
~/Library/Preferences/com.apple.commerce.plist
~/Library/Preferences/com.apple.QuickTimePlayerX.plist

Passwords
Generate strong passwords with several programs or directly from /dev/urandom:
$ openssl rand -base64 30
qb8ZWbUU2Ri3FOAPY/1wKSFAJwMXmpQM4mZU4YbO

$ gpg --gen-random -a 0 90 | fold -w 40
3e+kfHOvovHVXxZYPgu+OOWQ1g1ttbljr+kNGv7f
loD//RsjUXYGIjfPM/bT0itsoEstyGLVUsFns8wP
zYM8VRBga+TsnxWrS7lWKfH1uvVPowzkq9kXCdvJ

$ LANG=C tr -dc 'A-F0-9' < /dev/urandom | fold -w 40 | head -n 5
45D0371481EE5E5A5C1F68EA59E69F9CA52CB321
A30B37A00302643921F205621B145E7EAF520164
B6EF38A2DA1D0586D20105502AFFF0468EA5F16A
029D6EA9F76CD64D3356E342EA154BEFEBE23387
07F468F0569579A0A06471247CABC4F4C1386E24

$ tr -dc '[:alnum:]' < /dev/urandom | fold -w 40 | head -n5
zmj8S0iuxud8y8YHjzdg7Hefu6U1KAYBiLl3aE8v
nCNpuMkWohTjQHntTzbiLQJG5zLzEHWSWaYSwjtm
R2L6M909S3ih852IkJqQFMDawCiHcpPBxlllAPrt
aZOXKVUmxhzQwVSYb6nqAbGTVMFSJOLf094bFZAb
HfgwSNlkVBXwIPQST6E6x6vDNCCasMLSSOoTUfSK

$ tr -dc '[:lower:]' < /dev/urandom | fold -w 40 | head -n5
gfvkanntxutzwxficgvavbwdvttexdezdftvvtmn
lgrsuiugwkqbtbkyggcbpbqlynwbiyxzlabstqcf
ufctdlsbyonkowzpmotxiksnsbwdzkjrjsupoqvr
hjwibdjxtmuvqricljayzkgdfztcmapsgwsubggr
bjstlmvwjczakgeetkbmwbjnidbeaerhaonpkacg

$ tr -dc '[:upper:]' < /dev/urandom | fold -w 40 | head -n5
EUHZMAOBOLNFXUNNDSTLJTPDCPVQBPUEQOLRZUQZ
HVNVKBEPAAYMXRCGVCNEZLFHNUYMRYPTWPWOOZVM
TAHEUPQJTSYQVJVYSKLURESMKWEZONXLUDHWQODB
PRDITWMAXXZLTRXEEOGOSGAWUXYDGDRJYRHUWICM
VHERIQBLBPHSIUZSGYZRDHTNAPUGJMRODIKBWZRJ

$ tr -dc '[:graph:]' < /dev/urandom | fold -w 40 | head -n5
n\T2|zUz:\C,@z9!#p3!B/[t6m:B94}q&t(^)Ol~
J%MMDbAgGdP}zrSQO!3mrP3$w!.[Ng_xx-_[C<3g
^)6V&*<2""ZOgU.mBd]iInvFKiT<dq~y\O[cdDK`V
+RE]UYPIf3:StX`y#w,.iG~g""urD)'FnDIFI_q^)
6?HRillpgvvFDBAr4[:H{^oAL<`Em7$roF=2w;1~
You can also generate passwords, even memorable ones, using Keychain Access password assistant, or a command line equivalent like anders/pwgen.
Keychains are encrypted with a PBKDF2 derived key and are a pretty safe place to store credentials. See also Breaking into the OS X keychain. Also be aware that Keychain does not encrypt the names corresponding to password entries.
Alternatively, you can manage an encrypted passwords file yourself with GnuPG (see drduh/Purse and drduh/pwd.sh for example).
In addition to passwords, ensure eligible online accounts, such as GitHub, Google accounts, banking, have two factor authentication enabled.
Look to Yubikey for a two factor and private key (e.g., ssh, gpg) hardware token. See drduh/YubiKey-Guide and trmm.net/Yubikey. One of two Yubikey's slots can also be programmed to emit a long, static password (which can be used in combination with a short, memorized password, for example).
In Addition to Login and other PAMs, you can use Yubikey to secure your login and sudo, here is a pdf guide from Yubico. Yubikey are a bit pricey, there is cheaper alternative, but not as capable, U2F Zero. Here is a great guide to set it up
Backup
Always encrypt files locally before backing them up to external media or online services.
One way is to use a symmetric cipher with GPG and a password of your choosing. Files can also be encrypted to a public key with GPG, with the private key stored on YubiKey.
To compress and encrypt a directory:
$ tar zcvf - ~/Downloads | gpg -c > ~/Desktop/backup-$(date +%F-%H%M).tar.gz.gpg
tar: Removing leading '/' from member names
a Users/drduh/Downloads
a Users/drduh/Downloads/.DS_Store
a Users/drduh/Downloads/.localized
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
To decrypt and decompress the directory:
$ gpg -o ~/Desktop/decrypted-backup.tar.gz -d ~/Desktop/backup-2015-01-01-0000.tar.gz.gpg
gpg: AES256 encrypted data
gpg: encrypted with 1 passphrase

$ tar zxvf ~/Desktop/decrypted-backup.tar.gz
tar: Removing leading '/' from member names
x Users/drduh/._Downloads
x Users/drduh/Downloads/
x Users/drduh/Downloads/._.DS_Store
x Users/drduh/Downloads/.DS_Store
x Users/drduh/Downloads/.localized
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
You can also create and use encrypted volumes using Disk Utility or hdiutil:
$ hdiutil create ~/Desktop/encrypted.dmg -encryption -size 50M -volname ""secretStuff"" -fs JHFS+
Enter a new password to secure ""encrypted.dmg"":
Re-enter new password:
....................................
Created: /Users/drduh/Desktop/encrypted.img

$ hdiutil mount ~/Desktop/encrypted.dmg
Enter password to access ""encrypted.dmg"":
[...]
/Volumes/secretStuff

$ cp -v ~/Documents/passwords.txt /Volumes/secretStuff
[...]

$ hdiutil eject /Volumes/secretStuff
""disk4"" unmounted.
""disk4"" ejected.
See also the following applications and services: Tresorit, SpiderOak, Arq, Espionage, and restic.
Wi-Fi
macOS remembers access points it has connected to. Like all wireless devices, the Mac will broadcast all access point names it remembers (e.g., MyHomeNetwork) each time it looks for a network, such as when waking from sleep.
This is a privacy risk, so remove networks from the list in System Preferences > Network > Advanced when they are no longer needed.
Also see Signals from the Crowd: Uncovering Social Relationships through Smartphone Probes (pdf) and Wi-Fi told me everything about you (pdf).
Saved Wi-Fi information (SSID, last connection, etc.) can be found in:
/Library/Preferences/SystemConfiguration/com.apple.airport.preferences.plist

You may want to spoof the MAC address of the network card before connecting to new and untrusted wireless networks to mitigate passive fingerprinting:
$ sudo ifconfig en0 ether $(openssl rand -hex 6 | sed 's%\(..\)%\1:%g; s%.$%%')
It is also good to know that macOS will store Wi-Fi SSIDs and passwords in NVRAM, because Recovery Mode needs access to restore from the Internet. Be sure to either clear NVRAM or de-authenticate your Mac from your Apple account, which will clear the NVRAM, before passing a Mac along. (Resetting the SMC will clear some of the NVRAM, but not all.)
Note MAC addresses will reset to hardware defaults on each boot.
Also see feross/SpoofMAC.
Finally, WEP protection on wireless networks is not secure and you should favor connecting to WPA2 protected networks only to mitigate the risk of passive eavesdroppers.
SSH
For outgoing SSH connections, use hardware or password-protected keys, set up remote hosts and consider hashing them for added privacy. See drduh/config/ssh_config for recommended client options.
You can also use ssh to create an encrypted tunnel to send traffic through, similar to a VPN.
For example, to use Privoxy running on a remote host port 8118:
$ ssh -C -L 5555:127.0.0.1:8118 you@remote-host.tld

$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 5555

$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 5555
Or to use an ssh connection as a SOCKS proxy:
$ ssh -NCD 3000 you@remote-host.tld
By default, macOS does not have sshd or Remote Login enabled.
To enable sshd and allow incoming ssh connections:
$ sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist
Or use the System Preferences > Sharing menu.
If enabling sshd, be sure to disable password authentication and consider further hardening your configuration. See drduh/config/sshd_config for recommended options.
Confirm whether sshd is running:
$ sudo lsof -Pni TCP:22
Physical access
Keep your Mac physically secure at all times. Don't leave it unattended in hotels and such.
A skilled attacker with unsupervised physical access to your computer can infect the boot ROM to install a keylogger and steal your password - see Thunderstrike for an example.
A helpful tool is usbkill, which is an anti-forensic kill-switch that waits for a change on your USB ports and then immediately shuts down your computer.
Consider purchasing a privacy filter for your screen to thwart shoulder surfers.
Superglues or epoxy resins can also be used to disable physical access to computer ports. Nail polish and tamper-evidence seals can be applied to components to detect tampering.
System monitoring
OpenBSM audit
macOS has a powerful OpenBSM (Basic Security Module) auditing capability. You can use it to monitor process execution, network activity, and much more.
To tail audit logs, use the praudit utility:
$ sudo praudit -l /dev/auditpipe
header,201,11,execve(2),0,Thu Sep  1 12:00:00 2015, + 195 msec,exec arg,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,attribute,100755,root,wheel,16777220,986535,0,subject,drduh,root,wheel,root,wheel,412,100005,50511731,0.0.0.0,return,success,0,trailer,201,
header,88,11,connect(2),0,Thu Sep  1 12:00:00 2015, + 238 msec,argument,1,0x5,fd,socket-inet,2,443,173.194.74.104,subject,drduh,root,wheel,root,wheel,326,100005,50331650,0.0.0.0,return,failure : Operation now in progress,4354967105,trailer,88
header,111,11,OpenSSH login,0,Thu Sep  1 12:00:00 2015, + 16 msec,subject_ex,drduh,drduh,staff,drduh,staff,404,404,49271,::1,text,successful login drduh,return,success,0,trailer,111,
See the manual pages for audit, praudit, audit_control and other files in /etc/security
Note although man audit says the -s flag will synchronize the audit configuration, it appears necessary to reboot for changes to take effect.
See articles on ilostmynotes.blogspot.com and derflounder.wordpress.com for more information.
DTrace
Note System Integrity Protection interferes with DTrace, so it is not possible to use it in recent macOS versions without disabling SIP.

iosnoop monitors disk I/O
opensnoop monitors file opens
execsnoop monitors execution of processes
errinfo monitors failed system calls
dtruss monitors all system calls

See man -k dtrace for more information.
Execution
ps -ef lists information about all running processes.
You can also view processes with Activity Monitor.
launchctl list and sudo launchctl list list loaded and running user and system launch daemons and agents.
Network
List open network files:
$ sudo lsof -Pni
List contents of various network-related data structures:
$ sudo netstat -atln
Wireshark can be used from the command line with tshark.
Monitor DNS queries and replies:
$ tshark -Y ""dns.flags.response == 1"" -Tfields \
  -e frame.time_delta \
  -e dns.qry.name \
  -e dns.a \
  -Eseparator=,
Monitor HTTP requests and responses:
$ tshark -Y ""http.request or http.response"" -Tfields \
  -e ip.dst \
  -e http.request.full_uri \
  -e http.request.method \
  -e http.response.code \
  -e http.response.phrase \
  -Eseparator=/s
Monitor x509 (SSL/TLS) certificates:
$ tshark -Y ""ssl.handshake.certificate"" -Tfields \
  -e ip.src \
  -e x509sat.uTF8String \
  -e x509sat.printableString \
  -e x509sat.universalString \
  -e x509sat.IA5String \
  -e x509sat.teletexString \
  -Eseparator=/s -Equote=d
Also see the simple networking monitoring application BonzaiThePenguin/Loading.
Binary Whitelisting
google/santa is a security software developed for Google's corporate Macintosh fleet and open sourced.

Santa is a binary whitelisting/blacklisting system for macOS. It consists of a kernel extension that monitors for executions, a userland daemon that makes execution decisions based on the contents of a SQLite database, a GUI agent that notifies the user in case of a block decision and a command-line utility for managing the system and synchronizing the database with a server.

Santa uses the Kernel Authorization API to monitor and allow/disallow binaries from executing in the kernel. Binaries can be white- or black-listed by unique hash or signing developer certificate. Santa can be used to only allow trusted code execution, or to blacklist known malware from executing on a Mac, similar to Bit9 software for Windows.
Note Santa does not currently have a graphical user interface for managing rules. The following instructions are for advanced users only!
To install Santa, visit the Releases page and download the latest disk image, the mount it and install the contained package:
$ hdiutil mount ~/Downloads/santa-0.9.20.dmg

$ sudo installer -pkg /Volumes/santa-0.9.20/santa-0.9.20.pkg -tgt /
By default, Santa installs in ""Monitor"" mode (meaning, nothing gets blocked, only logged) and comes with two rules: one for Apple binaries and another for Santa software itself.
Verify Santa is running and its kernel module is loaded:
$ santactl status
>>> Daemon Info
  Mode                   | Monitor
  File Logging           | No
  Watchdog CPU Events    | 0  (Peak: 0.00%)
  Watchdog RAM Events    | 0  (Peak: 0.00MB)
>>> Kernel Info
  Kernel cache count     | 0
>>> Database Info
  Binary Rules           | 0
  Certificate Rules      | 2
  Events Pending Upload  | 0

$ ps -ef | grep ""[s]anta""
    0   786     1   0 10:01AM ??         0:00.39 /Library/Extensions/santa-driver.kext/Contents/MacOS/santad --syslog

$ kextstat | grep santa
  119    0 0xffffff7f822ff000 0x6000     0x6000     com.google.santa-driver (0.9.14) 693D8E4D-3161-30E0-B83D-66A273CAE026 <5 4 3 1>
Create a blacklist rule to prevent iTunes from executing:
$ sudo santactl rule --blacklist --path /Applications/iTunes.app/
Added rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Try to launch iTunes - it will be blocked.
$ open /Applications/iTunes.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/iTunes.app.

To remove the rule:
$ sudo santactl rule --remove --path /Applications/iTunes.app/
Removed rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Open iTunes:
$ open /Applications/iTunes.app/
[iTunes will open successfully]
Create a new, example C program:
$ cat <<EOF > foo.c
> #include <stdio.h>
> main() { printf(""Hello World\n‚Äù); }
> EOF
Compile the program with GCC (requires installation of Xcode or command-line tools):
$ gcc -o foo foo.c

$ file foo
foo: Mach-O 64-bit executable x86_64

$ codesign -d foo
foo: code object is not signed at all
Run it:
$ ./foo
Hello World
Toggle Santa into ""Lockdown"" mode, which only allows whitelisted binaries to run:
$ sudo defaults write /var/db/santa/config.plist ClientMode -int 2

Try to run the unsigned binary:
$ ./foo
bash: ./foo: Operation not permitted

Santa

The following application has been blocked from executing
because its trustworthiness cannot be determined.

Path:       /Users/demouser/foo
Identifier: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Parent:     bash (701)
To whitelist a specific binary, determine its SHA-256 sum:
$ santactl fileinfo /Users/demouser/foo
Path                 : /Users/demouser/foo
SHA-256              : 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
SHA-1                : 4506f3a8c0a5abe4cacb98e6267549a4d8734d82
Type                 : Executable (x86-64)
Code-signed          : No
Rule                 : Blacklisted (Unknown)
Add a whitelist rule:
$ sudo santactl rule --whitelist --sha256 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Added rule for SHA-256: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed.
Run it:
$ ./foo
Hello World
It's allowed and works!
Applications can also be whitelisted by developer certificate (so that new binary versions will not need to be manually whitelisted on each update). For example, download and run Google Chrome - it will be blocked by Santa in ""Lockdown"" mode:
$ curl -sO https://dl.google.com/chrome/mac/stable/GGRO/googlechrome.dmg

$ hdiutil mount googlechrome.dmg

$ cp -r /Volumes/Google\ Chrome/Google\ Chrome.app /Applications/

$ open /Applications/Google\ Chrome.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/Google Chrome.app.
Whitelist the application by its developer certificate (first item in the Signing Chain):
$ santactl fileinfo /Applications/Google\ Chrome.app/
Path                 : /Applications/Google Chrome.app/Contents/MacOS/Google Chrome
SHA-256              : 0eb08224d427fb1d87d2276d911bbb6c4326ec9f74448a4d9a3cfce0c3413810
SHA-1                : 9213cbc7dfaaf7580f3936a915faa56d40479f6a
Bundle Name          : Google Chrome
Bundle Version       : 2883.87
Bundle Version Str   : 55.0.2883.87
Type                 : Executable (x86-64)
Code-signed          : Yes
Rule                 : Blacklisted (Unknown)
Signing Chain:
     1. SHA-256             : 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
        SHA-1               : 85cee8254216185620ddc8851c7a9fc4dfe120ef
        Common Name         : Developer ID Application: Google Inc.
        Organization        : Google Inc.
        Organizational Unit : EQHXZ8M8AV
        Valid From          : 2012/04/26 07:10:10 -0700
        Valid Until         : 2017/04/27 07:10:10 -0700

     2. SHA-256             : 7afc9d01a62f03a2de9637936d4afe68090d2de18d03f29c88cfb0b1ba63587f
        SHA-1               : 3b166c3b7dc4b751c9fe2afab9135641e388e186
        Common Name         : Developer ID Certification Authority
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2012/02/01 14:12:15 -0800
        Valid Until         : 2027/02/01 14:12:15 -0800

     3. SHA-256             : b0b1730ecbc7ff4505142c49f1295e6eda6bcaed7e2c68c5be91b5a11001f024
        SHA-1               : 611e5b662c593a08ff58d14ae22452d198df6c60
        Common Name         : Apple Root CA
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2006/04/25 14:40:36 -0700
        Valid Until         : 2035/02/09 13:40:36 -0800
In this case, 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153 is the SHA-256 of Google‚Äôs Apple developer certificate (team ID EQHXZ8M8AV). To whitelist it:
$ sudo santactl rule --whitelist --certificate --sha256 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
Added rule for SHA-256: 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153.
Google Chrome should now launch, and subsequent updates to the application will continue to work as long as the code signing certificate doesn‚Äôt change or expire.
To disable ""Lockdown"" mode:
$ sudo defaults delete /var/db/santa/config.plist ClientMode
See /var/log/santa.log to monitor ALLOW and DENY execution decisions.
A log and configuration server for Santa is available in Zentral, an open source event monitoring solution and TLS server for osquery and Santa.
Zentral will support Santa in both MONITORING and LOCKDOWN operation mode. Clients need to be enrolled with a TLS connection to sync Santa Rules, all Santa events from endpoints are aggregated and logged back in Zentral. Santa events can trigger actions and notifications from within the Zentral Framework.
Note Python, Bash and other interpreters are whitelisted (since they are signed by Apple's developer certificate), so Santa will not be able to block such scripts from executing. Thus, a potential non-binary program which disables Santa is a weakness (not vulnerability, since it is so by design) to take note of.
Miscellaneous
Disable Diagnostics & Usage Data.
If you want to play music or watch videos, use VLC media player which is free and open source.
If you want to use torrents, use Transmission which is free and open source (note: like all software, even open source projects, malware may still find its way in). You may also wish to use a block list to avoid peering with known bad hosts - see Which is the best blocklist for Transmission and johntyree/3331662.
Manage default file handlers with duti, which can be installed with brew install duti. One reason to manage extensions is to prevent auto-mounting of remote filesystems in Finder (see Protecting Yourself From Sparklegate). Here are several recommended file handlers to manage:
$ duti -s com.apple.Safari afp

$ duti -s com.apple.Safari ftp

$ duti -s com.apple.Safari nfs

$ duti -s com.apple.Safari smb

$ duti -s com.apple.TextEdit public.unix-executable
Monitor system logs with the Console application or syslog -w or /usr/bin/log stream commands.
In systems prior to macOS Sierra (10.12), enable the tty_tickets flag in /etc/sudoers to restrict the sudo session to the Terminal window/tab that started it. To do so, use sudo visudo and add the line Defaults    tty_tickets.
Set your screen to lock as soon as the screensaver starts:
$ defaults write com.apple.screensaver askForPassword -int 1

$ defaults write com.apple.screensaver askForPasswordDelay -int 0
Expose hidden files and Library folder in Finder:
$ defaults write com.apple.finder AppleShowAllFiles -bool true

$ chflags nohidden ~/Library
Show all filename extensions (so that ""Evil.jpg.app"" cannot masquerade easily).
$ defaults write NSGlobalDomain AppleShowAllExtensions -bool true
Don't default to saving documents to iCloud:
$ defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false
Enable Secure Keyboard Entry in Terminal (unless you use YubiKey or applications such as TextExpander).
Disable crash reporter (the dialog which appears after an application crashes and prompts to report the problem to Apple):
$ defaults write com.apple.CrashReporter DialogType none
Disable Bonjour multicast advertisements:
$ sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES
Disable Handoff and Bluetooth features, if they aren't necessary.
Consider sandboxing your applications. See fG! Sandbox Guide (pdf) and s7ephen/OSX-Sandbox--Seatbelt--Profiles.
Did you know Apple has not shipped a computer with TPM since 2006?
macOS comes with this line in /etc/sudoers:
Defaults env_keep += ""HOME MAIL""

Which stops sudo from changing the HOME variable when you elevate privileges. This means it will execute as root the bash dotfiles in the non-root user's home directory when you run ""sudo bash"". It is advisable to comment this line out to avoid a potentially easy way for malware or a local attacker to escalate privileges to root.
If you want to retain the convenience of the root user having a non-root user's home directory, you can append an export line to /var/root/.bashrc, e.g.:
export HOME=/Users/blah
Set a custom umask:
$ sudo launchctl config user umask 077
Reboot, create a file in Finder and verify its permissions (macOS default allows 'group/other' read access):
$ ls -ld umask*
drwx------  2 kevin  staff       64 Dec  4 12:27 umask_testing_dir
-rw-------@ 1 kevin  staff  2026566 Dec  4 12:28 umask_testing_file
Related software

CISOfy/lynis - Cross-platform security auditing tool and assists with compliance testing and system hardening.
Dylib Hijack Scanner - Scan for applications that are either susceptible to dylib hijacking or have been hijacked.
F-Secure XFENCE (formerly Little Flocker) - ""Little Snitch for files""; prevents applications from accessing files.
Lockdown - Audits and remediates security configuration settings.
Zentral - A log and configuration server for santa and osquery. Run audit and probes on inventory, events, logfiles, combine with point-in-time alerting. A full Framework and Django web server build on top of the elastic stack (formerly known as ELK stack).
facebook/osquery - Can be used to retrieve low level system information.  Users can write SQL queries to retrieve system information.
google/grr - Incident response framework focused on remote live forensics.
jipegit/OSXAuditor - Analyzes artifacts on a running system, such as quarantined files, Safari, Chrome and Firefox history, downloads, HTML5 databases and localstore, social media and email accounts, and Wi-Fi access point names.
kristovatlas/osx-config-check - Checks your OSX machine against various hardened configuration settings.
libyal/libfvde - Library to access FileVault Drive Encryption (FVDE) (or FileVault2) encrypted volumes.
stronghold - Securely and easily configure your Mac from the terminal. Inspired by this guide.
yelp/osxcollector - Forensic evidence collection & analysis toolkit for OS X.
The Eclectic Light Company - Downloads - A collection of useful diagnostics and control applications and utilities for macOS.

Additional resources

Apple Open Source
Auditing and Exploiting Apple IPC
CIS Benchmarks
Demystifying the DMG File Format
Demystifying the i-Device NVMe NAND (New storage used by Apple)
Developing Mac OSX kernel rootkits
DoD Security Technical Implementation Guides for Mac OS
EFF Surveillance Self-Defense Guide
Extracting FileVault 2 Keys with Volatility
Fuzzing the macOS WindowServer for Exploitable Vulnerabilities
Hacker News discussion 2
Hacker News discussion
Harden the World: Mac OSX 10.11 El Capitan
Hidden backdoor API to root privileges in Apple OS X
How to Switch to the Mac
How to make macOS Spotlight fuck the fuck off and do your bidding
IOKit kernel code execution exploit
IPv6 Hardening Guide for OS X
Mac Developer Library: Secure Coding Guide
Mac Forensics: Mac OS X and the HFS+ File System (pdf)
Mac OS X Forensics - Technical Report (pdf)
Mac OS X and iOS Internals: To the Apple's Core by Jonathan Levin
MacAdmins on Slack
MacOS Hardening Guide - Appendix of *OS Internals: Volume III - Security & Insecurity Internals (pdf)
Managing Macs at Google Scale (LISA '13)
OS X 10.10 Yosemite: The Ars Technica Review
OS X Core Technologies Overview White Paper (pdf)
OS X Hardening: Securing a Large Global Mac Fleet (LISA '13)
OSX.Pirrit Mac Adware Part III: The DaVinci Code
Over The Air - Vol. 2, Pt. 1: Exploiting The Wi-Fi Stack on Apple Devices
Patrick Wardle's Objective-See blog
Remote code execution, git, and OS X
Reverse Engineering Mac OS X blog
Reverse Engineering Resources
Security Configuration For Mac OS X Version 10.6 Snow Leopard (pdf)
The EFI boot process
The Great DOM Fuzz-off of 2017
The Intel Mac boot process
The macOS Phishing Easy Button: AppleScript Dangers
There's a lot of vulnerable OS X applications out there (Sparkle Framework RCE)
Userland Persistence on Mac OS X
iCloud security and privacy overview
iSeeYou: Disabling the MacBook Webcam Indicator LED

",GitHub - drduh/macOS-Security-and-Privacy-Guide: Guide to securing and improving privacy on macOS
62,Python,"
spaCy: Industrial-strength NLP
spaCy is a library for advanced Natural Language Processing in Python and
Cython. It's built on the very latest research, and was designed from day one to
be used in real products. spaCy comes with
pretrained statistical models and word vectors, and
currently supports tokenization for 50+ languages. It features
state-of-the-art speed, convolutional neural network models for tagging,
parsing and named entity recognition and easy deep learning integration.
It's commercial open-source software, released under the MIT license.
üí´ Version 2.2 out now!
Check out the release notes here.











üìñ Documentation



Documentation





spaCy 101
New to spaCy? Here's everything you need to know!


Usage Guides
How to use spaCy and its features.


New in v2.2
New features, backwards incompatibilities and migration guide.


API Reference
The detailed reference for spaCy's API.


Models
Download statistical language models for spaCy.


Universe
Libraries, extensions, demos, books and courses.


Changelog
Changes and version history.


Contribute
How to contribute to the spaCy project and code base.



üí¨ Where to ask questions
The spaCy project is maintained by @honnibal and
@ines, along with core contributors
@svlandeg and
@adrianeboyd. Please understand that we won't
be able to provide individual support via email. We also believe that help is
much more valuable if it's shared publicly, so that more people can benefit from
it.



Type
Platforms




üö® Bug Reports
GitHub Issue Tracker


üéÅ Feature Requests
GitHub Issue Tracker


üë©‚Äçüíª Usage Questions
Stack Overflow ¬∑ Gitter Chat ¬∑ Reddit User Group


üóØ General Discussion
Gitter Chat ¬∑ Reddit User Group



Features

Non-destructive tokenization
Named entity recognition
Support for 50+ languages
pretrained statistical models and word vectors
State-of-the-art speed
Easy deep learning integration
Part-of-speech tagging
Labelled dependency parsing
Syntax-driven sentence segmentation
Built in visualizers for syntax and NER
Convenient string-to-hash mapping
Export to numpy data arrays
Efficient binary serialization
Easy model packaging and deployment
Robust, rigorously evaluated accuracy

üìñ For more details, see the
facts, figures and benchmarks.
Install spaCy
For detailed installation instructions, see the
documentation.

Operating system: macOS / OS X ¬∑ Linux ¬∑ Windows (Cygwin, MinGW, Visual
Studio)
Python version: Python 2.7, 3.5+ (only 64 bit)
Package managers: pip ¬∑ conda (via conda-forge)

pip
Using pip, spaCy releases are available as source packages and binary wheels (as
of v2.0.13).
pip install spacy
To install additional data tables for lemmatization in spaCy v2.2+ you can
run pip install spacy[lookups] or install
spacy-lookups-data
separately. The lookups package is needed to create blank models with
lemmatization data, and to lemmatize in languages that don't yet come with
pretrained models and aren't powered by third-party libraries.
When using pip it is generally recommended to install packages in a virtual
environment to avoid modifying system state:
python -m venv .env
source .env/bin/activate
pip install spacy
conda
Thanks to our great community, we've finally re-added conda support. You can now
install spaCy via conda-forge:
conda install -c conda-forge spacy
For the feedstock including the build recipe and configuration, check out
this repository. Improvements
and pull requests to the recipe and setup are always appreciated.
Updating spaCy
Some updates to spaCy may require downloading new statistical models. If you're
running spaCy v2.0 or higher, you can use the validate command to check if
your installed models are compatible and if not, print details on how to update
them:
pip install -U spacy
python -m spacy validate
If you've trained your own models, keep in mind that your training and runtime
inputs must match. After updating spaCy, we recommend retraining your models
with the new version.
üìñ For details on upgrading from spaCy 1.x to spaCy 2.x, see the
migration guide.
Download models
As of v1.7.0, models for spaCy can be installed as Python packages. This
means that they're a component of your application, just like any other module.
Models can be installed using spaCy's download command, or manually by
pointing pip to a path or URL.



Documentation





Available Models
Detailed model descriptions, accuracy figures and benchmarks.


Models Documentation
Detailed usage instructions.



# download best-matching version of specific model for your spaCy installation
python -m spacy download en_core_web_sm

# pip install .tar.gz archive from path or URL
pip install /Users/you/en_core_web_sm-2.2.0.tar.gz
pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz
Loading and using models
To load a model, use spacy.load() with the model name, a shortcut link or a
path to the model data directory.
import spacy
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is a sentence."")
You can also import a model directly via its full name and then call its
load() method with no arguments.
import spacy
import en_core_web_sm

nlp = en_core_web_sm.load()
doc = nlp(""This is a sentence."")
üìñ For more info and examples, check out the
models documentation.
Compile from source
The other way to install spaCy is to clone its
GitHub repository and build it from
source. That is the common way if you want to make changes to the code base.
You'll need to make sure that you have a development environment consisting of a
Python distribution including header files, a compiler,
pip,
virtualenv and
git installed. The compiler part is the trickiest. How to
do that depends on your system. See notes on Ubuntu, OS X and Windows for
details.
# make sure you are using the latest pip
python -m pip install -U pip
git clone https://github.com/explosion/spaCy
cd spaCy

python -m venv .env
source .env/bin/activate
export PYTHONPATH=`pwd`
pip install -r requirements.txt
python setup.py build_ext --inplace
Compared to regular install via pip, requirements.txt
additionally installs developer dependencies such as Cython. For more details
and instructions, see the documentation on
compiling spaCy from source and the
quickstart widget to get the right
commands for your platform and Python version.
Ubuntu
Install system-level dependencies via apt-get:
sudo apt-get install build-essential python-dev git
macOS / OS X
Install a recent version of XCode,
including the so-called ""Command Line Tools"". macOS and OS X ship with Python
and git preinstalled.
Windows
Install a version of the
Visual C++ Build Tools
or Visual Studio Express that
matches the version that was used to compile your Python interpreter. For
official distributions these are VS 2008 (Python 2.7), VS 2010 (Python 3.4) and
VS 2015 (Python 3.5).
Run tests
spaCy comes with an extensive test suite. In order to run the
tests, you'll usually want to clone the repository and build spaCy from source.
This will also install the required development dependencies and test utilities
defined in the requirements.txt.
Alternatively, you can find out where spaCy is installed and run pytest on
that directory. Don't forget to also install the test utilities via spaCy's
requirements.txt:
python -c ""import os; import spacy; print(os.path.dirname(spacy.__file__))""
pip install -r path/to/requirements.txt
python -m pytest <spacy-directory>
See the documentation for more details and
examples.
",GitHub - explosion/spaCy: üí´ Industrial-strength Natural Language Processing (NLP) with Python and Cython
63,Python,"ÊúÄËøëÈúÄË¶Å‰ªéÊñáÊú¨‰∏≠ÊäΩÂèñÁªìÊûÑÂåñ‰ø°ÊÅØÔºåÁî®Âà∞‰∫ÜÂæàÂ§ögithub‰∏äÁöÑÂåÖÔºåÈÅÇÊï¥ÁêÜ‰∫Ü‰∏Ä‰∏ãÔºåÂêéÁª≠‰ºö‰∏çÊñ≠Êõ¥Êñ∞„ÄÇ
ÂæàÂ§öÂåÖÈùûÂ∏∏ÊúâË∂£ÔºåÂÄºÂæóÊî∂ËóèÔºåÊª°Ë∂≥Â§ßÂÆ∂ÁöÑÊî∂ÈõÜÁôñÔºÅ
Â¶ÇÊûúËßâÂæóÊúâÁî®ÔºåËØ∑ÂàÜ‰∫´Âπ∂starÔºåË∞¢Ë∞¢ÔºÅ
Ê∂âÂèäÂÜÖÂÆπÂåÖÊã¨Ôºö‰∏≠Ëã±ÊñáÊïèÊÑüËØç„ÄÅËØ≠Ë®ÄÊ£ÄÊµã„ÄÅ‰∏≠Â§ñÊâãÊú∫/ÁîµËØùÂΩíÂ±ûÂú∞/ËøêËê•ÂïÜÊü•ËØ¢„ÄÅÂêçÂ≠óÊé®Êñ≠ÊÄßÂà´„ÄÅÊâãÊú∫Âè∑ÊäΩÂèñ„ÄÅË∫´‰ªΩËØÅÊäΩÂèñ„ÄÅÈÇÆÁÆ±ÊäΩÂèñ„ÄÅ‰∏≠Êó•Êñá‰∫∫ÂêçÂ∫ì„ÄÅ‰∏≠ÊñáÁº©ÂÜôÂ∫ì„ÄÅÊãÜÂ≠óËØçÂÖ∏„ÄÅËØçÊ±áÊÉÖÊÑüÂÄº„ÄÅÂÅúÁî®ËØç„ÄÅÂèçÂä®ËØçË°®„ÄÅÊö¥ÊÅêËØçË°®„ÄÅÁπÅÁÆÄ‰ΩìËΩ¨Êç¢„ÄÅËã±ÊñáÊ®°Êãü‰∏≠ÊñáÂèëÈü≥„ÄÅÊ±™Â≥∞Ê≠åËØçÁîüÊàêÂô®„ÄÅËÅå‰∏öÂêçÁß∞ËØçÂ∫ì„ÄÅÂêå‰πâËØçÂ∫ì„ÄÅÂèç‰πâËØçÂ∫ì„ÄÅÂê¶ÂÆöËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ÂìÅÁâåËØçÂ∫ì„ÄÅÊ±ΩËΩ¶Èõ∂‰ª∂ËØçÂ∫ì„ÄÅËøûÁª≠Ëã±ÊñáÂàáÂâ≤„ÄÅÂêÑÁßç‰∏≠ÊñáËØçÂêëÈáè„ÄÅÂÖ¨Âè∏ÂêçÂ≠óÂ§ßÂÖ®„ÄÅÂè§ËØóËØçÂ∫ì„ÄÅITËØçÂ∫ì„ÄÅË¥¢ÁªèËØçÂ∫ì„ÄÅÊàêËØ≠ËØçÂ∫ì„ÄÅÂú∞ÂêçËØçÂ∫ì„ÄÅÂéÜÂè≤Âêç‰∫∫ËØçÂ∫ì„ÄÅËØóËØçËØçÂ∫ì„ÄÅÂåªÂ≠¶ËØçÂ∫ì„ÄÅÈ•ÆÈ£üËØçÂ∫ì„ÄÅÊ≥ïÂæãËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ËØçÂ∫ì„ÄÅÂä®Áâ©ËØçÂ∫ì„ÄÅ‰∏≠ÊñáËÅäÂ§©ËØ≠Êñô„ÄÅ‰∏≠ÊñáË∞£Ë®ÄÊï∞ÊçÆ„ÄÅÁôæÂ∫¶‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜ„ÄÅÂè•Â≠êÁõ∏‰ººÂ∫¶ÂåπÈÖçÁÆóÊ≥ïÈõÜÂêà„ÄÅbertËµÑÊ∫ê„ÄÅÊñáÊú¨ÁîüÊàê&ÊëòË¶ÅÁõ∏ÂÖ≥Â∑•ÂÖ∑„ÄÅcocoNLP‰ø°ÊÅØÊäΩÂèñÂ∑•ÂÖ∑„ÄÅÂõΩÂÜÖÁîµËØùÂè∑Á†ÅÊ≠£ÂàôÂåπÈÖç„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶XLORE:‰∏≠Ëã±ÊñáË∑®ËØ≠Ë®ÄÁôæÁßëÁü•ËØÜÂõæË∞±„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁ≥ªÂàóÊä•Âëä„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê„ÄÅNLUÂ§™Èöæ‰∫ÜÁ≥ªÂàó„ÄÅËá™Âä®ÂØπËÅîÊï∞ÊçÆÂèäÊú∫Âô®‰∫∫„ÄÅÁî®Êà∑ÂêçÈªëÂêçÂçïÂàóË°®„ÄÅÁΩ™ÂêçÊ≥ïÂä°ÂêçËØçÂèäÂàÜÁ±ªÊ®°Âûã„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ËØ≠Êñô„ÄÅcs224nÊ∑±Â∫¶Â≠¶‰π†Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã„ÄÅ‰∏≠ÊñáÊâãÂÜôÊ±âÂ≠óËØÜÂà´„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ ËØ≠Êñô/Êï∞ÊçÆÈõÜ„ÄÅÂèòÈáèÂëΩÂêçÁ•ûÂô®„ÄÅÂàÜËØçËØ≠ÊñôÂ∫ì+‰ª£Á†Å„ÄÅ‰ªªÂä°ÂûãÂØπËØùËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÅASR ËØ≠Èü≥Êï∞ÊçÆÈõÜ + Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´Á≥ªÁªü„ÄÅÁ¨ëÂ£∞Ê£ÄÊµãÂô®„ÄÅMicrosoftÂ§öËØ≠Ë®ÄÊï∞Â≠ó/Âçï‰Ωç/Â¶ÇÊó•ÊúüÊó∂Èó¥ËØÜÂà´ÂåÖ„ÄÅ‰∏≠ÂçéÊñ∞ÂçéÂ≠óÂÖ∏Êï∞ÊçÆÂ∫ìÂèäapi(ÂåÖÊã¨Â∏∏Áî®Ê≠áÂêéËØ≠„ÄÅÊàêËØ≠„ÄÅËØçËØ≠ÂíåÊ±âÂ≠ó)„ÄÅÊñáÊ°£ÂõæË∞±Ëá™Âä®ÁîüÊàê„ÄÅSpaCy ‰∏≠ÊñáÊ®°Âûã„ÄÅCommon VoiceËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜÊñ∞Áâà„ÄÅÁ•ûÁªèÁΩëÁªúÂÖ≥Á≥ªÊäΩÂèñ„ÄÅÂü∫‰∫ébertÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥ÈîÆËØç(Keyphrase)ÊäΩÂèñÂåÖpke„ÄÅÂü∫‰∫éÂåªÁñóÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂü∫‰∫é‰æùÂ≠òÂè•Ê≥ï‰∏éËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÁöÑ‰∫ã‰ª∂‰∏âÂÖÉÁªÑÊäΩÂèñ„ÄÅ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê4‰∏áÂè•È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÅcnocrÔºöÁî®Êù•ÂÅö‰∏≠ÊñáOCRÁöÑPython3ÂåÖ„ÄÅ‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÁü•ËØÜÂõæË∞±È°πÁõÆ„ÄÅ‰∏≠ÊñánlpÁ´ûËµõÈ°πÁõÆÂèä‰ª£Á†ÅÊ±áÊÄª„ÄÅ‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ÊçÆ„ÄÅspeech-aligner: ‰ªé‚Äú‰∫∫Â£∞ËØ≠Èü≥‚ÄùÂèäÂÖ∂‚ÄúËØ≠Ë®ÄÊñáÊú¨‚Äù‰∫ßÁîüÈü≥Á¥†Á∫ßÂà´Êó∂Èó¥ÂØπÈΩêÊ†áÊ≥®ÁöÑÂ∑•ÂÖ∑„ÄÅAmpliGraph: Áü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†(Python)Â∫ìÔºöÁü•ËØÜÂõæË∞±Ê¶ÇÂøµÈìæÊé•È¢ÑÊµã„ÄÅScattertext ÊñáÊú¨ÂèØËßÜÂåñ(python)„ÄÅËØ≠Ë®Ä/Áü•ËØÜË°®Á§∫Â∑•ÂÖ∑ÔºöBERT & ERNIE„ÄÅ‰∏≠ÊñáÂØπÊØîËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜNLPÁöÑÂå∫Âà´ÁªºËø∞„ÄÅSynonyms‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖ„ÄÅHarvestTextÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñáÊú¨ÊåñÊéòÂ∑•ÂÖ∑ÔºàÊñ∞ËØçÂèëÁé∞-ÊÉÖÊÑüÂàÜÊûê-ÂÆû‰ΩìÈìæÊé•Á≠âÔºâ„ÄÅword2wordÔºö(Python)Êñπ‰æøÊòìÁî®ÁöÑÂ§öËØ≠Ë®ÄËØç-ËØçÂØπÈõÜÔºö62ÁßçËØ≠Ë®Ä/3,564‰∏™Â§öËØ≠Ë®ÄÂØπ„ÄÅËØ≠Èü≥ËØÜÂà´ËØ≠ÊñôÁîüÊàêÂ∑•ÂÖ∑Ôºö‰ªéÂÖ∑ÊúâÈü≥È¢ë/Â≠óÂπïÁöÑÂú®Á∫øËßÜÈ¢ëÂàõÂª∫Ëá™Âä®ËØ≠Èü≥ËØÜÂà´(ASR)ËØ≠ÊñôÂ∫ì„ÄÅÊûÑÂª∫ÂåªÁñóÂÆû‰ΩìËØÜÂà´ÁöÑÊ®°ÂûãÔºàÂåÖÂê´ËØçÂÖ∏ÂíåËØ≠ÊñôÊ†áÊ≥®Ôºâ„ÄÅÂçïÊñáÊ°£ÈùûÁõëÁù£ÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñ„ÄÅKashgari‰∏≠‰ΩøÁî®gpt-2ËØ≠Ë®ÄÊ®°Âûã„ÄÅÂºÄÊ∫êÁöÑÈáëËûçÊäïËµÑÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑„ÄÅÊñáÊú¨Ëá™Âä®ÊëòË¶ÅÂ∫ìTextTeaser: ‰ªÖÊîØÊåÅËã±Êñá„ÄÅ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ„ÄÅ‰∏Ä‰∫õÂÖ≥‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂü∫Êú¨Ê®°Âûã„ÄÅÂü∫‰∫é14WÊ≠åÊõ≤Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∞ùËØï--ÂäüËÉΩÂåÖÊã¨Ê≠åËØçÊé•ÈæôandÂ∑≤Áü•Ê≠åËØçÊâæÊ≠åÊõ≤‰ª•ÂèäÊ≠åÊõ≤Ê≠åÊâãÊ≠åËØç‰∏âËßíÂÖ≥Á≥ªÁöÑÈóÆÁ≠î„ÄÅÂü∫‰∫éSiamese bilstmÊ®°ÂûãÁöÑÁõ∏‰ººÂè•Â≠êÂà§ÂÆöÊ®°ÂûãÂπ∂Êèê‰æõËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÊµãËØïÊï∞ÊçÆÈõÜ„ÄÅÁî®TransformerÁºñËß£Á†ÅÊ®°ÂûãÂÆûÁé∞ÁöÑÊ†πÊçÆHacker NewsÊñáÁ´†Ê†áÈ¢òËá™Âä®ÁîüÊàêËØÑËÆ∫„ÄÅÁî®BERTËøõË°åÂ∫èÂàóÊ†áËÆ∞ÂíåÊñáÊú¨ÂàÜÁ±ªÁöÑÊ®°Êùø‰ª£Á†Å„ÄÅLitBankÔºöNLPÊï∞ÊçÆÈõÜ‚Äî‚ÄîÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆó‰∫∫ÊñáÂ≠¶Áßë‰ªªÂä°ÁöÑ100ÈÉ®Â∏¶Ê†áËÆ∞Ëã±ÊñáÂ∞èËØ¥ËØ≠Êñô„ÄÅÁôæÂ∫¶ÂºÄÊ∫êÁöÑÂü∫ÂáÜ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü„ÄÅËôöÂÅáÊñ∞ÈóªÊï∞ÊçÆÈõÜ„ÄÅFacebook: LAMAËØ≠Ë®ÄÊ®°ÂûãÂàÜÊûêÔºåÊèê‰æõTransformer-XL/BERT/ELMo/GPTÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªü‰∏ÄËÆøÈóÆÊé•Âè£„ÄÅCommonsenseQAÔºöÈù¢ÂêëÂ∏∏ËØÜÁöÑËã±ÊñáQAÊåëÊàò„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ËµÑÊñô„ÄÅÊï∞ÊçÆÂèäÂ∑•ÂÖ∑„ÄÅÂêÑÂ§ßÂÖ¨Âè∏ÂÜÖÈÉ®ÈáåÂ§ßÁâõÂàÜ‰∫´ÁöÑÊäÄÊúØÊñáÊ°£ PDF ÊàñËÄÖ PPT„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêSQLËØ≠Âè•ÔºàËã±ÊñáÔºâ„ÄÅ‰∏≠ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫ÔºàEDAÔºâÂ∑•ÂÖ∑„ÄÅËã±ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫Â∑•ÂÖ∑ „ÄÅÂü∫‰∫éÂåªËçØÁü•ËØÜÂõæË∞±ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅ‰∫¨‰∏úÂïÜÂìÅÁü•ËØÜÂõæË∞±„ÄÅÂü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆ„ÄÅÂü∫‰∫éËøúÁõëÁù£ÁöÑ‰∏≠ÊñáÂÖ≥Á≥ªÊäΩÂèñ„ÄÅËØ≠Èü≥ÊÉÖÊÑüÂàÜÊûê„ÄÅ‰∏≠ÊñáULMFiT-ÊÉÖÊÑüÂàÜÊûê-ÊñáÊú¨ÂàÜÁ±ª-ËØ≠ÊñôÂèäÊ®°Âûã„ÄÅ‰∏Ä‰∏™ÊãçÁÖßÂÅöÈ¢òÁ®ãÂ∫è„ÄÅ‰∏ñÁïåÂêÑÂõΩÂ§ßËßÑÊ®°‰∫∫ÂêçÂ∫ì„ÄÅ‰∏Ä‰∏™Âà©Áî®ÊúâË∂£‰∏≠ÊñáËØ≠ÊñôÂ∫ì qingyun ËÆ≠ÁªÉÂá∫Êù•ÁöÑ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫seqGAN„ÄÅÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥®„ÄÅÊïôËÇ≤Ë°å‰∏öÊñ∞ÈóªËØ≠ÊñôÂ∫ìÂåÖÂê´Ëá™Âä®ÊñáÊëòÂäüËÉΩ„ÄÅÂºÄÊîæ‰∫ÜÂØπËØùÊú∫Âô®‰∫∫-Áü•ËØÜÂõæË∞±-ËØ≠‰πâÁêÜËß£-Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèäÊï∞ÊçÆ„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºöÂü∫‰∫éÁôæÂ∫¶ÁôæÁßë‰∏≠ÊñáÈ°µÈù¢-ÊäΩÂèñ‰∏âÂÖÉÁªÑ‰ø°ÊÅØ-ÊûÑÂª∫‰∏≠ÊñáÁü•ËØÜÂõæË∞±„ÄÅmasr: ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´-Êèê‰æõÈ¢ÑËÆ≠ÁªÉÊ®°Âûã-È´òËØÜÂà´Áéá„ÄÅPythonÈü≥È¢ëÊï∞ÊçÆÂ¢ûÂπøÂ∫ì„ÄÅ‰∏≠ÊñáÂÖ®ËØçË¶ÜÁõñBERTÂèä‰∏§‰ªΩÈòÖËØªÁêÜËß£Êï∞ÊçÆ„ÄÅConvLabÔºöÂºÄÊ∫êÂ§öÂüüÁ´ØÂà∞Á´ØÂØπËØùÁ≥ªÁªüÂπ≥Âè∞„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊï∞ÊçÆÈõÜ„ÄÅÂü∫‰∫éÊúÄÊñ∞ÁâàÊú¨rasaÊê≠Âª∫ÁöÑÂØπËØùÁ≥ªÁªü„ÄÅÂü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñ„ÄÅ‰∏Ä‰∏™Â∞èÂûãÁöÑËØÅÂà∏Áü•ËØÜÂõæË∞±/Áü•ËØÜÂ∫ì„ÄÅÂ§çÁõòÊâÄÊúâNLPÊØîËµõÁöÑTOPÊñπÊ°à„ÄÅOpenCLaPÔºöÂ§öÈ¢ÜÂüüÂºÄÊ∫ê‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã‰ªìÂ∫ì„ÄÅUERÔºöÂü∫‰∫é‰∏çÂêåËØ≠Êñô+ÁºñÁ†ÅÂô®+ÁõÆÊ†á‰ªªÂä°ÁöÑ‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ªìÂ∫ì„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂêëÈáèÂêàÈõÜ„ÄÅÂü∫‰∫éÈáëËûç-Âè∏Ê≥ïÈ¢ÜÂüü(ÂÖºÊúâÈó≤ËÅäÊÄßË¥®)ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅg2pCÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ±âËØ≠ËØªÈü≥Ëá™Âä®Ê†áËÆ∞Ê®°Âùó„ÄÅZincbase Áü•ËØÜÂõæË∞±ÊûÑÂª∫Â∑•ÂÖ∑ÂåÖ„ÄÅËØóÊ≠åË¥®ÈáèËØÑ‰ª∑/ÁªÜÁ≤íÂ∫¶ÊÉÖÊÑüËØóÊ≠åËØ≠ÊñôÂ∫ì„ÄÅÂø´ÈÄüËΩ¨Âåñ„Äå‰∏≠ÊñáÊï∞Â≠ó„ÄçÂíå„ÄåÈòøÊãâ‰ºØÊï∞Â≠ó„Äç„ÄÅÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îËØ≠ÊñôÂ∫ì„ÄÅÂü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÅjieba_fast Âä†ÈÄüÁâàÁöÑjieba„ÄÅÊ≠£ÂàôË°®ËææÂºèÊïôÁ®ã„ÄÅ‰∏≠ÊñáÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜ„ÄÅÂü∫‰∫éBERTÁ≠âÊúÄÊñ∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊäΩÂèñÂºèÊëòË¶ÅÊèêÂèñ„ÄÅPythonÂà©Áî®Ê∑±Â∫¶Â≠¶‰π†ËøõË°åÊñáÊú¨ÊëòË¶ÅÁöÑÁªºÂêàÊåáÂçó„ÄÅÁü•ËØÜÂõæË∞±Ê∑±Â∫¶Â≠¶‰π†Áõ∏ÂÖ≥ËµÑÊñôÊï¥ÁêÜ„ÄÅÁª¥Âü∫Â§ßËßÑÊ®°Âπ≥Ë°åÊñáÊú¨ËØ≠Êñô„ÄÅStanfordNLP 0.2.0ÔºöÁ∫ØPythonÁâàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂåÖ„ÄÅNeuralNLP-NeuralClassifierÔºöËÖæËÆØÂºÄÊ∫êÊ∑±Â∫¶Â≠¶‰π†ÊñáÊú¨ÂàÜÁ±ªÂ∑•ÂÖ∑„ÄÅÁ´ØÂà∞Á´ØÁöÑÂ∞ÅÈó≠ÂüüÂØπËØùÁ≥ªÁªü„ÄÅ‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºöNeuroNER vs. BertNER„ÄÅÊñ∞Èóª‰∫ã‰ª∂Á∫øÁ¥¢ÊäΩÂèñ„ÄÅ2019Âπ¥ÁôæÂ∫¶ÁöÑ‰∏âÂÖÉÁªÑÊäΩÂèñÊØîËµõÔºö‚ÄúÁßëÂ≠¶Á©∫Èó¥Èòü‚ÄùÊ∫êÁ†Å„ÄÅÂü∫‰∫é‰æùÂ≠òÂè•Ê≥ïÁöÑÂºÄÊîæÂüüÊñáÊú¨Áü•ËØÜ‰∏âÂÖÉÁªÑÊäΩÂèñÂíåÁü•ËØÜÂ∫ìÊûÑÂª∫„ÄÅ‰∏≠ÊñáÁöÑGPT2ËÆ≠ÁªÉ‰ª£Á†Å„ÄÅML-NLP - Êú∫Âô®Â≠¶‰π†(Machine Learning)NLPÈù¢ËØï‰∏≠Â∏∏ËÄÉÂà∞ÁöÑÁü•ËØÜÁÇπÂíå‰ª£Á†ÅÂÆûÁé∞„ÄÅnlp4han:‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ(Êñ≠Âè•/ÂàÜËØç/ËØçÊÄßÊ†áÊ≥®/ÁªÑÂùó/Âè•Ê≥ïÂàÜÊûê/ËØ≠‰πâÂàÜÊûê/NER/NÂÖÉËØ≠Ê≥ï/HMM/‰ª£ËØçÊ∂àËß£/ÊÉÖÊÑüÂàÜÊûê/ÊãºÂÜôÊ£ÄÊü•„ÄÅXLMÔºöFacebookÁöÑË∑®ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã„ÄÅÁî®Âü∫‰∫éBERTÁöÑÂæÆË∞ÉÂíåÁâπÂæÅÊèêÂèñÊñπÊ≥ïÊù•ËøõË°åÁü•ËØÜÂõæË∞±ÁôæÂ∫¶ÁôæÁßë‰∫∫Áâ©ËØçÊù°Â±ûÊÄßÊäΩÂèñ„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁõ∏ÂÖ≥ÁöÑÂºÄÊîæ‰ªªÂä°-Êï∞ÊçÆÈõÜ-ÂΩìÂâçÊúÄ‰Ω≥ÁªìÊûú„ÄÅCoupletAI - Âü∫‰∫éCNN+Bi-LSTM+Attention ÁöÑËá™Âä®ÂØπÂØπËÅîÁ≥ªÁªü„ÄÅÊäΩË±°Áü•ËØÜÂõæË∞±„ÄÅMiningZhiDaoQACorpus - 580‰∏áÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îÊï∞ÊçÆÊåñÊéòÈ°πÁõÆ„ÄÇ
1. textfilter: ‰∏≠Ëã±ÊñáÊïèÊÑüËØçËøáÊª§ observerss/textfilter
 >>> f = DFAFilter()
 >>> f.add(""sexy"")
 >>> f.filter(""hello sexy baby"")
 hello **** baby

ÊïèÊÑüËØçÂåÖÊã¨ÊîøÊ≤ª„ÄÅËÑèËØùÁ≠âËØùÈ¢òËØçÊ±á„ÄÇÂÖ∂ÂéüÁêÜ‰∏ªË¶ÅÊòØÂü∫‰∫éËØçÂÖ∏ÁöÑÊü•ÊâæÔºàÈ°πÁõÆ‰∏≠ÁöÑkeywordÊñá‰ª∂ÔºâÔºåÂÜÖÂÆπÂæàÂä≤ÁàÜ„ÄÇ„ÄÇ„ÄÇ
2. langidÔºö97ÁßçËØ≠Ë®ÄÊ£ÄÊµã https://github.com/saffsd/langid.py

pip install langid

>>> import langid
>>> langid.classify(""This is a test"")
('en', -54.41310358047485)

3. langdetectÔºöÂè¶‰∏Ä‰∏™ËØ≠Ë®ÄÊ£ÄÊµãhttps://code.google.com/archive/p/language-detection/

pip install langdetect

from langdetect import detect
from langdetect import detect_langs

s1 = ""Êú¨ÁØáÂçöÂÆ¢‰∏ªË¶Å‰ªãÁªç‰∏§Ê¨æËØ≠Ë®ÄÊé¢ÊµãÂ∑•ÂÖ∑ÔºåÁî®‰∫éÂå∫ÂàÜÊñáÊú¨Âà∞Â∫ïÊòØ‰ªÄ‰πàËØ≠Ë®ÄÔºå""
s2 = 'We are pleased to introduce today a new technology'
print(detect(s1))
print(detect(s2))
print(detect_langs(s3))    # detect_langs()ËæìÂá∫Êé¢ÊµãÂá∫ÁöÑÊâÄÊúâËØ≠Ë®ÄÁ±ªÂûãÂèäÂÖ∂ÊâÄÂç†ÁöÑÊØî‰æã

ËæìÂá∫ÁªìÊûúÂ¶Ç‰∏ãÔºö Ê≥®ÔºöËØ≠Ë®ÄÁ±ªÂûã‰∏ªË¶ÅÂèÇËÄÉÁöÑÊòØISO 639-1ËØ≠Ë®ÄÁºñÁ†ÅÊ†áÂáÜÔºåËØ¶ËßÅISO 639-1ÁôæÂ∫¶ÁôæÁßë
Ë∑ü‰∏ä‰∏Ä‰∏™ËØ≠Ë®ÄÊ£ÄÊµãÊØîËæÉÔºåÂáÜÁ°ÆÁéá‰ΩéÔºåÊïàÁéáÈ´ò„ÄÇ
4. phone ‰∏≠ÂõΩÊâãÊú∫ÂΩíÂ±ûÂú∞Êü•ËØ¢Ôºö ls0f/phone

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

from phone import Phone
p  = Phone()
p.find(18100065143)
#return {'phone': '18100065143', 'province': '‰∏äÊµ∑', 'city': '‰∏äÊµ∑', 'zip_code': '200000', 'area_code': '021', 'phone_type': 'Áîµ‰ø°'}

ÊîØÊåÅÂè∑ÊÆµ: 13*,15*,18*,14[5,7],17[0,6,7,8]
ËÆ∞ÂΩïÊù°Êï∞: 360569 (updated:2017Âπ¥4Êúà)
‰ΩúËÄÖÊèê‰æõ‰∫ÜÊï∞ÊçÆphone.dat Êñπ‰æøÈùûpythonÁî®Êà∑LoadÊï∞ÊçÆ„ÄÇ
5. phoneÂõΩÈôÖÊâãÊú∫„ÄÅÁîµËØùÂΩíÂ±ûÂú∞Êü•ËØ¢ÔºöAfterShip/phone

npm install phone

import phone from 'phone';
phone('+852 6569-8900'); // return ['+85265698900', 'HKG']
phone('(817) 569-8900'); // return ['+18175698900, 'USA']

6. ngender Ê†πÊçÆÂêçÂ≠óÂà§Êñ≠ÊÄßÂà´Ôºöobserverss/ngender Âü∫‰∫éÊú¥Á¥†Ë¥ùÂè∂ÊñØËÆ°ÁÆóÁöÑÊ¶ÇÁéá

pip install ngender

>>> import ngender
>>> ngender.guess('ËµµÊú¨Â±±')
('male', 0.9836229687547046)
>>> ngender.guess('ÂÆã‰∏π‰∏π')
('female', 0.9759486128949907)

7. ÊäΩÂèñemailÁöÑÊ≠£ÂàôË°®ËææÂºè

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

email_pattern = '^[*#\u4e00-\u9fa5 a-zA-Z0-9_.-]+@[a-zA-Z0-9-]+(\.[a-zA-Z0-9-]+)*\.[a-zA-Z0-9]{2,6}$'
emails = re.findall(email_pattern, text, flags=0)

8. ÊäΩÂèñphone_numberÁöÑÊ≠£ÂàôË°®ËææÂºè

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

cellphone_pattern = '^((13[0-9])|(14[0-9])|(15[0-9])|(17[0-9])|(18[0-9]))\d{8}$'
phoneNumbers = re.findall(cellphone_pattern, text, flags=0)

9. ÊäΩÂèñË∫´‰ªΩËØÅÂè∑ÁöÑÊ≠£ÂàôË°®ËææÂºè
IDCards_pattern = r'^([1-9]\d{5}[12]\d{3}(0[1-9]|1[012])(0[1-9]|[12][0-9]|3[01])\d{3}[0-9xX])$'
IDs = re.findall(IDCards_pattern, text, flags=0)

10.  ‰∫∫ÂêçËØ≠ÊñôÂ∫ìÔºö wainshine/Chinese-Names-Corpus

‰∫∫ÂêçÊäΩÂèñÂäüËÉΩ python package cocoNLPÔºåÊ¨¢ËøéËØïÁî®

‰∏≠ÊñáÔºàÁé∞‰ª£„ÄÅÂè§‰ª£ÔºâÂêçÂ≠ó„ÄÅÊó•ÊñáÂêçÂ≠ó„ÄÅ‰∏≠ÊñáÁöÑÂßìÂíåÂêç„ÄÅÁß∞ÂëºÔºàÂ§ßÂß®Â¶à„ÄÅÂ∞èÂß®Â¶àÁ≠âÔºâ„ÄÅËã±Êñá->‰∏≠ÊñáÂêçÂ≠óÔºàÊùéÁ∫¶Áø∞Ôºâ„ÄÅÊàêËØ≠ËØçÂÖ∏

ÔºàÂèØÁî®‰∫é‰∏≠ÊñáÂàÜËØç„ÄÅÂßìÂêçËØÜÂà´Ôºâ
11. ‰∏≠ÊñáÁº©ÂÜôÂ∫ìÔºögithub
ÂÖ®ÂõΩ‰∫∫Â§ß: ÂÖ®ÂõΩ/n ‰∫∫Ê∞ë/n ‰ª£Ë°®Â§ß‰ºö/n
‰∏≠ÂõΩ: ‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩ/ns
Â•≥ÁΩëËµõ: Â•≥Â≠ê/n ÁΩëÁêÉ/n ÊØîËµõ/vn

12. Ê±âËØ≠ÊãÜÂ≠óËØçÂÖ∏Ôºökfcd/chaizi
Êº¢Â≠ó	ÊãÜÊ≥ï (‰∏Ä)	ÊãÜÊ≥ï (‰∫å)	ÊãÜÊ≥ï (‰∏â)
ÊãÜ	Êâã Êñ•	Êâå Êñ•	Êâç Êñ•

13. ËØçÊ±áÊÉÖÊÑüÂÄºÔºörainarch/SentiBridge
Â±±Ê≥âÊ∞¥	ÂÖÖÊ≤õ	0.400704566541	0.370067395878
ËßÜÈáé	        ÂÆΩÂπø	0.305762728932	0.325320747491
Â§ßÂ≥°Ë∞∑	ÊÉäÈô©	0.312137906517	0.378594957281

14. ‰∏≠ÊñáËØçÂ∫ì„ÄÅÂÅúÁî®ËØç„ÄÅÊïèÊÑüËØç dongxiexidian/Chinese
Ê≠§packageÁöÑÊïèÊÑüËØçÂ∫ìÂàÜÁ±ªÊõ¥ÁªÜÔºö
ÂèçÂä®ËØçÂ∫ìÔºå ÊïèÊÑüËØçÂ∫ìË°®ÁªüËÆ°Ôºå Êö¥ÊÅêËØçÂ∫ìÔºå Ê∞ëÁîüËØçÂ∫ìÔºå Ëâ≤ÊÉÖËØçÂ∫ì
15. Ê±âÂ≠óËΩ¨ÊãºÈü≥Ôºömozillazg/python-pinyin
ÊñáÊú¨Á∫†Èîô‰ºöÁî®Âà∞
16. ‰∏≠ÊñáÁπÅÁÆÄ‰Ωì‰∫íËΩ¨Ôºöskydark/nstools
17. Ëã±ÊñáÊ®°Êãü‰∏≠ÊñáÂèëÈü≥ÂºïÊìé funny chinese text to speech engineeÔºötinyfool/ChineseWithEnglish
say wo i ni
#ËØ¥ÔºöÊàëÁà±‰Ω†

Áõ∏ÂΩì‰∫éÁî®Ëã±ÊñáÈü≥Ê†áÔºåÊ®°Êãü‰∏≠ÊñáÂèëÈü≥„ÄÇ
18. Ê±™Â≥∞Ê≠åËØçÁîüÊàêÂô®Ôºöphunterlau/wangfeng-rnn
ÊàëÂú®ËøôÈáå‰∏≠ÁöÑÂ§úÈáå
Â∞±ÂÉè‰∏ÄÂú∫ÊòØ‰∏ÄÁßçÁîüÂëΩÁöÑÊÑèÊó™
Â∞±ÂÉèÊàëÁöÑÁîüÊ¥ªÂèòÂæóÂú®Êàë‰∏ÄÊ†∑
ÂèØÊàë‰ª¨ËøôÊòØ‰∏Ä‰∏™Áü•ÈÅì
ÊàëÂè™ÊòØ‰∏ÄÂ§©‰Ω†‰ºöÊÄéÂêó

19. Âêå‰πâËØçÂ∫ì„ÄÅÂèç‰πâËØçÂ∫ì„ÄÅÂê¶ÂÆöËØçÂ∫ìÔºöguotong1988/chinese_dictionary
20. Êó†Á©∫Ê†ºËã±Êñá‰∏≤ÂàÜÂâ≤„ÄÅÊäΩÂèñÂçïËØçÔºöwordinja
>>> import wordninja
>>> wordninja.split('derekanderson')
['derek', 'anderson']
>>> wordninja.split('imateapot')
['im', 'a', 'teapot']

21. IPÂú∞ÂùÄÊ≠£ÂàôË°®ËææÂºèÔºö
(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)\.(25[0-5]|2[0-4]\d|[0-1]\d{2}|[1-9]?\d)

22. ËÖæËÆØQQÂè∑Ê≠£ÂàôË°®ËææÂºèÔºö
[1-9]([0-9]{5,11})

23. ÂõΩÂÜÖÂõ∫ËØùÂè∑Á†ÅÊ≠£ÂàôË°®ËææÂºèÔºö
[0-9-()ÔºàÔºâ]{7,18}

24. Áî®Êà∑ÂêçÊ≠£ÂàôË°®ËææÂºèÔºö
[A-Za-z0-9_\-\u4e00-\u9fa5]+

25. Ê±ΩËΩ¶ÂìÅÁâå„ÄÅÊ±ΩËΩ¶Èõ∂‰ª∂Áõ∏ÂÖ≥ËØçÊ±áÔºö
ËßÅÊú¨repoÁöÑdataÊñá‰ª∂ [data](https://github.com/fighting41love/funNLP/tree/master/data)

26. Êó∂Èó¥ÊäΩÂèñÔºö

Â∑≤ÈõÜÊàêÂà∞ python package cocoNLP‰∏≠ÔºåÊ¨¢ËøéËØïÁî®

Âú®2016Âπ¥6Êúà7Êó•9:44ÊâßË°åÊ∏¨Ë©¶ÔºåÁªìÊûúÂ¶Ç‰∏ã

HiÔºåall„ÄÇ‰∏ãÂë®‰∏Ä‰∏ãÂçà‰∏âÁÇπÂºÄ‰ºö

>> 2016-06-13 15:00:00-false

Âë®‰∏ÄÂºÄ‰ºö

>> 2016-06-13 00:00:00-true

‰∏ã‰∏ãÂë®‰∏ÄÂºÄ‰ºö

>> 2016-06-20 00:00:00-true

java version
python version
27. ÂêÑÁßç‰∏≠ÊñáËØçÂêëÈáèÔºö github repo
‰∏≠ÊñáËØçÂêëÈáèÂ§ßÂÖ®
28. ÂÖ¨Âè∏ÂêçÂ≠óÂ§ßÂÖ®Ôºö github repo
29. Âè§ËØóËØçÂ∫ìÔºö github repo Êõ¥ÂÖ®ÁöÑÂè§ËØóËØçÂ∫ì
30. THUÊï¥ÁêÜÁöÑËØçÂ∫ìÔºö link
Â∑≤Êï¥ÁêÜÂà∞Êú¨repoÁöÑdataÊñá‰ª∂Â§π‰∏≠.
ITËØçÂ∫ì„ÄÅË¥¢ÁªèËØçÂ∫ì„ÄÅÊàêËØ≠ËØçÂ∫ì„ÄÅÂú∞ÂêçËØçÂ∫ì„ÄÅÂéÜÂè≤Âêç‰∫∫ËØçÂ∫ì„ÄÅËØóËØçËØçÂ∫ì„ÄÅÂåªÂ≠¶ËØçÂ∫ì„ÄÅÈ•ÆÈ£üËØçÂ∫ì„ÄÅÊ≥ïÂæãËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ËØçÂ∫ì„ÄÅÂä®Áâ©ËØçÂ∫ì

31. ‰∏≠ÊñáËÅäÂ§©ËØ≠Êñô link
ËØ•Â∫ìÊêúÈõÜ‰∫ÜÂåÖÂê´:Ë±ÜÁì£Â§öËΩÆ, PTTÂÖ´Âç¶ËØ≠Êñô, Èùí‰∫ëËØ≠Êñô, ÁîµËßÜÂâßÂØπÁôΩËØ≠Êñô, Ë¥¥ÂêßËÆ∫ÂùõÂõûÂ∏ñËØ≠Êñô,ÂæÆÂçöËØ≠Êñô,Â∞èÈªÑÈ∏°ËØ≠Êñô

32. ‰∏≠ÊñáË∞£Ë®ÄÊï∞ÊçÆ: github
ËØ•Êï∞ÊçÆÊñá‰ª∂‰∏≠ÔºåÊØè‰∏ÄË°å‰∏∫‰∏ÄÊù°jsonÊ†ºÂºèÁöÑË∞£Ë®ÄÊï∞ÊçÆÔºåÂ≠óÊÆµÈáä‰πâÂ¶Ç‰∏ãÔºö

rumorCode: ËØ•Êù°Ë∞£Ë®ÄÁöÑÂîØ‰∏ÄÁºñÁ†ÅÔºåÂèØ‰ª•ÈÄöËøáËØ•ÁºñÁ†ÅÁõ¥Êé•ËÆøÈóÆËØ•Ë∞£Ë®Ä‰∏æÊä•È°µÈù¢„ÄÇ
title: ËØ•Êù°Ë∞£Ë®ÄË¢´‰∏æÊä•ÁöÑÊ†áÈ¢òÂÜÖÂÆπ
informerName: ‰∏æÊä•ËÄÖÂæÆÂçöÂêçÁß∞
informerUrl: ‰∏æÊä•ËÄÖÂæÆÂçöÈìæÊé•
rumormongerName: ÂèëÂ∏ÉË∞£Ë®ÄËÄÖÁöÑÂæÆÂçöÂêçÁß∞
rumormongerUr: ÂèëÂ∏ÉË∞£Ë®ÄËÄÖÁöÑÂæÆÂçöÈìæÊé•
rumorText: Ë∞£Ë®ÄÂÜÖÂÆπ
visitTimes: ËØ•Ë∞£Ë®ÄË¢´ËÆøÈóÆÊ¨°Êï∞
result: ËØ•Ë∞£Ë®ÄÂÆ°Êü•ÁªìÊûú
publishTime: ËØ•Ë∞£Ë®ÄË¢´‰∏æÊä•Êó∂Èó¥

33. ÊÉÖÊÑüÊ≥¢Âä®ÂàÜÊûêÔºögithub
ËØçÂ∫ìÂ∑≤Êï¥ÁêÜÂà∞Êú¨repoÁöÑdataÊñá‰ª∂Â§π‰∏≠.
Êú¨repoÈ°πÁõÆÊòØ‰∏Ä‰∏™ÈÄöËøá‰∏é‰∫∫ÂØπËØùËé∑ÂæóÂÖ∂ÊÉÖÊÑüÂÄºÊ≥¢Âä®ÂõæË∞±, ÂÜÖÁî®ËØçÂ∫ìÂú®dataÊñá‰ª∂Â§π‰∏≠.

34. ÁôæÂ∫¶‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜÔºöÈìæÊé• ÊèêÂèñÁ†Å: 2dva
35. Âè•Â≠ê„ÄÅQAÁõ∏‰ººÂ∫¶ÂåπÈÖç:MatchZoo github
ÊñáÊú¨Áõ∏‰ººÂ∫¶ÂåπÈÖçÁÆóÊ≥ïÁöÑÈõÜÂêàÔºåÂåÖÂê´Â§ö‰∏™Ê∑±Â∫¶Â≠¶‰π†ÁöÑÊñπÊ≥ïÔºåÂÄºÂæóÂ∞ùËØï„ÄÇ
36. bertËµÑÊ∫êÔºö

bertËÆ∫Êñá‰∏≠ÊñáÁøªËØë: link



bertÂéü‰ΩúËÄÖÁöÑslides: link
ÊèêÂèñÁ†Å: iarj


ÊñáÊú¨ÂàÜÁ±ªÂÆûË∑µ: github


bert tutorialÊñáÊú¨ÂàÜÁ±ªÊïôÁ®ã: github


bert pytorchÂÆûÁé∞:  github


bertÁî®‰∫é‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´ tensorflowÁâàÊú¨: github


BERTÁîüÊàêÂè•ÂêëÈáèÔºåBERTÂÅöÊñáÊú¨ÂàÜÁ±ª„ÄÅÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆógithub


bert Âü∫‰∫é keras ÁöÑÂ∞ÅË£ÖÂàÜÁ±ªÊ†áÊ≥®Ê°ÜÊû∂ KashgariÔºåÂá†ÂàÜÈíüÂç≥ÂèØÊê≠Âª∫‰∏Ä‰∏™ÂàÜÁ±ªÊàñËÄÖÂ∫èÂàóÊ†áÊ≥®Ê®°Âûã: github


bert„ÄÅELMOÁöÑÂõæËß£Ôºö github


BERT: Pre-trained models and downstream applications: github


37. Texar - Toolkit for Text Generation and Beyond: github
Âü∫‰∫éTensorflowÁöÑÂºÄÊ∫êÂ∑•ÂÖ∑ÂåÖÔºåÊó®Âú®ÊîØÊåÅÂπøÊ≥õÁöÑÊú∫Âô®Â≠¶‰π†ÔºåÁâπÂà´ÊòØÊñáÊú¨ÁîüÊàê‰ªªÂä°ÔºåÂ¶ÇÊú∫Âô®ÁøªËØë„ÄÅÂØπËØù„ÄÅÊëòË¶Å„ÄÅÂÜÖÂÆπÂ§ÑÁΩÆ„ÄÅËØ≠Ë®ÄÂª∫Ê®°Á≠â
38. ‰∏≠Êñá‰∫ã‰ª∂ÊäΩÂèñÔºö github
‰∏≠ÊñáÂ§çÂêà‰∫ã‰ª∂ÊäΩÂèñÔºåÂåÖÊã¨Êù°‰ª∂‰∫ã‰ª∂„ÄÅÂõ†Êûú‰∫ã‰ª∂„ÄÅÈ°∫Êâø‰∫ã‰ª∂„ÄÅÂèçËΩ¨‰∫ã‰ª∂Á≠â‰∫ã‰ª∂ÊäΩÂèñÔºåÂπ∂ÂΩ¢Êàê‰∫ãÁêÜÂõæË∞±„ÄÇ
39. cocoNLP: github
‰∫∫Âêç„ÄÅÂú∞ÂùÄ„ÄÅÈÇÆÁÆ±„ÄÅÊâãÊú∫Âè∑„ÄÅÊâãÊú∫ÂΩíÂ±ûÂú∞ Á≠â‰ø°ÊÅØÁöÑÊäΩÂèñÔºårakeÁü≠ËØ≠ÊäΩÂèñÁÆóÊ≥ï„ÄÇ

pip install cocoNLP

>>> from cocoNLP.extractor import extractor

>>> ex = extractor()

>>> text = 'ÊÄ•ÂØªÁâπÊúóÊôÆÔºåÁî∑Â≠©Ôºå‰∫é2018Âπ¥11Êúà27Âè∑11Êó∂Âú®ÈôïË•øÁúÅÂÆâÂ∫∑Â∏ÇÊ±âÊª®Âå∫Ëµ∞Â§±„ÄÇ‰∏¢Â§±ÂèëÂûãÁü≠ÂèëÔºå...Â¶ÇÊúâÁ∫øÁ¥¢ÔºåËØ∑ËøÖÈÄü‰∏éË≠¶ÊñπËÅîÁ≥ªÔºö18100065143Ôºå132-6156-2938Ôºåbaizhantang@sina.com.cn Âíåyangyangfuture at gmail dot com'

# ÊäΩÂèñÈÇÆÁÆ±
>>> emails = ex.extract_email(text)
>>> print(emails)

['baizhantang@sina.com.cn', 'yangyangfuture@gmail.com.cn']
# ÊäΩÂèñÊâãÊú∫Âè∑
>>> cellphones = ex.extract_cellphone(text,nation='CHN')
>>> print(cellphones)

['18100065143', '13261562938']
# ÊäΩÂèñÊâãÊú∫ÂΩíÂ±ûÂú∞„ÄÅËøêËê•ÂïÜ
>>> cell_locs = [ex.extract_cellphone_location(cell,'CHN') for cell in cellphones]
>>> print(cell_locs)

cellphone_location [{'phone': '18100065143', 'province': '‰∏äÊµ∑', 'city': '‰∏äÊµ∑', 'zip_code': '200000', 'area_code': '021', 'phone_type': 'Áîµ‰ø°'}]
# ÊäΩÂèñÂú∞ÂùÄ‰ø°ÊÅØ
>>> locations = ex.extract_locations(text)
>>> print(locations)
['ÈôïË•øÁúÅÂÆâÂ∫∑Â∏ÇÊ±âÊª®Âå∫', 'ÂÆâÂ∫∑Â∏ÇÊ±âÊª®Âå∫', 'Ê±âÊª®Âå∫']
# ÊäΩÂèñÊó∂Èó¥ÁÇπ
>>> times = ex.extract_time(text)
>>> print(times)
time {""type"": ""timestamp"", ""timestamp"": ""2018-11-27 11:00:00""}
# ÊäΩÂèñ‰∫∫Âêç
>>> name = ex.extract_name(text)
>>> print(name)
ÁâπÊúóÊôÆ


40. ÂõΩÂÜÖÁîµËØùÂè∑Á†ÅÊ≠£ÂàôÂåπÈÖçÔºà‰∏âÂ§ßËøêËê•ÂïÜ+ËôöÊãüÁ≠âÔºâ: github
41. Ê∏ÖÂçéÂ§ßÂ≠¶XLORE:‰∏≠Ëã±ÊñáË∑®ËØ≠Ë®ÄÁôæÁßëÁü•ËØÜÂõæË∞±: link
‰∏äËø∞ÈìæÊé•‰∏≠ÂåÖÂê´‰∫ÜÊâÄÊúâÂÆû‰ΩìÂèäÂÖ≥Á≥ªÁöÑTTLÊñá‰ª∂ÔºåÊõ¥Â§öÊï∞ÊçÆÂ∞ÜÂú®ËøëÊúüÂèëÂ∏É„ÄÇ
Ê¶ÇÂøµÔºåÂÆû‰æãÔºåÂ±ûÊÄßÂíå‰∏ä‰∏ã‰ΩçÂÖ≥Á≥ªÊï∞ÁõÆ




ÁôæÂ∫¶
‰∏≠ÊñáÁª¥Âü∫
Ëã±ÊñáÁª¥Âü∫
ÊÄªÊï∞




Ê¶ÇÂøµÊï∞Èáè
32,009
150,241
326,518
508,768


ÂÆû‰æãÊï∞Èáè
1,629,591
640,622
1,235,178
3,505,391


Â±ûÊÄßÊï∞Èáè
157,370
45,190
26,723
229.283


InstanceOf
7,584,931
1,449,925
3,032,515
12,067,371


SubClassOf
2,784
191,577
555,538
749,899



Ë∑®ËØ≠Ë®ÄËøûÊé•ÔºàÊ¶ÇÂøµ/ÂÆû‰æãÔºâ




ÁôæÂ∫¶
‰∏≠ÊñáÁª¥Âü∫
Ëã±ÊñáÁª¥Âü∫




ÁôæÂ∫¶
-
10,216/336,890
4,846/303,108


‰∏≠ÊñáÁª¥Âü∫
10,216/336,890
-
28,921/454,579


Ëã±ÊñáÁª¥Âü∫
4,846/303,108
28,921/454,579
-



42. Ê∏ÖÂçéÂ§ßÂ≠¶‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁ≥ªÂàóÊä•ÂëäÔºö link
ÊØèÂπ¥‰ºöÂá∫AIÈ¢ÜÂüüÁõ∏ÂÖ≥ÁöÑÊä•ÂëäÔºåÂÜÖÂÆπÂåÖÂê´

Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ link
Áü•ËØÜÂõæË∞± link
Êï∞ÊçÆÊåñÊéò link
Ëá™Âä®È©æÈ©∂ link
Êú∫Âô®ÁøªËØë link
Âå∫ÂùóÈìæ link
Êú∫Âô®‰∫∫ link
ËÆ°ÁÆóÊú∫ÂõæÂΩ¢Â≠¶ link
3DÊâìÂç∞ link
‰∫∫ËÑ∏ËØÜÂà´ link
‰∫∫Â∑•Êô∫ËÉΩËäØÁâá link
Á≠âÁ≠â

43.Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÊñπÈù¢:
Ehud ReiterÊïôÊéàÁöÑÂçöÂÆ¢  ÂåóÂ§ß‰∏áÂ∞èÂÜõÊïôÊéàÂº∫ÂäõÊé®ËçêÔºåËØ•ÂçöÂÆ¢ÂØπNLGÊäÄÊúØ„ÄÅËØÑ‰ª∑‰∏éÂ∫îÁî®ËøõË°å‰∫ÜÊ∑±ÂÖ•ÁöÑÊé¢ËÆ®‰∏éÂèçÊÄù„ÄÇ
ÊñáÊú¨ÁîüÊàêÁõ∏ÂÖ≥ËµÑÊ∫êÂ§ßÂàóË°®
Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÔºöËÆ©Êú∫Âô®ÊéåÊè°Ëá™Âä®Âàõ‰ΩúÁöÑÊú¨È¢Ü - ÂºÄÊîæÂüüÂØπËØùÁîüÊàêÂèäÂú®ÂæÆËΩØÂ∞èÂÜ∞‰∏≠ÁöÑÂÆûË∑µ
ÊñáÊú¨ÁîüÊàêÊéßÂà∂
44.:
jiebaÂíåhanlpÂ∞±‰∏çÂøÖ‰ªãÁªç‰∫ÜÂêß„ÄÇ
45.NLPÂ§™Èöæ‰∫ÜÁ≥ªÂàó: github

Êù•Âà∞Êù®ËøáÊõæÁªèÁîüÊ¥ªËøáÁöÑÂú∞ÊñπÔºåÂ∞èÈæôÂ•≥Âä®ÊÉÖÂú∞ËØ¥Ôºö‚ÄúÊàë‰πüÊÉ≥ËøáËøáËøáÂÑøËøáËøáÁöÑÁîüÊ¥ª„ÄÇ‚Äù ‚Äã‚Äã‚Äã
Êù•Âà∞ÂÑøÂ≠êÁ≠âÊ†°ËΩ¶ÁöÑÂú∞ÊñπÔºåÈÇìË∂ÖÂØπÂ≠ô‰ø™ËØ¥Ôºö‚ÄúÊàë‰πüÊÉ≥Á≠âÁ≠âÁ≠âÁ≠âÁ≠âËøáÁöÑÈÇ£ËæÜËΩ¶„ÄÇ‚Äù
ËµµÊïèËØ¥ÔºöÊàë‰πüÊÉ≥ÊéßÂøåÂøåÂ∑±‰∏çÊÉ≥Êó†Âøå„ÄÇ
‰Ω†‰πüÊÉ≥ÁäØËåÉËåÉËåÉÁéÆÁê™ÁäØËøáÁöÑÈîôÂêó
ÂØπÂèôÊâìÂáªÊòØ‰∏ÄÊ¨°ÊÄßË°å‰∏∫Ôºü

46.Ëá™Âä®ÂØπËÅîÊï∞ÊçÆÂèäÊú∫Âô®‰∫∫:
70‰∏áÂØπËÅîÊï∞ÊçÆ link
‰ª£Á†Å link



‰∏äËÅî
‰∏ãËÅî




ÊÆ∑Âã§ÊÄïË¥ü‰∏âÊò•ÊÑè
ÊΩáÊ¥íÈöæ‰π¶‰∏ÄÂ≠óÊÑÅ


Â¶ÇÊ≠§Ê∏ÖÁßã‰ΩïÂêùÈÖí
ËøôËà¨ÊòéÊúà‰∏çÈ°ªÈí±



47.Áî®Êà∑ÂêçÈªëÂêçÂçïÂàóË°®Ôºö github
ÂåÖÂê´‰∫ÜÁî®Êà∑ÂêçÁ¶ÅÁî®ÂàóË°®ÔºåÊØîÂ¶Ç: link
administrator
administration
autoconfig
autodiscover
broadcasthost
domain
editor
guest
host
hostmaster
info
keybase.txt
localdomain
localhost
master
mail
mail0
mail1

48.ÁΩ™ÂêçÊ≥ïÂä°ÂêçËØçÂèäÂàÜÁ±ªÊ®°Âûã: github
ÂåÖÂê´856È°πÁΩ™ÂêçÁü•ËØÜÂõæË∞±, Âü∫‰∫é280‰∏áÁΩ™ÂêçËÆ≠ÁªÉÂ∫ìÁöÑÁΩ™ÂêçÈ¢ÑÊµã,Âü∫‰∫é20WÊ≥ïÂä°ÈóÆÁ≠îÂØπÁöÑ13Á±ªÈóÆÈ¢òÂàÜÁ±ª‰∏éÊ≥ïÂæãËµÑËÆØÈóÆÁ≠îÂäüËÉΩ

49.ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ËØ≠Êñô: github
3GËØ≠ÊñôÔºåÂåÖÂê´ÈÉ®ÂàÜÁΩëÁªúÊäìÂèñÁöÑÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÁöÑÊñáÁ´†ÔºåÂ∑≤ÁªèÂéªÈô§HTMLÔºåÂè™ÂåÖÂê´‰∫ÜÁ∫ØÊñáÊú¨„ÄÇÊØèË°å‰∏ÄÁØáÔºåÊòØJSONÊ†ºÂºèÔºånameÊòØÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÂêçÂ≠óÔºåaccountÊòØÂæÆ‰ø°ÂÖ¨‰ºóÂè∑IDÔºåtitleÊòØÈ¢òÁõÆÔºåcontentÊòØÊ≠£Êñá
50.cs224nÊ∑±Â∫¶Â≠¶‰π†Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ãÔºölink

ËØæÁ®ã‰∏≠Ê®°ÂûãÁöÑpytorchÂÆûÁé∞ link
Èù¢ÂêëÊ∑±Â∫¶Â≠¶‰π†Á†îÁ©∂‰∫∫ÂëòÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂÆû‰æãÊïôÁ®ã link

51.‰∏≠ÊñáÊâãÂÜôÊ±âÂ≠óËØÜÂà´Ôºögithub
52.‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ ËØ≠Êñô/Êï∞ÊçÆÈõÜÔºögithub
Á´ûÂìÅÔºöTHUOCLÔºàTHU Open Chinese LexiconÔºâ‰∏≠ÊñáËØçÂ∫ì
53.ÂèòÈáèÂëΩÂêçÁ•ûÂô®Ôºögithub link
54.ÂàÜËØçËØ≠ÊñôÂ∫ì+‰ª£Á†ÅÔºöÁôæÂ∫¶ÁΩëÁõòÈìæÊé•

ÊèêÂèñÁ†Å: pea6
kerasÂÆûÁé∞ÁöÑÂü∫‰∫éBi-LSTM + CRFÁöÑ‰∏≠ÊñáÂàÜËØç+ËØçÊÄßÊ†áÊ≥®
Âü∫‰∫éUniversal Transformer + CRF ÁöÑ‰∏≠ÊñáÂàÜËØçÂíåËØçÊÄßÊ†áÊ≥®
Âø´ÈÄüÁ•ûÁªèÁΩëÁªúÂàÜËØçÂåÖ java version

55. NLPÊñ∞‰π¶Êé®Ëçê„ÄäNatural Language Processing„Äãby Jacob EisensteinÔºö link
56. ‰ªªÂä°ÂûãÂØπËØùËã±ÊñáÊï∞ÊçÆÈõÜÔºö github
„ÄêÊúÄÂÖ®‰ªªÂä°ÂûãÂØπËØùÊï∞ÊçÆÈõÜ„Äë‰∏ªË¶Å‰ªãÁªç‰∫Ü‰∏Ä‰ªΩ‰ªªÂä°ÂûãÂØπËØùÊï∞ÊçÆÈõÜÂ§ßÂÖ®ÔºåËøô‰ªΩÊï∞ÊçÆÈõÜÂ§ßÂÖ®Ê∂µÁõñ‰∫ÜÂà∞ÁõÆÂâçÂú®‰ªªÂä°ÂûãÂØπËØùÈ¢ÜÂüüÁöÑÊâÄÊúâÂ∏∏Áî®Êï∞ÊçÆÈõÜÁöÑ‰∏ªË¶Å‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºå‰∏∫‰∫ÜÂ∏ÆÂä©Á†îÁ©∂ËÄÖÊõ¥Â•ΩÁöÑÊääÊè°È¢ÜÂüüËøõÂ±ïÁöÑËÑâÁªúÔºåÊàë‰ª¨‰ª•LeaderboardÁöÑÂΩ¢ÂºèÁªôÂá∫‰∫ÜÂá†‰∏™Êï∞ÊçÆÈõÜ‰∏äÁöÑState-of-the-artÂÆûÈ™åÁªìÊûú„ÄÇ
57. ASR ËØ≠Èü≥Êï∞ÊçÆÈõÜ + Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´Á≥ªÁªüÔºö github


Data Sets Êï∞ÊçÆÈõÜ


Ê∏ÖÂçéÂ§ßÂ≠¶THCHS30‰∏≠ÊñáËØ≠Èü≥Êï∞ÊçÆÈõÜ
data_thchs30.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè
test-noise.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè
resource.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè


Free ST Chinese Mandarin Corpus
ST-CMDS-20170001_1-OS.tar.gz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè


AIShell-1 ÂºÄÊ∫êÁâàÊï∞ÊçÆÈõÜ
data_aishell.tgz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè


Ê≥®ÔºöÊï∞ÊçÆÈõÜËß£ÂéãÊñπÊ≥ï
$ tar xzf data_aishell.tgz
$ cd data_aishell/wav
$ for tar in *.tar.gz;  do tar xvf $tar; done



Primewords Chinese Corpus Set 1
primewords_md_2018_set1.tar.gz
OpenSLRÂõΩÂÜÖÈïúÂÉè
OpenSLRÂõΩÂ§ñÈïúÂÉè




58. Á¨ëÂ£∞Ê£ÄÊµãÂô®Ôºö github
59. MicrosoftÂ§öËØ≠Ë®ÄÊï∞Â≠ó/Âçï‰Ωç/Â¶ÇÊó•ÊúüÊó∂Èó¥ËØÜÂà´ÂåÖÔºö [github](https://github.com/Microsoft/Recognizers-Text
60. chinese-xinhua ‰∏≠ÂçéÊñ∞ÂçéÂ≠óÂÖ∏Êï∞ÊçÆÂ∫ìÂèäapiÔºåÂåÖÊã¨Â∏∏Áî®Ê≠áÂêéËØ≠„ÄÅÊàêËØ≠„ÄÅËØçËØ≠ÂíåÊ±âÂ≠ó github
61. ÊñáÊ°£ÂõæË∞±Ëá™Âä®ÁîüÊàê github

TextGrapher - Text Content Grapher based on keyinfo extraction by NLP method„ÄÇËæìÂÖ•‰∏ÄÁØáÊñáÊ°£ÔºåÂ∞ÜÊñáÊ°£ËøõË°åÂÖ≥ÈîÆ‰ø°ÊÅØÊèêÂèñÔºåËøõË°åÁªìÊûÑÂåñÔºåÂπ∂ÊúÄÁªàÁªÑÁªáÊàêÂõæË∞±ÁªÑÁªáÂΩ¢ÂºèÔºåÂΩ¢ÊàêÂØπÊñáÁ´†ËØ≠‰πâ‰ø°ÊÅØÁöÑÂõæË∞±ÂåñÂ±ïÁ§∫

62. SpaCy ‰∏≠ÊñáÊ®°Âûã github

ÂåÖÂê´Parser, NER, ËØ≠Ê≥ïÊ†ëÁ≠âÂäüËÉΩ„ÄÇÊúâ‰∏Ä‰∫õËã±Êñápackage‰ΩøÁî®spacyÁöÑËã±ÊñáÊ®°ÂûãÁöÑÔºåÂ¶ÇÊûúË¶ÅÈÄÇÈÖç‰∏≠ÊñáÔºåÂèØËÉΩÈúÄË¶Å‰ΩøÁî®spacy‰∏≠ÊñáÊ®°Âûã„ÄÇ

63. Common VoiceËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜÊñ∞Áâà link

ÂåÖÊã¨Êù•Ëá™42,000ÂêçË¥°ÁåÆËÄÖË∂ÖËøá1,400Â∞èÊó∂ÁöÑËØ≠Èü≥Ê†∑Êú¨ÔºåÊ∂µgithub

64. Á•ûÁªèÁΩëÁªúÂÖ≥Á≥ªÊäΩÂèñ pytorch github

ÊöÇ‰∏çÊîØÊåÅ‰∏≠Êñá

65. Âü∫‰∫ébertÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´ pytorch github

ÊöÇ‰∏çÊîØÊåÅ‰∏≠Êñá

66. ÂÖ≥ÈîÆËØç(Keyphrase)ÊäΩÂèñÂåÖ pke github
pke: an open source python-based keyphrase extraction toolkit

ÊöÇ‰∏çÊîØÊåÅ‰∏≠ÊñáÔºåÊàë‰∫éËøëÊúüÂØπÂÖ∂ËøõË°å‰øÆÊîπÔºå‰ΩøÂÖ∂ÈÄÇÈÖç‰∏≠Êñá„ÄÇ
ËØ∑ÂÖ≥Ê≥®ÊàëÁöÑgithubÂä®ÊÄÅÔºåË∞¢Ë∞¢ÔºÅ

67. Âü∫‰∫éÂåªÁñóÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü github

ËØ•repoÂèÇËÄÉ‰∫Ügithub

68. Âü∫‰∫é‰æùÂ≠òÂè•Ê≥ï‰∏éËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÁöÑ‰∫ã‰ª∂‰∏âÂÖÉÁªÑÊäΩÂèñ github
69. ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê4‰∏áÂè•È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆ by ËãèÂ∑ûÂ§ßÂ≠¶Ê±âËØ≠‰æùÂ≠òÊ†ëÂ∫ìÔºàSUCDTÔºâ
Homepage
Êï∞ÊçÆ‰∏ãËΩΩËØ¶ËßÅhomepageÂ∫ïÈÉ®ÔºåÈúÄË¶ÅÁ≠æÁΩ≤ÂçèËÆÆÔºåÈúÄË¶ÅÈÇÆ‰ª∂Êé•Êî∂Ëß£ÂéãÂØÜÁ†Å„ÄÇ
70. cnocrÔºöÁî®Êù•ÂÅö‰∏≠ÊñáOCRÁöÑPython3ÂåÖÔºåËá™Â∏¶‰∫ÜËÆ≠ÁªÉÂ•ΩÁöÑËØÜÂà´Ê®°Âûã github
71. ‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÁü•ËØÜÂõæË∞±È°πÁõÆ github

‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÂõæË∞±ÊûÑÂª∫
Âü∫‰∫éÁü•ËØÜÂ∫ìÁöÑÊï∞ÊçÆÂõûÊ†á
Âü∫‰∫éËøúÁ®ãÁõëÁù£‰∏ébootstrappingÊñπÊ≥ïÁöÑ‰∫∫Áâ©ÂÖ≥Á≥ªÊäΩÂèñ
Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÁü•ËØÜÈóÆÁ≠îÁ≠âÂ∫îÁî®

72. ‰∏≠ÊñánlpÁ´ûËµõÈ°πÁõÆÂèä‰ª£Á†ÅÊ±áÊÄª github

ÊñáÊú¨ÁîüÊàê„ÄÅÊñáÊú¨ÊëòË¶ÅÔºöByte Cup 2018 ÂõΩÈôÖÊú∫Âô®Â≠¶‰π†Á´ûËµõ
Áü•ËØÜÂõæË∞±ÔºöÁëûÈáëÂåªÈô¢MMC‰∫∫Â∑•Êô∫ËÉΩËæÖÂä©ÊûÑÂª∫Áü•ËØÜÂõæË∞±Â§ßËµõ
ËßÜÈ¢ëËØÜÂà´ ÈóÆÁ≠îÔºö2018‰πãÊ±üÊùØÂÖ®ÁêÉ‰∫∫Â∑•Êô∫ËÉΩÂ§ßËµõ‚Ä®ÔºöËßÜÈ¢ëËØÜÂà´&ÈóÆÁ≠î

73. ‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ÊçÆ github

ÁÆÄ/ÁπÅ‰ΩìÊ±âÂ≠óÁ¨îÈ°∫
Áü¢ÈáèÁ¨îÁîª

74. speech-aligner: ‰ªé‚Äú‰∫∫Â£∞ËØ≠Èü≥‚ÄùÂèäÂÖ∂‚ÄúËØ≠Ë®ÄÊñáÊú¨‚ÄùÔºå‰∫ßÁîüÈü≥Á¥†Á∫ßÂà´Êó∂Èó¥ÂØπÈΩêÊ†áÊ≥®ÁöÑÂ∑•ÂÖ∑ github
75. AmpliGraph: Áü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†(Python)Â∫ìÔºöÁü•ËØÜÂõæË∞±Ê¶ÇÂøµÈìæÊé•È¢ÑÊµã github

ÂüÉÊ£ÆÂì≤Âá∫ÂìÅÔºåÁõÆÂâçÂ∞ö‰∏çÊîØÊåÅ‰∏≠Êñá

76. Scattertext ÊñáÊú¨ÂèØËßÜÂåñ(python) github

ÂæàÂ•ΩÁî®ÁöÑÂ∑•ÂÖ∑ÂåÖÔºåÁÆÄÂçï‰øÆÊîπÂêéÂèØÊîØÊåÅ‰∏≠Êñá
ËÉΩÂê¶ÂàÜÊûêÂá∫Êüê‰∏™Á±ªÂà´ÁöÑÊñáÊú¨‰∏éÂÖ∂‰ªñÊñáÊú¨ÁöÑÁî®ËØçÂ∑ÆÂºÇ

77. ËØ≠Ë®Ä/Áü•ËØÜË°®Á§∫Â∑•ÂÖ∑ÔºöBERT & ERNIE github

ÁôæÂ∫¶Âá∫ÂìÅÔºåERNIE‰πüÂè∑Áß∞Âú®Â§öÈ°πnlp‰ªªÂä°‰∏≠ÂáªË¥•‰∫Übert

78. ‰∏≠ÊñáÂØπÊØîËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜNLPÁöÑÂå∫Âà´ÁªºËø∞ link
79. Synonyms‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖ github

Synonyms ‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖÔºåÂèØ‰ª•Áî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ÁöÑÂæàÂ§ö‰ªªÂä°ÔºöÊñáÊú¨ÂØπÈΩêÔºåÊé®ËçêÁÆóÊ≥ïÔºåÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåËØ≠‰πâÂÅèÁßªÔºåÂÖ≥ÈîÆÂ≠óÊèêÂèñÔºåÊ¶ÇÂøµÊèêÂèñÔºåËá™Âä®ÊëòË¶ÅÔºåÊêúÁ¥¢ÂºïÊìéÁ≠â

80. HarvestTextÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñáÊú¨ÊåñÊéòÂ∑•ÂÖ∑ÔºàÊñ∞ËØçÂèëÁé∞-ÊÉÖÊÑüÂàÜÊûê-ÂÆû‰ΩìÈìæÊé•Á≠âÔºâ github
81. word2wordÔºö(Python)Êñπ‰æøÊòìÁî®ÁöÑÂ§öËØ≠Ë®ÄËØç-ËØçÂØπÈõÜÔºö62ÁßçËØ≠Ë®Ä/3,564‰∏™Â§öËØ≠Ë®ÄÂØπ github
82. ËØ≠Èü≥ËØÜÂà´ËØ≠ÊñôÁîüÊàêÂ∑•ÂÖ∑Ôºö‰ªéÂÖ∑ÊúâÈü≥È¢ë/Â≠óÂπïÁöÑÂú®Á∫øËßÜÈ¢ëÂàõÂª∫Ëá™Âä®ËØ≠Èü≥ËØÜÂà´(ASR)ËØ≠ÊñôÂ∫ì github
83. ASRËØ≠Èü≥Â§ßËæûÂÖ∏/ËØçÂÖ∏Ôºö github
84. ÊûÑÂª∫ÂåªÁñóÂÆû‰ΩìËØÜÂà´ÁöÑÊ®°ÂûãÔºåÂåÖÂê´ËØçÂÖ∏ÂíåËØ≠ÊñôÊ†áÊ≥®ÔºåÂü∫‰∫épython: github
85. ÂçïÊñáÊ°£ÈùûÁõëÁù£ÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñÔºö github
86. Kashgari‰∏≠‰ΩøÁî®gpt-2ËØ≠Ë®ÄÊ®°Âûã github
87.  ÂºÄÊ∫êÁöÑÈáëËûçÊäïËµÑÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑ github
88. ÊñáÊú¨Ëá™Âä®ÊëòË¶ÅÂ∫ìTextTeaser: ‰ªÖÊîØÊåÅËã±Êñá github
89. ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ github
90. ‰∏Ä‰∫õÂÖ≥‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂü∫Êú¨Ê®°Âûã github
91. Âü∫‰∫é14WÊ≠åÊõ≤Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∞ùËØïÔºåÂäüËÉΩÂåÖÊã¨Ê≠åËØçÊé•ÈæôÔºåÂ∑≤Áü•Ê≠åËØçÊâæÊ≠åÊõ≤‰ª•ÂèäÊ≠åÊõ≤Ê≠åÊâãÊ≠åËØç‰∏âËßíÂÖ≥Á≥ªÁöÑÈóÆÁ≠î github
92. Âü∫‰∫éSiamese bilstmÊ®°ÂûãÁöÑÁõ∏‰ººÂè•Â≠êÂà§ÂÆöÊ®°Âûã,Êèê‰æõËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÊµãËØïÊï∞ÊçÆÈõÜ github

Êèê‰æõ‰∫Ü10‰∏á‰∏™ËÆ≠ÁªÉÊ†∑Êú¨

93. Áî®TransformerÁºñËß£Á†ÅÊ®°ÂûãÂÆûÁé∞ÁöÑÊ†πÊçÆHacker NewsÊñáÁ´†Ê†áÈ¢òËá™Âä®ÁîüÊàêËØÑËÆ∫ github
94. Áî®BERTËøõË°åÂ∫èÂàóÊ†áËÆ∞ÂíåÊñáÊú¨ÂàÜÁ±ªÁöÑÊ®°Êùø‰ª£Á†Å github
95. LitBankÔºöNLPÊï∞ÊçÆÈõÜ‚Äî‚ÄîÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆó‰∫∫ÊñáÂ≠¶Áßë‰ªªÂä°ÁöÑ100ÈÉ®Â∏¶Ê†áËÆ∞Ëã±ÊñáÂ∞èËØ¥ËØ≠Êñô github
96. ÁôæÂ∫¶ÂºÄÊ∫êÁöÑÂü∫ÂáÜ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü github
97. ËôöÂÅáÊñ∞ÈóªÊï∞ÊçÆÈõÜ fake news corpus github
98. Facebook: LAMAËØ≠Ë®ÄÊ®°ÂûãÂàÜÊûêÔºåÊèê‰æõTransformer-XL/BERT/ELMo/GPTÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªü‰∏ÄËÆøÈóÆÊé•Âè£ github
99. CommonsenseQAÔºöÈù¢ÂêëÂ∏∏ËØÜÁöÑËã±ÊñáQAÊåëÊàò link
100. ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ËµÑÊñô„ÄÅÊï∞ÊçÆÂèäÂ∑•ÂÖ∑ github
101. ÂêÑÂ§ßÂÖ¨Âè∏ÂÜÖÈÉ®ÈáåÂ§ßÁâõÂàÜ‰∫´ÁöÑÊäÄÊúØÊñáÊ°£ PDF ÊàñËÄÖ PPT github
102. Ëá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêSQLËØ≠Âè•ÔºàËã±ÊñáÔºâ github
103. ‰∏≠ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫ÔºàEDAÔºâÂ∑•ÂÖ∑ github

 Ëã±ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫Â∑•ÂÖ∑ github

104. Âü∫‰∫éÂåªËçØÁü•ËØÜÂõæË∞±ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü github
105. ‰∫¨‰∏úÂïÜÂìÅÁü•ËØÜÂõæË∞± github

Âü∫‰∫é‰∫¨‰∏úÁΩëÁ´ôÁöÑ1300ÁßçÂïÜÂìÅ‰∏ä‰∏ãÁ∫ßÊ¶ÇÂøµÔºåÁ∫¶10‰∏áÂïÜÂìÅÂìÅÁâåÔºåÁ∫¶65‰∏áÂìÅÁâåÈîÄÂîÆÂÖ≥Á≥ªÔºåÂïÜÂìÅÊèèËø∞Áª¥Â∫¶Á≠âÁü•ËØÜÂ∫ìÔºåÂü∫‰∫éËØ•Áü•ËØÜÂ∫ìÂèØ‰ª•ÊîØÊåÅÂïÜÂìÅÂ±ûÊÄßÂ∫ìÊûÑÂª∫ÔºåÂïÜÂìÅÈîÄÂîÆÈóÆÁ≠îÔºåÂìÅÁâåÁâ©ÂìÅÁîü‰∫ßÁ≠âÁü•ËØÜÊü•ËØ¢ÊúçÂä°Ôºå‰πüÂèØÁî®‰∫éÊÉÖÊÑüÂàÜÊûêÁ≠â‰∏ãÊ∏∏Â∫îÁî®Ôºé

106. Âü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆ github

Âü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆÔºåÂåÖÊã¨È£ûË°åÂô®„ÄÅÂ§™Á©∫Ë£ÖÂ§áÁ≠â8Â§ßÁ±ªÔºå100‰ΩôÂ∞èÁ±ªÔºåÂÖ±ËÆ°5800È°πÁöÑÂÜõ‰∫ãÊ≠¶Âô®Áü•ËØÜÂ∫ìÔºåËØ•È°πÁõÆ‰∏ç‰ΩøÁî®ÂõæÊï∞ÊçÆÂ∫ìËøõË°åÂ≠òÂÇ®ÔºåÈÄöËøájiebaËøõË°åÈóÆÂè•Ëß£ÊûêÔºåÈóÆÂè•ÂÆû‰ΩìÈ°πËØÜÂà´ÔºåÂü∫‰∫éÊü•ËØ¢Ê®°ÊùøÂÆåÊàêÂ§öÁ±ªÈóÆÈ¢òÁöÑÊü•ËØ¢Ôºå‰∏ªË¶ÅÊòØÊèê‰æõ‰∏ÄÁßçÂ∑•‰∏öÁïåÁöÑÈóÆÁ≠îÊÄùÊÉ≥demo„ÄÇ

107. Âü∫‰∫éËøúÁõëÁù£ÁöÑ‰∏≠ÊñáÂÖ≥Á≥ªÊäΩÂèñ github
108. ËØ≠Èü≥ÊÉÖÊÑüÂàÜÊûê github
109. ‰∏≠ÊñáULMFiT ÊÉÖÊÑüÂàÜÊûê ÊñáÊú¨ÂàÜÁ±ª ËØ≠ÊñôÂèäÊ®°Âûã github
110. ‰∏Ä‰∏™ÊãçÁÖßÂÅöÈ¢òÁ®ãÂ∫è„ÄÇËæìÂÖ•‰∏ÄÂº†ÂåÖÂê´Êï∞Â≠¶ËÆ°ÁÆóÈ¢òÁöÑÂõæÁâáÔºåËæìÂá∫ËØÜÂà´Âá∫ÁöÑÊï∞Â≠¶ËÆ°ÁÆóÂºè‰ª•ÂèäËÆ°ÁÆóÁªìÊûú github
111. ‰∏ñÁïåÂêÑÂõΩÂ§ßËßÑÊ®°‰∫∫ÂêçÂ∫ì github
112. ‰∏Ä‰∏™Âà©Áî®ÊúâË∂£‰∏≠ÊñáËØ≠ÊñôÂ∫ì qingyun ËÆ≠ÁªÉÂá∫Êù•ÁöÑ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫ github

‰ΩøÁî®‰∫ÜÈùí‰∫ëËØ≠Êñô10‰∏áËØ≠ÊñôÔºåÊú¨repo‰∏≠‰πüÊúâËØ•ËØ≠ÊñôÁöÑÈìæÊé•

113. ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫Ôºå Ê†πÊçÆËá™Â∑±ÁöÑËØ≠ÊñôËÆ≠ÁªÉÂá∫Ëá™Â∑±ÊÉ≥Ë¶ÅÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂèØ‰ª•Áî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅÂú®Á∫øÈóÆÁ≠î„ÄÅÊô∫ËÉΩËÅäÂ§©Á≠âÂú∫ÊôØ github

Ê†πÊçÆËá™Â∑±ÁöÑËØ≠ÊñôËÆ≠ÁªÉÂá∫Ëá™Â∑±ÊÉ≥Ë¶ÅÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ÔºåÂèØ‰ª•Áî®‰∫éÊô∫ËÉΩÂÆ¢Êúç„ÄÅÂú®Á∫øÈóÆÁ≠î„ÄÅÊô∫ËÉΩËÅäÂ§©Á≠âÂú∫ÊôØ„ÄÇÂä†ÂÖ•seqGANÁâàÊú¨„ÄÇ
repo‰∏≠Êèê‰æõ‰∫Ü‰∏Ä‰ªΩË¥®Èáè‰∏çÂ§™È´òÁöÑËØ≠Êñô

114. ÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥® github

ÂõΩÂÆ∂ÁªüËÆ°Â±Ä‰∏≠ÁöÑÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥®ÔºåÈ´òÂæ∑Âú∞ÂõæÁöÑÂùêÊ†áÂíåË°åÊîøÂå∫ÂüüËæπÁïåËåÉÂõ¥ÔºåÂú®ÊµèËßàÂô®ÈáåÈù¢ËøêË°åjs‰ª£Á†ÅÈááÈõÜÁöÑ2019Âπ¥ÂèëÂ∏ÉÁöÑÊúÄÊñ∞Êï∞ÊçÆÔºåÂê´ÈááÈõÜÊ∫êÁ†ÅÔºåÊèê‰æõcsvÊ†ºÂºèÊï∞ÊçÆÔºåÊîØÊåÅcsvËΩ¨ÊàêÁúÅÂ∏ÇÂå∫Â§öÁ∫ßËÅîÂä®js‰ª£Á†Å
ÂùêÊ†á„ÄÅËæπÁïåËåÉÂõ¥„ÄÅÂêçÁß∞„ÄÅÊãºÈü≥„ÄÅË°åÊîøÂå∫Á≠âÂ§öÁ∫ßÂú∞ÂùÄ

115. ÊïôËÇ≤Ë°å‰∏öÊñ∞Èóª Ëá™Âä®ÊñáÊëò ËØ≠ÊñôÂ∫ì github
116. ÂºÄÊîæ‰∫ÜÂØπËØùÊú∫Âô®‰∫∫„ÄÅÁü•ËØÜÂõæË∞±„ÄÅËØ≠‰πâÁêÜËß£„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèäÊï∞ÊçÆ github

Âè¶‰∏Ä‰∏™qaÂØπÁöÑÊú∫Âô®‰∫∫ Amodel-for-Retrivalchatbot - ÂÆ¢ÊúçÊú∫Âô®‰∫∫ÔºåChinese Retreival chatbotÔºà‰∏≠ÊñáÊ£ÄÁ¥¢ÂºèÊú∫Âô®‰∫∫Ôºâ

117. ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºöÂü∫‰∫éÁôæÂ∫¶ÁôæÁßë‰∏≠ÊñáÈ°µÈù¢ÔºåÊäΩÂèñ‰∏âÂÖÉÁªÑ‰ø°ÊÅØÔºåÊûÑÂª∫‰∏≠ÊñáÁü•ËØÜÂõæË∞± github
118. masr: ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´ÔºåÊèê‰æõÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÈ´òËØÜÂà´Áéá github
119. PythonÈü≥È¢ëÊï∞ÊçÆÂ¢ûÂπøÂ∫ì github
120. ‰∏≠ÊñáÂÖ®ËØçË¶ÜÁõñBERTÂèä‰∏§‰ªΩÈòÖËØªÁêÜËß£Êï∞ÊçÆ github

DRCDÊï∞ÊçÆÈõÜÁî±‰∏≠ÂõΩÂè∞ÊπæÂè∞ËææÁ†îÁ©∂Èô¢ÂèëÂ∏ÉÔºåÂÖ∂ÂΩ¢Âºè‰∏éSQuADÁõ∏ÂêåÔºåÊòØÂü∫‰∫éÁπÅ‰Ωì‰∏≠ÊñáÁöÑÊäΩÂèñÂºèÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜ„ÄÇ
CMRC 2018Êï∞ÊçÆÈõÜÊòØÂìàÂ∑•Â§ßËÆØÈ£ûËÅîÂêàÂÆûÈ™åÂÆ§ÂèëÂ∏ÉÁöÑ‰∏≠ÊñáÊú∫Âô®ÈòÖËØªÁêÜËß£Êï∞ÊçÆ„ÄÇÊ†πÊçÆÁªôÂÆöÈóÆÈ¢òÔºåÁ≥ªÁªüÈúÄË¶Å‰ªéÁØáÁ´†‰∏≠ÊäΩÂèñÂá∫ÁâáÊÆµ‰Ωú‰∏∫Á≠îÊ°àÔºåÂΩ¢Âºè‰∏éSQuADÁõ∏Âêå„ÄÇ

121. ConvLabÔºöÂºÄÊ∫êÂ§öÂüüÁ´ØÂà∞Á´ØÂØπËØùÁ≥ªÁªüÂπ≥Âè∞ github
122. ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊï∞ÊçÆÈõÜ github
123. Âü∫‰∫éÊúÄÊñ∞ÁâàÊú¨rasaÊê≠Âª∫ÁöÑÂØπËØùÁ≥ªÁªü github
124. Âü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñ github

Entity and Relation Extraction Based on TensorFlow and BERT. Âü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñÔºå2019ËØ≠Ë®Ä‰∏éÊô∫ËÉΩÊäÄÊúØÁ´ûËµõ‰ø°ÊÅØÊäΩÂèñ‰ªªÂä°Ëß£ÂÜ≥ÊñπÊ°à„ÄÇSchema based Knowledge Extraction, SKE 2019

125. ‰∏Ä‰∏™Â∞èÂûãÁöÑËØÅÂà∏Áü•ËØÜÂõæË∞±/Áü•ËØÜÂ∫ì github
126. Â§çÁõòÊâÄÊúâNLPÊØîËµõÁöÑTOPÊñπÊ°à github
127. OpenCLaPÔºöÂ§öÈ¢ÜÂüüÂºÄÊ∫ê‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã‰ªìÂ∫ì github
ÂåÖÂê´Â¶Ç‰∏ãËØ≠Ë®ÄÊ®°ÂûãÂèäÁôæÂ∫¶ÁôæÁßëÊï∞ÊçÆ

Ê∞ë‰∫ãÊñá‰π¶BERT	bert-base	ÂÖ®ÈÉ®Ê∞ë‰∫ãÊñá‰π¶	2654‰∏áÁØáÊñá‰π¶	22554ËØç	370MB
Âàë‰∫ãÊñá‰π¶BERT	bert-base	ÂÖ®ÈÉ®Âàë‰∫ãÊñá‰π¶	663‰∏áÁØáÊñá‰π¶	22554ËØç	370MB
ÁôæÂ∫¶ÁôæÁßëBERT	bert-base	ÁôæÂ∫¶ÁôæÁßë	903‰∏áÁØáËØçÊù°	22166ËØç	367MB

128. UERÔºöÂü∫‰∫é‰∏çÂêåËØ≠Êñô„ÄÅÁºñÁ†ÅÂô®„ÄÅÁõÆÊ†á‰ªªÂä°ÁöÑ‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ªìÂ∫ìÔºàÂåÖÊã¨BERT„ÄÅGPT„ÄÅELMOÁ≠âÔºâ github

Âü∫‰∫éPyTorchÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÊ°ÜÊû∂ÔºåÊîØÊåÅÂØπÁºñÁ†ÅÂô®ÔºåÁõÆÊ†á‰ªªÂä°Á≠âËøõË°å‰ªªÊÑèÁöÑÁªÑÂêàÔºå‰ªéËÄåÂ§çÁé∞Â∑≤ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÊàñÂú®Â∑≤ÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏äËøõ‰∏ÄÊ≠•ÊîπËøõ„ÄÇÂü∫‰∫éUERËÆ≠ÁªÉ‰∫Ü‰∏çÂêåÊÄßË¥®ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºà‰∏çÂêåËØ≠Êñô„ÄÅÁºñÁ†ÅÂô®„ÄÅÁõÆÊ†á‰ªªÂä°ÔºâÔºåÊûÑÊàê‰∫Ü‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰ªìÂ∫ìÔºåÈÄÇÁî®‰∫é‰∏çÂêåÁöÑÂú∫ÊôØ„ÄÇ

129. ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂêëÈáèÂêàÈõÜ github

ÂåÖÊã¨Â≠óÂêëÈáè,ÊãºÈü≥ÂêëÈáè,ËØçÂêëÈáè,ËØçÊÄßÂêëÈáè,‰æùÂ≠òÂÖ≥Á≥ªÂêëÈáè.ÂÖ±5ÁßçÁ±ªÂûãÁöÑÂêëÈáè

130. Âü∫‰∫éÈáëËûç-Âè∏Ê≥ïÈ¢ÜÂüü(ÂÖºÊúâÈó≤ËÅäÊÄßË¥®)ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫ github

ÂÖ∂‰∏≠ÁöÑ‰∏ªË¶ÅÊ®°ÂùóÊúâ‰ø°ÊÅØÊäΩÂèñ„ÄÅNLU„ÄÅNLG„ÄÅÁü•ËØÜÂõæË∞±Á≠âÔºåÂπ∂‰∏îÂà©Áî®DjangoÊï¥Âêà‰∫ÜÂâçÁ´ØÂ±ïÁ§∫,ÁõÆÂâçÂ∑≤ÁªèÂ∞ÅË£Ö‰∫ÜnlpÂíåkgÁöÑrestfulÊé•Âè£

131. g2pCÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ±âËØ≠ËØªÈü≥Ëá™Âä®Ê†áËÆ∞Ê®°Âùó github
132. Zincbase Áü•ËØÜÂõæË∞±ÊûÑÂª∫Â∑•ÂÖ∑ÂåÖ github
133. ËØóÊ≠åË¥®ÈáèËØÑ‰ª∑/ÁªÜÁ≤íÂ∫¶ÊÉÖÊÑüËØóÊ≠åËØ≠ÊñôÂ∫ì github
134. Âø´ÈÄüËΩ¨Âåñ„Äå‰∏≠ÊñáÊï∞Â≠ó„ÄçÂíå„ÄåÈòøÊãâ‰ºØÊï∞Â≠ó„Äç github

‰∏≠Êñá„ÄÅÈòøÊãâ‰ºØÊï∞Â≠ó‰∫íËΩ¨
‰∏≠Êñá‰∏éÈòøÊãâ‰ºØÊï∞Â≠óÊ∑∑ÂêàÁöÑÊÉÖÂÜµÔºåÂú®ÂºÄÂèë‰∏≠

135. ÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îËØ≠ÊñôÂ∫ì github

Ë∂ÖËøá580‰∏áÁöÑÈóÆÈ¢òÔºå938‰∏áÁöÑÁ≠îÊ°àÔºå5800‰∏™ÂàÜÁ±ªÊ†áÁ≠æ„ÄÇÂü∫‰∫éËØ•ÈóÆÁ≠îËØ≠ÊñôÂ∫ìÔºåÂèØÊîØÊåÅÂ§öÁßçÂ∫îÁî®ÔºåÂ¶ÇÈó≤ËÅäÈóÆÁ≠îÔºåÈÄªËæëÊåñÊéò

136. Âü∫‰∫éÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü github

BERTÂÅöÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÂíåÂè•Â≠êÁõ∏‰ººÂ∫¶ÔºåÂàÜ‰∏∫onlineÂíåoutlineÊ®°Âºè

137. jieba_fast Âä†ÈÄüÁâàÁöÑjieba github

‰ΩøÁî®cpythonÈáçÂÜô‰∫ÜjiebaÂàÜËØçÂ∫ì‰∏≠ËÆ°ÁÆóDAGÂíåHMM‰∏≠ÁöÑvitrebiÂáΩÊï∞ÔºåÈÄüÂ∫¶ÂæóÂà∞Â§ßÂπÖÊèêÂçá

138. Ê≠£ÂàôË°®ËææÂºèÊïôÁ®ã github
139. ‰∏≠ÊñáÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜ github
140. Âü∫‰∫éBERTÁ≠âÊúÄÊñ∞ËØ≠Ë®ÄÊ®°ÂûãÁöÑÊäΩÂèñÂºèÊëòË¶ÅÊèêÂèñ github
141. PythonÂà©Áî®Ê∑±Â∫¶Â≠¶‰π†ËøõË°åÊñáÊú¨ÊëòË¶ÅÁöÑÁªºÂêàÊåáÂçó link
142. Áü•ËØÜÂõæË∞±Ê∑±Â∫¶Â≠¶‰π†Áõ∏ÂÖ≥ËµÑÊñôÊï¥ÁêÜ github

Ê∑±Â∫¶Â≠¶‰π†‰∏éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÅÁü•ËØÜÂõæË∞±„ÄÅÂØπËØùÁ≥ªÁªü„ÄÇÂåÖÊã¨Áü•ËØÜËé∑Âèñ„ÄÅÁü•ËØÜÂ∫ìÊûÑÂª∫„ÄÅÁü•ËØÜÂ∫ìÂ∫îÁî®‰∏âÂ§ßÊäÄÊúØÁ†îÁ©∂‰∏éÂ∫îÁî®

143. Áª¥Âü∫Â§ßËßÑÊ®°Âπ≥Ë°åÊñáÊú¨ËØ≠Êñô github

85ÁßçËØ≠Ë®Ä„ÄÅ1620ÁßçËØ≠Ë®ÄÂØπ„ÄÅ135MÂØπÁÖßÂè•

144. StanfordNLP 0.2.0ÔºöÁ∫ØPythonÁâàËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂåÖ link
145. NeuralNLP-NeuralClassifierÔºöËÖæËÆØÂºÄÊ∫êÊ∑±Â∫¶Â≠¶‰π†ÊñáÊú¨ÂàÜÁ±ªÂ∑•ÂÖ∑ github
146. Á´ØÂà∞Á´ØÁöÑÂ∞ÅÈó≠ÂüüÂØπËØùÁ≥ªÁªü github
147. ‰∏≠ÊñáÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÔºöNeuroNER vs. BertNER github
148. Êñ∞Èóª‰∫ã‰ª∂Á∫øÁ¥¢ÊäΩÂèñ github

An exploration for Eventline (important news Rank organized by pulic time)ÔºåÈíàÂØπÊüê‰∏Ä‰∫ã‰ª∂ËØùÈ¢ò‰∏ãÁöÑÊñ∞ÈóªÊä•ÈÅìÈõÜÂêàÔºåÈÄöËøá‰ΩøÁî®docrankÁÆóÊ≥ïÔºåÂØπÊñ∞ÈóªÊä•ÈÅìËøõË°åÈáçË¶ÅÊÄßËØÜÂà´ÔºåÂπ∂ÈÄöËøáÊñ∞ÈóªÊä•ÈÅìÊó∂Èó¥ÊåëÈÄâÂá∫Êó∂Èó¥Á∫ø‰∏äÈáçË¶ÅÊñ∞Èóª

149. 2019Âπ¥ÁôæÂ∫¶ÁöÑ‰∏âÂÖÉÁªÑÊäΩÂèñÊØîËµõÔºå‚ÄúÁßëÂ≠¶Á©∫Èó¥Èòü‚ÄùÊ∫êÁ†Å(Á¨¨7Âêç) github
150. Âü∫‰∫é‰æùÂ≠òÂè•Ê≥ïÁöÑÂºÄÊîæÂüüÊñáÊú¨Áü•ËØÜ‰∏âÂÖÉÁªÑÊäΩÂèñÂíåÁü•ËØÜÂ∫ìÊûÑÂª∫ github
151. ‰∏≠ÊñáÁöÑGPT2ËÆ≠ÁªÉ‰ª£Á†Å github
152. ML-NLP - Êú∫Âô®Â≠¶‰π†(Machine Learning)„ÄÅNLPÈù¢ËØï‰∏≠Â∏∏ËÄÉÂà∞ÁöÑÁü•ËØÜÁÇπÂíå‰ª£Á†ÅÂÆûÁé∞ github
153. nlp4han:‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ(Êñ≠Âè•/ÂàÜËØç/ËØçÊÄßÊ†áÊ≥®/ÁªÑÂùó/Âè•Ê≥ïÂàÜÊûê/ËØ≠‰πâÂàÜÊûê/NER/NÂÖÉËØ≠Ê≥ï/HMM/‰ª£ËØçÊ∂àËß£/ÊÉÖÊÑüÂàÜÊûê/ÊãºÂÜôÊ£ÄÊü• github
154. XLMÔºöFacebookÁöÑË∑®ËØ≠Ë®ÄÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã github
155. Áî®Âü∫‰∫éBERTÁöÑÂæÆË∞ÉÂíåÁâπÂæÅÊèêÂèñÊñπÊ≥ïÊù•ËøõË°åÁü•ËØÜÂõæË∞±ÁôæÂ∫¶ÁôæÁßë‰∫∫Áâ©ËØçÊù°Â±ûÊÄßÊäΩÂèñ github
156. ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁõ∏ÂÖ≥ÁöÑÂºÄÊîæ‰ªªÂä°ÔºåÊï∞ÊçÆÈõÜ, ‰ª•ÂèäÂΩìÂâçÊúÄ‰Ω≥ÁªìÊûú github
157. CoupletAI - Âü∫‰∫éCNN+Bi-LSTM+Attention ÁöÑËá™Âä®ÂØπÂØπËÅîÁ≥ªÁªü github
158. ÊäΩË±°Áü•ËØÜÂõæË∞±ÔºåÁõÆÂâçËßÑÊ®°50‰∏áÔºåÊîØÊåÅÂêçËØçÊÄßÂÆû‰Ωì„ÄÅÁä∂ÊÄÅÊÄßÊèèËø∞„ÄÅ‰∫ã‰ª∂ÊÄßÂä®‰ΩúËøõË°åÊäΩË±° github
159. MiningZhiDaoQACorpus - 580‰∏áÁôæÂ∫¶Áü•ÈÅìÈóÆÁ≠îÊï∞ÊçÆÊåñÊéòÈ°πÁõÆ github
160. brat rapid annotation tool: Â∫èÂàóÊ†áÊ≥®Â∑•ÂÖ∑ link
161. Â§ßËßÑÊ®°‰∏≠ÊñáÁü•ËØÜÂõæË∞±Êï∞ÊçÆÔºöÔºö1.4‰∫øÂÆû‰Ωì github
162. Êï∞ÊçÆÂ¢ûÂº∫Âú®Êú∫Âô®ÁøªËØëÂèäÂÖ∂‰ªñnlp‰ªªÂä°‰∏≠ÁöÑÂ∫îÁî®ÂèäÊïàÊûú link
163. allennlpÈòÖËØªÁêÜËß£:ÊîØÊåÅÂ§öÁßçÊï∞ÊçÆÂíåÊ®°Âûã github
164. PDFË°®Ê†ºÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑ github
165. GraphbrainÔºöAIÂºÄÊ∫êËΩØ‰ª∂Â∫ìÂíåÁßëÁ†îÂ∑•ÂÖ∑ÔºåÁõÆÁöÑÊòØ‰øÉËøõËá™Âä®ÊÑè‰πâÊèêÂèñÂíåÊñáÊú¨ÁêÜËß£‰ª•ÂèäÁü•ËØÜÁöÑÊé¢Á¥¢ÂíåÊé®Êñ≠ github
166. ÁÆÄÂéÜËá™Âä®Á≠õÈÄâÁ≥ªÁªü github
167. Âü∫‰∫éÂëΩÂêçÂÆû‰ΩìËØÜÂà´ÁöÑÁÆÄÂéÜËá™Âä®ÊëòË¶Å github
168. ‰∏≠ÊñáËØ≠Ë®ÄÁêÜËß£ÊµãËØÑÂü∫ÂáÜÔºåÂåÖÊã¨‰ª£Ë°®ÊÄßÁöÑÊï∞ÊçÆÈõÜ&Âü∫ÂáÜÊ®°Âûã&ËØ≠ÊñôÂ∫ì&ÊéíË°åÊ¶ú github
169. Ê†ëÊ¥û OCR ÊñáÂ≠óËØÜÂà´ github

‰∏Ä‰∏™c++ OCR github


170. ‰ªéÂåÖÂê´Ë°®Ê†ºÁöÑÊâ´ÊèèÂõæÁâá‰∏≠ËØÜÂà´Ë°®Ê†ºÂíåÊñáÂ≠ó github
171. ËØ≠Â£∞ËøÅÁßª github
172. PythonÂè£ËØ≠Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ(Ëã±Êñá) github
173. similarityÔºöÁõ∏‰ººÂ∫¶ËÆ°ÁÆóÂ∑•ÂÖ∑ÂåÖÔºåjavaÁºñÂÜô github

Áî®‰∫éËØçËØ≠„ÄÅÁü≠ËØ≠„ÄÅÂè•Â≠ê„ÄÅËØçÊ≥ïÂàÜÊûê„ÄÅÊÉÖÊÑüÂàÜÊûê„ÄÅËØ≠‰πâÂàÜÊûêÁ≠âÁõ∏ÂÖ≥ÁöÑÁõ∏‰ººÂ∫¶ËÆ°ÁÆó

174. Êµ∑Èáè‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉALBERTÊ®°Âûã github
175. Transformers 2.0 github

ÊîØÊåÅTensorFlow 2.0 Âíå PyTorch ÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) 8ÁßçÊû∂ÊûÑ/33ÁßçÈ¢ÑËÆ≠ÁªÉÊ®°Âûã/102ÁßçËØ≠Ë®Ä

176. Âü∫‰∫éÂ§ßËßÑÊ®°Èü≥È¢ëÊï∞ÊçÆÈõÜAudiosetÁöÑÈü≥È¢ëÂ¢ûÂº∫ github
177. PoplarÔºöÁΩëÈ°µÁâàËá™ÁÑ∂ËØ≠Ë®ÄÊ†áÊ≥®Â∑•ÂÖ∑ github
178. ÂõæÁâáÊñáÂ≠óÂéªÈô§ÔºåÂèØÁî®‰∫éÊº´ÁîªÁøªËØë github
179. 186ÁßçËØ≠Ë®ÄÁöÑÊï∞Â≠óÂè´Ê≥ïÂ∫ì github
180. AmazonÂèëÂ∏ÉÂü∫‰∫éÁü•ËØÜÁöÑ‰∫∫-‰∫∫ÂºÄÊîæÈ¢ÜÂüüÂØπËØùÊï∞ÊçÆÈõÜ github
181. ‰∏≠ÊñáÊñáÊú¨Á∫†ÈîôÊ®°Âùó‰ª£Á†Å github
182. ÁπÅÁÆÄ‰ΩìËΩ¨Êç¢ github
183. PythonÂÆûÁé∞ÁöÑÂ§öÁßçÊñáÊú¨ÂèØËØªÊÄßËØÑ‰ª∑ÊåáÊ†á github
184. Á±ª‰ºº‰∫é‰∫∫Âêç/Âú∞Âêç/ÁªÑÁªáÊú∫ÊûÑÂêçÁöÑÂëΩÂêç‰ΩìËØÜÂà´Êï∞ÊçÆÈõÜ github
185. ‰∏úÂçóÂ§ßÂ≠¶„ÄäÁü•ËØÜÂõæË∞±„ÄãÁ†îÁ©∂ÁîüËØæÁ®ã(ËµÑÊñô) github
186. Ëã±ÊñáÊãºÂÜôÊ£ÄÊü•Â∫ì github
from spellchecker import SpellChecker

spell = SpellChecker()

# find those words that may be misspelled
misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])

for word in misspelled:
    # Get the one `most likely` answer
    print(spell.correction(word))

    # Get a list of `likely` options
    print(spell.candidates(word))

187. wwsearchÊòØ‰ºÅ‰∏öÂæÆ‰ø°ÂêéÂè∞Ëá™Á†îÁöÑÂÖ®ÊñáÊ£ÄÁ¥¢ÂºïÊìé github
188. CHAMELEONÔºöÊ∑±Â∫¶Â≠¶‰π†Êñ∞ÈóªÊé®ËçêÁ≥ªÁªüÂÖÉÊû∂ÊûÑ github
189. 8ÁØáËÆ∫ÊñáÊ¢≥ÁêÜBERTÁõ∏ÂÖ≥Ê®°ÂûãËøõÂ±ï‰∏éÂèçÊÄù github
190. DocSearchÔºöÂÖçË¥πÊñáÊ°£ÊêúÁ¥¢ÂºïÊìé github
191. LIDAÔºöËΩªÈáè‰∫§‰∫íÂºèÂØπËØùÊ†áÊ≥®Â∑•ÂÖ∑ github
192. aili - the fastest in-memory index in the East ‰∏úÂçäÁêÉÊúÄÂø´Âπ∂ÂèëÁ¥¢Âºï github
","GitHub - fighting41love/funNLP: ‰∏≠Ëã±ÊñáÊïèÊÑüËØç„ÄÅËØ≠Ë®ÄÊ£ÄÊµã„ÄÅ‰∏≠Â§ñÊâãÊú∫/ÁîµËØùÂΩíÂ±ûÂú∞/ËøêËê•ÂïÜÊü•ËØ¢„ÄÅÂêçÂ≠óÊé®Êñ≠ÊÄßÂà´„ÄÅÊâãÊú∫Âè∑ÊäΩÂèñ„ÄÅË∫´‰ªΩËØÅÊäΩÂèñ„ÄÅÈÇÆÁÆ±ÊäΩÂèñ„ÄÅ‰∏≠Êó•Êñá‰∫∫ÂêçÂ∫ì„ÄÅ‰∏≠ÊñáÁº©ÂÜôÂ∫ì„ÄÅÊãÜÂ≠óËØçÂÖ∏„ÄÅËØçÊ±áÊÉÖÊÑüÂÄº„ÄÅÂÅúÁî®ËØç„ÄÅÂèçÂä®ËØçË°®„ÄÅÊö¥ÊÅêËØçË°®„ÄÅÁπÅÁÆÄ‰ΩìËΩ¨Êç¢„ÄÅËã±ÊñáÊ®°Êãü‰∏≠ÊñáÂèëÈü≥„ÄÅÊ±™Â≥∞Ê≠åËØçÁîüÊàêÂô®„ÄÅËÅå‰∏öÂêçÁß∞ËØçÂ∫ì„ÄÅÂêå‰πâËØçÂ∫ì„ÄÅÂèç‰πâËØçÂ∫ì„ÄÅÂê¶ÂÆöËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ÂìÅÁâåËØçÂ∫ì„ÄÅÊ±ΩËΩ¶Èõ∂‰ª∂ËØçÂ∫ì„ÄÅËøûÁª≠Ëã±ÊñáÂàáÂâ≤„ÄÅÂêÑÁßç‰∏≠ÊñáËØçÂêëÈáè„ÄÅÂÖ¨Âè∏ÂêçÂ≠óÂ§ßÂÖ®„ÄÅÂè§ËØóËØçÂ∫ì„ÄÅITËØçÂ∫ì„ÄÅË¥¢ÁªèËØçÂ∫ì„ÄÅÊàêËØ≠ËØçÂ∫ì„ÄÅÂú∞ÂêçËØçÂ∫ì„ÄÅÂéÜÂè≤Âêç‰∫∫ËØçÂ∫ì„ÄÅËØóËØçËØçÂ∫ì„ÄÅÂåªÂ≠¶ËØçÂ∫ì„ÄÅÈ•ÆÈ£üËØçÂ∫ì„ÄÅÊ≥ïÂæãËØçÂ∫ì„ÄÅÊ±ΩËΩ¶ËØçÂ∫ì„ÄÅÂä®Áâ©ËØçÂ∫ì„ÄÅ‰∏≠ÊñáËÅäÂ§©ËØ≠Êñô„ÄÅ‰∏≠ÊñáË∞£Ë®ÄÊï∞ÊçÆ„ÄÅÁôæÂ∫¶‰∏≠ÊñáÈóÆÁ≠îÊï∞ÊçÆÈõÜ„ÄÅÂè•Â≠êÁõ∏‰ººÂ∫¶ÂåπÈÖçÁÆóÊ≥ïÈõÜÂêà„ÄÅbertËµÑÊ∫ê„ÄÅÊñáÊú¨ÁîüÊàê&ÊëòË¶ÅÁõ∏ÂÖ≥Â∑•ÂÖ∑„ÄÅcocoNLP‰ø°ÊÅØÊäΩÂèñÂ∑•ÂÖ∑„ÄÅÂõΩÂÜÖÁîµËØùÂè∑Á†ÅÊ≠£ÂàôÂåπÈÖç„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶XLORE:‰∏≠Ëã±ÊñáË∑®ËØ≠Ë®ÄÁôæÁßëÁü•ËØÜÂõæË∞±„ÄÅÊ∏ÖÂçéÂ§ßÂ≠¶‰∫∫Â∑•Êô∫ËÉΩÊäÄÊúØÁ≥ªÂàóÊä•Âëä„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàê„ÄÅNLUÂ§™Èöæ‰∫ÜÁ≥ªÂàó„ÄÅËá™Âä®ÂØπËÅîÊï∞ÊçÆÂèäÊú∫Âô®‰∫∫„ÄÅÁî®Êà∑ÂêçÈªëÂêçÂçïÂàóË°®„ÄÅÁΩ™ÂêçÊ≥ïÂä°ÂêçËØçÂèäÂàÜÁ±ªÊ®°Âûã„ÄÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ËØ≠Êñô„ÄÅcs224nÊ∑±Â∫¶Â≠¶‰π†Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜËØæÁ®ã„ÄÅ‰∏≠ÊñáÊâãÂÜôÊ±âÂ≠óËØÜÂà´„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ ËØ≠Êñô/Êï∞ÊçÆÈõÜ„ÄÅÂèòÈáèÂëΩÂêçÁ•ûÂô®„ÄÅÂàÜËØçËØ≠ÊñôÂ∫ì+‰ª£Á†Å„ÄÅ‰ªªÂä°ÂûãÂØπËØùËã±ÊñáÊï∞ÊçÆÈõÜ„ÄÅASR ËØ≠Èü≥Êï∞ÊçÆÈõÜ + Âü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁöÑ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´Á≥ªÁªü„ÄÅÁ¨ëÂ£∞Ê£ÄÊµãÂô®„ÄÅMicrosoftÂ§öËØ≠Ë®ÄÊï∞Â≠ó/Âçï‰Ωç/Â¶ÇÊó•ÊúüÊó∂Èó¥ËØÜÂà´ÂåÖ„ÄÅ‰∏≠ÂçéÊñ∞ÂçéÂ≠óÂÖ∏Êï∞ÊçÆÂ∫ìÂèäapi(ÂåÖÊã¨Â∏∏Áî®Ê≠áÂêéËØ≠„ÄÅÊàêËØ≠„ÄÅËØçËØ≠ÂíåÊ±âÂ≠ó)„ÄÅÊñáÊ°£ÂõæË∞±Ëá™Âä®ÁîüÊàê„ÄÅSpaCy ‰∏≠ÊñáÊ®°Âûã„ÄÅCommon VoiceËØ≠Èü≥ËØÜÂà´Êï∞ÊçÆÈõÜÊñ∞Áâà„ÄÅÁ•ûÁªèÁΩëÁªúÂÖ≥Á≥ªÊäΩÂèñ„ÄÅÂü∫‰∫ébertÁöÑÂëΩÂêçÂÆû‰ΩìËØÜÂà´„ÄÅÂÖ≥ÈîÆËØç(Keyphrase)ÊäΩÂèñÂåÖpke„ÄÅÂü∫‰∫éÂåªÁñóÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÅÂü∫‰∫é‰æùÂ≠òÂè•Ê≥ï‰∏éËØ≠‰πâËßíËâ≤Ê†áÊ≥®ÁöÑ‰∫ã‰ª∂‰∏âÂÖÉÁªÑÊäΩÂèñ„ÄÅ‰æùÂ≠òÂè•Ê≥ïÂàÜÊûê4‰∏áÂè•È´òË¥®ÈáèÊ†áÊ≥®Êï∞ÊçÆ„ÄÅcnocrÔºöÁî®Êù•ÂÅö‰∏≠ÊñáOCRÁöÑPython3ÂåÖ„ÄÅ‰∏≠Êñá‰∫∫Áâ©ÂÖ≥Á≥ªÁü•ËØÜÂõæË∞±È°πÁõÆ„ÄÅ‰∏≠ÊñánlpÁ´ûËµõÈ°πÁõÆÂèä‰ª£Á†ÅÊ±áÊÄª„ÄÅ‰∏≠ÊñáÂ≠óÁ¨¶Êï∞ÊçÆ„ÄÅspeech-aligner: ‰ªé‚Äú‰∫∫Â£∞ËØ≠Èü≥‚ÄùÂèäÂÖ∂‚ÄúËØ≠Ë®ÄÊñáÊú¨‚Äù‰∫ßÁîüÈü≥Á¥†Á∫ßÂà´Êó∂Èó¥ÂØπÈΩêÊ†áÊ≥®ÁöÑÂ∑•ÂÖ∑„ÄÅAmpliGraph: Áü•ËØÜÂõæË∞±Ë°®Á§∫Â≠¶‰π†(Python)Â∫ìÔºöÁü•ËØÜÂõæË∞±Ê¶ÇÂøµÈìæÊé•È¢ÑÊµã„ÄÅScattertext ÊñáÊú¨ÂèØËßÜÂåñ(python)„ÄÅËØ≠Ë®Ä/Áü•ËØÜË°®Á§∫Â∑•ÂÖ∑ÔºöBERT & ERNIE„ÄÅ‰∏≠ÊñáÂØπÊØîËã±ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜNLPÁöÑÂå∫Âà´ÁªºËø∞„ÄÅSynonyms‰∏≠ÊñáËøë‰πâËØçÂ∑•ÂÖ∑ÂåÖ„ÄÅHarvestTextÈ¢ÜÂüüËá™ÈÄÇÂ∫îÊñáÊú¨ÊåñÊéòÂ∑•ÂÖ∑ÔºàÊñ∞ËØçÂèëÁé∞-ÊÉÖÊÑüÂàÜÊûê-ÂÆû‰ΩìÈìæÊé•Á≠âÔºâ„ÄÅword2wordÔºö(Python)Êñπ‰æøÊòìÁî®ÁöÑÂ§öËØ≠Ë®ÄËØç-ËØçÂØπÈõÜÔºö62ÁßçËØ≠Ë®Ä/3,564‰∏™Â§öËØ≠Ë®ÄÂØπ„ÄÅËØ≠Èü≥ËØÜÂà´ËØ≠ÊñôÁîüÊàêÂ∑•ÂÖ∑Ôºö‰ªéÂÖ∑ÊúâÈü≥È¢ë/Â≠óÂπïÁöÑÂú®Á∫øËßÜÈ¢ëÂàõÂª∫Ëá™Âä®ËØ≠Èü≥ËØÜÂà´(ASR)ËØ≠ÊñôÂ∫ì„ÄÅÊûÑÂª∫ÂåªÁñóÂÆû‰ΩìËØÜÂà´ÁöÑÊ®°ÂûãÔºàÂåÖÂê´ËØçÂÖ∏ÂíåËØ≠ÊñôÊ†áÊ≥®Ôºâ„ÄÅÂçïÊñáÊ°£ÈùûÁõëÁù£ÁöÑÂÖ≥ÈîÆËØçÊäΩÂèñ„ÄÅKashgari‰∏≠‰ΩøÁî®gpt-2ËØ≠Ë®ÄÊ®°Âûã„ÄÅÂºÄÊ∫êÁöÑÈáëËûçÊäïËµÑÊï∞ÊçÆÊèêÂèñÂ∑•ÂÖ∑„ÄÅÊñáÊú¨Ëá™Âä®ÊëòË¶ÅÂ∫ìTextTeaser: ‰ªÖÊîØÊåÅËã±Êñá„ÄÅ‰∫∫Ê∞ëÊó•Êä•ËØ≠ÊñôÂ§ÑÁêÜÂ∑•ÂÖ∑ÈõÜ„ÄÅ‰∏Ä‰∫õÂÖ≥‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁöÑÂü∫Êú¨Ê®°Âûã„ÄÅÂü∫‰∫é14WÊ≠åÊõ≤Áü•ËØÜÂ∫ìÁöÑÈóÆÁ≠îÂ∞ùËØï--ÂäüËÉΩÂåÖÊã¨Ê≠åËØçÊé•ÈæôandÂ∑≤Áü•Ê≠åËØçÊâæÊ≠åÊõ≤‰ª•ÂèäÊ≠åÊõ≤Ê≠åÊâãÊ≠åËØç‰∏âËßíÂÖ≥Á≥ªÁöÑÈóÆÁ≠î„ÄÅÂü∫‰∫éSiamese bilstmÊ®°ÂûãÁöÑÁõ∏‰ººÂè•Â≠êÂà§ÂÆöÊ®°ÂûãÂπ∂Êèê‰æõËÆ≠ÁªÉÊï∞ÊçÆÈõÜÂíåÊµãËØïÊï∞ÊçÆÈõÜ„ÄÅÁî®TransformerÁºñËß£Á†ÅÊ®°ÂûãÂÆûÁé∞ÁöÑÊ†πÊçÆHacker NewsÊñáÁ´†Ê†áÈ¢òËá™Âä®ÁîüÊàêËØÑËÆ∫„ÄÅÁî®BERTËøõË°åÂ∫èÂàóÊ†áËÆ∞ÂíåÊñáÊú¨ÂàÜÁ±ªÁöÑÊ®°Êùø‰ª£Á†Å„ÄÅLitBankÔºöNLPÊï∞ÊçÆÈõÜ‚Äî‚ÄîÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåËÆ°ÁÆó‰∫∫ÊñáÂ≠¶Áßë‰ªªÂä°ÁöÑ100ÈÉ®Â∏¶Ê†áËÆ∞Ëã±ÊñáÂ∞èËØ¥ËØ≠Êñô„ÄÅÁôæÂ∫¶ÂºÄÊ∫êÁöÑÂü∫ÂáÜ‰ø°ÊÅØÊäΩÂèñÁ≥ªÁªü„ÄÅËôöÂÅáÊñ∞ÈóªÊï∞ÊçÆÈõÜ„ÄÅFacebook: LAMAËØ≠Ë®ÄÊ®°ÂûãÂàÜÊûêÔºåÊèê‰æõTransformer-XL/BERT/ELMo/GPTÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÁªü‰∏ÄËÆøÈóÆÊé•Âè£„ÄÅCommonsenseQAÔºöÈù¢ÂêëÂ∏∏ËØÜÁöÑËã±ÊñáQAÊåëÊàò„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ËµÑÊñô„ÄÅÊï∞ÊçÆÂèäÂ∑•ÂÖ∑„ÄÅÂêÑÂ§ßÂÖ¨Âè∏ÂÜÖÈÉ®ÈáåÂ§ßÁâõÂàÜ‰∫´ÁöÑÊäÄÊúØÊñáÊ°£ PDF ÊàñËÄÖ PPT„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêSQLËØ≠Âè•ÔºàËã±ÊñáÔºâ„ÄÅ‰∏≠ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫ÔºàEDAÔºâÂ∑•ÂÖ∑„ÄÅËã±ÊñáNLPÊï∞ÊçÆÂ¢ûÂº∫Â∑•ÂÖ∑ „ÄÅÂü∫‰∫éÂåªËçØÁü•ËØÜÂõæË∞±ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÅ‰∫¨‰∏úÂïÜÂìÅÁü•ËØÜÂõæË∞±„ÄÅÂü∫‰∫émongodbÂ≠òÂÇ®ÁöÑÂÜõ‰∫ãÈ¢ÜÂüüÁü•ËØÜÂõæË∞±ÈóÆÁ≠îÈ°πÁõÆ„ÄÅÂü∫‰∫éËøúÁõëÁù£ÁöÑ‰∏≠ÊñáÂÖ≥Á≥ªÊäΩÂèñ„ÄÅËØ≠Èü≥ÊÉÖÊÑüÂàÜÊûê„ÄÅ‰∏≠ÊñáULMFiT-ÊÉÖÊÑüÂàÜÊûê-ÊñáÊú¨ÂàÜÁ±ª-ËØ≠ÊñôÂèäÊ®°Âûã„ÄÅ‰∏Ä‰∏™ÊãçÁÖßÂÅöÈ¢òÁ®ãÂ∫è„ÄÅ‰∏ñÁïåÂêÑÂõΩÂ§ßËßÑÊ®°‰∫∫ÂêçÂ∫ì„ÄÅ‰∏Ä‰∏™Âà©Áî®ÊúâË∂£‰∏≠ÊñáËØ≠ÊñôÂ∫ì qingyun ËÆ≠ÁªÉÂá∫Êù•ÁöÑ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅ‰∏≠ÊñáËÅäÂ§©Êú∫Âô®‰∫∫seqGAN„ÄÅÁúÅÂ∏ÇÂå∫ÈïáË°åÊîøÂå∫ÂàíÊï∞ÊçÆÂ∏¶ÊãºÈü≥Ê†áÊ≥®„ÄÅÊïôËÇ≤Ë°å‰∏öÊñ∞ÈóªËØ≠ÊñôÂ∫ìÂåÖÂê´Ëá™Âä®ÊñáÊëòÂäüËÉΩ„ÄÅÂºÄÊîæ‰∫ÜÂØπËØùÊú∫Âô®‰∫∫-Áü•ËØÜÂõæË∞±-ËØ≠‰πâÁêÜËß£-Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂ∑•ÂÖ∑ÂèäÊï∞ÊçÆ„ÄÅ‰∏≠ÊñáÁü•ËØÜÂõæË∞±ÔºöÂü∫‰∫éÁôæÂ∫¶ÁôæÁßë‰∏≠ÊñáÈ°µÈù¢-ÊäΩÂèñ‰∏âÂÖÉÁªÑ‰ø°ÊÅØ-ÊûÑÂª∫‰∏≠ÊñáÁü•ËØÜÂõæË∞±„ÄÅmasr: ‰∏≠ÊñáËØ≠Èü≥ËØÜÂà´-Êèê‰æõÈ¢ÑËÆ≠ÁªÉÊ®°Âûã-È´òËØÜÂà´Áéá„ÄÅPythonÈü≥È¢ëÊï∞ÊçÆÂ¢ûÂπøÂ∫ì„ÄÅ‰∏≠ÊñáÂÖ®ËØçË¶ÜÁõñBERTÂèä‰∏§‰ªΩÈòÖËØªÁêÜËß£Êï∞ÊçÆ„ÄÅConvLabÔºöÂºÄÊ∫êÂ§öÂüüÁ´ØÂà∞Á´ØÂØπËØùÁ≥ªÁªüÂπ≥Âè∞„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÊï∞ÊçÆÈõÜ„ÄÅÂü∫‰∫éÊúÄÊñ∞ÁâàÊú¨rasaÊê≠Âª∫ÁöÑÂØπËØùÁ≥ªÁªü„ÄÅÂü∫‰∫éTensorFlowÂíåBERTÁöÑÁÆ°ÈÅìÂºèÂÆû‰ΩìÂèäÂÖ≥Á≥ªÊäΩÂèñ„ÄÅ‰∏Ä‰∏™Â∞èÂûãÁöÑËØÅÂà∏Áü•ËØÜÂõæË∞±/Áü•ËØÜÂ∫ì„ÄÅÂ§çÁõòÊâÄÊúâNLPÊØîËµõÁöÑTOPÊñπÊ°à„ÄÅOpenCLaPÔºöÂ§öÈ¢ÜÂüüÂºÄÊ∫ê‰∏≠ÊñáÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã‰ªìÂ∫ì„ÄÅ‰∏≠ÊñáËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂêëÈáèÂêàÈõÜ„ÄÅÂü∫‰∫éÈáëËûç-Âè∏Ê≥ïÈ¢ÜÂüü(ÂÖºÊúâÈó≤ËÅäÊÄßË¥®)ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫„ÄÅg2pCÔºöÂü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ±âËØ≠ËØªÈü≥Ëá™Âä®Ê†áËÆ∞Ê®°Âùó„ÄÅZincbase Áü•ËØÜÂõæË∞±ÊûÑÂª∫Â∑•ÂÖ∑ÂåÖ„ÄÅËØóÊ≠åË¥®ÈáèËØÑ‰ª∑/ÁªÜÁ≤íÂ∫¶ÊÉÖÊÑüËØóÊ≠åËØ≠ÊñôÂ∫ì„ÄÇ"
64,Python,"This repository is archived.
This repository is archived and will not receive any updates or accept issues or pull requests.
To report bugs in reddit.com please make a post in /r/bugs.
If you have found a bug that can in some way compromise the security of the
site or its users, please exercise responsible
disclosure and e-mail
security@reddit.com.

API
For notices about reddit API changes and discussion of reddit API client development, subscribe to the /r/redditdev and /r/changelog subreddits.
To learn more about reddit's API, check out our automated API documentation and the API wiki page. Please use a unique User-Agent string and take care to abide by our API rules.
Quickstart
To set up your own instance of reddit see the install guide.
",GitHub - reddit-archive/reddit: historical code from reddit.com
65,Python,"Âä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†

Êú¨‰π¶ÁΩëÂùÄÔºözh.d2l.ai | 1.0.0Áâàrc0ÂèëÂ∏É | Â¶Ç‰ΩïÂÆâË£ÖÂíå‰ΩøÁî®‰π¶‰∏≠Ê∫ê‰ª£Á†Å
Êõ¥Êñ∞
Ëã±ÊñáÁâàÂÖ®Èù¢ÊîπËøõ‰∫ÜÈ¢ÑÂ§áÁü•ËØÜ‰∏ÄÁ´†Ôºå
Êñ∞Â¢û‰∫ÜÊé®ËçêÁ≥ªÁªü‰∏ÄÁ´†ÂíåÊ∑±Â∫¶Â≠¶‰π†ÁöÑÊï∞Â≠¶‰∏ÄÁ´†„ÄÇ
Ê¨¢ËøéÂÖ≥Ê≥®Ëã±ÊñáÁâàÂºÄÊ∫êÈ°πÁõÆÔºöhttps://github.com/d2l-ai/d2l-en
Ëã±ÊñáÁâà Dive into Deep Learning
Âä†Â∑ûÂ§ßÂ≠¶‰ºØÂÖãÂà©ÂàÜÊ†° 2019 Âπ¥Êò•Â≠¶Êúü Introduction to Deep Learning ËØæÁ®ãÊïôÊùêÔºà‰∏≠ÊñáÁâàËØæ‰ª∂ÔºàÂÜÖÂê´ÊïôÂ≠¶ËßÜÈ¢ëÂú∞ÂùÄÔºâÔºâ„ÄÇ
Ëã±ÊñáÁâàÂºïÁî®
BibTeX entry:
@book{zhang2019dive,
    title={Dive into Deep Learning},
    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
    note={\url{http://www.d2l.ai}},
    year={2019}
}

Ë¥°ÁåÆ
ÊÑüË∞¢Á§æÂå∫Ë¥°ÁåÆËÄÖ‰ª¨‰∏∫ÊØè‰∏Ä‰ΩçËØªËÄÖÊîπËøõËøôÊú¨ÂºÄÊ∫ê‰π¶„ÄÇ
Â¶Ç‰ΩïË¥°ÁåÆ | Ëá¥Ë∞¢ | ËÆ®ËÆ∫ÊàñÊä•ÂëäÈóÆÈ¢ò | ÂÖ∂‰ªñ
",GitHub - d2l-ai/d2l-zh: „ÄäÂä®ÊâãÂ≠¶Ê∑±Â∫¶Â≠¶‰π†„ÄãÔºöÈù¢Âêë‰∏≠ÊñáËØªËÄÖ„ÄÅËÉΩËøêË°å„ÄÅÂèØËÆ®ËÆ∫„ÄÇËã±ÊñáÁâàÂç≥‰ºØÂÖãÂà©‚ÄúÊ∑±Â∫¶Â≠¶‰π†ÂØºËÆ∫‚ÄùÊïôÊùê„ÄÇ
66,Python,"Mask R-CNN for Object Detection and Segmentation
This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.

The repository includes:

Source code of Mask R-CNN built on FPN and ResNet101.
Training code for MS COCO
Pre-trained weights for MS COCO
Jupyter notebooks to visualize the detection pipeline at every step
ParallelModel class for multi-GPU training
Evaluation on MS COCO metrics (AP)
Example of training on your own dataset

The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well.
This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here.
Getting Started


demo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images.
It includes code to run object detection and instance segmentation on arbitrary images.


train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.


(model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.


inspect_data.ipynb. This notebook visualizes the different pre-processing steps
to prepare the training data.


inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.


inspect_weights.ipynb
This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.


Step by Step Detection
To help with debugging and understanding the model, there are 3 notebooks
(inspect_data.ipynb, inspect_model.ipynb,
inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples:
1. Anchor sorting and filtering
Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.

2. Bounding Box Refinement
This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.

3. Mask Generation
Examples of generated masks. These then get scaled and placed on the image in the right location.

4.Layer activations
Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).

5. Weight Histograms
Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.

6. Logging to TensorBoard
TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.

6. Composing the different pieces into a final result

Training on MS COCO
We're providing pre-trained weights for MS COCO to make it easier to start. You can
use those weights as a starting point to train your own variation on the network.
Training and evaluation code is in samples/coco/coco.py. You can import this
module in Jupyter notebook (see the provided notebooks for examples) or you
can run it directly from the command line as such:
# Train a new model starting from pre-trained COCO weights
python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco

# Train a new model starting from ImageNet weights
python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet

# Continue training a model that you had trained earlier
python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5

# Continue training the last model you trained. This will find
# the last trained weights in the model directory.
python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last

You can also run the COCO evaluation code with:
# Run COCO evaluation on the last trained model
python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last

The training schedule, learning rate, and other parameters should be set in samples/coco/coco.py.
Training on Your Own Dataset
Start by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application.
In summary, to train the model on your own dataset you'll need to extend two classes:
Config
This class contains the default configuration. Subclass it and modify the attributes you need to change.
Dataset
This class provides a consistent way to work with any dataset.
It allows you to use new datasets for training without having to change
the code of the model. It also supports loading multiple datasets at the
same time, which is useful if the objects you want to detect are not
all available in one dataset.
See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py.
Differences from the Official Paper
This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.


Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.


Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation.
To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset.
We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more,
and only 0.01% differed by 10px or more.


Learning Rate: The paper uses a learning rate of 0.02, but we found that to be
too high, and often causes the weights to explode, especially when using a small batch
size. It might be related to differences between how Caffe and TensorFlow compute
gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient
clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively.
We found that smaller learning rates converge faster anyway so we go with that.


Citation
Use this bibtex to cite this repository:
@misc{matterport_maskrcnn_2017,
  title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},
  author={Waleed Abdulla},
  year={2017},
  publisher={Github},
  journal={GitHub repository},
  howpublished={\url{https://github.com/matterport/Mask_RCNN}},
}

Contributing
Contributions to this repository are welcome. Examples of things you can contribute:

Speed Improvements. Like re-writing some Python code in TensorFlow or Cython.
Training on other datasets.
Accuracy Improvements.
Visualizations and examples.

You can also join our team and help us build even more projects like this one.
Requirements
Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt.
MS COCO Requirements:
To train or test on MS COCO, you'll also need:

pycocotools (installation instructions below)
MS COCO Dataset
Download the 5K minival
and the 35K validation-minus-minival
subsets. More details in the original Faster R-CNN implementation.

If you use Docker, the code has been verified to work on
this Docker container.
Installation


Clone this repository


Install dependencies
pip3 install -r requirements.txt


Run setup from the repository root directory
python3 setup.py install


Download pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.


(Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).

Linux: https://github.com/waleedka/coco
Windows: https://github.com/philferriere/cocoapi.
You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)



Projects Using this Model
If you extend this model to other datasets or build projects that use it, we'd love to hear from you.
4K Video Demo by Karol Majek.

Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.

Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.

Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science Bowl
Code is in the samples/nucleus directory.

Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.

Reconstructing 3D buildings from aerial LiDAR
A proof of concept project by Esri, in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.

Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy
A project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.
 
Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery
Research project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.

Mask-RCNN Shiny
A computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.

Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.

GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ond≈ôej Pe≈°ek.

",GitHub - matterport/Mask_RCNN: Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow
67,Python,"ZeroNet   
Decentralized websites using Bitcoin crypto and the BitTorrent network - https://zeronet.io
Why?

We believe in open, free, and uncensored network and communication.
No single point of failure: Site remains online so long as at least 1 peer is
serving it.
No hosting costs: Sites are served by visitors.
Impossible to shut down: It's nowhere because it's everywhere.
Fast and works offline: You can access the site even if Internet is
unavailable.

Features

Real-time updated sites
Namecoin .bit domains support
Easy to setup: unpack & run
Clone websites in one click
Password-less BIP32
based authorization: Your account is protected by the same cryptography as your Bitcoin wallet
Built-in SQL server with P2P data synchronization: Allows easier site development and faster page load times
Anonymity: Full Tor network support with .onion hidden services instead of IPv4 addresses
TLS encrypted connections
Automatic uPnP port opening
Plugin for multiuser (openproxy) support
Works with any browser/OS

How does it work?

After starting zeronet.py you will be able to visit zeronet sites using
http://127.0.0.1:43110/{zeronet_address} (eg.
http://127.0.0.1:43110/1HeLLo4uzjaLetFx6NH3PMwFP3qbRbTf3D).
When you visit a new zeronet site, it tries to find peers using the BitTorrent
network so it can download the site files (html, css, js...) from them.
Each visited site is also served by you.
Every site contains a content.json file which holds all other files in a sha512 hash
and a signature generated using the site's private key.
If the site owner (who has the private key for the site address) modifies the
site, then he/she signs the new content.json and publishes it to the peers.
Afterwards, the peers verify the content.json integrity (using the
signature), they download the modified files and publish the new content to
other peers.

Slideshow about ZeroNet cryptography, site updates, multi-user sites ¬ª
Frequently asked questions ¬ª
ZeroNet Developer Documentation ¬ª
Screenshots


More screenshots in ZeroNet docs ¬ª
How to join
Windows

Download ZeroNet-py3-win64.zip (18MB)
Unpack anywhere
Run ZeroNet.exe

macOS

Download ZeroNet-dist-mac.zip (13.2MB)
Unpack anywhere
Run ZeroNet.app

Linux (x86-64bit)

wget https://github.com/HelloZeroNet/ZeroNet-linux/archive/dist-linux64/ZeroNet-py3-linux64.tar.gz
tar xvpfz ZeroNet-py3-linux64.tar.gz
cd ZeroNet-linux-dist-linux64/
Start with: ./ZeroNet.sh
Open the ZeroHello landing page in your browser by navigating to: http://127.0.0.1:43110/

Tip: Start with ./ZeroNet.sh --ui_ip '*' --ui_restrict your.ip.address to allow remote connections on the web interface.
Install from source

wget https://github.com/HelloZeroNet/ZeroNet/archive/py3/ZeroNet-py3.tar.gz
tar xvpfz ZeroNet-py3.tar.gz
cd ZeroNet-py3
sudo apt-get update
sudo apt-get install python3-pip
sudo python3 -m pip install -r requirements.txt
Start with: python3 zeronet.py
Open the ZeroHello landing page in your browser by navigating to: http://127.0.0.1:43110/

Current limitations

No torrent-like file splitting for big file support (big file support added)
No more anonymous than Bittorrent (built-in full Tor support added)
File transactions are not compressed or encrypted yet (TLS encryption added)
No private sites

How can I create a ZeroNet site?
Shut down zeronet if you are running it already
$ zeronet.py siteCreate
...
- Site private key: 23DKQpzxhbVBrAtvLEc2uvk7DZweh4qL3fn3jpM3LgHDczMK2TtYUq
- Site address: 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
...
- Site created!
$ zeronet.py
...
Congratulations, you're finished! Now anyone can access your site using
http://127.0.0.1:43110/13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
Next steps: ZeroNet Developer Documentation
How can I modify a ZeroNet site?

Modify files located in data/13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2 directory.
After you're finished:

$ zeronet.py siteSign 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
- Signing site: 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2...
Private key (input hidden):

Enter the private key you got when you created the site, then:

$ zeronet.py sitePublish 13DNDkMUExRf9Xa9ogwPKqp7zyHFEqbhC2
...
Site:13DNDk..bhC2 Publishing to 3/10 peers...
Site:13DNDk..bhC2 Successfuly published to 3 peers
- Serving files....

That's it! You've successfully signed and published your modifications.

Help keep this project alive

Bitcoin: 1QDhxQ6PraUZa21ET5fYUCPgdrwBomnFgX
Paypal: https://zeronet.io/docs/help_zeronet/donate/

Sponsors

Better macOS/Safari compatibility made possible by BrowserStack.com

Thank you!

More info, help, changelog, zeronet sites: https://www.reddit.com/r/zeronet/
Come, chat with us: #zeronet @ FreeNode or on gitter
Email: hello@zeronet.io (PGP: CB9613AE)

",GitHub - HelloZeroNet/ZeroNet: ZeroNet - Decentralized websites using Bitcoin crypto and BitTorrent network
68,Python,"
Apache Airflow








Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.
When workflows are defined as code, they become more maintainable,
versionable, testable, and collaborative.
Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.
Table of contents

Getting started
Beyond the Horizon
Principles
User Interface
Contributing
Who uses Apache Airflow?
Who Maintains Apache Airflow?
Can I use the Apache Airflow logo in my presentation?
Links

Getting started
Please visit the Airflow Platform documentation (latest stable release) for help with installing Airflow, getting a quick start, or a more complete tutorial.
Documentation of GitHub master (latest development branch): ReadTheDocs Documentation
For further information, please visit the Airflow Wiki.
Beyond the Horizon
Airflow is not a data streaming solution. Tasks do not move data from
one to the other (though tasks can exchange metadata!). Airflow is not
in the Spark Streaming
or Storm space, it is more comparable to
Oozie or
Azkaban.
Workflows are expected to be mostly static or slowly changing. You can think
of the structure of the tasks in your workflow as slightly more dynamic
than a database structure would be. Airflow workflows are expected to look
similar from a run to the next, this allows for clarity around
unit of work and continuity.
Principles

Dynamic:  Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.
Extensible:  Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.
Elegant:  Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine.
Scalable:  Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.

User Interface


DAGs: Overview of all DAGs in your environment.



Tree View: Tree representation of a DAG that spans across time.



Graph View: Visualization of a DAG's dependencies and their current status for a specific run.



Task Duration: Total time spent on different tasks over time.



Gantt View: Duration and overlap of a DAG.



Code View:  Quick way to view source code of a DAG.



Contributing
Want to help build Apache Airflow? Check out our contributing documentation.
Who uses Apache Airflow?
As the Apache Airflow community grows, we'd like to keep track of who is using
the platform. Please send a PR with your company name and @githubhandle
if you may.
Currently officially using Airflow:

4G Capital [@posei]
6play [@lemourA, @achaussende, @d-nguyen, @julien-gm]
8fit [@nicor88, @frnzska]
90 Seconds [@aaronmak]
99 [@fbenevides, @gustavoamigo & @mmmaia]
AdBOOST [AdBOOST]
Adobe [@mishikaSingh, @ramandumcs, @vardancse]
Agari [@r39132]
Agoda [@akki]
Airbnb [@mistercrunch, @artwr]
AirDNA
Airfinity [@sibowyer]
Airtel [@harishbisht]
Alan [@charles-go]
allegro.pl [@kretes]
AloPeyk [@blcksrx, @AloPeyk]
AltX [@pedromduarte]
AMPATH[@AMPATH, @fatmali]
Apigee [@btallman]
ARGO Labs [@California Data Collaborative]
ARMEDANGELS [@swiffer]
Arquivei [@arquivei]
Arrive
Asana [@chang, @dima-asana, @jdavidheiser, @ricardoandresrojas]
Astronomer [@schnie, @ashb, @kaxil, @dimberman, @andriisoldatenko, @ryw, @andrewhharmon]
Auth0 [@sicarul]
Automattic [@anandnalya, @bperson, @khrol, @xyu]
Away [@trunsky]
Azri Solutions [@userimack]
Bagelcode
BalanceHero [@swalloow]
Banco de Formaturas [@guiligan]
BandwidthX [@dineshdsharma]
Basetis
BBM
Beamly [@christopheralcock]
Beeswax
Bellhops
BelugaDB [@fabio-nukui & @joao-sallaberry & @lucianoviola & @tmatuki]
Betterment [@betterment]
Bexs Bank [@felipefb & @ilarsen]
BigQuant [@bigquant]
BlaBlaCar [@puckel & @wmorin]
Blacklane [@serkef]
Bloc [@dpaola2]
Bloomberg [@dimberman]
Blue Yonder [@blue-yonder]
BlueApron [@jasonjho & @matthewdavidhauser]
Bluecore [@JLDLaughlin]
Bluekiri [@Bluekiri]
Boda Telecom Suite - CE [@erssebaggala, @bodastage]
Bodastage Solutions [@erssebaggala, @bodastage]
Bombora Inc [@jeffkpayne, @pakelley, @dNavalta, @austynh, @TheOriginalAlex]
Bonial International GmbH
Bonnier Broadcasting [@wileeam]
BounceX [@JoshFerge, @hudsonrio, @ronniekritou]
Braintree [@coopergillan, @curiousjazz77, @raymondberg]
Branch [@sdebarshi, @dmitrig01]
Caesars Entertainment
California Data Collaborative powered by ARGO Labs
Capital One [@anoopengineer]
Carbonite [@ajbosco]
CarLabs [@sganz & @odannyc]
CAVA [@minh5 & @patchus]
Celect [@superdosh & @chadcelect]
Censys [@zakird, @dadrian, & @andrewsardone]
Change.org [@change, @vijaykramesh]
Chartboost [@cgelman & @dclubb]
Checkr [@tongboh]
Children's Hospital of Philadelphia Division of Genomic Diagnostics [@genomics-geek]
Cinimex DataLab [@kdubovikov]
City of San Diego [@MrMaksimize, @andrell81 & @arnaudvedy]
City of Toronto [@CityofToronto, @radumas]
ciValue [@chencivalue, @YoavGaudin, @saleem-boshnak]
Civey [@WesleyBatista]
Clairvoyant [@shekharv]
Classmethod, Inc. [@shoito]
Cleartax [@anks & @codebuff]
Clover Health [@gwax & @vansivallab]
Collectivehealth Inc. [@retornam]
Compass [@wdhorton]
ConnectWise [@jacobeturpin]
ContaAzul [@bern4rdelli, @renanleme & @sabino]
Cotap [@maraca & @richardchew]
Craig@Work
Crealytics
Credit Karma [@preete-dixit-ck & @harish-gaggar-ck & @greg-finley-ck]
Creditas [@dcassiano]
CreditCards.com[@vmAggies &  @jay-wallaby]
Cryptalizer.com
Custom Ink [@david-dalisay, @dmartin11 & @mpeteuil]
Cyscale [@ocical]
Dailymotion [@germaintanguy & @hc]
Danamica [@testvinder]
Data Reply [@kaxil]
DataCamp [@dgrtwo]
DataFox [@sudowork]
Dentsu Inc. [@bryan831 & @loozhengyuan]
Digital First Media [@duffn & @mschmo & @seanmuth]
DigitalOcean [@ajbosco]
DoorDash
Dotmodus [@dannylee12]
Drivy [@AntoineAugusti]
Easy Taxi [@caique-lima & @diraol]
EllisDon [@d2kalra & @zbasama]
Endesa [@drexpp]
Enigma [@hydrosquall]
Datamaran [@valexharo]
Etsy [@mchalek]
evo.company [@orhideous]
Experity (formerly DocuTAP) [@cloneluke & @tobyjoliver]
Fathom Health
Firestone Inventing [@zihengCat]
Flipp [@sethwilsonwishabi]
Format [@format & @jasonicarter]
FreshBooks [@DinoCow]
Freshworks [@shaikshakeel]
FullContact
Fuller, Inc. [@wutali & @sh-tech]
Fundera [@andyxhadji]
G Adventures [@chchtv11, @tgumbley, @tomwross]
GameWisp [@tjbiii & @theryanwalls]
GeneCards [@oferze]
Gentner Lab [@neuromusic]
Get Simpl [@rootcss]
GitLab [@tlapiana & @tayloramurphy]
Glassdoor [@syvineckruyk & @sid88in]
Global Fashion Group [@GFG]
GoDataDriven [@BasPH, @danielvdende, @ffinfo, @Fokko, @gglanzani, @hgrif, @jrderuiter, @NielsZeilemaker]
GovTech GDS [@chrissng & @datagovsg]
Grab [@calvintran]
Gradeup [@gradeup]
Grand Rounds [@richddr, @timz1290, @wenever, & @runongirlrunon]
Groupalia [@jesusfcr]
Groupon [@stevencasey]
Growbots[@exploy]
GSN Games
Gusto [@frankhsu]
Handshake [@mhickman]
Handy [@marcintustin / @mtustin-handy]
happn [@pcorbel]
HAVAN [@botbiz]
HBC Digital [@tmccartan & @dmateusp]
HBO[@yiwang]
Healthjump [@miscbits]
HelloFresh [@tammymendt & @davidsbatista & @iuriinedostup]
Hipages [@arihantsurana]
Holimetrix [@thibault-ketterer]
Hootsuite
Hostnfly [@CyrilLeMat & @pierrechopin & @alexisrosuel]
HotelQuickly [@zinuzoid]
Huq Industries [@huqindustries, @alepuccetti, @turbomerl]
Iflix [@ChaturvediSulabh]
IFTTT [@apurvajoshi]
iHeartRadio[@yiwang]
imgix [@dclubb]
ING
Instacart ü•ï [@arp1t & @code-sauce & @jasonlew & @j4p3 & @lubert & @mmontagna & @RyanAD &@zzadeh]
Intercom [@fox & @paulvic]
Interia
Investorise [@svenvarkel]
iS2.co [@iS2co]
Jampp
Jeitto [@BrennerPablo & @ds-mauri]
Jetlore [@bderose]
JobTeaser [@stefani75 &  @knil-sama]
JULO [@sepam & @tenapril & @verzqy]
Kalibrr [@charlesverdad]
Kargo [@chaithra-yenikapati, @akarsh3007 & @dineshanchan]
Karmic [@hyw]
King [@nathadfield]
King Abdullah Petroleum Studies and Research Center(KAPSARC) [@saianupkumarp]
Kiwi.com [@underyx]
Kogan.com [@geeknam]
Korbit [@jensenity]
KPN B.V. [@biyanisuraj & @gmic]
Kroton Educacional
Lemann Foundation [@fernandosjp]
LeMans Corporation [@alloydwhitlock] & [@tinyrye]
LendUp [@lendup]
LetsBonus [@jesusfcr & @OpringaoDoTurno]
Liberty Global [@LibertyGlobal]
liligo [@tromika]
LingoChamp [@haitaoyao]
Logitravel Group
Los Angeles Times [@standyro]
LokSuvidha [@saurabhwahile]
Lucid [@jbrownlucid & @kkourtchikov]
Lumos Labs [@rfroetscher & @zzztimbo]
Lyft [@feng-tao, @milton0825, @astahlman,
@youngyjd, @ArgentFalcon]
M4U [@msantino]
Madrone [@mbreining & @scotthb]
Markovian [@al-xv, @skogsbaeck, @waltherg]
Mercadoni [@demorenoc]
Mercari [@yu-iskw]
MFG Labs
MiNODES [@dice89, @diazcelsa]
Modernizing Medicine[@kehv1n, @dalupus]
Multiply [@nrhvyc]
mytaxi [@mytaxi]
National Bank of Canada [@brilhana]
Neoway [@neowaylabs]
Nerdwallet
New Relic [@marcweil]
Newzoo [@newzoo-nexus]
NEXT Trucking [@earthmancash2, @kppullin]
Nextdoor [@SivaPandeti, @zshapiro & @jthomas123]
Nine [@TheZepto]
OdysseyPrime [@davideberdin]
OfferUp
OneFineStay [@slangwald]
Open Knowledge International @vitorbaptista
Optum - UnitedHealthGroup [@hiteshrd]
Outcome Health [@mikethoun, @rolandotribo]
Overstock [@mhousley & @mct0006]
OVH [@ncrocfer & @anthonyolea]
Pagar.me [@pagarme]
Palo Alto Networks [@PaloAltoNetworks]
Pandora Media [@Acehaidrey & @wolfier]
PayFit [@pcorbel]
PAYMILL [@paymill & @matthiashuschle]
PayPal [@r39132 & @jhsenjaliya]
Pecan [@ohadmata]
Pernod-Ricard [@romain-nio]
Plaid [@plaid, @AustinBGibbons & @jeeyoungk]
Playbuzz [@clintonboys & @dbn]
PMC [@andrewm4894]
Poshmark
Postmates [@syeoryn]
Premise [@jmccallum-premise]
Pronto Tools [@zkan & @mesodiar]
proton.ai [@prmsolutions]
Publicis Pixelpark [@feluelle]
PubNub [@jzucker2]
PXYData [@patchus]
Qplum [@manti]
Quantopian [@eronarn]
Qubole [@msumit]
Quizlet [@quizlet]
Quora
Ra√≠zen [@rudlac & @guifneves]
REA Group
Reddit [@reddit]
Reverb[@reverbdotcom]
Revolut [@sztanko & @nautilus28]
Robinhood [@vineet-rh]
Scaleway [@kdeldycke]
Seasoned [@joshuacano] & [@mmyers] & [@tjward]
Secret Escapes [@secretescapes]
Semantics3 [@abishekk92]
Sense360 [@kamilmroczek]
Sentry.io [@tiopi]
Shopkick [@shopkick]
Sidecar [@getsidecar]
SimilarWeb [@similarweb]
Skyscanner [@skyscanner]
SmartNews [@takus]
SnapTravel
SocialCops [@vinayak-mehta & @sharky93]
Soci√©t√© g√©n√©rale [@medmrgh & @s83]
Spotahome [@spotahome]
SpotHero [@benjigoldberg]
Spotify [@znichols]
Square
Stackspace
StoneCo [@lgwacker]
Strava [@strava, @dhuang & @liamstewart]
Stripe [@jbalogh]
Strongmind [@tomchapin & @wongstein]
Surfline [@jawang35]
T2 Systems [@unclaimedpants]
Tails.com [@alanmcruickshank]
TEK [@telac]
Telefonica Innovation Alpha [@Alpha-Health]
Telia Company
Tesla [@thoralf-gutierrez]
The Home Depot[@apekshithr]
THE ICONIC [@revathijay] [@ilikedata]
Thinking Machines [@marksteve]
Thinknear [@d3cay1, @ccson, & @ababian]
ThoughtWorks [@sann3]
Thumbtack [@natekupp]
Tictail
Tile [@ranjanmanish]
Tinder [@kbendick]
TokenAnalyst [@simonohanlon101, @ankitchiplunkar, @sidshekhar, @sp6pe]
Tokopedia [@topedmaria]
Trocafone [@idontdomath & @gseva & @ordonezf & @PalmaLeandro]
Twine Labs [@ivorpeles]
Twitter [@aoen]
Ubisoft [@Walkoss]
United Airlines [@ilopezfr]
Upsight
VeeR VR [@pishilong]
Veikkaus [@hixus]
Vente-Exclusive.com [@alexvanboxel]
Vevo [@csetiawan & @jerrygillespie]
Vidio
Ville de Montr√©al@VilledeMontreal]
Vnomics [@lpalum]
Walmart Labs [@bharathpalaksha, @vipul007ravi]
Waze [@waze]
WePay [@criccomini & @mtagle]
WeTransfer [@coredipper & @higee & @azclub]
Whistle Labs [@ananya77041]
WiseBanyan
Wooga
Wrike [@eliseealex & teoretic6]
Xero [@yan9yu & adamantnz]
Xoom
Yahoo!
Yieldr [@ggeorgiadis]
Zapier [@drknexus & @statwonk]
Zego [@ruimffl, @james-welly, @ken-payne]
Zendesk
Zenly [@cerisier & @jbdalido]
Zymergen
Zynga

Who Maintains Apache Airflow?
Airflow is the work of the community,
but the core committers/maintainers
are responsible for reviewing and merging PRs as well as steering conversation around new feature requests.
If you would like to become a maintainer, please review the Apache Airflow
committer requirements.
Can I use the Apache Airflow logo in my presentation?
Yes! Be sure to abide by the Apache Foundation trademark policies and the Apache Airflow Brandbook. The most up to date logos are found in this repo and on the Apache Software Foundation website.
Links

Documentation
Chat
More

","GitHub - apache/airflow: Apache Airflow - A platform to programmatically author, schedule, and monitor workflows"
69,Python,"

This repository provides tutorial code for deep learning researchers to learn PyTorch. In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish Official Pytorch Tutorial.

Table of Contents
1. Basics

PyTorch Basics
Linear Regression
Logistic Regression
Feedforward Neural Network

2. Intermediate

Convolutional Neural Network
Deep Residual Network
Recurrent Neural Network
Bidirectional Recurrent Neural Network
Language Model (RNN-LM)

3. Advanced

Generative Adversarial Networks
Variational Auto-Encoder
Neural Style Transfer
Image Captioning (CNN-RNN)

4. Utilities

TensorBoard in PyTorch


Getting Started
$ git clone https://github.com/yunjey/pytorch-tutorial.git
$ cd pytorch-tutorial/tutorials/PATH_TO_PROJECT
$ python main.py

Dependencies

Python 2.7 or 3.5+
PyTorch 0.4.0+


Author
Yunjey Choi/ @yunjey
",GitHub - yunjey/pytorch-tutorial: PyTorch Tutorial for Deep Learning Researchers
70,Python,"Glances - An eye on your system












Summary
Glances is a cross-platform monitoring tool which aims to present a
large amount of monitoring information through a curses or Web
based interface. The information dynamically adapts depending on the
size of the user interface.

It can also work in client/server mode. Remote monitoring could be done
via terminal, Web interface or API (XML-RPC and RESTful). Stats can also
be exported to files or external time/value databases.

Glances is written in Python and uses libraries to grab information from
your system. It is based on an open architecture where developers can
add new plugins or exports modules.

Requirements

python 2.7,>=3.4
psutil>=5.3.0 (better with latest version)

Optional dependencies:

bernhard (for the Riemann export module)
bottle (for Web server mode)
cassandra-driver (for the Cassandra export module)
couchdb (for the CouchDB export module)
docker (for the Docker monitoring support) [Linux/macOS-only]
elasticsearch (for the Elastic Search export module)
hddtemp (for HDD temperature monitoring support) [Linux-only]
influxdb (for the InfluxDB export module)
kafka-python (for the Kafka export module)
netifaces (for the IP plugin)
nvidia-ml-py3 (for the GPU plugin)
pika (for the RabbitMQ/ActiveMQ export module)
potsdb (for the OpenTSDB export module)
prometheus_client (for the Prometheus export module)
py-cpuinfo (for the Quicklook CPU info module)
pygal (for the graph export module)
pymdstat (for RAID support) [Linux-only]
pySMART.smartx (for HDD Smart support) [Linux-only]
pysnmp (for SNMP support)
pystache (for the action script feature)
pyzmq (for the ZeroMQ export module)
requests (for the Ports, Cloud plugins and RESTful export module)
scandir (for the Folders plugin) [Only for Python < 3.5]
statsd (for the StatsD export module)
wifi (for the wifi plugin) [Linux-only]
zeroconf (for the autodiscover mode)

Note for Python 2.6 users
Glances no longer supports Python 2.6. Please upgrade
to a minimum Python version of 2.7/3.4+ or downgrade to Glances 2.6.2 (last version
with Python 2.6 support).
Note for CentOS Linux 6 and 7 users
Python 2.7 and 3.4 are now available via SCL repositories. See:
https://lists.centos.org/pipermail/centos-announce/2015-December/021555.html.

Installation
There are several methods to test/install Glances on your system. Choose your weapon!

Glances Auto Install script: the total way
To install both dependencies and the latest Glances production ready version
(aka master branch), just enter the following command line:
curl -L https://bit.ly/glances | /bin/bash
or
wget -O- https://bit.ly/glances | /bin/bash
Note: This is only supported on some GNU/Linux distributions and Mac OS X.
If you want to support other distributions, please contribute to glancesautoinstall.

PyPI: The simple way
Glances is on PyPI. By using PyPI, you will be using the latest
stable version.
To install, simply use pip:
pip install glances
Note: Python headers are required to install psutil. For example,
on Debian/Ubuntu you need to install first the python-dev package.
For Fedora/CentOS/RHEL install first python-devel package. For Windows,
just install psutil from the binary installation file.
Note 2 (for the Wifi plugin): If you want to use the Wifi plugin, you need
to install the wireless-tools package on your system.
You can also install the following libraries in order to use optional
features (like the Web interface, exports modules...):
pip install 'glances[action,browser,cloud,cpuinfo,docker,export,folders,gpu,graph,ip,raid,snmp,web,wifi]'
To upgrade Glances to the latest version:
pip install --upgrade glances
pip install --upgrade glances[...]
If you need to install Glances in a specific user location, use:
export PYTHONUSERBASE=~/mylocalpath
pip install --user glances

Docker: the funny way
A Glances container is available. It includes the latest development
HEAD version. You can use it to monitor your server and all your other
containers!
Get the Glances container:
docker pull nicolargo/glances
Run the container in console mode:
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host --network host -it docker.io/nicolargo/glances
Additionally, if you want to use your own glances.conf file, you can
create your own Dockerfile:
FROM nicolargo/glances
COPY glances.conf /glances/conf/glances.conf
CMD python -m glances -C /glances/conf/glances.conf $GLANCES_OPT
Alternatively, you can specify something along the same lines with
docker run options:
docker run -v `pwd`/glances.conf:/glances/conf/glances.conf -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host -it docker.io/nicolargo/glances
Where `pwd`/glances.conf is a local directory containing your glances.conf file.
Run the container in Web server mode (notice the GLANCES_OPT environment
variable setting parameters for the glances startup command):
docker run -d --restart=""always"" -p 61208-61209:61208-61209 -e GLANCES_OPT=""-w"" -v /var/run/docker.sock:/var/run/docker.sock:ro --pid host docker.io/nicolargo/glances

GNU/Linux
Glances is available on many Linux distributions, so you should be
able to install it using your favorite package manager. Be aware that
when you use this method the operating system package for Glances
may not be the latest version.

FreeBSD
To install the binary package:
# pkg install py27-glances
To install Glances from ports:
# cd /usr/ports/sysutils/py-glances/
# make install clean

macOS
If you do not want to use the glancesautoinstall script, follow this procedure.
macOS users can install Glances using Homebrew or MacPorts.

Homebrew
$ brew install glances

MacPorts
$ sudo port install glances

Windows
Install Python for Windows (Python 2.7.9+ and 3.4+ ship with pip) and
then run the following command:
$ pip install glances
Alternatively, you could clone the repository and install with the following command.
$ git clone https://github.com/nicolargo/glances.git
$ cd glances
$ python setup.py install

Android
You need a rooted device and the Termux application (available on the
Google Play Store).
Start Termux on your device and enter:
$ apt update
$ apt upgrade
$ apt install clang python python-dev
$ pip install bottle
$ pip install glances
And start Glances:
$ glances
You can also run Glances in server mode (-s or -w) in order to remotely
monitor your Android device.

Source
To install Glances from source:
$ wget https://github.com/nicolargo/glances/archive/vX.Y.tar.gz -O - | tar xz
$ cd glances-*
# python setup.py install
Note: Python headers are required to install psutil.

Chef
An awesome Chef cookbook is available to monitor your infrastructure:
https://supermarket.chef.io/cookbooks/glances (thanks to Antoine Rouyer)

Puppet
You can install Glances using Puppet: https://github.com/rverchere/puppet-glances

Ansible
A Glances Ansible role is available: https://galaxy.ansible.com/zaxos/glances-ansible-role/

Usage
For the standalone mode, just run:
$ glances
For the Web server mode, run:
$ glances -w
and enter the URL http://<ip>:61208 in your favorite web browser.
For the client/server mode, run:
$ glances -s
on the server side and run:
$ glances -c <ip>
on the client one.
You can also detect and display all Glances servers available on your
network or defined in the configuration file:
$ glances --browser
You can also display raw stats on stdout:
$ glances --stdout cpu.user,mem.used,load
cpu.user: 30.7
mem.used: 3278204928
load: {'cpucore': 4, 'min1': 0.21, 'min5': 0.4, 'min15': 0.27}
cpu.user: 3.4
mem.used: 3275251712
load: {'cpucore': 4, 'min1': 0.19, 'min5': 0.39, 'min15': 0.27}
...
or in a CSV format thanks to the stdout-csv option:
$ glances --stdout-csv now,cpu.user,mem.used,load
now,cpu.user,mem.used,load.cpucore,load.min1,load.min5,load.min15
2018-12-08 22:04:20 CEST,7.3,5948149760,4,1.04,0.99,1.04
2018-12-08 22:04:23 CEST,5.4,5949136896,4,1.04,0.99,1.04
...
and RTFM, always.

Documentation
For complete documentation have a look at the readthedocs website.
If you have any question (after RTFM!), please post it on the official Q&A forum.

Gateway to other services
Glances can export stats to: CSV file, JSON file, InfluxDB, Cassandra, CouchDB,
OpenTSDB, Prometheus, StatsD, ElasticSearch, RabbitMQ/ActiveMQ,
ZeroMQ, Kafka, Riemann and RESTful server.

How to contribute ?
If you want to contribute to the Glances project, read this wiki page.
There is also a chat dedicated to the Glances developers:



Donation
If this project help you, you can give me a tip ;)


Author
Nicolas Hennion (@nicolargo) <nicolas@nicolargo.com>


License
Glances is distributed under the LGPL version 3 license. See COPYING for more details.
","GitHub - nicolargo/glances: Glances an Eye on your system. A top/htop alternative for GNU/Linux, BSD, Mac OS and Windows operating systems."
71,Python,"


Magenta is a research project exploring the role of machine learning
in the process of creating art and music.  Primarily this
involves developing new deep learning and reinforcement learning
algorithms for generating songs, images, drawings, and other materials. But it's also
an exploration in building smart tools and interfaces that allow
artists and musicians to extend (not replace!) their processes using
these models.  Magenta was started by some researchers and engineers
from the Google Brain team,
but many others have contributed significantly to the project. We use
TensorFlow and release our models and
tools in open source on this GitHub.  If you‚Äôd like to learn more
about Magenta, check out our blog,
where we post technical details.  You can also join our discussion
group.
This is the home for our Python TensorFlow library. To use our models in the browser with TensorFlow.js, head to the Magenta.js repository.
Getting Started

Installation
Using Magenta
Playing a MIDI Instrument
Development Environment (Advanced)

Installation
Magenta maintains a pip package for easy
installation. We recommend using Anaconda to install it, but it can work in any
standard Python environment. We support both Python 2 (>= 2.7) and Python 3 (>= 3.5).
These instructions will assume you are using Anaconda.
Automated Install (w/ Anaconda)
If you are running Mac OS X or Ubuntu, you can try using our automated
installation script. Just paste the following command into your terminal.
curl https://raw.githubusercontent.com/tensorflow/magenta/master/magenta/tools/magenta-install.sh > /tmp/magenta-install.sh
bash /tmp/magenta-install.sh
After the script completes, open a new terminal window so the environment
variable changes take effect.
The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!
Note that you will need to run source activate magenta to use Magenta every
time you open a new terminal window.
Manual Install (w/o Anaconda)
If the automated script fails for any reason, or you'd prefer to install by
hand, do the following steps.
Install the Magenta pip package:
pip install magenta
NOTE: In order to install the rtmidi package that we depend on, you may need to install headers for some sound libraries. On Linux, this command should install the necessary packages:
sudo apt-get install build-essential libasound2-dev libjack-dev
The Magenta libraries are now available for use within Python programs and
Jupyter notebooks, and the Magenta scripts are installed in your path!
Using Magenta
You can now train our various models and use them to generate music, audio, and images. You can
find instructions for each of the models by exploring the models directory.
To get started, create your own melodies with TensorFlow using one of the various configurations of our Melody RNN model; a recurrent neural network for predicting melodies.
Playing a MIDI Instrument
After you've trained one of the models above, you can use our MIDI interface to play with it interactively.
We also have created several demos that provide a UI for this interface, making it easier to use (e.g., the browser-based AI Jam).
Development Environment
If you want to develop on Magenta, you'll need to set up the full Development Environment.
First, clone this repository:
git clone https://github.com/tensorflow/magenta.git
Next, install the dependencies by changing to the base directory and executing the setup command:
pip install -e .
You can now edit the files and run scripts by calling Python as usual. For example, this is how you would run the melody_rnn_generate script from the base directory:
python magenta/models/melody_rnn/melody_rnn_generate --config=...
You can also install the (potentially modified) package with:
pip install .
Before creating a pull request, please also test your changes with:
pip install pytest-pylint
pytest
PIP Release
To build a new version for pip, bump the version and then run:
python setup.py test
python setup.py bdist_wheel --universal
twine upload dist/magenta-N.N.N-py2.py3-none-any.whl
",GitHub - tensorflow/magenta: Magenta: Music and Art Generation with Machine Intelligence
72,Python,"
Take Note!  This version of the Hosts file generator, and tests, are for Python 3.5+ only.











Unified hosts file with base extensions
This repository consolidates several reputable hosts files, and merges them
into a unified hosts file with duplicates removed.  A variety of tailored hosts files are provided.

Last updated: December 02 2019.
Here's the raw hosts file with base extensions containing 43,154 entries.
Logo by @Tobaloidee.

List of all hosts file variants
This repository offers 15 different host file variants, in addition to the base variant.
The Non GitHub mirror is the link to use for some hosts file managers like
Hostsman for Windows that don't work
with Github download links.



Host file recipe
Readme
Raw hosts
Unique domains
Non Github mirror




Unified hosts = (adware + malware)
Readme
link
43,154
link


Unified hosts + fakenews
Readme
link
44,096
link


Unified hosts + gambling
Readme
link
45,426
link


Unified hosts + porn
Readme
link
58,797
link


Unified hosts + social
Readme
link
45,629
link


Unified hosts + fakenews + gambling
Readme
link
46,368
link


Unified hosts + fakenews + porn
Readme
link
59,739
link


Unified hosts + fakenews + social
Readme
link
46,571
link


Unified hosts + gambling + porn
Readme
link
61,069
link


Unified hosts + gambling + social
Readme
link
47,901
link


Unified hosts + porn + social
Readme
link
61,271
link


Unified hosts + fakenews + gambling + porn
Readme
link
62,011
link


Unified hosts + fakenews + gambling + social
Readme
link
48,843
link


Unified hosts + fakenews + porn + social
Readme
link
62,213
link


Unified hosts + gambling + porn + social
Readme
link
63,543
link


Unified hosts + fakenews + gambling + porn + social
Readme
link
64,485
link



Expectation: These unified hosts files should serve all devices, regardless
of OS.
Sources of hosts data unified in this variant
Updated hosts files from the following locations are always unified and
included:



Host file source
Description
Home page
Raw hosts
Update frequency
License
Issues




Steven Black's ad-hoc list
Additional sketch domains as I come across them.
link
raw
occasionally
MIT
issues


Malware Domain List
Malware Domain List is a non-commercial community project.
link
raw
weekly
'can be used for free by anyone'
issues


add.Dead
Dead sites based on hostsfile.org content.
link
raw
occasionally
GPLv3+
issues


hostsVN
Hosts block ads of Vietnamese
link
raw
occasionally
MIT
issues


add.Spam
Spam sites based on hostsfile.org content.
link
raw
occasionally
GPLv3+
issues


Dan Pollock ‚Äì someonewhocares
How to make the internet not suck (as much).
link
raw
frequently
non-commercial with attribution
issues


MVPS hosts file
The purpose of this site is to provide the user with a high quality custom HOSTS file.
link
raw
monthly
CC BY-NC-SA 4.0
issues


yoyo.org
Blocking with ad server and tracking server hostnames.
link
raw
frequently

issues


Mitchell Krog's - Badd Boyz Hosts
Sketchy domains and Bad Referrers from my Nginx and Apache Bad Bot and Spam Referrer Blockers
link
raw
weekly
MIT
issues


CoinBlocker
Simple lists that can help prevent cryptomining in the browser or other applications
link
raw
frequently
GPLv3
issues


UncheckyAds
Windows installers ads sources sites based on https://unchecky.com/ content.
link
raw
occasionally

issues


add.2o7Net
2o7Net tracking sites based on hostsfile.org content.
link
raw
occasionally
GPLv3+
issues


KADhosts
Fraud/adware/scam websites.
link
raw
frequently
CC BY-SA 4.0
issues


AdAway
AdAway is an open source ad blocker for Android using the hosts file.
link
raw
occasionally
CC BY 3.0
issues


add.Risk
Risk content sites based on hostsfile.org content.
link
raw
occasionally
GPLv3+
issues


Tiuxo hostlist - ads
Categorized hosts files for DNS based content blocking
link
raw
occasional
CC BY 4.0
issues



Extensions
The unified hosts file is optionally extensible.  Extensions are used to include domains by category.  Currently we offer the following categories: fakenews, social, gambling, and porn.
Extensions are optional, and can be combined in various ways wth the base hosts file.  The combined products are stored in the alternates folder.
Data for extensions is stored in the extensions folder. You manage extensions by curating this
folder tree, where you will find the data for fakenews, social, gambling, and porn extension data that we maintain and provide for you.
Generate your own unified hosts file
To generate your own unified hosts file you will need Python 3.5 or later.
First install the dependencies with:
pip3 install --user -r requirements.txt

Note we recommend the --user flag which installs the required dependencies at the user level. More information about it can be found on pip documentation.
To run unit tests, in the top level directory, run:
python3 testUpdateHostsFile.py

The updateHostsFile.py script will generate a unified hosts file based on the sources in the
local data/ subfolder.  The script will prompt you whether it should fetch updated versions
(from locations defined by the update.json text file in each source's folder). Otherwise, it
will use the hosts file that's already there.
Usage
Using Python 3:
python3 updateHostsFile.py [--auto] [--replace] [--ip nnn.nnn.nnn.nnn] [--extensions ext1 ext2 ext3]

Command line options:
--help, or -h: display help.
--auto, or -a: run the script without prompting. When --auto is invoked,

Hosts data sources, including extensions, are updated.
No extensions are included by default.  Use the --extensions or -e flag
to include any you want.
Your active hosts file is not replaced unless you include the --replace
flag.

--backup, or -b: Make a backup of existing hosts file(s) as you generate
over them.
--extensions <ext1> <ext2> <ext3>, or -e <ext1> <ext2> <ext3>: the names
of subfolders below the extensions folder containing additional
category-specific hosts files to include in the amalgamation. Example:
--extensions porn or -e social porn.
--flush-dns-cache, or -f: skip the prompt for flushing the DNS cache.
Only active when --replace is also active.
--ip nnn.nnn.nnn.nnn, or -i nnn.nnn.nnn.nnn: the IP address to use as the
target.  Default is 0.0.0.0.
--keepdomaincomments, or -k: true (default) or false, keep the comments
that appear on the same line as domains.  The default is true.
--noupdate, or -n: skip fetching updates from hosts data sources.
--output <subfolder>, or -o <subfolder>: place the generated source file
in a subfolder.  If the subfolder does not exist, it will be created.
--replace, or -r: trigger replacing your active hosts
--skipstatichosts, or -s: false (default) or true, omit the standard
section at the top, containing lines like 127.0.0.1 localhost.  This is
useful for configuring proximate DNS services on the local network.
--compress, or -c: false (default) or true, Compress the hosts file
ignoring non-necessary lines (empty lines and comments) and putting multiple
domains in each line. Reducing the number of lines of the hosts file improves
the performances under Windows (with DNS Client service enabled).
--minimise, or -m: false (default) or true, like --compress, but puts
each domain on a separate line. This is necessary because many implementations
of URL blockers that rely on hosts files do not conform to the standard which
allows multiple hosts on a single line.
How do I control which sources are unified?
Add one or more  additional sources, each in a subfolder of the data/
folder, and specify the url key in its update.json file.
Add one or more optional extensions, which originate from subfolders of the
extensions/ folder.  Again the url in update.json controls where this
extension finds its updates.
Create an optional blacklist file. The contents of this file (containing a
listing of additional domains in hosts file format) are appended to the
unified hosts file during the update process. A sample blacklist is
included, and may be modified as you need.

NOTE: The blacklist is not tracked by git, so any changes you make won't
be overridden when you git pull   this repo from origin in the future.

How do I include my own custom domain mappings?
If you have custom hosts records, place them in file myhosts.  The contents
of this file are prepended to the unified hosts file during the update
process.
The myhosts file is not tracked by git, so any changes you make won't be
overridden when you git pull this repo from origin in the future.
How do I prevent domains from being included?
The domains you list in the whitelist file are excluded from the final hosts
file.
The whitelist uses partial matching.  Therefore if you whitelist
google-analytics.com, that domain and all its subdomains won't be merged
into the final hosts file.
The whitelist is not tracked by git, so any changes you make won't be
overridden when you git pull this repo  from origin in the future.
How can I contribute hosts records?
If you discover sketchy domains you feel should be included here, here are some ways to contribute them.
Option 1: contact one of our hosts sources
The best way to get new domains included is to submit an issue to any of the data providers whose home pages are listed here. This is best because once you submit new domains, they will be curated and updated by the dedicated folks who maintain these sources.
Option 2: add your domains to Steven Black's personal data file
Fork this hosts this repo and add your links to https://github.com/StevenBlack/hosts/blob/master/data/StevenBlack/hosts.
Then, submit a pull request.
WARNING: this is less desirable than Option 1 because the ongoing curation falls on us. So this creates more work for us.
Option 3: create your own hosts list as a repo on Github
If you're able to curate your own collection of sketchy domains, then curate your own hosts list.  Then signal the existence of your repo as a new issue and we may include your new repo into the collection of sources we pull whenever we create new versions.
What is a hosts file?
A hosts file, named hosts (with no file extension), is a plain-text file
used by all operating systems to map hostnames to IP addresses.
In most operating systems, the hosts file is preferential to DNS.
Therefore if a domain name is resolved by the hosts file, the request never
leaves your computer.
Having a smart hosts file goes a long way towards blocking malware, adware,
and other irritants.
For example, to nullify requests to some doubleclick.net servers, adding these
lines to your hosts file will do it:
# block doubleClick's servers
0.0.0.0 ad.ae.doubleclick.net
0.0.0.0 ad.ar.doubleclick.net
0.0.0.0 ad.at.doubleclick.net
0.0.0.0 ad.au.doubleclick.net
0.0.0.0 ad.be.doubleclick.net
# etc...

We recommend using 0.0.0.0 instead of 127.0.0.1
Traditionally most host files use 127.0.0.1, the loopback address, to establish an IP connection to the local machine.
We prefer to use 0.0.0.0, which is defined as a non-routable meta-address used to designate an invalid, unknown, or non applicable target.
Using 0.0.0.0 is empirically faster, possibly because there's no wait for a timeout resolution. It also does not
interfere with a web server that may be running on the local PC.
Why not use 0 instead of 0.0.0.0?
We tried that.  Using 0 doesn't work universally.
Location of your hosts file
To modify your current hosts file, look for it in the following places and modify it with a text
editor.
mac OS (until 10.14.x macOS Mojave), iOS, Android, Linux: /etc/hosts file.
macOS Catalina: /private/etc/hosts file.
Windows: %SystemRoot%\system32\drivers\etc\hosts file.
Gentoo
Gentoo users may find sb-hosts in ::pf4public Gentoo overlay
Updating hosts file on Windows
On Linux and Mac OS X, run the Python script. On Windows more
work is required due to compatibility issues so it's preferable to run the batch file as follows:
updateHostsWindows.bat

This file MUST be run in command prompt with administrator privileges in
the repository directory. In addition to updating the hosts file, it can also
replace the existing hosts file, and reload the DNS cache. It goes without
saying that in order for this to work, you must be connected to the internet.
To open a command prompt as administrator in the repository's directory, do the following:
Windows XP: Start -> Run -> cmd
Windows Vista, 7: Start Button -> type cmd -> right-click Command Prompt ->
""Run as Administrator""
Windows 8: Start -> Swipe Up -> All Apps -> Windows System -> right-click Command Prompt ->
""Run as Administrator""
Windows 10: Start Button -> type cmd -> right-click Command Prompt ->
""Run as Administrator""
You can also refer to the ""Third-Party Hosts Managers"" section for further recommended solutions from third parties.
Reloading hosts file
Your operating system will cache DNS lookups. You can either reboot or run the following commands to
manually flush your DNS cache once the new hosts file is in place.



The Google Chrome browser may require manually cleaning up its DNS Cache on chrome://net-internals/#dns page to thereafter see the changes in your hosts file. See: https://superuser.com/questions/723703



Windows
Open a command prompt with administrator privileges and run this command:
ipconfig /flushdns




If you want to use a huge hosts file by merging hphosts (NOT INCLUDED HERE) you need to DISABLE and STOP Dnscache service before you replace hosts file in Windows Systems. You have been warned.



Before flushing the DNS cache, open a command prompt with administrator privileges and run this command:
sc config ""Dnscache"" start= disabled
sc stop ""Dnscache""

Linux
Open a Terminal and run with root privileges:
Debian/Ubuntu sudo service network-manager restart
Linux Mint sudo /etc/init.d/dns-clean start
Linux with systemd: sudo systemctl restart network.service
Fedora Linux: sudo systemctl restart NetworkManager.service
Arch Linux/Manjaro with Network Manager: sudo systemctl restart NetworkManager.service
Arch Linux/Manjaro with Wicd: sudo systemctl restart wicd.service
RHEL/Centos: sudo /etc/init.d/network restart
FreeBSD: sudo service nscd restart
To enable the nscd daemon initially, it is recommended that you run the following commands:
sudo sysrc nscd_enable=""YES""
sudo service nscd start

Then modify the hosts line in your /etc/nsswitch.conf file to the following:
hosts: cache files dns

Others: Consult this wikipedia article.
Mac OS X
Open a Terminal and run:
sudo dscacheutil -flushcache;sudo killall -HUP mDNSResponder

Release management
This repository uses Release-It!, an excellent CLI release
tool for Github repos and npm packages, to automate creating releases.
This is why the package.json and
.release-it.json files are bundled.
Goals of this unified hosts file
The goals of this repo are to:


automatically combine high-quality lists of hosts,


provide situation-appropriate extensions,


de-dupe the resultant combined list,


and keep the resultant file reasonably sized.


A high-quality source is defined here as one that is actively curated.  A
hosts source should be frequently updated by its maintainers with both
additions and removals.  The larger the hosts file, the higher the level of
curation is expected.
For example, the (huge) hosts file from hosts-file.net
is not included here because it is very large (780,000+ entries)
and doesn't currently display a corresponding high level of curation activity.
It is expected that this unified hosts file will serve both desktop and mobile
devices under a variety of operating systems.
Third-Party Hosts Managers


Unified Hosts AutoUpdate (for Windows): The Unified Hosts AutUpdate package is purpose-built for this unified hosts project as well as in active development by community members. You can install and uninstall any blacklist  and keep it automatically up to date, and can be placed in a shared network location and deployed across an organization via group policies. And since it is in active development by community members, your bug reports, feature requests, and other feedback are most welcome.


ViHoMa is a Visual Hosts file Manager, written in Java, by Christian Mart√≠nez.  Check it out!


Interesting Applications


Hostile is a nifty command line utility to easily add or remove domains from your hosts file.  If our hosts files are too aggressive for you, you can use hostile to remove domains, or you can use hostile in a bash script to automate a post process each time you download fresh versions of hosts.


macOS Scripting for Configuration, Backup and Restore helps customizing, re-installing and using macOS. It also provides a script to install and update the hosts file using this project on macOS. In combination with a launchd it updates the hosts file every x days (default is 4). To install both download the github repo and run the install script from the directory one level up.


Pi-hole is a network-wide DHCP server and ad blocker that runs on Raspberry Pi. Pi-hole uses this repository as one of its sources.     This is a very interesting project to setup yourself, or you can buy one pre-loaded.


Block ads and malware via local BIND9 DNS server (for Debian, Raspbian & Ubuntu): Set up a local DNS server with a /etc/bind/named.conf.blocked file, sourced from here.


Block ads, malware, and deploy parental controls via local DualServer DNS/DHCP server (for BSD, Windows & Linux): Set up a blacklist for everyone on your network using the power of the unified hosts reformatted for DualServer. And if you're on Windows, this project also maintains an update script to make updating DualServer's blacklist even easier.


Blocking ads and malwares with unbound ‚Äì Unbound  is a validating, recursive, and caching DNS resolver.


DNSMasq conversion script This github gist has a short shell script (bash, will work on any 'nix) and uses 'wget' & 'awk' present in most distros, to fetch a specified hosts file and convert it the format required by dnsmasq. Supports ipv4 and ipv6. Designed to be used as either a shell script, or can be dropped into /etc/cron.weekly (or wherever suits). Script is short and easily edited, also has a short document attached with notes on dnsmasq setup.


Contribute!
Please read our Contributing Guide. Among other things, this explains how we organize files and folders in this repository.
We are always interested in discovering well-curated sources of hosts.  If you find one, please open an issue to draw our attention.
Before you create or respond to any issue, please read our code of conduct.
","GitHub - StevenBlack/hosts: Extending and consolidating hosts files from several well-curated sources like adaway.org, mvps.org, malwaredomainlist.com, someonewhocares.org, and potentially others.  You can optionally invoke extensions to block additional sites by category."
73,Python,"
  
 
 
 
 
 



Version:4.4.0rc5 (cliffs)

Web:http://celeryproject.org/

Download:https://pypi.org/project/celery/

Source:https://github.com/celery/celery/

Keywords:task, queue, job, async, rabbitmq, amqp, redis,
python, distributed, actors




Donations
This project relies on your generous donations.
If you are using Celery to create a commercial product, please consider becoming our backer or our sponsor to ensure Celery's future.

For enterprise
Available as part of the Tidelift Subscription.
The maintainers of celery and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. Learn more.

What's a Task Queue?
Task queues are used as a mechanism to distribute work across threads or
machines.
A task queue's input is a unit of work, called a task, dedicated worker
processes then constantly monitor the queue for new work to perform.
Celery communicates via messages, usually using a broker
to mediate between clients and workers. To initiate a task a client puts a
message on the queue, the broker then delivers the message to a worker.
A Celery system can consist of multiple workers and brokers, giving way
to high availability and horizontal scaling.
Celery is written in Python, but the protocol can be implemented in any
language. In addition to Python there's node-celery for Node.js,
a PHP client and gocelery for golang.
Language interoperability can also be achieved by using webhooks
in such a way that the client enqueues an URL to be requested by a worker.

What do I need?
Celery version 4.3 runs on,

Python (2.7, 3.4, 3.5, 3.6, 3.7)
PyPy2.7 (6.0)
PyPy3.5 (6.0)

This is the last version to support Python 2.7,
and from the next version (Celery 5.x) Python 3.5 or newer is required.
If you're running an older version of Python, you need to be running
an older version of Celery:

Python 2.6: Celery series 3.1 or earlier.
Python 2.5: Celery series 3.0 or earlier.
Python 2.4 was Celery series 2.2 or earlier.

Celery is a project with minimal funding,
so we don't support Microsoft Windows.
Please don't open any issues related to that platform.
Celery is usually used with a message broker to send and receive messages.
The RabbitMQ, Redis transports are feature complete,
but there's also experimental support for a myriad of other solutions, including
using SQLite for local development.
Celery can run on a single machine, on multiple machines, or even
across datacenters.

Get Started
If this is the first time you're trying to use Celery, or you're
new to Celery 4.2 coming from previous versions then you should read our
getting started tutorials:

First steps with Celery

Tutorial teaching you the bare minimum needed to get started with Celery.


Next steps

A more complete overview, showing more features.




Celery is...

Simple

Celery is easy to use and maintain, and does not need configuration files.
It has an active, friendly community you can talk to for support,
like at our mailing-list, or the IRC channel.
Here's one of the simplest applications you can make:
from celery import Celery

app = Celery('hello', broker='amqp://guest@localhost//')

@app.task
def hello():
    return 'hello world'



Highly Available

Workers and clients will automatically retry in the event
of connection loss or failure, and some brokers support
HA in way of Primary/Primary or Primary/Replica replication.


Fast

A single Celery process can process millions of tasks a minute,
with sub-millisecond round-trip latency (using RabbitMQ,
py-librabbitmq, and optimized settings).


Flexible

Almost every part of Celery can be extended or used on its own,
Custom pool implementations, serializers, compression schemes, logging,
schedulers, consumers, producers, broker transports, and much more.




It supports...


Message Transports


RabbitMQ, Redis, Amazon SQS



Concurrency


Prefork, Eventlet, gevent, single threaded (solo)



Result Stores


AMQP, Redis
memcached
SQLAlchemy, Django ORM
Apache Cassandra, IronCache, Elasticsearch



Serialization


pickle, json, yaml, msgpack.
zlib, bzip2 compression.
Cryptographic message signing.






Framework Integration
Celery is easy to integrate with web frameworks, some of which even have
integration packages:



Django
not needed

Pyramid
pyramid_celery

Pylons
celery-pylons

Flask
not needed

web2py
web2py-celery

Tornado
tornado-celery




The integration packages aren't strictly necessary, but they can make
development easier, and sometimes they add important hooks like closing
database connections at fork.

Documentation
The latest documentation is hosted at Read The Docs, containing user guides,
tutorials, and an API reference.
ÊúÄÊñ∞ÁöÑ‰∏≠ÊñáÊñáÊ°£ÊâòÁÆ°Âú® https://www.celerycn.io/ ‰∏≠ÔºåÂåÖÂê´Áî®Êà∑ÊåáÂçó„ÄÅÊïôÁ®ã„ÄÅAPIÊé•Âè£Á≠â„ÄÇ

Installation
You can install Celery either via the Python Package Index (PyPI)
or from source.
To install using pip:
$ pip install -U Celery


Bundles
Celery also defines a group of bundles that can be used
to install Celery and the dependencies for a given feature.
You can specify these in your requirements or on the pip
command-line by using brackets. Multiple bundles can be specified by
separating them by commas.
$ pip install ""celery[librabbitmq]""

$ pip install ""celery[librabbitmq,redis,auth,msgpack]""

The following bundles are available:

Serializers


celery[auth]:for using the auth security serializer.

celery[msgpack]:for using the msgpack serializer.

celery[yaml]:for using the yaml serializer.




Concurrency


celery[eventlet]:for using the eventlet pool.

celery[gevent]:for using the gevent pool.




Transports and Backends


celery[librabbitmq]:for using the librabbitmq C library.


celery[redis]:for using Redis as a message transport or as a result backend.


celery[sqs]:for using Amazon SQS as a message transport.


celery[tblib]:for using the task_remote_tracebacks feature.


celery[memcache]:for using Memcached as a result backend (using pylibmc)


celery[pymemcache]:for using Memcached as a result backend (pure-Python implementation).


celery[cassandra]:for using Apache Cassandra as a result backend with DataStax driver.


celery[azureblockblob]:for using Azure Storage as a result backend (using azure-storage)


celery[s3]:for using S3 Storage as a result backend.


celery[couchbase]:for using Couchbase as a result backend.


celery[arangodb]:for using ArangoDB as a result backend.


celery[elasticsearch]:for using Elasticsearch as a result backend.


celery[riak]:for using Riak as a result backend.


celery[cosmosdbsql]:for using Azure Cosmos DB as a result backend (using pydocumentdb)


celery[zookeeper]:for using Zookeeper as a message transport.


celery[sqlalchemy]:for using SQLAlchemy as a result backend (supported).


celery[pyro]:for using the Pyro4 message transport (experimental).


celery[slmq]:for using the SoftLayer Message Queue transport (experimental).


celery[consul]:for using the Consul.io Key/Value store as a message transport or result backend (experimental).


celery[django]:specifies the lowest version possible for Django support.
You should probably not use this in your requirements, it's here
for informational purposes only.





Downloading and installing from source
Download the latest version of Celery from PyPI:
https://pypi.org/project/celery/
You can install it by doing the following,:
$ tar xvfz celery-0.0.0.tar.gz
$ cd celery-0.0.0
$ python setup.py build
# python setup.py install

The last command must be executed as a privileged user if
you aren't currently using a virtualenv.

Using the development version

With pip
The Celery development version also requires the development
versions of kombu, amqp, billiard, and vine.
You can install the latest snapshot of these using the following
pip commands:
$ pip install https://github.com/celery/celery/zipball/master#egg=celery
$ pip install https://github.com/celery/billiard/zipball/master#egg=billiard
$ pip install https://github.com/celery/py-amqp/zipball/master#egg=amqp
$ pip install https://github.com/celery/kombu/zipball/master#egg=kombu
$ pip install https://github.com/celery/vine/zipball/master#egg=vine


With git
Please see the Contributing section.

Getting Help

Mailing list
For discussions about the usage, development, and future of Celery,
please join the celery-users mailing list.

IRC
Come chat with us on IRC. The #celery channel is located at the Freenode
network.

Bug tracker
If you have any suggestions, bug reports, or annoyances please report them
to our issue tracker at https://github.com/celery/celery/issues/

Wiki
https://github.com/celery/celery/wiki

Credits

Contributors
This project exists thanks to all the people who contribute. Development of
celery happens at GitHub: https://github.com/celery/celery
You're highly encouraged to participate in the development
of celery. If you don't like GitHub (for some reason) you're welcome
to send regular patches.
Be sure to also read the Contributing to Celery section in the
documentation.


Backers
Thank you to all our backers! üôè [Become a backer]


Sponsors
Support this project by becoming a sponsor. Your logo will show up here with a
link to your website. [Become a sponsor]



License
This software is licensed under the New BSD License. See the LICENSE
file in the top distribution directory for the full license text.
",GitHub - celery/celery: Distributed Task Queue (development branch)
74,Python,"Machine Learning From Scratch
About
Python implementations of some of the fundamental Machine Learning models and algorithms from scratch.
The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible
but rather to present the inner workings of them in a transparent and accessible way.
Table of Contents

Machine Learning From Scratch

About
Table of Contents
Installation
Examples

Polynomial Regression
Classification With CNN
Density-Based Clustering
Generating Handwritten Digits
Deep Reinforcement Learning
Image Reconstruction With RBM
Evolutionary Evolved Neural Network
Genetic Algorithm
Association Analysis


Implementations

Supervised Learning
Unsupervised Learning
Reinforcement Learning
Deep Learning


Contact



Installation
$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install

Examples
Polynomial Regression
$ python mlfromscratch/examples/polynomial_regression.py





    Figure: Training progress of a regularized polynomial regression model fitting 
    temperature data measured in Link√∂ping, Sweden 2016.

Classification With CNN
$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058





    Figure: Classification of the digit dataset using CNN.

Density-Based Clustering
$ python mlfromscratch/examples/dbscan.py





    Figure: Clustering of the moons dataset using DBSCAN.

Generating Handwritten Digits
$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762





    Figure: Training progress of a Generative Adversarial Network generating 
    handwritten digits.

Deep Reinforcement Learning
$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450





    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.

Image Reconstruction With RBM
$ python mlfromscratch/examples/restricted_boltzmann_machine.py





    Figure: Shows how the network gets better during training at reconstructing 
    the digit 2 in the MNIST dataset.

Evolutionary Evolved Neural Network
$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%





    Figure: Classification of the digit dataset by a neural network which has
    been evolutionary evolved.

Genetic Algorithm
$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']

Association Analysis
$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -> 2 (support: 0.43, confidence: 1.0)
    4 -> 2 (support: 0.57, confidence: 0.8)
    [1, 4] -> 2 (support: 0.29, confidence: 1.0)

Implementations
Supervised Learning

Adaboost
Bayesian Regression
Decision Tree
Elastic Net
Gradient Boosting
K Nearest Neighbors
Lasso Regression
Linear Discriminant Analysis
Linear Regression
Logistic Regression
Multi-class Linear Discriminant Analysis
Multilayer Perceptron
Naive Bayes
Neuroevolution
Particle Swarm Optimization of Neural Network
Perceptron
Polynomial Regression
Random Forest
Ridge Regression
Support Vector Machine
XGBoost

Unsupervised Learning

Apriori
Autoencoder
DBSCAN
FP-Growth
Gaussian Mixture Model
Generative Adversarial Network
Genetic Algorithm
K-Means
Partitioning Around Medoids
Principal Component Analysis
Restricted Boltzmann Machine

Reinforcement Learning

Deep Q-Network

Deep Learning

Neural Network
Layers

Activation Layer
Average Pooling Layer
Batch Normalization Layer
Constant Padding Layer
Convolutional Layer
Dropout Layer
Flatten Layer
Fully-Connected (Dense) Layer
Fully-Connected RNN Layer
Max Pooling Layer
Reshape Layer
Up Sampling Layer
Zero Padding Layer


Model Types

Convolutional Neural Network
Multilayer Perceptron
Recurrent Neural Network



Contact
If there's some implementation you would like to see here or if you're just feeling social,
feel free to email me or connect with me on LinkedIn.
",GitHub - eriklindernoren/ML-From-Scratch: Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.
75,Python,"








IPython: Productive Interactive Computing

Overview
Welcome to IPython.  Our full documentation is available on ipython.readthedocs.io and contains information on how to install, use, and
contribute to the project.
IPython versions and Python Support
Starting with IPython 7.10, IPython follows NEP 29
IPython 7.10+ requires Python version 3.6 and above.
IPython 7.0 requires Python version 3.5 and above.
IPython 6.x requires Python version 3.3 and above.
IPython 5.x LTS is the compatible release for Python 2.7.
If you require Python 2 support, you must use IPython 5.x LTS. Please
update your project configurations and requirements as necessary.
The Notebook, Qt console and a number of other pieces are now parts of Jupyter.
See the Jupyter installation docs
if you want to use these.

Development and Instant running
You can find the latest version of the development documentation on readthedocs.
You can run IPython from this directory without even installing it system-wide
by typing at the terminal:
$ python -m IPython

Or see the development installation docs
for the latest revision on read the docs.
Documentation and installation instructions for older version of IPython can be
found on the IPython website

IPython requires Python version 3 or above
Starting with version 6.0, IPython does not support Python 2.7, 3.0, 3.1, or
3.2.
For a version compatible with Python 2.7, please install the 5.x LTS Long Term
Support version.
If you are encountering this error message you are likely trying to install or
use IPython from source. You need to checkout the remote 5.x branch. If you are
using git the following should work:
$ git fetch origin
$ git checkout 5.x

If you encounter this error message with a regular install of IPython, then you
likely need to update your package manager, for example if you are using pip
check the version of pip with:
$ pip --version

You will need to update pip to the version 9.0.1 or greater. If you are not using
pip, please inquiry with the maintainers of the package for your package
manager.
For more information see one of our blog posts:

https://blog.jupyter.org/release-of-ipython-5-0-8ce60b8d2e8e
As well as the following Pull-Request for discussion:

https://github.com/ipython/ipython/pull/9900
This error does also occur if you are invoking setup.py directly ‚Äì¬†which you
should not ‚Äì¬†or are using easy_install If this is the case, use pip
install . instead of setup.py install , and pip install -e . instead
of setup.py develop If you are depending on IPython as a dependency you may
also want to have a conditional dependency on IPython depending on the Python
version:
install_req = ['ipython']
if sys.version_info[0] < 3 and 'bdist_wheel' not in sys.argv:
    install_req.remove('ipython')
    install_req.append('ipython<6')

setup(
    ...
    install_requires=install_req
)


Alternatives to IPython
IPython may not be to your taste; if that's the case there might be similar
project that you might want to use:

the classic Python REPL.
bpython
mypython
ptpython and ptipython <https://pypi.org/project/ptpython/>
xonsh <https://xon.sh/>

","GitHub - ipython/ipython: Official repository for IPython itself. Other repos in the IPython organization contain things like the website, documentation builds, etc."
76,Python,"Êïô‰Ω†Áî® Python Êù•Áé©ÂæÆ‰ø°Ë∑≥‰∏ÄË∑≥
  

Ê∏∏ÊàèÊ®°Âºè

2017 Âπ¥ 12 Êúà 28 Êó•‰∏ãÂçàÔºåÂæÆ‰ø°ÂèëÂ∏É‰∫Ü 6.6.1 ÁâàÊú¨ÔºåÂä†ÂÖ•‰∫Ü„ÄåÂ∞èÊ∏∏Êàè„ÄçÂäüËÉΩÔºåÂπ∂Êèê‰æõ‰∫ÜÂÆòÊñπ DEMO„ÄåË∑≥‰∏ÄË∑≥„Äç„ÄÇËøôÊòØ‰∏Ä‰∏™ 2.5D ÊèíÁîªÈ£éÊ†ºÁöÑÁõäÊô∫Ê∏∏ÊàèÔºåÁé©ÂÆ∂ÂèØ‰ª•ÈÄöËøáÊåâÂéãÂ±èÂπïÊó∂Èó¥ÁöÑÈïøÁü≠Êù•ÊéßÂà∂Ëøô‰∏™„ÄåÂ∞è‰∫∫„ÄçË∑≥Ë∑ÉÁöÑË∑ùÁ¶ª„ÄÇÂàÜÊï∞Ë∂äÈ´òÔºåÈÇ£‰πàÂú®Â•ΩÂèãÊéíË°åÊ¶úÊõ¥Âä†Èù†Ââç„ÄÇÈÄöËøá Python ËÑöÊú¨Ëá™Âä®ËøêË°åÔºåËÆ©‰Ω†ËΩªÊùæÈú∏Ê¶ú„ÄÇ


ÂèØËÉΩÂàöÂºÄÂßã‰∏äÊâãÁöÑÊó∂ÂÄôÔºåÂõ†‰∏∫Êó∂Èó¥Ë∑ùÁ¶ª‰πãÈó¥ÁöÑÂÖ≥Á≥ªÊääÊè°‰∏çÊÅ∞ÂΩìÔºåÂè™ËÉΩË∑≥Âá∫Âá†‰∏™Â∞±ÊéâÂà∞‰∫ÜÂè∞Â≠ê‰∏ãÈù¢„ÄÇÂ¶ÇÊûúËÉΩÂà©Áî®ÂõæÂÉèËØÜÂà´Á≤æÁ°ÆÊµãÈáèÂá∫Ëµ∑ÂßãÂíåÁõÆÊ†áÁÇπ‰πãÈó¥ÊµãË∑ùÁ¶ªÔºåÂ∞±ÂèØ‰ª•‰º∞ËÆ°ÊåâÂéãÁöÑÊó∂Èó¥Êù•Á≤æÁ°ÆË∑≥Ë∑É„ÄÇ
ÂéüÁêÜËØ¥Êòé
Áî±‰∫éÂæÆ‰ø°Ê£ÄÊµãÈùûÂ∏∏‰∏•ÂéâÔºåËøôÈáåÁöÑÈò≤Á¶Å‰ª£Á†ÅÂèØËÉΩÂ∑≤Áªè‰∏çËµ∑‰ΩúÁî®Ôºå‰∏ªË¶Å‰æõÂ≠¶‰π†Áî®ÈÄî


Â∞ÜÊâãÊú∫ÁÇπÂáªÂà∞„ÄäË∑≥‰∏ÄË∑≥„ÄãÂ∞èÁ®ãÂ∫èÁïåÈù¢


Áî® ADB Â∑•ÂÖ∑Ëé∑ÂèñÂΩìÂâçÊâãÊú∫Êà™ÂõæÔºåÂπ∂Áî® ADB Â∞ÜÊà™Âõæ pull ‰∏äÊù•


adb shell screencap -p /sdcard/autojump.png
adb pull /sdcard/autojump.png .

ËÆ°ÁÆóÊåâÂéãÊó∂Èó¥


ÊâãÂä®ÁâàÔºöÁî® Matplotlib ÊòæÁ§∫Êà™ÂõæÔºåÁî®Èº†Ê†áÂÖàÁÇπÂáªËµ∑ÂßãÁÇπ‰ΩçÁΩÆÔºåÁÑ∂ÂêéÁÇπÂáªÁõÆÊ†á‰ΩçÁΩÆÔºåËÆ°ÁÆóÂÉèÁ¥†Ë∑ùÁ¶ªÔºõ
Ëá™Âä®ÁâàÔºöÈù†Ê£ãÂ≠êÁöÑÈ¢úËâ≤Êù•ËØÜÂà´Ê£ãÂ≠êÔºåÈù†Â∫ïËâ≤ÂíåÊñπÂùóÁöÑËâ≤Â∑ÆÊù•ËØÜÂà´Ê£ãÁõòÔºõ


Áî® ADB Â∑•ÂÖ∑ÁÇπÂáªÂ±èÂπïËìÑÂäõ‰∏ÄË∑≥

adb shell input swipe x y x y time(ms)
‰ΩøÁî®ÊïôÁ®ã
Áõ∏ÂÖ≥ËΩØ‰ª∂Â∑•ÂÖ∑ÂÆâË£ÖÂíå‰ΩøÁî®Ê≠•È™§ËØ∑ÂèÇËÄÉ Android Âíå iOS Êìç‰ΩúÊ≠•È™§
Ëé∑ÂèñÊ∫êÁ†Å
- git clone https://github.com/wangshub/wechat_jump_game.git


ÈùûÂ∏∏Êé®Ëçê‰ΩøÁî®Python3ÔºåÈÅøÂÖçÁºñÁ†ÅÂèäimportÈóÆÈ¢ò
PR Ë¶ÅÊ±Ç
ËØ∑ÈÄâÊã© merge Ëøõ master ÂàÜÊîØÔºåÂπ∂‰∏îÊ†áÈ¢òÂÜô‰∏äÁÆÄÁü≠ÊèèËø∞Ôºå‰æãÂ≠ê
[‰ºòÂåñ] ‰ΩøÁî®PEP8‰ºòÂåñ‰ª£Á†Å
ÁâàÊú¨ËØ¥Êòé

master ÂàÜÊîØÔºöÁ®≥ÂÆöÁâàÊú¨ÔºåÂ∑≤ÈÄöËøáÊµãËØï
dev ÂàÜÊîØÔºöÂºÄÂèëÁâàÊú¨ÔºåÂåÖÂê´‰∏Ä‰∫õËæÉÁ®≥ÂÆöÁöÑÊñ∞ÂäüËÉΩÔºåÁ¥ØËÆ°Â§ö‰∏™ÂäüËÉΩÂπ∂ÊµãËØïÈÄöËøáÂêéÂêàÂπ∂Ëá≥ prod ÂàÜÊîØ
ÂÖ∂‰ªñÂàÜÊîØÔºöÂäüËÉΩÂºÄÂèë (feature) ÊàñÈóÆÈ¢ò‰øÆÂ§ç (bugfix)ÔºåÂ±û‰∫éÊúÄÊñ∞Â∞ùÈ≤úÁâàÊú¨ÔºåÂèØËÉΩÂ§Ñ‰∫éÂºÄÂèë‰∏≠ÁöÑÁä∂ÊÄÅÔºåÂü∫Êú¨ÂÆåÊàêÂêéÂêàÂπ∂Ëá≥ dev ÂàÜÊîØ

FAQ

ËØ¶ËßÅ Wiki-FAQ

Êõ¥Êñ∞Êó•Âøó

ËØ¶ËßÅ changelog

ÂºÄÂèëËÄÖÂàóË°®

ËØ¶ËßÅ contributors

‰∫§ÊµÅ


314659953 (1000 ‰∫∫)


176740763 (500 ‰∫∫)


ÊàñËÄÖÂÖ≥Ê≥®ÊàëÁöÑÂæÆ‰ø°ÂÖ¨‰ºóÂè∑ÂêéÂè∞ÁïôË®Ä



",GitHub - wangshub/wechat_jump_game: ÂæÆ‰ø°„ÄäË∑≥‰∏ÄË∑≥„ÄãPython ËæÖÂä©
77,Python,"pyspider   
A Powerful Spider(Web Crawler) System in Python. TRY IT NOW!

Write script in Python
Powerful WebUI with script editor, task monitor, project manager and result viewer
MySQL, CouchDB, MongoDB, Redis, SQLite, Elasticsearch; PostgreSQL with SQLAlchemy as database backend
RabbitMQ, Redis and Kombu as message queue
Task priority, retry, periodical, recrawl by age, etc...
Distributed architecture, Crawl Javascript pages, Python 2.{6,7}, 3.{3,4,5,6} support, etc...

Tutorial: http://docs.pyspider.org/en/latest/tutorial/
Documentation: http://docs.pyspider.org/
Release notes: https://github.com/binux/pyspider/releases
Sample Code
from pyspider.libs.base_handler import *


class Handler(BaseHandler):
    crawl_config = {
    }

    @every(minutes=24 * 60)
    def on_start(self):
        self.crawl('http://scrapy.org/', callback=self.index_page)

    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('a[href^=""http""]').items():
            self.crawl(each.attr.href, callback=self.detail_page)

    def detail_page(self, response):
        return {
            ""url"": response.url,
            ""title"": response.doc('title').text(),
        }

Installation

pip install pyspider
run command pyspider, visit http://localhost:5000/

WARNING: WebUI is open to the public by default, it can be used to execute any command which may harm your system. Please use it in an internal network or enable need-auth for webui.
Quickstart: http://docs.pyspider.org/en/latest/Quickstart/
Contribute

Use It
Open Issue, send PR
User Group
‰∏≠ÊñáÈóÆÁ≠î

TODO
v0.4.0

 a visual scraping interface like portia

License
Licensed under the Apache License, Version 2.0
",GitHub - binux/pyspider: A Powerful Spider(Web Crawler) System in Python.
78,Python,"
The Uncompromising Code Formatter











‚ÄúAny color you like.‚Äù

Black is the uncompromising Python code formatter. By using it, you agree to cede
control over minutiae of hand-formatting. In return, Black gives you speed,
determinism, and freedom from pycodestyle nagging about formatting. You will save time
and mental energy for more important matters.
Blackened code looks the same regardless of the project you're reading. Formatting
becomes transparent after a while and you can focus on the content instead.
Black makes code review faster by producing the smallest diffs possible.
Try it out now using the Black Playground. Watch the
PyCon 2019 talk to learn more.

Contents: Installation and usage |
Code style | pyproject.toml |
Editor integration | blackd |
Version control integration |
Ignoring unmodified files | Used by |
Testimonials | Show your style |
Contributing | Change Log |
Authors

Installation and usage
Installation
Black can be installed by running pip install black. It requires Python 3.6.0+ to
run but you can reformat Python 2 code with it, too.
Usage
To get started right away with sensible defaults:
black {source_file_or_directory}

Command line options
Black doesn't provide many options. You can list them by running black --help:
black [OPTIONS] [SRC]...

Options:
  -c, --code TEXT                 Format the code passed in as a string.
  -l, --line-length INTEGER       How many characters per line to allow.
                                  [default: 88]
  -t, --target-version [py27|py33|py34|py35|py36|py37|py38]
                                  Python versions that should be supported by
                                  Black's output. [default: per-file auto-
                                  detection]
  --py36                          Allow using Python 3.6-only syntax on all
                                  input files.  This will put trailing commas
                                  in function signatures and calls also after
                                  *args and **kwargs. Deprecated; use
                                  --target-version instead. [default: per-file
                                  auto-detection]
  --pyi                           Format all input files like typing stubs
                                  regardless of file extension (useful when
                                  piping source on standard input).
  -S, --skip-string-normalization
                                  Don't normalize string quotes or prefixes.
  --check                         Don't write the files back, just return the
                                  status.  Return code 0 means nothing would
                                  change.  Return code 1 means some files
                                  would be reformatted.  Return code 123 means
                                  there was an internal error.
  --diff                          Don't write the files back, just output a
                                  diff for each file on stdout.
  --fast / --safe                 If --fast given, skip temporary sanity
                                  checks. [default: --safe]
  --include TEXT                  A regular expression that matches files and
                                  directories that should be included on
                                  recursive searches.  An empty value means
                                  all files are included regardless of the
                                  name.  Use forward slashes for directories
                                  on all platforms (Windows, too).  Exclusions
                                  are calculated first, inclusions later.
                                  [default: \.pyi?$]
  --exclude TEXT                  A regular expression that matches files and
                                  directories that should be excluded on
                                  recursive searches.  An empty value means no
                                  paths are excluded. Use forward slashes for
                                  directories on all platforms (Windows, too).
                                  Exclusions are calculated first, inclusions
                                  later.  [default: /(\.eggs|\.git|\.hg|\.mypy
                                  _cache|\.nox|\.tox|\.venv|_build|buck-
                                  out|build|dist)/]
  -q, --quiet                     Don't emit non-error messages to stderr.
                                  Errors are still emitted, silence those with
                                  2>/dev/null.
  -v, --verbose                   Also emit messages to stderr about files
                                  that were not changed or were ignored due to
                                  --exclude=.
  --version                       Show the version and exit.
  --config PATH                   Read configuration from PATH.
  -h, --help                      Show this message and exit.

Black is a well-behaved Unix-style command-line tool:

it does nothing if no sources are passed to it;
it will read from standard input and write to standard output if - is used as the
filename;
it only outputs messages to users on standard error;
exits with code 0 unless an internal error occurred (or --check was used).

NOTE: This is a beta product
Black is already successfully used by many projects, small and big. It
also sports a decent test suite. However, it is still very new. Things will probably be
wonky for a while. This is made explicit by the ""Beta"" trove classifier, as well as by
the ""b"" in the version number. What this means for you is that until the formatter
becomes stable, you should expect some formatting to change in the future. That being
said, no drastic stylistic changes are planned, mostly responses to bug reports.
Also, as a temporary safety measure, Black will check that the reformatted code still
produces a valid AST that is equivalent to the original. This slows it down. If you're
feeling confident, use --fast.
The Black code style
Black reformats entire files in place. It is not configurable. It doesn't take
previous formatting into account. It doesn't reformat blocks that start with
# fmt: off and end with # fmt: on. # fmt: on/off have to be on the same level of
indentation. It also recognizes YAPF's block comments
to the same effect, as a courtesy for straddling code.
How Black wraps lines
Black ignores previous formatting and applies uniform horizontal and vertical
whitespace to your code. The rules for horizontal whitespace can be summarized as: do
whatever makes pycodestyle happy. The coding style used by Black can be viewed as a
strict subset of PEP 8.
As for vertical whitespace, Black tries to render one full expression or simple
statement per line. If this fits the allotted line length, great.
# in:

j = [1,
     2,
     3
]

# out:

j = [1, 2, 3]
If not, Black will look at the contents of the first outer matching brackets and put
that in a separate indented line.
# in:

ImportantClass.important_method(exc, limit, lookup_lines, capture_locals, extra_argument)

# out:

ImportantClass.important_method(
    exc, limit, lookup_lines, capture_locals, extra_argument
)
If that still doesn't fit the bill, it will decompose the internal expression further
using the same rule, indenting matching brackets every time. If the contents of the
matching brackets pair are comma-separated (like an argument list, or a dict literal,
and so on) then Black will first try to keep them on the same line with the matching
brackets. If that doesn't work, it will put all of them in separate lines.
# in:

def very_important_function(template: str, *variables, file: os.PathLike, engine: str, header: bool = True, debug: bool = False):
    """"""Applies `variables` to the `template` and writes to `file`.""""""
    with open(file, 'w') as f:
        ...

# out:

def very_important_function(
    template: str,
    *variables,
    file: os.PathLike,
    engine: str,
    header: bool = True,
    debug: bool = False,
):
    """"""Applies `variables` to the `template` and writes to `file`.""""""
    with open(file, ""w"") as f:
        ...
You might have noticed that closing brackets are always dedented and that a trailing
comma is always added. Such formatting produces smaller diffs; when you add or remove an
element, it's always just one line. Also, having the closing bracket dedented provides a
clear delimiter between two distinct sections of the code that otherwise share the same
indentation level (like the arguments list and the docstring in the example above).
If a data structure literal (tuple, list, set, dict) or a line of ""from"" imports cannot
fit in the allotted length, it's always split into one element per line. This minimizes
diffs as well as enables readers of code to find which commit introduced a particular
entry. This also makes Black compatible with isort with
the following configuration.

A compatible `.isort.cfg`
[settings]
multi_line_output=3
include_trailing_comma=True
force_grid_wrap=0
use_parentheses=True
line_length=88

The equivalent command line is:
$ isort --multi-line=3 --trailing-comma --force-grid-wrap=0 --use-parentheses --line-width=88 [ file.py ]


Line length
You probably noticed the peculiar default line length. Black defaults to 88 characters
per line, which happens to be 10% over 80. This number was found to produce
significantly shorter files than sticking with 80 (the most popular), or even 79 (used
by the standard library). In general,
90-ish seems like the wise choice.
If you're paid by the line of code you write, you can pass --line-length with a lower
number. Black will try to respect that. However, sometimes it won't be able to without
breaking other rules. In those rare cases, auto-formatted code will exceed your allotted
limit.
You can also increase it, but remember that people with sight disabilities find it
harder to work with line lengths exceeding 100 characters. It also adversely affects
side-by-side diff review on typical screen resolutions. Long lines also make it harder
to present code neatly in documentation or talk slides.
If you're using Flake8, you can bump max-line-length to 88 and forget about it.
Alternatively, use Bugbear's B950 warning
instead of E501 and keep the max line length at 80 which you are probably already using.
You'd do it like this:
[flake8]
max-line-length = 80
...
select = C,E,F,W,B,B950
ignore = E203, E501, W503
You'll find Black's own .flake8 config file is configured like this. Explanation of
why W503 and E203 are disabled can be found further in this documentation. And if you're
curious about the reasoning behind B950,
Bugbear's documentation
explains it. The tl;dr is ""it's like highway speed limits, we won't bother you if you
overdo it by a few km/h"".
If you're looking for a minimal, black-compatible flake8 configuration:
[flake8]
max-line-length = 88
extend-ignore = E203
Empty lines
Black avoids spurious vertical whitespace. This is in the spirit of PEP 8 which says
that in-function vertical whitespace should only be used sparingly.
Black will allow single empty lines inside functions, and single and double empty
lines on module level left by the original editors, except when they're within
parenthesized expressions. Since such expressions are always reformatted to fit minimal
space, this whitespace is lost.
It will also insert proper spacing before and after function definitions. It's one line
before and after inner functions and two lines before and after module-level functions
and classes. Black will not put empty lines between function/class definitions and
standalone comments that immediately precede the given function/class.
Black will enforce single empty lines between a class-level docstring and the first
following field or method. This conforms to
PEP 257.
Black won't insert empty lines after function docstrings unless that empty line is
required due to an inner function starting immediately after.
Trailing commas
Black will add trailing commas to expressions that are split by comma where each
element is on its own line. This includes function signatures.
Unnecessary trailing commas are removed if an expression fits in one line. This makes it
1% more likely that your line won't exceed the allotted line length limit. Moreover, in
this scenario, if you added another argument to your call, you'd probably fit it in the
same line anyway. That doesn't make diffs any larger.
One exception to removing trailing commas is tuple expressions with just one element. In
this case Black won't touch the single trailing comma as this would unexpectedly
change the underlying data type. Note that this is also the case when commas are used
while indexing. This is a tuple in disguise: numpy_array[3, ].
One exception to adding trailing commas is function signatures containing *, *args,
or **kwargs. In this case a trailing comma is only safe to use on Python 3.6. Black
will detect if your file is already 3.6+ only and use trailing commas in this situation.
If you wonder how it knows, it looks for f-strings and existing use of trailing commas
in function signatures that have stars in them. In other words, if you'd like a trailing
comma in this situation and Black didn't recognize it was safe to do so, put it there
manually and Black will keep it.
Strings
Black prefers double quotes ("" and """""") over single quotes (' and '''). It
will replace the latter with the former as long as it does not result in more backslash
escapes than before.
Black also standardizes string prefixes, making them always lowercase. On top of that,
if your code is already Python 3.6+ only or it's using the unicode_literals future
import, Black will remove u from the string prefix as it is meaningless in those
scenarios.
The main reason to standardize on a single form of quotes is aesthetics. Having one kind
of quotes everywhere reduces reader distraction. It will also enable a future version of
Black to merge consecutive string literals that ended up on the same line (see
#26 for details).
Why settle on double quotes? They anticipate apostrophes in English text. They match the
docstring standard described in
PEP 257. An empty
string in double quotes ("""") is impossible to confuse with a one double-quote
regardless of fonts and syntax highlighting used. On top of this, double quotes for
strings are consistent with C which Python interacts a lot with.
On certain keyboard layouts like US English, typing single quotes is a bit easier than
double quotes. The latter requires use of the Shift key. My recommendation here is to
keep using whatever is faster to type and let Black handle the transformation.
If you are adopting Black in a large project with pre-existing string conventions
(like the popular
""single quotes for data, double quotes for human-readable strings""),
you can pass --skip-string-normalization on the command line. This is meant as an
adoption helper, avoid using this for new projects.
Numeric literals
Black standardizes most numeric literals to use lowercase letters for the syntactic
parts and uppercase letters for the digits themselves: 0xAB instead of 0XAB and
1e10 instead of 1E10. Python 2 long literals are styled as 2L instead of 2l to
avoid confusion between l and 1.
Line breaks & binary operators
Black will break a line before a binary operator when splitting a block of code over
multiple lines. This is so that Black is compliant with the recent changes in the
PEP 8
style guide, which emphasizes that this approach improves readability.
This behaviour may raise W503 line break before binary operator warnings in style
guide enforcement tools like Flake8. Since W503 is not PEP 8 compliant, you should
tell Flake8 to ignore these warnings.
Slices
PEP 8
recommends
to treat : in slices as a binary operator with the lowest priority, and to leave an
equal amount of space on either side, except if a parameter is omitted (e.g.
ham[1 + 1 :]). It also states that for extended slices, both : operators have to
have the same amount of spacing, except if a parameter is omitted (ham[1 + 1 ::]).
Black enforces these rules consistently.
This behaviour may raise E203 whitespace before ':' warnings in style guide
enforcement tools like Flake8. Since E203 is not PEP 8 compliant, you should tell
Flake8 to ignore these warnings.
Parentheses
Some parentheses are optional in the Python grammar. Any expression can be wrapped in a
pair of parentheses to form an atom. There are a few interesting cases:

if (...):
while (...):
for (...) in (...):
assert (...), (...)
from X import (...)
assignments like:

target = (...)
target: type = (...)
some, *un, packing = (...)
augmented += (...)



In those cases, parentheses are removed when the entire statement fits in one line, or
if the inner expression doesn't have any delimiters to further split on. If there is
only a single delimiter and the expression starts or ends with a bracket, the
parenthesis can also be successfully omitted since the existing bracket pair will
organize the expression neatly anyway. Otherwise, the parentheses are added.
Please note that Black does not add or remove any additional nested parentheses that
you might want to have for clarity or further code organization. For example those
parentheses are not going to be removed:
return not (this or that)
decision = (maybe.this() and values > 0) or (maybe.that() and values < 0)
Call chains
Some popular APIs, like ORMs, use call chaining. This API style is known as a
fluent interface. Black formats
those by treating dots that follow a call or an indexing operation like a very low
priority delimiter. It's easier to show the behavior than to explain it. Look at the
example:
def example(session):
    result = (
        session.query(models.Customer.id)
        .filter(
            models.Customer.account_id == account_id,
            models.Customer.email == email_address,
        )
        .order_by(models.Customer.id.asc())
        .all()
    )
Typing stub files
PEP 484 describes the syntax for type hints in Python. One of the use cases for typing
is providing type annotations for modules which cannot contain them directly (they might
be written in C, or they might be third-party, or their implementation may be overly
dynamic, and so on).
To solve this,
stub files with the .pyi file extension
can be used to describe typing information for an external module. Those stub files omit
the implementation of classes and functions they describe, instead they only contain the
structure of the file (listing globals, functions, and classes with their members). The
recommended code style for those files is more terse than PEP 8:

prefer ... on the same line as the class/function signature;
avoid vertical whitespace between consecutive module-level functions, names, or
methods and fields within a single class;
use a single blank line between top-level class definitions, or none if the classes
are very small.

Black enforces the above rules. There are additional guidelines for formatting .pyi
file that are not enforced yet but might be in a future version of the formatter:

all function bodies should be empty (contain ... instead of the body);
do not use docstrings;
prefer ... over pass;
for arguments with a default, use ... instead of the actual default;
avoid using string literals in type annotations, stub files support forward references
natively (like Python 3.7 code with from __future__ import annotations);
use variable annotations instead of type comments, even for stubs that target older
versions of Python;
for arguments that default to None, use Optional[] explicitly;
use float instead of Union[int, float].

pyproject.toml
Black is able to read project-specific default values for its command line options
from a pyproject.toml file. This is especially useful for specifying custom
--include and --exclude patterns for your project.
Pro-tip: If you're asking yourself ""Do I need to configure anything?"" the answer is
""No"". Black is all about sensible defaults.
What on Earth is a pyproject.toml file?
PEP 518 defines pyproject.toml as a
configuration file to store build system requirements for Python projects. With the help
of tools like Poetry or
Flit it can fully replace the need for
setup.py and setup.cfg files.
Where Black looks for the file
By default Black looks for pyproject.toml starting from the common base directory of
all files and directories passed on the command line. If it's not there, it looks in
parent directories. It stops looking when it finds the file, or a .git directory, or a
.hg directory, or the root of the file system, whichever comes first.
If you're formatting standard input, Black will look for configuration starting from
the current working directory.
You can also explicitly specify the path to a particular file that you want with
--config. In this situation Black will not look for any other file.
If you're running with --verbose, you will see a blue message if a file was found and
used.
Please note blackd will not use pyproject.toml configuration.
Configuration format
As the file extension suggests, pyproject.toml is a
TOML file. It contains separate sections for
different tools. Black is using the [tool.black] section. The option keys are the
same as long names of options on the command line.
Note that you have to use single-quoted strings in TOML for regular expressions. It's
the equivalent of r-strings in Python. Multiline strings are treated as verbose regular
expressions by Black. Use [ ] to denote a significant space character.

Example `pyproject.toml`
[tool.black]
line-length = 88
target-version = ['py37']
include = '\.pyi?$'
exclude = '''

(
  /(
      \.eggs         # exclude a few common directories in the
    | \.git          # root of the project
    | \.hg
    | \.mypy_cache
    | \.tox
    | \.venv
    | _build
    | buck-out
    | build
    | dist
  )/
  | foo.py           # also separately exclude a file named foo.py in
                     # the root of the project
)
'''

Lookup hierarchy
Command-line options have defaults that you can see in --help. A pyproject.toml can
override those defaults. Finally, options provided by the user on the command line
override both.
Black will only ever use one pyproject.toml file during an entire run. It doesn't
look for multiple files, and doesn't compose configuration from different levels of the
file hierarchy.
Editor integration
Emacs
Use proofit404/blacken or
Elpy.
PyCharm/IntelliJ IDEA

Install black.

$ pip install black

Locate your black installation folder.

On macOS / Linux / BSD:
$ which black
/usr/local/bin/black  # possible location
On Windows:
$ where black
%LocalAppData%\Programs\Python\Python36-32\Scripts\black.exe  # possible location

Open External tools in PyCharm/IntelliJ IDEA

On macOS:
PyCharm -> Preferences -> Tools -> External Tools
On Windows / Linux / BSD:
File -> Settings -> Tools -> External Tools


Click the + icon to add a new external tool with the following values:

Name: Black
Description: Black is the uncompromising Python code formatter.
Program: <install_location_from_step_2>
Arguments: ""$FilePath$""



Format the currently opened file by selecting Tools -> External Tools -> black.

Alternatively, you can set a keyboard shortcut by navigating to
Preferences or Settings -> Keymap -> External Tools -> External Tools - Black.



Optionally, run Black on every file save:

Make sure you have the
File Watcher plugin
installed.
Go to Preferences or Settings -> Tools -> File Watchers and click + to add a
new watcher:

Name: Black
File type: Python
Scope: Project Files
Program: <install_location_from_step_2>
Arguments: $FilePath$
Output paths to refresh: $FilePath$
Working directory: $ProjectFileDir$




Uncheck ""Auto-save edited files to trigger the watcher""



Wing IDE
Wing supports black via the OS Commands tool, as explained in the Wing documentation on
pep8 formatting. The detailed procedure is:

Install black.

$ pip install black

Make sure it runs from the command line, e.g.

$ black --help

In Wing IDE, activate the OS Commands panel and define the command black to
execute black on the currently selected file:


Use the Tools -> OS Commands menu selection
click on + in OS Commands -> New: Command line..

Title: black
Command Line: black %s
I/O Encoding: Use Default
Key Binding: F1
 Raise OS Commands when executed
 Auto-save files before execution
 Line mode




Select a file in the editor and press F1 , or whatever key binding you selected
in step 3, to reformat the file.

Vim
Commands and shortcuts:

:Black to format the entire file (ranges not supported);
:BlackUpgrade to upgrade Black inside the virtualenv;
:BlackVersion to get the current version of Black inside the virtualenv.

Configuration:

g:black_fast (defaults to 0)
g:black_linelength (defaults to 88)
g:black_skip_string_normalization (defaults to 0)
g:black_virtualenv (defaults to ~/.vim/black or ~/.local/share/nvim/black)

To install with vim-plug:
Plug 'psf/black'

or with Vundle:
Plugin 'psf/black'

or you can copy the plugin from
plugin/black.vim.
mkdir -p ~/.vim/pack/python/start/black/plugin
curl https://raw.githubusercontent.com/psf/black/master/plugin/black.vim -o ~/.vim/pack/python/start/black/plugin/black.vim

Let me know if this requires any changes to work with Vim 8's builtin packadd, or
Pathogen, and so on.
This plugin requires Vim 7.0+ built with Python 3.6+ support. It needs Python 3.6 to
be able to run Black inside the Vim process which is much faster than calling an
external command.
On first run, the plugin creates its own virtualenv using the right Python version and
automatically installs Black. You can upgrade it later by calling :BlackUpgrade and
restarting Vim.
If you need to do anything special to make your virtualenv work and install Black (for
example you want to run a version from master), create a virtualenv manually and point
g:black_virtualenv to it. The plugin will use it.
To run Black on save, add the following line to .vimrc or init.vim:
autocmd BufWritePre *.py execute ':Black'

To run Black on a key press (e.g. F9 below), add this:
nnoremap <F9> :Black<CR>

How to get Vim with Python 3.6? On Ubuntu 17.10 Vim comes with Python 3.6 by
default. On macOS with Homebrew run: brew install vim --with-python3. When building
Vim from source, use: ./configure --enable-python3interp=yes. There's many guides
online how to do this.
Visual Studio Code
Use the
Python extension
(instructions).
SublimeText 3
Use sublack plugin.
Jupyter Notebook Magic
Use blackcellmagic.
Python Language Server
If your editor supports the Language Server Protocol (Atom,
Sublime Text, Visual Studio Code and many more), you can use the
Python Language Server with the
pyls-black plugin.
Atom/Nuclide
Use python-black.
Kakoune
Add the following hook to your kakrc, then run black with :format.
hook global WinSetOption filetype=python %{
    set-option window formatcmd 'black -q  -'
}

Thonny
Use Thonny-black-code-format.
Other editors
Other editors will require external contributions.
Patches welcome! ‚ú® üç∞ ‚ú®
Any tool that can pipe code through Black using its stdio mode (just
use - as the file name).
The formatted code will be returned on stdout (unless --check was passed). Black
will still emit messages on stderr but that shouldn't affect your use case.
This can be used for example with PyCharm's or IntelliJ's
File Watchers.
blackd
blackd is a small HTTP server that exposes Black's functionality over a simple
protocol. The main benefit of using it is to avoid paying the cost of starting up a new
Black process every time you want to blacken a file.
Usage
blackd is not packaged alongside Black by default because it has additional
dependencies. You will need to do pip install black[d] to install it.
You can start the server on the default port, binding only to the local interface by
running blackd. You will see a single line mentioning the server's version, and the
host and port it's listening on. blackd will then print an access log similar to most
web servers on standard output, merged with any exception traces caused by invalid
formatting requests.
blackd provides even less options than Black. You can see them by running
blackd --help:
Usage: blackd [OPTIONS]

Options:
  --bind-host TEXT                Address to bind the server to.
  --bind-port INTEGER             Port to listen on
  --version                       Show the version and exit.
  -h, --help                      Show this message and exit.

There is no official blackd client tool (yet!). You can test that blackd is working
using curl:
blackd --bind-port 9090 &  # or let blackd choose a port
curl -s -XPOST ""localhost:9090"" -d ""print('valid')""

Protocol
blackd only accepts POST requests at the / path. The body of the request should
contain the python source code to be formatted, encoded according to the charset field
in the Content-Type request header. If no charset is specified, blackd assumes
UTF-8.
There are a few HTTP headers that control how the source is formatted. These correspond
to command line flags for Black. There is one exception to this: X-Protocol-Version
which if present, should have the value 1, otherwise the request is rejected with
HTTP 501 (Not Implemented).
The headers controlling how code is formatted are:

X-Line-Length: corresponds to the --line-length command line flag.
X-Skip-String-Normalization: corresponds to the --skip-string-normalization
command line flag. If present and its value is not the empty string, no string
normalization will be performed.
X-Fast-Or-Safe: if set to fast, blackd will act as Black does when passed the
--fast command line flag.
X-Python-Variant: if set to pyi, blackd will act as Black does when passed the
--pyi command line flag. Otherwise, its value must correspond to a Python version or
a set of comma-separated Python versions, optionally prefixed with py. For example,
to request code that is compatible with Python 3.5 and 3.6, set the header to
py3.5,py3.6.
X-Diff: corresponds to the --diff command line flag. If present, a diff of the
formats will be output.

If any of these headers are set to invalid values, blackd returns a HTTP 400 error
response, mentioning the name of the problematic header in the message body.
Apart from the above, blackd can produce the following response codes:

HTTP 204: If the input is already well-formatted. The response body is empty.
HTTP 200: If formatting was needed on the input. The response body contains the
blackened Python code, and the Content-Type header is set accordingly.
HTTP 400: If the input contains a syntax error. Details of the error are returned in
the response body.
HTTP 500: If there was any kind of error while trying to format the input. The
response body contains a textual representation of the error.

The response headers include a X-Black-Version header containing the version of
Black.
Version control integration
Use pre-commit. Once you
have it installed, add this to the
.pre-commit-config.yaml in your repository:
repos:
  - repo: https://github.com/psf/black
    rev: stable
    hooks:
      - id: black
        language_version: python3.6
Then run pre-commit install and you're ready to go.
Avoid using args in the hook. Instead, store necessary configuration in
pyproject.toml so that editors and command-line usage of Black all behave consistently
for your project. See Black's own pyproject.toml for an example.
If you're already using Python 3.7, switch the language_version accordingly. Finally,
stable is a tag that is pinned to the latest release on PyPI. If you'd rather run on
master, this is also an option.
Ignoring unmodified files
Black remembers files it has already formatted, unless the --diff flag is used or
code is passed via standard input. This information is stored per-user. The exact
location of the file depends on the Black version and the system on which Black is
run. The file is non-portable. The standard location on common operating systems is:

Windows:
C:\\Users\<username>\AppData\Local\black\black\Cache\<version>\cache.<line-length>.<file-mode>.pickle
macOS:
/Users/<username>/Library/Caches/black/<version>/cache.<line-length>.<file-mode>.pickle
Linux:
/home/<username>/.cache/black/<version>/cache.<line-length>.<file-mode>.pickle

file-mode is an int flag that determines whether the file was formatted as 3.6+ only,
as .pyi, and whether string normalization was omitted.
To override the location of these files on macOS or Linux, set the environment variable
XDG_CACHE_HOME to your preferred location. For example, if you want to put the cache
in the directory you're running Black from, set XDG_CACHE_HOME=.cache. Black will
then write the above files to .cache/black/<version>/.
Used by
The following notable open-source projects trust Black with enforcing a consistent
code style: pytest, tox, Pyramid, Django Channels, Hypothesis, attrs, SQLAlchemy,
Poetry, PyPA applications (Warehouse, Pipenv, virtualenv), pandas, Pillow, every Datadog
Agent Integration, Home Assistant.
Are we missing anyone? Let us know.
Testimonials
Dusty Phillips,
writer:

Black is opinionated so you don't have to be.

Hynek Schlawack, creator of attrs, core developer of
Twisted and CPython:

An auto-formatter that doesn't suck is all I want for Xmas!

Carl Meyer, Django core developer:

At least the name is good.

Kenneth Reitz, creator of requests and
pipenv:

This vastly improves the formatting of our code. Thanks a ton!

Show your style
Use the badge in your project's README.md:
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
Using the badge in README.rst:
.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
    :target: https://github.com/psf/black

Looks like this:

License
MIT
Contributing to Black
In terms of inspiration, Black is about as configurable as gofmt. This is
deliberate.
Bug reports and fixes are always welcome! However, before you suggest a new feature or
configuration knob, ask yourself why you want it. If it enables better integration with
some workflow, fixes an inconsistency, speeds things up, and so on - go for it! On the
other hand, if your answer is ""because I don't like a particular formatting"" then you're
not ready to embrace Black yet. Such changes are unlikely to get accepted. You can
still try but prepare to be disappointed.
More details can be found in CONTRIBUTING.
Change Log
19.10b0


added support for PEP 572 assignment expressions (#711)


added support for PEP 570 positional-only arguments (#943)


added support for async generators (#593)


added support for pre-splitting collections by putting an explicit trailing comma
inside (#826)


added black -c as a way to format code passed from the command line (#761)


--safe now works with Python 2 code (#840)


fixed grammar selection for Python 2-specific code (#765)


fixed feature detection for trailing commas in function definitions and call sites
(#763)


# fmt: off/# fmt: on comment pairs placed multiple times within the same block of
code now behave correctly (#1005)


Black no longer crashes on Windows machines with more than 61 cores (#838)


Black no longer crashes on standalone comments prepended with a backslash (#767)


Black no longer crashes on from ... import blocks with comments (#829)


Black no longer crashes on Python 3.7 on some platform configurations (#494)


Black no longer fails on comments in from-imports (#671)


Black no longer fails when the file starts with a backslash (#922)


Black no longer merges regular comments with type comments (#1027)


Black no longer splits long lines that contain type comments (#997)


removed unnecessary parentheses around yield expressions (#834)


added parentheses around long tuples in unpacking assignments (#832)


added parentheses around complex powers when they are prefixed by a unary operator
(#646)


fixed bug that led Black format some code with a line length target of 1 (#762)


Black no longer introduces quotes in f-string subexpressions on string boundaries
(#863)


if Black puts parenthesis around a single expression, it moves comments to the
wrapped expression instead of after the brackets (#872)


blackd now returns the version of Black in the response headers (#1013)


blackd can now output the diff of formats on source code when the X-Diff header is
provided (#969)


19.3b0


new option --target-version to control which Python versions Black-formatted code
should target (#618)


deprecated --py36 (use --target-version=py36 instead) (#724)


Black no longer normalizes numeric literals to include _ separators (#696)


long del statements are now split into multiple lines (#698)


type comments are no longer mangled in function signatures


improved performance of formatting deeply nested data structures (#509)


Black now properly formats multiple files in parallel on Windows (#632)


Black now creates cache files atomically which allows it to be used in parallel
pipelines (like xargs -P8) (#673)


Black now correctly indents comments in files that were previously formatted with
tabs (#262)


blackd now supports CORS (#622)


18.9b0


numeric literals are now formatted by Black (#452, #461, #464, #469):


numeric literals are normalized to include _ separators on Python 3.6+ code


added --skip-numeric-underscore-normalization to disable the above behavior and
leave numeric underscores as they were in the input


code with _ in numeric literals is recognized as Python 3.6+


most letters in numeric literals are lowercased (e.g., in 1e10, 0x01)


hexadecimal digits are always uppercased (e.g. 0xBADC0DE)




added blackd, see its documentation for more info (#349)


adjacent string literals are now correctly split into multiple lines (#463)


trailing comma is now added to single imports that don't fit on a line (#250)


cache is now populated when --check is successful for a file which speeds up
consecutive checks of properly formatted unmodified files (#448)


whitespace at the beginning of the file is now removed (#399)


fixed mangling pweave and
Spyder IDE special comments (#532)


fixed unstable formatting when unpacking big tuples (#267)


fixed parsing of __future__ imports with renames (#389)


fixed scope of # fmt: off when directly preceding yield and other nodes (#385)


fixed formatting of lambda expressions with default arguments (#468)


fixed async for statements: Black no longer breaks them into separate lines (#372)


note: the Vim plugin stopped registering ,= as a default chord as it turned out to
be a bad idea (#415)


18.6b4

hotfix: don't freeze when multiple comments directly precede # fmt: off (#371)

18.6b3


typing stub files (.pyi) now have blank lines added after constants (#340)


# fmt: off and # fmt: on are now much more dependable:


they now work also within bracket pairs (#329)


they now correctly work across function/class boundaries (#335)


they now work when an indentation block starts with empty lines or misaligned
comments (#334)




made Click not fail on invalid environments; note that Click is right but the
likelihood we'll need to access non-ASCII file paths when dealing with Python source
code is low (#277)


fixed improper formatting of f-strings with quotes inside interpolated expressions
(#322)


fixed unnecessary slowdown when long list literals where found in a file


fixed unnecessary slowdown on AST nodes with very many siblings


fixed cannibalizing backslashes during string normalization


fixed a crash due to symbolic links pointing outside of the project directory (#338)


18.6b2


added --config (#65)


added -h equivalent to --help (#316)


fixed improper unmodified file caching when -S was used


fixed extra space in string unpacking (#305)


fixed formatting of empty triple quoted strings (#313)


fixed unnecessary slowdown in comment placement calculation on lines without comments


18.6b1


hotfix: don't output human-facing information on stdout (#299)


hotfix: don't output cake emoji on non-zero return code (#300)


18.6b0


added --include and --exclude (#270)


added --skip-string-normalization (#118)


added --verbose (#283)


the header output in --diff now actually conforms to the unified diff spec


fixed long trivial assignments being wrapped in unnecessary parentheses (#273)


fixed unnecessary parentheses when a line contained multiline strings (#232)


fixed stdin handling not working correctly if an old version of Click was used (#276)


Black now preserves line endings when formatting a file in place (#258)


18.5b1


added --pyi (#249)


added --py36 (#249)


Python grammar pickle caches are stored with the formatting caches, making Black
work in environments where site-packages is not user-writable (#192)


Black now enforces a PEP 257 empty line after a class-level docstring (and/or
fields) and the first method


fixed invalid code produced when standalone comments were present in a trailer that
was omitted from line splitting on a large expression (#237)


fixed optional parentheses being removed within # fmt: off sections (#224)


fixed invalid code produced when stars in very long imports were incorrectly wrapped
in optional parentheses (#234)


fixed unstable formatting when inline comments were moved around in a trailer that was
omitted from line splitting on a large expression (#238)


fixed extra empty line between a class declaration and the first method if no class
docstring or fields are present (#219)


fixed extra empty line between a function signature and an inner function or inner
class (#196)


18.5b0


call chains are now formatted according to the
fluent interfaces style (#67)


data structure literals (tuples, lists, dictionaries, and sets) are now also always
exploded like imports when they don't fit in a single line (#152)


slices are now formatted according to PEP 8 (#178)


parentheses are now also managed automatically on the right-hand side of assignments
and return statements (#140)


math operators now use their respective priorities for delimiting multiline
expressions (#148)


optional parentheses are now omitted on expressions that start or end with a bracket
and only contain a single operator (#177)


empty parentheses in a class definition are now removed (#145, #180)


string prefixes are now standardized to lowercase and u is removed on Python 3.6+
only code and Python 2.7+ code with the unicode_literals future import (#188, #198,
#199)


typing stub files (.pyi) are now formatted in a style that is consistent with PEP
484 (#207, #210)


progress when reformatting many files is now reported incrementally


fixed trailers (content with brackets) being unnecessarily exploded into their own
lines after a dedented closing bracket (#119)


fixed an invalid trailing comma sometimes left in imports (#185)


fixed non-deterministic formatting when multiple pairs of removable parentheses were
used (#183)


fixed multiline strings being unnecessarily wrapped in optional parentheses in long
assignments (#215)


fixed not splitting long from-imports with only a single name


fixed Python 3.6+ file discovery by also looking at function calls with unpacking.
This fixed non-deterministic formatting if trailing commas where used both in function
signatures with stars and function calls with stars but the former would be
reformatted to a single line.


fixed crash on dealing with optional parentheses (#193)


fixed ""is"", ""is not"", ""in"", and ""not in"" not considered operators for splitting
purposes


fixed crash when dead symlinks where encountered


18.4a4

don't populate the cache on --check (#175)

18.4a3


added a ""cache""; files already reformatted that haven't changed on disk won't be
reformatted again (#109)


--check and --diff are no longer mutually exclusive (#149)


generalized star expression handling, including double stars; this fixes
multiplication making expressions ""unsafe"" for trailing commas (#132)


Black no longer enforces putting empty lines behind control flow statements (#90)


Black now splits imports like ""Mode 3 + trailing comma"" of isort (#127)


fixed comment indentation when a standalone comment closes a block (#16, #32)


fixed standalone comments receiving extra empty lines if immediately preceding a
class, def, or decorator (#56, #154)


fixed --diff not showing entire path (#130)


fixed parsing of complex expressions after star and double stars in function calls
(#2)


fixed invalid splitting on comma in lambda arguments (#133)


fixed missing splits of ternary expressions (#141)


18.4a2


fixed parsing of unaligned standalone comments (#99, #112)


fixed placement of dictionary unpacking inside dictionary literals (#111)


Vim plugin now works on Windows, too


fixed unstable formatting when encountering unnecessarily escaped quotes in a string
(#120)


18.4a1


added --quiet (#78)


added automatic parentheses management (#4)


added pre-commit integration (#103, #104)


fixed reporting on --check with multiple files (#101, #102)


fixed removing backslash escapes from raw strings (#100, #105)


18.4a0


added --diff (#87)


add line breaks before all delimiters, except in cases like commas, to better comply
with PEP 8 (#73)


standardize string literals to use double quotes (almost) everywhere (#75)


fixed handling of standalone comments within nested bracketed expressions; Black
will no longer produce super long lines or put all standalone comments at the end of
the expression (#22)


fixed 18.3a4 regression: don't crash and burn on empty lines with trailing whitespace
(#80)


fixed 18.3a4 regression: # yapf: disable usage as trailing comment would cause
Black to not emit the rest of the file (#95)


when CTRL+C is pressed while formatting many files, Black no longer freaks out with
a flurry of asyncio-related exceptions


only allow up to two empty lines on module level and only single empty lines within
functions (#74)


18.3a4


# fmt: off and # fmt: on are implemented (#5)


automatic detection of deprecated Python 2 forms of print statements and exec
statements in the formatted file (#49)


use proper spaces for complex expressions in default values of typed function
arguments (#60)


only return exit code 1 when --check is used (#50)


don't remove single trailing commas from square bracket indexing (#59)


don't omit whitespace if the previous factor leaf wasn't a math operator (#55)


omit extra space in kwarg unpacking if it's the first argument (#46)


omit extra space in
Sphinx auto-attribute comments
(#68)


18.3a3


don't remove single empty lines outside of bracketed expressions (#19)


added ability to pipe formatting from stdin to stdin (#25)


restored ability to format code with legacy usage of async as a name (#20, #42)


even better handling of numpy-style array indexing (#33, again)


18.3a2


changed positioning of binary operators to occur at beginning of lines instead of at
the end, following
a recent change to PEP 8
(#21)


ignore empty bracket pairs while splitting. This avoids very weirdly looking
formattings (#34, #35)


remove a trailing comma if there is a single argument to a call


if top level functions were separated by a comment, don't put four empty lines after
the upper function


fixed unstable formatting of newlines with imports


fixed unintentional folding of post scriptum standalone comments into last statement
if it was a simple statement (#18, #28)


fixed missing space in numpy-style array indexing (#33)


fixed spurious space after star-based unary expressions (#31)


18.3a1


added --check


only put trailing commas in function signatures and calls if it's safe to do so. If
the file is Python 3.6+ it's always safe, otherwise only safe if there are no *args
or **kwargs used in the signature or call. (#8)


fixed invalid spacing of dots in relative imports (#6, #13)


fixed invalid splitting after comma on unpacked variables in for-loops (#23)


fixed spurious space in parenthesized set expressions (#7)


fixed spurious space after opening parentheses and in default arguments (#14, #17)


fixed spurious space after unary operators when the operand was a complex expression
(#15)


18.3a0


first published version, Happy üç∞ Day 2018!


alpha quality


date-versioned (see: https://calver.org/)


Authors
Glued together by ≈Åukasz Langa.
Maintained with Carol Willing,
Carl Meyer,
Jelle Zijlstra,
Mika Naylor, and
Zsolt Dollenstein.
Multiple contributions by:

Abdur-Rahmaan Janhangeer
Adam Johnson
Alexander Huynh
Andrew Thorp
Andrey
Andy Freeland
Anthony Sottile
Arjaan Buijk
Artem Malyshev
Asger Hautop Drewsen
Augie Fackler
Aviskar KC
Benjamin Woodruff
Brandt Bucher
Charles Reid
Christian Heimes
Chuck Wooters
Daniel Hahler
Daniel M. Capella
Daniele Esposti
dylanjblack
Eli Treuherz
Florent Thiery
hauntsaninja
Hugo van Kemenade
Ivan Kataniƒá
Jason Fried
jgirardet
Joe Antonakakis
Jon Dufresne
Jonas Obrist
Josh Bode
Juan Luis Cano Rodr√≠guez
Katie McLaughlin
Lawrence Chan
Linus Groh
Luka Sterbic
Mariatta
Matt VanEseltine
Michael Flaxman
Michael J. Sullivan
Michael McClimon
Miguel Gaiowski
Mike
Min ho Kim
Miroslav Shubernetskiy
Neraste
Ofek Lev
Osaetin Daniel
Pablo Galindo
Peter Bengtsson
pmacosta
Rishikesh Jha
Stavros Korokithakis
Stephen Rosen
Sunil Kapil
Thom Lu
Tom Christie
Tzu-ping Chung
Utsav Shah
vezeli
Vishwas B Sharma
Yngve H√∏iseth
Yurii Karabas

",GitHub - psf/black: The uncompromising Python code formatter
79,Python,"TensorFlow Course



This repository aims to provide simple and ready-to-use tutorials for TensorFlow.
Each tutorial includes source code and most of them are associated with a documentation.

Table of Contents


Download Free TensorFlow Roadmap EBook
What is TensorFlow?


Motivation
Why use TensorFlow?
What's the point of this repository?


TensorFlow Installation and Setup the Environment
TensorFlow Tutorials
Warm-up
Basics
Basic Machine Learning
Neural Networks


Some Useful Tutorials
Contributing
Pull Request Process
Final Note


Developers
Acknowledgement



Download Free TensorFlow Roadmap EBook



What is TensorFlow?
TensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.
TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015.
The current stable release as of September 27, 2018 is 1.11.0

Motivation
There are different motivations for this open source project. TensorFlow (as we write this document) is one of / the best deep learning frameworks available. The question that should be asked is why has this repository been created when there are so many other tutorials about TensorFlow available on the web?

Why use TensorFlow?
Deep Learning is in very high interest these days - there's a crucial need for rapid and optimized implementations of the algorithms and architectures. TensorFlow is designed to facilitate this goal.
The strong advantage of TensorFlow is it flexibility in designing highly modular models which can also be a disadvantage for beginners since a lot of the pieces must be considered together when creating the model.
This issue has been facilitated as well by developing high-level APIs such as Keras and Slim which abstract a lot of the pieces used in designing machine learning algorithms.
The interesting thing about TensorFlow is that it can be found anywhere these days. Lots of the researchers and developers are using it and its community is growing at the speed of light! So many issues can be dealt with easily since they're usually the same issues that a lot of other people run into considering the large number of people involved in the TensorFlow community.

What's the point of this repository?
Developing open source projects for the sake of just developing something is not the reason behind this effort.
Considering the large number of tutorials that are being added to this large community, this repository has been created to break the jump-in and jump-out process that usually happens to most of the open source projects, but why and how?
First of all, what's the point of putting effort into something that most of the people won't stop by and take a look? What's the point of creating something that does not help anyone in the developers and researchers community? Why spend time for something that can easily be forgotten? But how we try to do it? Even up to this
very moment there are countless tutorials on TensorFlow whether on the model design or TensorFlow
workflow.
Most of them are too complicated or suffer from a lack of documentation. There are only a few available tutorials which are concise and well-structured and provide enough insight for their specific implemented models.
The goal of this project is to help the community with structured tutorials and simple and optimized code implementations to provide better insight about how to use TensorFlow quick and effectively.
It is worth noting that, the main goal of this project is to provide well-documented tutorials and less-complicated code!

TensorFlow Installation and Setup the Environment

In order to install TensorFlow please refer to the following link:


TensorFlow Installation



The virtual environment installation is recommended in order to prevent package conflict and having the capacity to customize the working environment.

TensorFlow Tutorials
The tutorials in this repository are partitioned into relevant categories.


Warm-up



#
topic
Source Code
¬†



1
Start-up
Welcome  / IPython
Documentation





Basics



#
topic
Source Code
¬†



2
TensorFLow Basics
Basic Math Operations   / IPython
Documentation

3
TensorFLow Basics
TensorFlow Variables   / IPython
Documentation





Basic Machine Learning



#
topic
Source Code
¬†



4
Linear Models
Linear Regression  / IPython
Documentation

5
Predictive Models
Logistic Regression  / IPython
Documentation

6
Support Vector Machines
Linear SVM  / IPython
¬†

7
Support Vector Machines
MultiClass Kernel SVM  / IPython
¬†





Neural Networks



#
topic
Source Code
¬†



8
Multi Layer Perceptron
Simple Multi Layer Perceptron   / IPython
¬†

9
Convolutional Neural Network
Simple Convolutional Neural Networks
Documentation

10
Recurrent Neural Network
RNN  / IPython
¬†




Some Useful Tutorials


TensorFlow Examples - TensorFlow tutorials and code examples for beginners
Sungjoon's TensorFlow-101 - TensorFlow tutorials written in Python with Jupyter Notebook
Terry Um‚Äôs TensorFlow Exercises - Re-create the codes from other TensorFlow examples
Classification on time series - Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data



Contributing
When contributing to this repository, please first discuss the change you wish to make via issue,
email, or any other method with the owners of this repository before making a change. For typos, please
do not create a pull request. Instead, declare them in issues or email the repository owner.
Please note we have a code of conduct, please follow it in all your interactions with the project.

Pull Request Process
Please consider the following criterions in order to help us in a better way:


The pull request is mainly expected to be a code script suggestion or improvement.
A pull request related to non-code-script sections is expected to make a significant difference in the documentation. Otherwise, it is expected to be announced in the issues section.
Ensure any install or build dependencies are removed before the end of the layer when doing a build and creating a pull request.
Add comments with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.
You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.



Final Note
We are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.
For contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate
your kind feedback and elaborate code inspections.

Developers
Creator: Machine Learning Mindset [Blog, GitHub, Twitter]
Developer: Amirsina Torfi [GitHub, Personal Website, Linkedin ]

Acknowledgement
I have taken huge efforts in this project for hopefully being a small part of TensorFlow world. However, it would not have been plausible without the kind support and help of my friend and colleague Domenick Poster for his valuable advices. He helped me for having a better understanding of TensorFlow and my special appreciation goes to him.
",GitHub - machinelearningmindset/TensorFlow-Course: Simple and ready-to-use tutorials for TensorFlow
